text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9747977,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,115051,-0.032639617800900446
"Clinical and Informatics Research in Medical Terminologies A. Extraction of information from drug labels using natural language processing Drug package inserts (drug labels) are a comprehensive, up-to-date and authoritative source of drug information that is publicly available. To unleash the knowledge in the drug labels, they need to be transformed into standardized data structure and encoded in standard terminologies. Only then can the knowledge be used to drive applications such as clinical decision support. This collaborative project with the FDA uses natural language processing (NLP) and machine learning to extract information from the drug labels and create mappings to standard terminologies. The output will support the FDAs drug label indexing initiative to increase the usefulness of drug labels. Following on the success of the Text Analysis Conference (TAC) 2018, I am hosting the challenge for a second year in collaboration with FDA. The submissions in the 2018 challenge included promising methodologies using deep learning. My research team is exploring the use of neural networks to improve our existing processing pipeline.   B. Use of medical terminologies to support clinical research. Social media is becoming an important source of patient reported data for data mining and data analytics research. I have used medical terminologies for the analysis of data posted by patients to study the adverse drug events and effectiveness of antidepressants. Medical terminologies and common data elements are important tools to allow sharing of clinical research data. I have studied their application in data sharing involving HIV-infected patients.  C. Creating maps between commonly used terminologies Mapping provides a solution to the problem caused by the use of multiple coding systems for the same kind of information. One example is the use of SNOMED CT and ICD-10-CM for coding medical diagnosis and problems. Using various computational methods supplemented by expert review, I have developed maps between SNOMED CT and the different flavors and versions of ICD codes. This will help to facilitate data re-use and data integration. I have also studied the potential benefits of using maps in data encoding. I am also studying various algorithmic approaches to create mappings between SNOMED CT and ICD-10-PCS, including lexical matching, ontological alignment and indirect mapping. This has led to the creation of the publicly available MAGPIE tool (Map-Assisted Generation of Procedure and Intervention Encoding) launched in May 2019.  D. Facilitating adoption of terminology standards According to the Meaningful Use and subsequent Improving Interoperability incentive programs, SNOMED CT and RxNorm are terminologies required for the certification of electronic health record systems. I have studied the practical barriers of adoption of these terminologies and created useful resources to help with implementation. I studied the usage pattern of SNOMED CT terms in the problem lists of large health care providers and published a list of the most commonly used terms as the CORE Problem List Subset of SNOMED CT. The CORE subset is not only a useful resource for SNOMED CT implementers, it is also frequently used for terminology research and other purposes, and cited in multiple publications. RxTerms is another resource that I have developed to overcome data entry problems with RxNorm. n/a",Clinical and Informatics Research in Medical Terminologies,10016029,ZIALM010013,"['Adoption', 'Adverse drug event', 'Algorithms', 'Antidepressive Agents', 'Big Data', 'Certification', 'Clinical Informatics', 'Clinical Research', 'Code', 'Collaborations', 'Common Data Element', 'Communication', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Analytics', 'Data Reporting', 'Data Science', 'Data Sources', 'Diagnosis', 'Drug Labeling', 'Effectiveness', 'Electronic Health Record', 'Generations', 'HIV', 'Health Personnel', 'ICD-10-CM', 'International Classification of Disease Codes', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Intervention', 'Knowledge', 'Machine Learning', 'Maps', 'Medical', 'Methodology', 'Methods', 'Natural Language Processing', 'Ontology', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Procedures', 'Publications', 'Publishing', 'Research', 'Resources', 'Retrieval', 'SNOMED Clinical Terms', 'Semantics', 'Source', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'care outcomes', 'clinical decision support', 'data integration', 'data mining', 'data sharing', 'data structure', 'deep learning', 'improved', 'incentive program', 'indexing', 'information organization', 'insight', 'interoperability', 'lexical', 'neural network', 'off-label drug', 'off-label use', 'social media', 'success', 'symposium', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2019,681191,-0.038026583897062156
"Natural language processing for precision medicine and clinical and consumer health question The Repository for Informed Decision Making, Clinical Question Answering and Consumer Health Question Answering projects are addressing the above objectives by developing knowledge-based and machine learning approaches to extraction and structuring of information in biomedical literature and other types of text (such as clinical notes and registered clinical trials) for the following types of information: 1) the diseases and conditions; 2) the numbers, co-morbidities, and socio-demographic characteristics of study subjects/participants, such as species, gender, smoking status and alcohol consumption; 3) the therapeutic and diagnostic interventions; 4) the study and publication types; 5) the end-points and the outcomes of the studies; 6) drug interactions and 7) adverse drug reactions.  In FY2019, we have developed a number of approaches to facilitate understanding information requests sent to NLM customer services and long queries submitted to MedlinePlus search engine. Information requests sent to customer services are often several paragraphs long and provide the background and context that the customers believe will help understand their needs. For example, customers often describe several generations of their families affected by a disease and ask if their children will have it. The long MedlinePlus queries consist of one or two sentences and are often formed as questions. Both of these request forms are usually ungrammatical and rife with misspellings, abbreviations and informal language. We have developed a spellchecker for consumer language that is performing adequately on the misspellings important to understanding of the needs. After correcting spelling, our system employs three modules: a knowledge-based and a supervised machine learning method to understand the main points of the request, such as the disease or a drug of interest and the type of information about it. The systems extract the main points, which we found are sufficient to automatically search MedlinePlus and find authoritative and relevant pages for 65% of the requests. The third approach is to find similar questions that already have authoritative answers, e.g., provided by NIH institutes. In FY2019, the prototype consumer health question answering system https://chiqa.nlm.nih.gov/ was used to create data collection for a community-wide evaluation of approaches to question answering, which attracted 70 academic, industry and government teams from five continents.  Our clinical question answering system is based on the framework for asking well-formed questions developed by the evidence-based medicine experts. Their analysis showed that presenting a clinical information need as four-part question frame: patient characteristics/problem; planned intervention; comparison; and desired outcome, helps formulate search engine queries that lead to relevant results. We developed methods for automatic extraction of question frames from information requests, automatic query formulation and automatic extraction of answers from retrieval results. The LHC CQA1.0 system extracts the bottom-line advice from biomedical publications and aligns the question frames and the answers to find the best answer. The CQA 1.0 system is currently used to support development of evidence-based care plans at the NIH Clinical Center, to provide bottom-line for retrieved images in the LHC Open-i system and to provide summaries of the biomedical articles in the LHC Open Summarizer. n/a",Natural language processing for precision medicine and clinical and consumer health question,10016943,ZIALM010009,"['Abbreviations', 'Address', 'Affect', 'Alcohol consumption', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Clinical Trials', 'Communities', 'Comorbidity', 'Data Collection', 'Decision Making', 'Development', 'Diagnostic', 'Disease', 'Drug Interactions', 'Evaluation', 'Evidence Based Medicine', 'Family', 'Formulation', 'Gender', 'General Population', 'Generations', 'Government', 'Growth', 'Health', 'Image', 'Industry', 'Institutes', 'Intervention', 'Language', 'Lead', 'Literature', 'Machine Learning', 'MedlinePlus', 'Methods', 'Natural Language Processing', 'Outcome', 'Outcome Study', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Publications', 'Resources', 'Retrieval', 'Review Literature', 'Role', 'Services', 'Smoking Status', 'Source', 'Structure', 'Study Subject', 'System', 'Text', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Update', 'adverse drug reaction', 'base', 'clinical decision support', 'evidence base', 'interest', 'knowledge base', 'learning strategy', 'precision medicine', 'prototype', 'repository', 'search engine', 'sociodemographics', 'spelling', 'study characteristics', 'supervised learning', 'systematic review']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2019,533106,-0.02312125180695561
"Query Log Analysis for Improving User Access to NCBI Web Services Over the last decade, the online search for biological information has progressed rapidly and has become an integral part of any scientific discovery process. Today, it is virtually impossible to conduct R&D in biomedicine without relying on the kind of Web resources developed and maintained by the NCBI. Indeed, each day millions of users search for biological information via NCBIs online Entrez system. However, finding data relevant to a users information need is not always easy in Entrez. Improving our understanding of the growing population of Entrez users, their information needs and the way in which they meet these needs opens opportunities to improve information services and information access provided by NCBI.   Among all Entrez databases, PubMed is the most used one and often serves as an entry point for people to access related data in other Entrez databases. Tools to aid searching PubMed are query suggestion, expansion, and spelling correction. Dedicated best match algorithms aid navigational queries by ignoring minor errors and aid informational searches using machine learning to combine relevant signals such as article popularity, publication date and type, and query-document relevance score. Additional valuable aids including identifying related articles and author name disambiguation.  PubMed Labs provides a place to trial and improve new search features. It features a clean and mobile-friendly design tailored specifically towards small screen devices and a platform for users to provide feedback guiding future work.  While search has usually focused on full documents or references, the value of sentence search is rising. It can identify specific statements rather than whole articles on a general topic. Our new tool, LitSense, provides sentence level search, making sense of biomedical literature at sentence level.  A specific use of sentence similarity is to aid the curation efforts in the Conserved Domain Database (CDD). To this end, LitSense has been used to both finds sentences in PubMed articles already used to create CDD summaries and identify new sentences closely related to existing CDD summaries.  For using sentences in Deep Learning tasks, BioSentVec is the first sentence encoder specifically for the biomedical domain. It better captures biomedical semantics than general domain encoders.  Of course, word embeddings remain the primary method of using Deep Learning in NLP tasks. BioWordVec uses subword information and MeSH to generate biomedical word embeddings that can significantly improve performance. Biomedical terminology often includes important subword information. The semantic information available in ontologies such as MeSH is meaningful. A generic word embedding cannot take advantage of this valuable supplemental information.  These machine learning methods benefit from having a large amount of text available. To that end a Web API serves BioC versions of the PMC Open Access Subset and Author Manuscripts. This is a continuously updated complement to our existing FTP service. The documents are available in either JSON or XML and both ASCII and Unicode encodings are available. n/a",Query Log Analysis for Improving User Access to NCBI Web Services,10007518,ZIALM000001,"['Algorithms', 'Biological', 'Complement', 'Data', 'Databases', 'Devices', 'Extensible Markup Language', 'Feedback', 'Future', 'Goals', 'Information Services', 'Internet', 'Link', 'Literature', 'Machine Learning', 'Manuscripts', 'MeSH Thesaurus', 'Methods', 'Minor', 'Molecular Biology', 'Names', 'Ontology', 'Performance', 'Population', 'Process', 'PubMed', 'Publications', 'Reference Values', 'Research', 'Semantics', 'Services', 'Signal Transduction', 'Suggestion', 'System', 'Terminology', 'Text', 'Update', 'Work', 'deep learning', 'design', 'formycin triphosphate', 'improved', 'learning strategy', 'navigation aid', 'online resource', 'research and development', 'spelling', 'tool', 'virtual', 'web services']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2019,2139093,-0.03979676434576566
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9818711,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'learning strategy', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2019,320194,-0.046094762044782314
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9636581,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2019,489919,0.007136903464686255
"Ethical Considerations for Language Modeling within Brain-Computer Interfaces Project Summary Machine learning (ML) and Natural Language Processing (NLP) have the potential to transform communication for patients with neurodegenerative disease through personalized and real-time augmentative and alternative communication (AAC) devices. Individuals with severe communication impairments who can no longer control their daily conversations or participate in previous life roles want AAC devices. And they want them to work – to be reliable, effective, and fast. ML and NLP are emerging as promising tools to bridge current technology and next generation devices for individuals with the most severe speech and physical impairments, like the RSVP Keyboard™, a brain-computer interface (BCI) being developed by the parent grant. BCI systems for communication are referred to as AAC-BCIs. NLP efforts to combine large public data sets with private data sets, such as personal email messages, promise to give individuals with communication impairments their own personalized language models, models that are sufficiently robust to get closer to real-time communication. The focus on getting AAC-BCIs to work with machine learning, however, has led to a critical oversight in the field: an inadequate understanding of why individuals want next-generation devices and what trade-offs they are willing to make for faster and more personalized communication. The turn to ML brings this oversight into sharp relief. Individuals should provide input about the data sets used to construct their personal language models, but this raises important ethical questions about what individuals value, how they understand their identity, and what trade-offs they are willing to make relative to their personalized communication data. The goal of this supplement is to fill this gap in understanding so that researchers can implement ML into next generation AAC-BCI systems in a way that is sensitive to the ethical concerns of future users. There are four components to this ethics supplement: (1) to design a toolbox of ethics vignettes tailored to ethical concerns raised by both BCI communication and ML; (2) to administer monthly vignette-based online ethics surveys to individuals with severe communication impairments due to motor neuron disease (e.g., ALS) (n=25) or movement disorders (e.g., Parkinson's disease) (n=25); (3) to conduct semi-structured vignette-based interviews with individuals with pre- clinical or mild communication impairment due to motor neuron disease (n=10) or movement disorder (n=10). Components (2) and (3) will employ an iterative, parallel mixed-method approach. Trends in Likert-style online responses to ethics vignettes in the severe communication impairment cohort will be used to inform and modify the semi-structured interview prompts asked of the pre-clinical or mild impairment cohort. In parallel, themes emerging from direct content analysis of interviews will be used to refine online survey questions. Results of this iterative, mix-methods approach will be used (4) to outline a framework of core ethical domains and preliminary tools (vignettes and discussion prompts) that AAC-BCI researchers can use to assess ethical concerns while developing and iteratively refining communication technology for personalized language models. Project Narrative The populations of US citizens with severe speech and physical impairments secondary to neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies. Bioethical issues about privacy, agency and identity must be included in technology development and implementation as the parent grant implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Ethical Considerations for Language Modeling within Brain-Computer Interfaces,9929337,R01DC009834,"['Address', 'Administrative Supplement', 'Affect', 'Attention', 'Attitude', 'Augmentative and Alternative Communication', 'Award', 'Bioethical Issues', 'Bioethics', 'Clinical', 'Code', 'Cognitive', 'Communication', 'Communication impairment', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disease', 'Electroencephalography', 'Electronic Mail', 'Encapsulated', 'Engineering', 'Ensure', 'Ethical Analysis', 'Ethical Issues', 'Ethics', 'Foundations', 'Future', 'Goals', 'Home environment', 'Impairment', 'Individual', 'Informed Consent', 'Interview', 'Language', 'Letters', 'Life', 'Link', 'Literature', 'Locked-In Syndrome', 'Machine Learning', 'Medical', 'Medical Technology', 'Methods', 'Modeling', 'Monkeys', 'Motor Neuron Disease', 'Movement', 'Movement Disorders', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Oregon', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Patient advocacy', 'Patients', 'Population', 'Privacy', 'Privatization', 'Public Health', 'Reporting', 'Research Personnel', 'Review Literature', 'Role', 'Secondary to', 'Self-Help Devices', 'Source', 'Speech', 'Structure', 'Surveys', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'User-Computer Interface', 'Voice', 'Work', 'advocacy organizations', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'cohort', 'communication device', 'computer science', 'design', 'expectation', 'informant', 'neurophysiology', 'next generation', 'novel', 'parent grant', 'pre-clinical', 'recruit', 'research and development', 'response', 'signal processing', 'skills', 'spelling', 'technology development', 'technology validation', 'tool', 'trend', 'uptake']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,153834,-0.04796779512456646
"2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE 2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE  PROJECT SUMMARY/ABSTRACT A 1.5-day symposium, StoneLab will be held December 6-7, 2019 at AUA Headquarters in Linthicum, Maryland, near BWI Airport. The novel StoneLab Symposium will be a meeting where kidney stone researchers can come together with experts in biomedical engineering, physics, chemistry, artificial intelligence, medical device design and development. We plan to encourage researchers to “think outside the box” in the search for advances in kidney stone treatment. The target audience for this meeting includes: urologists, nephrologists, basic scientists, early-career investigators, residents, research fellows, and project management members of urology research teams in the area of nephrolithiasis and related technology development. It is essential that the urology community, and especially trainees, early-career physician- scientists, and researchers be provided with accessible opportunities to gain understanding of new knowledge and recent advances in not only their own fields, but in related fields that they may not otherwise be aware of, and leverage this for new research frameworks that can help improve the treatment of kidney stone disease. A critical aspect of the StoneLab Sympoisum is the delivery of talks from outside the normal realm of kidney stone research by leading basic scientists, followed by discussions led by kidney stone researchers and surgeons. The intent of these discussions is to spur the development of collaborations and the generation of new ideas and avenues for exploration. Another goal of the meeting is to help identify future funding needs and growth areas for kidney stone research. The R13 support requested in this application will encourage early investigators to participate in the workshop and stimulate their development as researchers and surgeon- scientists. This symposium will have a major impact by bringing together premier kidney stone researchers, along with experts in nephrology, biomedical engineering, physics, chemistry, artificial intelligence, and medical device design and development, to explore solutions for the most pressing translational science issues facing kidney stone researchers today. The Principal Investigator (PI), Carolyn J.M. Best, PhD, is AUA Director of Research. The Program Planning Committee consists of a diverse multidisciplinary team of distinguished scientists and urologists, most of whom also serve in leadership roles of several AUA-affiliated subspecialty societies: Khurshid Ghani, MD, MS (Chair); Ben H. Chew, MD, MSc (Co-Chair); Thomas Chi, MD; Benjamin Canales, MD, MPH; Gary Curhan, MD, ScD; Amy Krambeck, MD; Dirk Lange, PhD; Manoj Monga, MD, FACS; Kristina Penniston, PhD,RD; and Aria Olumi, MD (ex-officio). 2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE  PROJECT NARRATIVE/PUBLIC HEALTH RELEVANCE STATEMENT This application seeks support to enable the American Urological Association (AUA) to invite early-career investigators (and late-stage trainees) to participate in a novel interdisciplinary symposium where premier kidney stone researchers, along with experts in nephrology, can come together with experts in biomedical engineering, physics, chemistry, artificial intelligence, and medical device design and development with the aim of “thinking outside the box” in the search for advances in kidney stone treatment. This meeting will explore solutions for the most pressing translational science issues facing kidney stone researchers today, research in interdisciplinary fields that might be leveraged for new research to improve treatment of kidney stone disease, and help identify future funding needs and growth areas for kidney stone research. A critical aspect of the StoneLab Sympoisum is the delivery of talks from outside the normal realm of kidney stone research by leading basic scientists, followed by discussions led by kidney stone researchers and surgeons with the intent to spur the development of collaborations and the generation of new ideas and avenues for exploration.",2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE,9914630,U13DK124023,"['American', 'Area', 'Artificial Intelligence', 'Award', 'Awareness', 'Basic Science', 'Big Data', 'Big Data Methods', 'Biology', 'Biomedical Engineering', 'Chemistry', 'Collaborations', 'Communities', 'Development', 'Development Plans', 'Device or Instrument Development', 'Discipline', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Fostering', 'Funding', 'Future', 'Generations', 'Genomics', 'Goals', 'Grant', 'Growth', 'Kidney Calculi', 'Knowledge', 'Lead', 'Leadership', 'Machine Learning', 'Maryland', 'Medical', 'Medical Device Designs', 'Methodology', 'Miniaturization', 'Nephrolithiasis', 'Nephrology', 'North America', 'Operative Surgical Procedures', 'Optics', 'Physicians', 'Physics', 'Prevalence', 'Principal Investigator', 'Privatization', 'Research', 'Research Personnel', 'Robotics', 'Role', 'Science', 'Scientist', 'Societies', 'Surgeon', 'Technology', 'Translational Research', 'Travel', 'United States', 'Urologist', 'Urology', 'career', 'career development', 'improved', 'innovation', 'interest', 'medical specialties', 'meetings', 'member', 'multidisciplinary', 'next generation', 'next generation sequencing', 'novel', 'programs', 'public health relevance', 'symposium', 'technology development', 'urologic']",NIDDK,AMERICAN UROLOGICAL ASSOCIATION,U13,2019,10000,-0.01868138057761571
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9724345,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'random forest', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2019,292500,-0.01134404325773589
"Informatics, Machine Learning & Biomedical Data Science Over the past year, we have been active in: (1) developing computationally efficient methods and algorithms to solve known problems in the analysis of biomedical and clinical data and study complex interactions in biological systems; (2) developing knowledge-based data management systems for the discovery and curation of biomedical knowledge, including distributed annotation systems and laboratory information management systems;  (3) applying predictive-analytic models to scientific and administrative domains; and (4) consulting with NIH leadership to provide evidence-based solutions to improve the grant application and review process.  Specifically, in 2019, collaborative efforts in support of these goals included the following:  - In a partnership with Dr. John Tsang of the NIAID Laboratory of Systems Biology, HPCIO is conducting a multifaceted project to profile the immune system using the latest high-throughput, multiplexed technologies and systems approaches. One of the goals of this collaboration is to develop novel computational methodologies that can exploit inter-subject heterogeneity and measurements at various scales to assess the roles of the immune system in health and disease.  Currently, we are developing a multi-level linear model utilizing ATAC-seq and RNA-seq data from monogenic disease patients to dissect the gene regulatory network.  The results will provide insights on the impacts specific genetic lesions have on downstream genes and how they are manifested at the clinical phenotypical level.   - HPCIO has entered into a wide-ranging collaboration to provide data science support and innovation to Dr. Leorey Saligan and the Symptom Management Branch of NINR.   An existing effort to identify radiotherapy-related fatigue genes from a PCR assay of oxidative stress using machine-learning methods has largely been completed.   In addition, a number of new efforts were initiated in 2019.   To provide guidance for future scRNA experiments to identify clusters of cells and their marker genes associated with wound healing in mice, an analysis of a similar experiment deposited to the Sequence Read Archive was conducted.  We also initiated a machine-learning analysis of mouse proteomic data to identify targets or pathways that may be involved in both the generation of radiation-related fatigue and rescue through taltirelin.    - HPCIO is working with Dr. Timothy Myers of NIAID to develop a laboratory information management system (LIMS) that can more effectively organize the NIAID Genomic Technology Sections (GTS) project and experiment data and workflow. The new LIMS being developed facilitates the standardization and management of procedures, workflows, and data management for NIAID investigators experiments with next-gen sequencing data, to enhance tracking of relevant information in each step and to improve reproducibility of results and identification of operational issues. Modules have been developed to automate and streamline GTSs custom project and experiment creation workflow.  We envision that this system can serve as a model and be readily customized for other labs with similar needs.  HPCIO also adapted this LIMS for Dr. John Tsang and Dr. Craig Martens of NIAID as demonstration projects for their respective labs.  - In collaboration with NINDS Spinal Circuits and Plasticity Unit, HPCIO is helping develop novel methods of analyzing single-cell RNAseq data collected from mice undergoing treatment for spinal injuries. We are working with NINDS researchers to help understand the pathways involved during spinal cord injury and healing.   - HPCIO is working with NCI Occupational & Environmental Epidemiology Branch to develop methodologies to incorporate occupational risk factors into epidemiological models. We are enlarging the training data to improve our novel classifiers for coding free text job descriptions into the 840 codes of the 2010 U.S. Standard Occupational Classification System.  Our classifier is being embedded within the data collection software of the NCI Agriculture Health Study, a large scale epidemiological study, to automatically code job descriptions as they are entered by study participants.  - In collaboration with the Membrane Transport Biophysics Section of NINDS, HPCIO is 1) developing  a computational tool to accurately identify the boundaries of the lysosomes in fluorescence microscopy and 2) using the fluorescence ration to measure lysosomal pH within each organelle for better understanding of the lysosomal pH regulation.    - A freely available plasmid database that is inter-operable with popular freeware is currently being developed for the NIDA Optogenetics and Transgenic Technology Core. The Plasmid Manager offers a versatile yet simple platform for scientists to store and analyze their plasmid data. Motivated by the need for a more comprehensive approach to archiving plasmid data, the database platform is enriched with numerous components beyond the repository, serving as an informatics platform designed to enhance the efficiency and analytic capabilities of scientists.   - The Human Salivary Proteome Wiki is a community-driven Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. Many features and external contents have been incorporated over the last few years to make it easier for users to extract different kinds of information from the wiki.  We are actively adding new data to the system to allow users to discern the origin of proteins found in saliva, whether they may come from the different salivary glands or from blood plasma, for instance.  Current efforts also include working with major stakeholders to engage with the research community and to gather feedback from them.  - In collaboration with CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process. A Web-based automated referral tool, called ART, was developed and deployed to help PIs and SROs to identify the most relevant study section(s) or special emphasis panel(s) based on the scientific content of an application. In addition, HPCIO is analyzing text from quick feedback surveys on peer review. HPCIO has developed a system to capture the sentiment of reviewer comments in quick feedback surveys and classify these comments with sentiment score into broad categories.   In 2019, HPCIO continued to maintain ART and retrain the machine-learning models to reflect new and changing study sections.  In support of the ARGO initiative, we represented study sections through Word2Vec and generated distance metrics as well as diagrams, allowing leadership to analyze the relationship between study sections.  We developed an interactive tool that allows users to curate grant applications for measures of scientific rigor n/a","Informatics, Machine Learning & Biomedical Data Science",10016051,ZIHCT000200,"['ATAC-seq', 'Agricultural Health Study', 'Algorithms', 'Applications Grants', 'Archives', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Biophysics', 'Categories', 'Cells', 'Classification', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computational Linguistics', 'Computer software', 'Computing Methodologies', 'Consult', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Database Management Systems', 'Databases', 'Deposition', 'Disease', 'Environmental Epidemiology', 'Evaluation', 'Fatigue', 'Feedback', 'Fluorescence', 'Fluorescence Microscopy', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Grant Review Process', 'Harvest', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Immune system', 'Informatics', 'Intramural Research Program', 'Job Description', 'Knowledge', 'Laboratories', 'Leadership', 'Lesion', 'Linear Models', 'Lysosomes', 'Machine Learning', 'Management Information Systems', 'Martens', 'Measurement', 'Measures', 'Mendelian disorder', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Mus', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Nature', 'Occupational', 'Occupational Epidemiology', 'Online Systems', 'Organelles', 'Oxidative Stress', 'Participant', 'Pathway interactions', 'Patients', 'Peer Review', 'Plasma', 'Plasmids', 'Predictive Analytics', 'Procedures', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Published Comment', 'Radiation', 'Radiation therapy', 'Regulation', 'Regulator Genes', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Research Support', 'Risk Factors', 'Role', 'Saliva', 'Salivary', 'Salivary Glands', 'Scientist', 'Spinal', 'Spinal Injuries', 'Spinal cord injury', 'Standardization', 'Strategic Planning', 'Study Section', 'Surveys', 'System', 'Systems Biology', 'Technology', 'Text', 'Training', 'Transgenic Organisms', 'Translational Research', 'Transmembrane Transport', 'United States National Institutes of Health', 'Vision', 'Visualization software', 'Work', 'Wound Healing', 'annotation  system', 'base', 'biological systems', 'biomedical informatics', 'clinical phenotype', 'computerized tools', 'data management', 'data mining', 'design', 'epidemiological model', 'epidemiology study', 'evidence base', 'experimental study', 'healing', 'improved', 'information organization', 'innovation', 'insight', 'interactive tool', 'interdisciplinary approach', 'interoperability', 'knowledge base', 'learning strategy', 'next generation sequencing', 'novel', 'optogenetics', 'programs', 'repository', 'research and development', 'single cell analysis', 'software development', 'symptom management', 'text searching', 'tool', 'transcriptome sequencing', 'web portal', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2019,2918456,-0.01986365798383597
"Harmonized Data-Derived Resources for the Alzheimer's Disease and Related Dementias Community Current data harmonization work has been underway to maximize the utility of both datasets and analytics workflows across ADRD domains, we have also attempted to improve and standardize compute infrastructure to accomplish these tasks. All publicly available genomics data in the ADRD space is currently being aggregated and standardized as per Nalls et al 2019. Clinical data from longitudinal ADRD studies has been accessed as is being harmonized to mirror work represented in Iwaki et al 2019. Additionally, we have been initiating testing of hybrid cloud infrastructure to support this work, maximizing the power of google cloud and the NIH's biowulf compute resource to ensure efficient analytics. n/a",Harmonized Data-Derived Resources for the Alzheimer's Disease and Related Dementias Community,10011287,ZIAAG000534,"['Address', 'Adopted', 'Alzheimer&apos', 's disease related dementia', 'Area', 'Artificial Intelligence', 'Biological', 'Cells', 'Clinical Data', 'Cloud Computing', 'Communities', 'Complex', 'Data', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Distal', 'Ensure', 'Ethics', 'Genomics', 'Growth', 'Home environment', 'Human', 'Hybrids', 'Image', 'In Situ', 'Infrastructure', 'Institution', 'Investigation', 'Machine Learning', 'Mediation', 'Medical Genetics', 'Methods', 'Nature', 'Patients', 'Privacy', 'Process', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Speed', 'Standardization', 'Testing', 'Time', 'United States National Institutes of Health', 'Vision', 'Work', 'computing resources', 'data access', 'data integration', 'data integrity', 'data management', 'data visualization', 'deep learning', 'design', 'genomic data', 'improved', 'interest', 'molecular imaging', 'multimodality', 'next generation']",NIA,NATIONAL INSTITUTE ON AGING,ZIA,2019,135238,-0.012616797696609729
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,9885663,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Traction', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'success', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2019,206139,-0.0016654730253907936
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,-0.019937461886953247
"Image-guided robot for high-throughput microinjection of Drosophila embryos PROJECT SUMMARY This proposal is submitted in response to the NIH Development of Animal Models and Related Biological Materials for Research (R21) program. The proposal develops an image-guided robotic platform that performs the automated delivery of molecular genetic tools and non-genetically encoded reagents such as chemical libraries, fluorescent dyes to monitor cellular processes, functionalized magnetic beads, or nanoparticles into thousands of Drosophila embryos in a single experimental session. The proposed work builds on recent engineering innovations in our collaborative group which has developed image-guided robotic systems that can precisely interface with single cells in intact tissue. The two Specific Aims provide for a systematic development of the proposed technologies. AIM 1 first engineers a robotic platform (‘Autoinjector’) that can scan and image Drosophila embryos in arrays of egg laying plates. We will utilize machine learning algorithms for automated detection of embryos, followed by thresholding and morphology analysis to detect embryo centroids and annotate injection sites. In AIM 2, we will utilize microprocessor-controlled fluidic circuits for programmatic delivery of femtoliter to nanoliter volumes of reagents into individual embryos. We will quantify the efficacy of the Autoinjector by comparing the survival, fertility, and transformation rates of transposon or PhiC31-mediated transgenesis to manual microinjection datasets. Finally, we will demonstrate the efficient delivery of sgRNAs and mutagenesis in the presence of Cas9. This project fits very well within the goals of the program by engineering a novel tool for producing and improving animal models. The Autoinjector will accelerate Drosophila research and empower scientists to perform novel experiments and genome-scale functional genomics screens that are currently too inefficient or labor intensive to be conducted on a large scale and may additionally enable other novel future applications. PROJECT NARRATIVE This proposal develops a technology platform that will enable automated microinjection of molecular genetic tools and non-genetically encoded tools such as chemical libraries, fluorescent dyes, functionalized magnetic beads, or nanoparticles, into thousands of Drosophila embryos in a single experimental session. The successful development of this technology will empower Drosophila biologists to perform screens and develop new applications that are currently too inefficient or labor intensive to contemplate and will accelerate research into the function of the nervous system and the molecular and genetic underpinnings of numerous diseases in this important animal model.",Image-guided robot for high-throughput microinjection of Drosophila embryos,9806367,R21OD028214,"['Animal Model', 'Biocompatible Materials', 'Biological Assay', 'Caliber', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Collection', 'Computer Vision Systems', 'Cryopreservation', 'Data Set', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Drosophila melanogaster', 'Embryo', 'Engineering', 'Expenditure', 'Exploratory/Developmental Grant', 'Fertility', 'Fluorescent Dyes', 'Future', 'Gene Transfer Techniques', 'Genetic', 'Goals', 'Guide RNA', 'Image', 'Individual', 'Injections', 'Investigation', 'Laboratories', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Mediating', 'Methods', 'Microinjections', 'Microprocessor', 'Microscope', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Morphology', 'Motivation', 'Mutagenesis', 'Needles', 'Nervous System Physiology', 'Performance', 'Process', 'Reagent', 'Research', 'Resources', 'Robot', 'Robotics', 'Scanning', 'Scientist', 'Signaling Molecule', 'Site', 'Space Perception', 'System', 'Technology', 'Tissues', 'Transgenes', 'Transgenic Organisms', 'United States National Institutes of Health', 'Work', 'animal model development', 'base', 'biological research', 'cost', 'egg', 'experience', 'experimental study', 'functional genomics', 'gene product', 'genetic manipulation', 'genome-wide', 'image guided', 'improved', 'innovation', 'machine learning algorithm', 'magnetic beads', 'mutant', 'mutation screening', 'nanolitre', 'nanoparticle', 'novel', 'novel strategies', 'programs', 'response', 'robotic system', 'screening', 'small molecule libraries', 'stem', 'technology development', 'tool']",OD,UNIVERSITY OF MINNESOTA,R21,2019,184118,-0.03138953387787271
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9671422,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Ingestion', 'Kinetics', 'Laboratories', 'Locomotion', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'automated image analysis', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'machine learning algorithm', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2019,475637,-0.03077357854704085
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9702053,R21GM128020,"['Address', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'learning algorithm', 'machine learning algorithm', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'preservation', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2019,206250,-0.00861472693343973
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9731665,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'informatics\xa0tool', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2019,599962,0.006469984356060583
"NIDDK Extramural Digital Pathology Repository System In recent years, new technology and data processing capabilities have removed barriers to integration of molecular and histopathological data sets. Several clinical research networks across the National Institute of Diabetes, Digestive and Kidney Diseases extramural programs have put Digital Pathology Repositories (DPRs) into place with digital whole slide images (WSI) available to support standardization of classical diagnostic criteria across clinical sites. A developing line of investigation is the “mining” of these digital sets to identify features which correlate with disease. Computer assisted-image analysis for feature detection and feature recognition are key components of such investigation. The Centralized NIDDK Digital Pathology Repository will serve as an online repository to facilitate standardized archiving of WSI with the goal of providing controlled access for standardization, discovery and validation research efforts. n/a",NIDDK Extramural Digital Pathology Repository System,10032690,5N94019F00322,"['Archives', 'Artificial Intelligence', 'Clinical', 'Clinical Research', 'Computer-Assisted Image Analysis', 'Data Set', 'Diabetes Mellitus', 'Diagnostic', 'Digestive System Disorders', 'Disease', 'Extramural Activities', 'Future', 'Goals', 'Institutes', 'Investigation', 'Kidney Diseases', 'Machine Learning', 'Metadata', 'Mining', 'Molecular', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Research', 'Standardization', 'System', 'Validation', 'clinical research site', 'computerized data processing', 'digital', 'digital pathology', 'feature detection', 'new technology', 'online repository', 'programs', 'repository', 'whole slide imaging']",NICHD, ,N02,2019,95738,-0.006400753911367781
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9656981,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2019,431185,-0.01521201482146519
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,9859232,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'One-Step dentin bonding system', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2019,345016,-0.005875502975969051
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9772886,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics\xa0tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2019,663419,-0.008547886947396952
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or “position” on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9988039,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,49496,-0.03067863578012151
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9654021,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2019,1354555,-0.02546562253978306
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9823400,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2019,583885,-0.02546562253978306
"Big Omics Data Engine 2 Supercomputer Computational and data science has transformed biomedical scientific discovery: its approaches are embedded into a wide range of workflows for diseases such as schizophrenia, depression, Alzheimer's, epilepsy, influenza, autism, drug addiction, pediatric cardiac care, Inflammatory Bowel Disease, prostate cancer and multiple myleloma. Sixty-one basic and translational researchers at Mount Sinai representing over $100 million in NIH funding, along with their collaborators from 75 external institutions, have utilized the Big Omics Data Engine (BODE) supercomputer to elucidate significant scientific findings in over 167 publications, including high impact journals such as Nature and Science, with 2,427 citations in three years. These researchers have also shared the data generated on BODE throughout their consortia and into national data sharing repositories. BODE is nearing the end of its vendor maintainable life, and researchers need increased computational throughput and storage space. To empower researchers to not only continue their inquiries, but to also tackle more complex scientific questions with decreased time to solution, we propose the Big Omics Data Engine 2 Supercomputer (BODE2). BODE2 will contain a total of 3,200 Intel Cascade Lake cores with 15 terabytes of memory and 14 petabytes of raw storage, and will leverage an existing 250 terabytes of SSDs. An instrument of this size is not available elsewhere affordably. With the proposed instrument, researchers will be able to take advantage of three major benefits: (1) the ability to receive results faster for overall greater scientific throughput; (2) the ability to increase the fidelity of their simulations and analyses; and (3) the ability to migrate research applications seamlessly to the software environment for greater scientific productivity. As with data produced on BODE, BODE2 data products will also be shared with the broader scientific community. BODE2 will provide the critical infrastructure needed by the wide range of researchers and clinicians for the genetics and population analysis, gene expression, machine learning and structural and chemical biology approaches used to make advances in these diseases. A specialized Big Omics Data Engine 2 Supercomputer instrument will provide necessary computational and data science infrastructure for 61 research projects with 75 collaborating institutions in diverse areas such as Alzheimer's, autism, schizophrenia, drug addiction, influenza, pediatric cardiac care, depression, epilepsy, prostate cancer and multiple myeloma. Data generated from this instrument will be shared in national databases.",Big Omics Data Engine 2 Supercomputer,9708160,S10OD026880,"['Alzheimer&apos', 's Disease', 'Biology', 'Cardiac', 'Caring', 'Chemicals', 'Childhood', 'Communities', 'Complex', 'Computational Science', 'Computer software', 'Data', 'Data Science', 'Disease', 'Drug Addiction', 'Environment', 'Epilepsy', 'Funding', 'Gene Expression', 'Inflammatory Bowel Diseases', 'Influenza', 'Infrastructure', 'Institution', 'Journals', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Memory', 'Mental Depression', 'Nature', 'Population Analysis', 'Productivity', 'Publications', 'Research', 'Research Personnel', 'Schizophrenia', 'Science', 'Structure', 'Time', 'United States National Institutes of Health', 'Vendor', 'autism spectrum disorder', 'data sharing', 'genetic analysis', 'instrument', 'petabyte', 'repository', 'simulation', 'supercomputer', 'terabyte', 'translational scientist']",OD,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,S10,2019,1998264,-0.05782524696755212
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9672444,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2019,1515934,-0.034130186709346345
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,10012073,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2019,77975,-0.034130186709346345
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9911854,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2019,15000,-0.029585486882065356
"Big Data, Big Models, and Big Bias?: A decision making framework for vital rate estimates based on extrapolation Project Summary/Abstract The past decades have seen a flurry of methods that use extrapolation, smoothing, and other forms of information sharing to compensate for limited and incomplete data. In places without comprehensive vital registration or public health monitoring systems, extrapolation and information sharing techniques are particularly appealing, since there typically is simply not enough data available to produce estimates with sufficient temporal or spatial resolution to influence public health decision making. This proposal reframes uncertainty in extrapolated estimates of vital rates in terms of decision-making. A decision-making framework (i) is grounded in familiar language for policymakers and public health officials, (ii)characterizes consequential and inconsequential model decisions based on variability in outcomes, and (iii) in- corporates both extrapolation and sampling uncertainty. A cornerstone of this project is a novel collaboration with researchers and policymakers at the World Bank. Through this collaboration, we will pilot the proposed decision-making tools and conduct experiments with local and national policymakers in realistic settings. Project Narrative Predictions based on machine learning models are increasingly common inputs into decision making processes across scientific domains. In this proposal I develop and evaluate strategies for making public health decisions based on predicted vital rates, particularly in places without full coverage civil registration. Results from the project will improve strategies for allocating resources for disease surveillance and health monitoring in scarce resource settings.","Big Data, Big Models, and Big Bias?: A decision making framework for vital rate estimates based on extrapolation",9780910,DP2MH122405,"['Big Data', 'Collaborations', 'Consequentialism', 'Data', 'Decision Making', 'Disease Surveillance', 'Health', 'Language', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Process', 'Public Health', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'System', 'Techniques', 'Uncertainty', 'World Bank', 'base', 'experimental study', 'improved', 'novel', 'tool']",NIMH,UNIVERSITY OF WASHINGTON,DP2,2019,2332500,-0.03941669460300103
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9786702,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2019,431816,-0.009631316623124933
"Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack Project Summary: Tufts University physics professor Allan Cormack pioneered the field of tomography. His seminal work, from 1963 and 1964, provided both the mathematical foundations of computerized tomography (CT), and tangible proof-of-concept by engineering a rudimentary CT scanner. Taken together, this effort represented the first practical method to ""see into"" an object without physically breaking it open. Along with the engineer Godfrey Hounsfield, he won the 1979 Nobel Prize in Physiology or Medicine for these contributions. Since then, tomography has broadened to include a wide range of modalities and problems. This field is unique for the rich interplay among applications in medicine, security, earth sciences, industry, physics, and the mathematics required to solve these problems. This international conference at Tufts, “Modern Challenges in Imaging: In the Footsteps of Allan Cormack” will honor the achievements of Cormack and reflect this diversity in the field by gathering top international researchers in mathematics, engineering, science, and medicine to communicate the most current research and challenges in the field. This will include work on mathematical models of emerging modalities, tomographic machine learning, dynamic methods, and spectral imaging with applications include medicine and security. The best research from the conference will be disseminated in a special issue of the journal Inverse Problems. Talks will be posted on the conference website. The organizers will recruit a diverse set of experienced participants and trainees, and the conference will be advertised in a range of publications reflecting the scientific and demographic diversity of the field. This conference is unique in that it combines high-level mathematical participants with experts in medical and industrial CT. It is structured to encourage participants from different fields to talk with each other, broaden their horizons, and make connections between problems and methodologies in the various fields. Several of the plenary talks will provide introductions to the areas. Trainees will be integrated into the conference through an informal welcome lunch and a poster session to introduce them to researchers in the field. This supports goals 1, 4, and 5, of the NIBIB: Researchers will present innovative biomedical technologies, engineering solutions, and mathematical methods to better image the body and objects more generally. The synergy between research areas will support the translation of technologies from the academic sphere to medical utility. The training opportunities for graduate students and beginners support the training of the next generation of diverse scientists. Project Narrative This conference will bring together medical, scientific, engineering, and applied mathematical researchers to present their newest research for a range of tomographic problems. Graduate students and beginners will be encouraged to participate and learn by being offered introductory talks, a student poster session, a welcome event, and an informal atmosphere. The conference will be structured so researchers will learn about important challenges in practical tomography as well as new techniques and methods, thereby creating synergies and research connections among the areas.",Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack,9837131,R13EB028700,"['Achievement', 'Advertising', 'Algorithms', 'Area', 'Biomedical Technology', 'Communication', 'Development', 'Earth science', 'Engineering', 'Environment', 'Event', 'Fertilization', 'Foundations', 'Goals', 'Image', 'Individual', 'Industrialization', 'Industry', 'International', 'Journals', 'Lead', 'Learning', 'Lightning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modality', 'Modernization', 'National Institute of Biomedical Imaging and Bioengineering', 'Nobel Prize', 'Outcome', 'Participant', 'Physics', 'Physiology', 'Population', 'Problem Solving', 'Publications', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Security', 'Seminal', 'Societies', 'Structure', 'Students', 'Techniques', 'Technology', 'Time', 'Training Support', 'Translations', 'Underrepresented Groups', 'Universities', 'Work', 'X-Ray Computed Tomography', 'cohort', 'demographics', 'design', 'experience', 'graduate student', 'higher level mathematics', 'informal atmosphere', 'innovation', 'mathematical methods', 'mathematical model', 'meetings', 'member', 'next generation', 'posters', 'professor', 'recruit', 'spectrograph', 'symposium', 'synergism', 'tomography', 'training opportunity', 'web site']",NIBIB,TUFTS UNIVERSITY MEDFORD,R13,2019,10000,-0.04583755128311914
"NLM Scrubber: NLM's Software Application to De-identify Clinical Text Documents Narrative clinical reports contain a rich set of clinical knowledge that could be invaluable for clinical research. However, they may also contain personally identifiable information (PII) that make those clinical reports classified as PHI, which is associated with use restrictions and risks to privacy. Computational de-identification seeks to remove all instances of PII in such narrative text in order to produce de-identified documents, which would no longer be classified as PHI and can be used in clinical research with fewer constraints and with almost no risk to privacy. Computational de-identification uses artificial intelligence methods including pattern recognition and computational linguistic techniques to recognize words and other alphanumeric tokens denoting PII (e.g., names, addresses, and telephone and social security numbers) in the text, and replace them with labels such as NAME and ADDRESS. In this way, both patient privacy is protected and clinical knowledge is preserved.  After exploring existing de-identification tools, the U.S. National Library of Medicine (NLM) began developing a new software application called NLM Scrubber, which is capable of de-identifying many types of clinical reports with high accuracy. The software design is based on both deterministic and probabilistic artificial intelligence methods utilizing large dictionaries of personal names, addresses, and organizations. The application accepts narrative reports in plain text or in HL7 format. When the input reports are formatted as HL7 messages, the application software leverages patient information embedded in HL7 segments to find such information in the text portion of the HL7 message.  NLM Scrubber has been downloaded by a number of organizations for testing and use, including IBM, Google, Fred Hutch Cancer Research Center, Harvard Medical School, Florida International University, University of Bristol, University College Dublin, and Oak Ridge National Laboratory. National Cancer Institute (NCI) along with the state cancer registries working with NCI have also been voiced their interests in using NLM Scrubber to de-identify narrative pathology reports in Surveillance, Epidemiology and End Results (SEER) database.  Our current focus is making NLM-Scrubber answer various needs of clinical scientists and clinical data managers without a deep understanding of the underlying technology. In this term, we improved run-time performance of the system and provided a graphical user interface for easier use. Our users can now provide white and black lists of terms to better preserve clinical information and to better preserve patient privacy, respectively. In the new version of NLM-Scrubber, the users can also preserve certain allowable PII, producing the so-called Limited Data Sets.   While NLM-Scrubber can be used for de-identifying all clinical reports repository-wide, it can also be used in various modes tailored to the user and their context, including on-demand cohort-specific de-identification and de-identification with patient and provider identifiers. NLM-Scrubber enables clinical scientists, the users of de-identified data, to be part of the process and shape the de-identification output based on their needs. n/a",NLM Scrubber: NLM's Software Application to De-identify Clinical Text Documents,10016024,ZIALM010002,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Clinical', 'Clinical Data', 'Clinical Research', 'Computational Linguistics', 'Computer software', 'Data', 'Data Set', 'Databases', 'Dictionary', 'Florida', 'Goals', 'Guidelines', 'Health', 'International', 'Knowledge', 'Label', 'Laboratories', 'Laws', 'Methods', 'NCI Center for Cancer Research', 'Names', 'National Cancer Institute', 'Output', 'Pathology Report', 'Patients', 'Pattern Recognition', 'Performance', 'Personally Identifiable Information', 'Policies', 'Privacy', 'Process', 'Provider', 'Regulation', 'Reporting', 'Risk', 'Running', 'SEER Program', 'Scientist', 'Shapes', 'Social Security Number', 'Software Design', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Text', 'Time', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Voice', 'base', 'cohort', 'college', 'graphical user interface', 'improved', 'interest', 'medical schools', 'neoplasm registry', 'patient privacy', 'preservation', 'repository', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2019,918126,-0.045721895184335865
"Exploring the evolving relationship between tobacco, marijuana and e-cigarettes Abstract The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana products (respectively). In order to understand this changing landscape we need new, ﬂexible, and responsive research methods capable of rapidly providing insights into product initiation patterns, use patterns, and cessation strategies. Social media — here deﬁned as including internet discussion forums — provides a ready-made source of abundant, naturalistic, longitudinal, publicly accessible, ﬁrst-person narratives with which to understand health behaviours and attitudes. We propose to use a combination of qualitative methods and automated natural language processing techniques to investigate online discussion forums devoted to tobacco, marijuana, and e-cigarettes in order to understand user trajectories through the three product categories. PROJECT NARRATIVE The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana (respectively). In order to make sense of this rapidly changing landscape, we need new, ﬂexible, and responsive research methods capable of providing insights into tobacco, marijuana, and e- cigarette product use patterns. We propose to use a combination of qualitative and automated natural language processing techniques to investigate online discussion forums related to tobacco, marijuana, and e-cigarettes in order to better understand user trajectories through these different product classes.","Exploring the evolving relationship between tobacco, marijuana and e-cigarettes",9788381,R21DA043775,"['Adolescent and Young Adult', 'Adult', 'Age', 'Algorithms', 'Attitude to Health', 'Categories', 'Chronic Bronchitis', 'Code', 'Consumption', 'Data', 'Data Science', 'Devices', 'Educational Status', 'Electronic cigarette', 'Health', 'Health behavior', 'High School Student', 'Individual', 'Internet', 'Manuals', 'Marijuana', 'Modeling', 'Multiple Marriages', 'Natural Language Processing', 'Pattern', 'Persons', 'Population', 'Qualitative Methods', 'Reporting', 'Research', 'Research Methodology', 'Resources', 'Role', 'Sampling', 'Smoking', 'Source', 'Surgeon', 'Techniques', 'Therapeutic', 'Tobacco', 'Tobacco use', 'Training', 'Work', 'base', 'cigarette smoking', 'combustible cigarette', 'electronic cigarette use', 'flexibility', 'high school', 'innovation', 'insight', 'man', 'marijuana use', 'nicotine replacement', 'smoking cessation', 'social media']",NIDA,UNIVERSITY OF UTAH,R21,2019,225147,-0.030786117319816866
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9786847,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2019,747591,-0.02038040755009343
"The next generation of RNA-Seq simulators for benchmarking analyses Abstract: RNA-Sequencing (RNA-Seq) has established itself as the primary method for studying transcription in basic research, with an emerging role in the clinic – currently upwards of 5,000 publications using the technology are indexed in PubMed. However, the interpretation of RNA-Seq requires several complex operations including alignment, quantification, normalization and statistical analyses of various types. Since its inception a large number of algorithms have appeared for each step, creating a very confusing landscape for investigators. In order to determine the best analysis practices, numerous benchmarking studies have emerged which leverage real RNA-Seq data made from well-studied RNA samples, such as the Genetic European Variation in Health and Disease (GEUVADIS) consortium data. These valuable RNA-Seq datasets contain the biases and errors introduced by sequencing biochemistry—factors that any analysis method must account for and overcome. However, the utility of such datasets for benchmarking analysis methods is limited by the fact that we do not know the underlying truth (e.g. the true number of RNA molecules from each transcript in the original sample). Therefore researchers tend to rely heavily on simulated data, since we know everything about the true composition of these samples. There are dozens of DNA simulators aimed at benchmarking applications such as variant calling. And while the need for simulators is just as strong in RNA analysis, there are only a scant few RNA-Seq simulators available. Furthermore, the available RNA- Seq simulators are based on simplifying assumptions that greatly restrict their utility for benchmarking anything but the most upstream steps in the analysis pipeline (e.g. alignment). The further downstream the analysis method is, the more accurately the true nature of real data and its technical biases need to be modeled in order to draw meaningful conclusions. For example, no simulator generates data from a diploid genome, which would be necessary to evaluate allele specific quantification. Given our extensive experience with RNA-Seq analysis and transcriptomics in general, and our success at building the BEERS simulator, and our track record of authorship on all comprehensive RNA-Seq aligner benchmarking studies published to date, we are ideally situated to develop the next generation of open-source RNA-Seq simulator which aims to model all sources of technical variability. Furthermore, the simulator will model biological variability with an empirical approach based on using real data to configure the simulator’s parameters, which is a natural problem for machine learning. There are eleven steps in RNA-Seq library preparation which introduce bias, all of which will be modeled by the software in an object-oriented modular framework. Project Narrative: There have been many algorithms developed for every step of the RNA-Seq analysis pipeline with no easy way to compare between them. Simulated data are useful for this purpose, but to date there are very few RNA-Seq simulators available and all make too many simplifying assumptions to be used for anything but the most upstream steps in the pipeline, e.g. alignment. We propose to develop the next generation of open-source RNA-Seq simulator, which will capture all of the biochemical processes in a modular fashion and model all of the sources of technical variation.",The next generation of RNA-Seq simulators for benchmarking analyses,9730605,R21LM012763,"['Affect', 'Algorithms', 'Alleles', 'Alternative Splicing', 'Authorship', 'Basic Science', 'Benchmarking', 'Biochemical', 'Biochemical Process', 'Biochemical Reaction', 'Biological', 'Biological Models', 'Clinic', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'DNA', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Development', 'Diploidy', 'Disease', 'Enzymes', 'European', 'Genetic', 'Genetic Transcription', 'Genome', 'Goals', 'Guanine + Cytosine Composition', 'Health', 'In Vitro', 'Libraries', 'Machine Learning', 'Methods', 'Modeling', 'Morphologic artifacts', 'Nature', 'Output', 'Preparation', 'Process', 'Protein Isoforms', 'Protocols documentation', 'PubMed', 'Public Domains', 'Publications', 'Publishing', 'RNA', 'RNA Splicing', 'RNA analysis', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Sequencing Biochemistry', 'Signal Transduction', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Transcript', 'Variant', 'Work', 'analog', 'analysis pipeline', 'base', 'biochemical model', 'design', 'digital', 'experience', 'experimental study', 'flexibility', 'indexing', 'next generation', 'open source', 'operation', 'power analysis', 'success', 'tool', 'transcriptome sequencing', 'transcriptomics']",NLM,UNIVERSITY OF PENNSYLVANIA,R21,2019,181125,-0.017343075760630326
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,9851583,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'learning strategy', 'mHealth', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2019,741688,-0.025630263706124434
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9676043,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,308000,-0.01829767591492799
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. n/a",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9882672,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,149810,-0.028406506885671465
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. • ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. • UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types • U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database • St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9853317,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Infrastructure', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'multidimensional data', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2019,855741,-0.01868199687959115
"Acquisition of a next-generation computing cluster We request funds to purchase our next-generation computing cluster to support computationally intensive NIH-funded research at Washington University in St. Louis. This system will become the foundation of the Center for High Performance Computing (CHPC) to support our active, diverse user community. It has been designed to meet our current and future computing needs. It adds additional capabilities to support emerging fields such as “Deep Learning”. The CHPC currently supports over 775 users from 300 different groups across 33 departments. 58 papers have cited the CHPC. The Center has a proven funding model and is economically sustainable. The Center has partnered with other University organizations to offer training workshops, not only on the use of the cluster, but also on introductory programming for users with no prior programming experience. If this proposal is funded, we will be able to continue to support this ever-growing diverse community of researchers. The proposed system would replace critical components including the management node, the login nodes, the storage, and upgrade the Infiniband networking. We would add substantial upgrades to our computing power with state-of-the-art processors, increased memory capacity for growing jobs, General Purpose Graphical Processing units (GPGPUs), and new capabilities for “Deep Learning”. Nearly all fields of NIH-funded research are faced with increasingly large data sets that require additional computing power to analyze. We propose building a next-generation computing cluster to support this research. Our Center has a proven track record in supporting a large, diverse group of users in all aspects of their computationally demanding research.",Acquisition of a next-generation computing cluster,9707936,S10OD025200,"['Communities', 'Educational workshop', 'Foundations', 'Funding', 'Future', 'High Performance Computing', 'Memory', 'Modeling', 'Occupations', 'Paper', 'Research', 'Research Personnel', 'System', 'Training', 'United States National Institutes of Health', 'Universities', 'Washington', 'cluster computing', 'deep learning', 'design', 'experience', 'next generation']",OD,WASHINGTON UNIVERSITY,S10,2019,597200,-0.004308370390077534
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9737676,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Quality', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2019,804907,-0.011010621701897406
"Academy of Aphasia Research and Training Symposium PROJECT SUMMARY/ABSTRACT The annual Academy of Aphasia meeting is the premier conference for researchers in the field of language processing and aphasia. Since the first meeting in 1963, this international meeting has brought together an interdisciplinary group of linguists, psychologists, neurologists, and speech-language pathologists to discuss the latest research in the field of aphasia, including theoretical, clinical, and rehabilitation aspects of this language disorder. The topics at the conference range widely but almost always cover all aspects of language processing including phonological processing, lexical-semantic processing, syntactic processing, orthographic processing, bilingualism, computational modeling, non-invasive and invasive brain imaging, language recovery, neuroplasticity, and rehabilitation. In this proposal, we aim to include two special initiatives that will take place during the annual academy of aphasia conference. The first initiative involves a formal mentoring program for young investigators entering the field of aphasia research. In this program, selected student/post-doctoral fellows from interdisciplinary backgrounds who are first authors at the conference are paired with a mentor. This mentor will provide specific feedback about the fellow's presentation and general mentorship to the fellow about research and academic careers. This program is currently occurring as part of the conference and has been growing at the annual meeting with very positive feedback. Additionally, a formal mentoring meeting will allow a structured format for discussion about a career in aphasia research. The second initiative will be a three hour seminar (New Frontiers in Aphasia Research) that covers the background and approach of a state of the art methodology (e.g., fNIRS, graph theoretical metric, machine learning approaches) that has an application to the study of aphasia. These workshops will be recorded and, consequently, uploaded to the academy website/youtube channel for dissemination to aphasia researchers and the public. This workshop will allow conference attendees to understand the conceptual and methodological aspect of a particular scientific approach that can be implemented in their study of aphasia. Given the highly interdisciplinary nature of aphasia research, these workshops will bridge the communication between aphasia researchers and scientists and experts who have developed new approaches to study the brain. This meeting already allows a valuable opportunity for cross-pollination of research ideas and will now provide a platform for the training the next generation of scientists interested in pursuing the nature of aphasia and associated language disorders in adults. PROJECT NARRATIVE Approximately 100,000 individuals suffer from aphasia each year. The academy of aphasia is an organization of clinicians, scientists and practitioners who study this communication disorder and develop interventions to treat individuals with aphasia. The members comprise a very interdisciplinary group of linguists, psychologists, speech and language clinicians, neurologists and neuroscientists. The annual academy of aphasia meeting is the premier venue to share state of the art methodologies for application in the study of aphasia to improve the research in the development of diagnosis and treatment approaches to alleviate aphasia. This conference is also the ideal venue to educate and train the next generation of aphasia researchers who are well trained from a theoretical, technical and clinical standpoint and are committed to expand the impact of research in aphasia.",Academy of Aphasia Research and Training Symposium,9731439,R13DC017375,"['Academy', 'Adult', 'Aphasia', 'Brain', 'Brain imaging', 'Chicago', 'Clinical', 'Collaborations', 'Committee Membership', 'Communication', 'Communication impairment', 'Computer Simulation', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Discipline', 'Educational workshop', 'Feedback', 'Fellowship', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Governing Board', 'Grant', 'Graph', 'Hour', 'Image Analysis', 'Individual', 'International', 'Intervention', 'Language', 'Language Disorders', 'Learning', 'Lesion', 'Linguistics', 'Machine Learning', 'Manuscripts', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neurologic', 'Neurologist', 'Neuronal Plasticity', 'Neurosciences', 'Non-aphasic', 'Orthography', 'Outcome', 'Pathologist', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychologist', 'Psychology', 'Publishing', 'Reading', 'Recovery', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Support', 'Research Training', 'Rest', 'Retrieval', 'Scientist', 'Secure', 'Speech', 'Speech Pathologist', 'Speech Perception', 'Stroke', 'Structure', 'Students', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Training and Education', 'Transcranial magnetic stimulation', 'Travel', 'Videotape', 'Work', 'Writing', 'base', 'bilingualism', 'career', 'career networking', 'experience', 'frontier', 'improved', 'improved outcome', 'interest', 'language processing', 'lexical', 'meetings', 'member', 'neuroimaging', 'next generation', 'novel', 'novel strategies', 'outcome forecast', 'phonology', 'programs', 'relating to nervous system', 'semantic processing', 'success', 'symposium', 'syntax', 'tenure track', 'web site']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R13,2019,39939,-0.04948843815967116
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,9882822,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2019,390806,-0.011381059089660589
"Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!) PROJECT SUMMARY/ABSTRACT Our society faces significant challenges in providing quality health care that is accessible by each person and is sensitive to each person's individual lifestyle and individual health needs. Due to recent advances in sensing technologies that have improved in accuracy, increased in throughput, and reduced in cost, it has become relatively easy to gather high resolution behavioral and individualized health data at scale. The resulting big datasets can be analyzed to understand the link between behavior and health and to design healthy behavior interventions. In this emerging area, however, very few courses are currently available for teaching researchers and practitioners about the foundational principles and best practices behind collecting, storing, analyzing, and using behavior- based sensor data. Teaching these skills can help the next generation of students thrive in the increasingly digital world.  The goal of this application is to design online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to WSU faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.  This contribution is significant because not only large research groups but even individual investigators can create large data sets that provide valuable, in-the-moment information about human behavior. They need to be able to handle the challenges that arise when working with sensor- based behavior data. Because students will receive hands-on training with actual sensor datasets and analysis tools, they will know how to get the best results from available tools and will be able to interpret the significance of analysis results.  Our proposed online course program, called AHA!, builds on the investigators' extensive experience and ongoing collaboration at Washington State University on the development of smart home and mobile health app design, activity recognition, scalable biological data mining, and the use of these technologies for clinical applications. Our approach will be to design online course modules to train individuals in the analysis of behavior-based sensor data using clinical case studies (Aim 1). We will design an educational program that involves students from diverse backgrounds and that is findable, accessible, interoperable, and reusable (Aim 2). Finally, we will conduct a thorough evaluation to monitor success and incrementally improve the program (Aim 3). All of the materials will be designed for continued use beyond the funding period of the program. PROJECT NARRATIVE  This program focuses on the development and dissemination of online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to Washington State University faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.",Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!),9696381,R25EB024327,"['Address', 'Aging', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological', 'Case Study', 'Charge', 'Chronic Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Development', 'Discipline', 'E-learning', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Evaluation', 'FAIR principles', 'Face', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'General Population', 'Goals', 'Health', 'Human', 'Immersion Investigative Technique', 'Individual', 'Interdisciplinary Study', 'Life Style', 'Link', 'Longevity', 'Machine Learning', 'Methods', 'Mobile Health Application', 'Monitor', 'Performance', 'Persons', 'Precision Medicine Initiative', 'Pythons', 'Rehabilitation Nursing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Site', 'Societies', 'Structure', 'Students', 'Suggestion', 'Techniques', 'Technology', 'Training', 'Universities', 'Washington', 'Work', 'base', 'behavior influence', 'behavioral health', 'biocomputing', 'career networking', 'clinical application', 'cognitive rehabilitation', 'cost', 'course development', 'course module', 'data mining', 'design', 'digital', 'experience', 'health care quality', 'health data', 'improved', 'innovation', 'learning materials', 'learning strategy', 'mHealth', 'next generation', 'online course', 'programs', 'recruit', 'responsible research conduct', 'scale up', 'sensor', 'sensor technology', 'skills', 'smart home', 'statistics', 'success', 'synergism', 'tool', 'web page']",NIBIB,WASHINGTON STATE UNIVERSITY,R25,2019,150529,-0.051842477577493405
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health.",New Jersey Alliance for Clinical Translational Science: NJ ACTS,9831333,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Community Health Centers', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2019,4776211,-0.020275062727404397
"Data Science and Sharing Team This is the third annual report for the Data Science and Sharing Team (DSST). Both the DSST and our sister group, the Machine Learning Team have now been at full staff for a full year and this has resulted in a considerable acceleration in productivity and demands for our services. Below is a summary of our activities for the past fiscal year.  Staff Transitions   After two years of excellent work on our team, our two original staff members, John Lee and Dylan Nielson, have been offered and accepted higher level positions within other intramural research groups. Dr. Lee accepted a position on the AFNI team under Dr. Robert Cox in November of 2018, and Dr. Nielson became the lead staff scientist for the Mood Brain & Development Unit (MBDU) under Dr. Argyris Stringaris in August of 2019. We see these transitions as a critical part of our core missions.   Many NIMH intramural research groups struggle to recruit quantitatively oriented candidates with strong programming skills as these candidates often choose to work in the for-profit technology sector which can offer higher salaries. Our team is fortunate in that we receive many inquiries from these candidates. We find that they are eager to work on our team given that we lie at the intersection of impactful health research and scientific progress while offering ample opportunities for training at the cutting edge of data science.   When our staff members advance their skill sets to a degree that they can compete for supervisory level positions and still choose to remain within our institute, we see this as a significant step toward achieving the mission of our team and strengthening the NIMH overall.  Intramural Data Sharing  Helping intramural investigators share their research data remains one of our team's core missions. One particularly large and impactful dataset that we have recently made publicly available from Armin Raznahan's section: 1,500 structural MRI scans collected longitudinally on 792 participants between the ages of 5 and 25. These data were collected at the NIMH between 1990-2010 and have proved very difficult to make publicly available. We are very happy to help finally make these data available to the community on the NIMH Data Archive: http://dx.doi.org/10.15154/1504177 https://nda.nih.gov/edit_collection.html?id=3142   Our team also continues to collaborate with Joyce Chung and the NIMH Clinical Director's Office on the Healthy Research Volunteers Protocol that was approved in October of 2017 (NCT03304665). Over 90 participants have been scanned to date using a standardized MRI protocol that was designed in collaboration with Vinai Roopchansingh from the NIMH fMRI Core Facility. The data from these participants is now publicly available on the NIMH Data Archive: https://nda.nih.gov/edit_collection.html?id=2843  These participants are also being actively referred to other NIMH protocols for which they qualify which we hope will lead to a rich, longitudinal dataset of healthy control data that is publicly accessible.   Exploring Cross Scanner Harmonization - ABCD  Several members of the NIMH extramural staff approached our team to consult with them on the effectiveness of current techniques for homogenization of data collected on different MRI scanners. To test these techniques, we used a dataset recently released by the Adolescent Brain Cognitive Development (ABCD) project. This work is now a manuscript available as a preprint (https://doi.org/10.1101/309260) and was recently presented by Dylan Nielson at the Organization for Human Brain Mapping in Rome and is available in poster format: http://doi.org/10.5281/zenodo.3366310  Aggregating Worldwide MRI Scanner Quality Metrics - MRIQC  MRIQC is a software tool for assessing the quality of structural and functional MRI data. We collaborated with the authors of MRIQC to build a database that receives scan quality information that is automatically uploaded every time MRIQC is run. That database, which we maintain, now contains anonymized quality control statistics on over 250,000 scans collected around the world. This database allows investigators to compare the data they have collected on their own scanners with a vast trove of similar data, to assess the quality of their scans and help guide changes in collection protocols and decide which scans need to be manually reviewed. These data show interesting relationships related to scan metadata such as manufacturer, flip angle, and tesla strength. These data were recently presented at the Human Brain Mapping Meeting in Rome and are available in poster format: http://doi.org/10.5281/zenodo.3366303  An open repository for positron emission tomography (PET) data in BIDS format  In 2017 Robert Innis, a leader in the standardization of PET nomenclature, approached our group to collaborate on building the necessary tools and infrastructure to make PET data sharing easier and more common. In collaboration with Melanie Ganz, who authored the majority of the PET BIDS standard, Russ Poldrack of the Stanford Center for Reproducible Neuroscience, and Doug Greeve of MGH, we are working to create a PET repository based on the OpenNeuro platform.   Quantifying Open Science and Data Sharing at NIMH  Funding bodies and journals are increasingly encouraging or requiring data sharing, however it is unknown how successful these policies have been over time. Even when data are shared publicly, it is difficult to determine if the data are properly organized and contain sufficient information for independent scientists to successfully use it to ask novel questions. Using a text mining approach, we are working to estimate the proportion of publications funded by the NIMH (both extramural and intramural) that provide references or links to shared data or code. In the coming months, we will also measure the impact of data sharing by assessing the rates of secondary use. In collaboration with the non-profit group ImpactStory, the results of this work will be publicized with an interactive tool for tracking which NIMH funded publications are publicly available, have open code, and have open data. This work will be presented at the MetaScience 2019 conference in Stanford in September. We hope these tools will provide the foundation for NIMH and other institutions to measure and incentivize effective data and code sharing that leads to reuse. n/a",Data Science and Sharing Team,10016962,ZICMH002960,"['Acceleration', 'Adolescent', 'Age', 'Annual Reports', 'Brain', 'Brain Mapping', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Communities', 'Consult', 'Core Facility', 'Data', 'Data Science', 'Data Set', 'Databases', 'Development', 'Effectiveness', 'Extramural Activities', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Health', 'Human', 'Incentives', 'Infrastructure', 'Institutes', 'Institution', 'Intramural Research', 'Journals', 'Lead', 'Link', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Manufacturer Name', 'Manuscripts', 'Measures', 'Metadata', 'Mission', 'Moods', 'National Institute of Mental Health', 'Neurosciences', 'Nomenclature', 'Participant', 'Policies', 'Positioning Attribute', 'Positron-Emission Tomography', 'Productivity', 'Protocols documentation', 'Publications', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Rome', 'Running', 'Scanning', 'Scientist', 'Services', 'Sister', 'Software Tools', 'Standardization', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Wages', 'Work', 'base', 'cognitive development', 'data archive', 'data sharing', 'design', 'interactive tool', 'longitudinal dataset', 'meetings', 'member', 'novel', 'open data', 'posters', 'recruit', 'repository', 'skills', 'statistics', 'symposium', 'text searching', 'tool', 'training opportunity', 'volunteer']",NIMH,NATIONAL INSTITUTE OF MENTAL HEALTH,ZIC,2019,996241,-0.0050341244718430656
"Accelerating Multi-modal Biomarker Discovery in Translational Research with Cloud Data Integration Project Summary/Abstract Cytobank is the leading cloud-based platform for analysis and storage of single cell flow and mass cytometry data, technologies that are essential for investigating the interplay between the immune system and disease conditions including cancer. There are numerous data analysis steps between raw data and insight especially for many single-cell technologies, where the data analysis is complex, highly expert-driven and/or reliant on novel computational methodologies. Cytobank already makes major contributions (1) centralizing single-cell cytometry data, (2) providing data analysis traceability that removes knowledge sharing complexities, and (3) establishing a platform that increases access to cutting edge algorithms and makes complex machine learning methods easy for biologists to use. However, as the amount, complexity, and different types of single cell data and other associated data increases and the number of workflows and single-cell algorithms to analyze the data also increases, the need for open and easy access to existing and new tools and secure, complete storage of the workflows and the resulting data has increased to the point of being critical for supporting basic and translational research collaborations and enabling them to efficiently achieve their objectives including biomarker discovery and development. The proposed project significantly extends the capabilities of the Cytobank platform. This will benefit the community by (1) enabling scalable and secure access to a number of new single-cell data analysis tools that will result in new automated workflows, and (2) enable more efficient cross platform knowledge generation with increased meta-analysis capabilities across experiments and data types. The potential of this project is that thousands of scientists around the world will be able to more easily leverage additional single-cell cytometry, transcript, and other data in their translational research data analysis including automating analysis that has primarily been dominated by expert-driven annotation, thus providing a central repository and knowledge management framework that will accelerate biomarker discovery and precision medicine. Project Narrative Single-cell biology and Immunotherapy are exploding and generating larger and more complex datasets in combination clinical trials. To take full advantage of these revolutions, the iteration and dissemination of advanced single-cell data analysis algorithms (many of whose development was funded by the NIH) needs to scale at the same rate as single-cell data generation technologies are scaling, and multi-omics data analysis and visualization must be integrated and automated. This project will greatly accelerate scientific research, transparency, and reproducibility by significantly lowering the barrier to perform complex data analysis of multiple types of high-dimensional data, providing the biomedical research community with access to powerful tools needed in immuno-oncology, autoimmunity and other high-impact disease areas.",Accelerating Multi-modal Biomarker Discovery in Translational Research with Cloud Data Integration,9672504,R44GM117914,"['Algorithmic Analysis', 'Algorithms', 'Area', 'Autoimmunity', 'B-Cell Acute Lymphoblastic Leukemia', 'Basic Science', 'Biological Markers', 'Biomedical Research', 'Cells', 'Cellular biology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Follicular Lymphoma', 'Foundations', 'Funding', 'Generations', 'Immune System Diseases', 'Immune system', 'Immunologic Monitoring', 'Immunology', 'Immunooncology', 'Immunotherapy', 'Information Resources Management', 'Knowledge', 'Label', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Meta-Analysis', 'Modality', 'Modeling', 'Multiomic Data', 'Outcome', 'Patients', 'Phase', 'Population', 'Positioning Attribute', 'Regimen', 'Relapse', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Scientist', 'Secure', 'System', 'Target Populations', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Toxic effect', 'Transcript', 'Translational Research', 'Treatment Efficacy', 'United States National Institutes of Health', 'Work', 'anti-cancer', 'automated analysis', 'base', 'biomarker development', 'biomarker discovery', 'biomarker validation', 'clinically actionable', 'cloud based', 'cost effective', 'cytokine', 'data integration', 'data management', 'data visualization', 'experimental study', 'high dimensionality', 'immunotherapy trials', 'improved', 'insight', 'learning strategy', 'multidimensional data', 'multimodal data', 'multimodality', 'novel', 'outcome prediction', 'personalized medicine', 'population based', 'precision medicine', 'predict clinical outcome', 'predictive marker', 'predictive modeling', 'relapse prediction', 'repository', 'response', 'single cell analysis', 'single cell technology', 'single-cell RNA sequencing', 'synergism', 'tool', 'transcriptome sequencing', 'transcriptomics', 'tumor']",NIGMS,"CYTOBANK, INC.",R44,2019,652516,-0.019634740858293677
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9665255,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease Surveillance', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Infrastructure', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2019,461012,-0.04011880366589098
"A Multigenerational Longitudinal Panel for Aging Research Summary/Abstract This project will construct a Multigenerational Longitudinal Panel (IPUMS-MLP) of unprecedented scale and scope. Using cutting-edge automatic record linkage technology and drawing on complete count U.S. census data available from IPUMS for the period 1850 to 1940, the project will construct millions of individual life histories and trace millions of families over multiple generations. This infrastructure will provide the most comprehensive view of long-run changes in life-course dynamics available for any place in the world and will transform our understanding of processes of population aging. The work will require significant innovation and new technical infrastructure to accommodate the massive scale of the database. These data will allow investigators to directly observe changes in aging processes and life-course transitions during the period in which U.S. society was being transformed by industrialization, urbanization, immigration, demographic transition, and economic collapse. Investigators will be able to follow individuals over time to evaluate the impact of early-life conditions on later outcomes, trace life-course transitions into adulthood and old age, and observe family change over multiple generations. IPUMS-MLP will enrich existing aging surveys by providing data on multiple generations of forebears of survey respondents; likewise, it will enrich existing historical databases by enabling them to connect with descendants across multiple generations. Leveraging billions of dollars of federal investments in census data and transactional records from a variety of administrative sources, this project is a highly cost-effective use of scarce resources to develop shared infrastructure for research, education, and policy-making on health and aging. Project Narrative The proposed work is directly relevant to the core mission of the Population and Social Processes branch of NIA: the new data will advance fundamental knowledge about the causes and consequences of changes in health and well-being of the older population and will support research on the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. For example, the data will enable examinations of the impact of lead exposure to late onset Alzheimer’s disease, the socioeconomic and health effects of early-life income support, intergenerational transmission of health and wellbeing over multiple generations, and the impact of early-life cognitive capacity on later-life health and economic outcomes.",A Multigenerational Longitudinal Panel for Aging Research,9769604,R01AG057679,"['Adult', 'Age', 'Aging', 'Big Data', 'Censuses', 'Characteristics', 'Communities', 'Custom', 'Data', 'Data Quality', 'Data Security', 'Databases', 'Demographic Transitions', 'Economics', 'Education', 'Elderly', 'Exposure to', 'Family', 'Family member', 'Future', 'Genealogy', 'Generations', 'Health', 'Household', 'Immigration', 'Income', 'Individual', 'Industrialization', 'Infrastructure', 'Institution', 'Investments', 'Knowledge', 'Late Onset Alzheimer Disease', 'Life', 'Life Cycle Stages', 'Link', 'Machine Learning', 'Metadata', 'Methods', 'Military Personnel', 'Mission', 'Names', 'Neighborhoods', 'Older Population', 'Outcome', 'Personal Satisfaction', 'Persons', 'Policy Making', 'Population', 'Population Process', 'Process', 'Public Policy', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Resources', 'Respondent', 'Running', 'Sampling', 'Selection Bias', 'Social Processes', 'Social Security', 'Societies', 'Source', 'Structure', 'Surveys', 'Technology', 'Time', 'Urbanization', 'War', 'Weight', 'Woman', 'Work', 'aging population', 'base', 'cognitive capacity', 'cost effective', 'data integration', 'economic outcome', 'experience', 'health economics', 'improved', 'innovation', 'intergenerational', 'lead exposure', 'learning strategy', 'life history', 'longitudinal dataset', 'machine learning algorithm', 'novel', 'parallel processing', 'social', 'socioeconomics', 'tool', 'transmission process']",NIA,UNIVERSITY OF MINNESOTA,R01,2019,681232,-0.025441118310533112
"Administrative Supplement to the OAIC Pepper Center Coordinating Center We wish to advantage of 2 new key opportunities that could significantly enhance achievement of the overall goals of the OIAC Coordinating Center (OAIC CC) and 2 key, unexpected administrative needs. Project 1) Develop, test and implement an innovative set of tools to perform Integrative Data Analysis (IDA) for combining and analyzing independent data sets across the OAIC network An over-arching goal of the OAIC CC is to build collaborations between OAICs that unlock synergy. Each of the OAICs has many small/medium-sized completed studies relevant to the OAIC theme, and that have measured key domains of physical function. Combining these studies could provide large, powerful databases for answering critical questions not possible with individual studies. However, this is currently not possible because different measurement instruments are often used across centers and across studies. This project overcomes this critical limitation by taking advantage of 2 newly available technologies and an ongoing study. IDA is a set of strategies in which two or more independent data sets which contain measures addressing similar domains but using different measurement instruments are combined into one and then statistically analyzed. The proposed project is timely because it leverages an ongoing clinical study to validate new procedures for harmonizing measures of physical and cognitive function across 20 Pepper center studies. The resources created by the project will significantly enhance collaboration across the OAIC program network, benefiting researchers at all OAICs, and can be disseminated to other NIA center programs. Project 2) Develop a robust, interactive database of OAIC Program accomplishments that will automatically be updated via an efficient, streamlined, electronic annual reporting process.  It is widely believed that the NIA-funded Pepper Center program has been highly productive. However, there is no means of assessing the overall effectiveness of the Pepper Center, or of ‘cataloging’ its impressive accomplishments. This project will take advantage of new open-source technology to efficiently develop a robust, comprehensive, searchable, interactive database of past accomplishments. It will also develop a streamlined electronic Annual Directory Report template, and link it to the new OAIC database so that it is automatically updated each year. Achieving the goals of this project will reduce administrative burden for sites, facilitate NIA review of performance of centers, and create an annually updated database of OAIC accomplishments, projects, publications, and outcomes, and facilitate collaborations between centers and investigators across NIA programs. This application also requests support for 2 key, unexpected administrative needs that have arisen: 1) Increase in funding amount for the annual OAIC CC Multi-center pilot project. 2) Support for additional Pepper Centers that will soon be added to the OAIC network. Relevance Statement for OAIC Coordinating Center Administrative Supplement The Coordinating Center of the OAIC coordinates the activities of all the individual centers in the NIA- funded, OAIC network; its over-arching goal is to build collaborations between the individual OAICs and thereby unlock synergy and enable projects that could not be undertaken by any single OAIC center. This administrative supplement application proposes 2 developmental projects that will significantly enhance the capabilities of the OAIC to achieve these goals and which takes advantage of newly available methods and technology. This also includes additional support for the possible increase in the number of Pepper Centers and an increase in the pilot award budget.",Administrative Supplement to the OAIC Pepper Center Coordinating Center,9961004,U24AG059624,"['Achievement', 'Address', 'Administrative Supplement', 'Aging', 'Annual Reports', 'Award', 'Budgets', 'Capsicum', 'Cataloging', 'Catalogs', 'Clinical', 'Clinical Research', 'Cognition', 'Collaborations', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Directories', 'Effectiveness', 'Elderly', 'Equipment and supply inventories', 'Evaluation', 'Funding', 'Goals', 'Health', 'Individual', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Online Systems', 'Outcome', 'Participant', 'Performance', 'Physical Function', 'Pilot Projects', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Update', 'Walking', 'analytical method', 'analytical tool', 'base', 'cognitive function', 'cost effective', 'data modeling', 'forest', 'innovation', 'instrument', 'interest', 'lifestyle intervention', 'new technology', 'novel', 'open source', 'programs', 'recruit', 'response', 'synergism', 'theories', 'tool']",NIA,WAKE FOREST UNIVERSITY HEALTH SCIENCES,U24,2019,149775,-0.011782065884316063
"Scalable tools to effectively translate genomic discoveries into the clinic PROJECT SUMMARY We are in the midst of a genomic revolution; more than 250,000 human genomes have been sequenced, generating over a petabase of genomic data. While these new data hold great promise to impact health, there is a disconnect between genomic discovery and clinical care. Providers frequently misinterpret genomic information, patients often don't understand their own test results, and genomic information about disease risk is infrequently shared between patients and family members. Importantly, ineffective communication and data misinterpretation has devastating consequences- including unnecessary organ removal, missed disease prevention opportunities, and premature death. We are addressing these genomic care gaps by developing and testing tools that optimize the integration of whole-exome and whole-genome sequencing (WES, WGS) for general clinical practice. My vision for improving genomic medicine is based on my work within multidisciplinary consortia and addresses the National Human Genome Research Institute's priority research area of improving the effectiveness of healthcare. In the proposed work we will test the effectiveness of a multilevel genomic e-Health intervention in cancer (Aim 1). Our intervention 1) educates physicians and patients about genomics, 2) enables direct-to-patient return-of- results, 3) provides physicians with patient-specific results and resources for interpretation, and 4) facilitates sharing of genomic results within families. We hypothesize that intervention use will result in higher rates of uptake of high-quality, genetically guided care. We will test our hypothesis in a randomized controlled trial among academic and community physicians who use WES for their patients. Next, we will use an iterative process, with stakeholder engagement, to adapt and pilot test our tool for Spanish and Mandarin speaking patients and for patients who have diabetes (Aim 2). Finally, we will create and assess new, moderated, social networks as a platform for genomic information sharing (Aim 3). Our hypothesis is that providers, patients and family members will engage with the genomic information sharing social networks and find them to be highly useful. Our general approach includes 1) creating the secure social networks, 2) integrating the networks into our e-Health intervention, and 3) using complementary methods, such as interviews and natural language processing, to assess stakeholders' network-related attitudes and network information quality. If successful, we will be well positioned to widely disseminate our e-Health tools. In sum, this work stands to transform how people obtain, process and share genomic information in the context of clinical care. Our tools reconceive genetic communication to allow for multi-directional flow of information, connects multiple stakeholders with one another, and integrates high-quality dynamic web-based resources to improve genomic care. In creating and deploying tools that both respond to and leverage the complexities of our information environment, we intend to transform genomic research and clinical practice. PROJECT NARRATIVE/ RELEVANCE OF PROJECT TO RESEARCH AND PUBLIC HEALTH Widespread utilization of genomic sequencing in medicine creates an urgent need to educate providers and patients. Currently, providers frequently misinterpret genomic information and patients often don't understand their own test results. In order to address this critical need, we propose to design and test multiple e-Health communication tools that will help providers and patients to better understand genomic data, lead to higher quality patient care, and facilitate genomic information sharing within families.",Scalable tools to effectively translate genomic discoveries into the clinic,9815293,R35HG010721,"['Address', 'Area', 'Attitude', 'Caring', 'Cessation of life', 'Clinic', 'Communication', 'Communication Tools', 'Community Physician', 'Data', 'Diabetes Mellitus', 'Effectiveness', 'Environment', 'Excision', 'Family', 'Family member', 'Genetic', 'Genome', 'Genomic medicine', 'Genomics', 'Health', 'Healthcare', 'Human Genome', 'Information Networks', 'Intervention', 'Interview', 'Lead', 'Malignant Neoplasms', 'Medicine', 'Methods', 'National Human Genome Research Institute', 'Natural Language Processing', 'Organ', 'Patient Care', 'Patients', 'Physicians', 'Positioning Attribute', 'Process', 'Provider', 'Public Health', 'Randomized Controlled Trials', 'Research', 'Research Priority', 'Resources', 'Secure', 'Social Network', 'Sum', 'Test Result', 'Testing', 'Translating', 'Vision', 'Work', 'base', 'clinical care', 'clinical practice', 'design', 'disorder prevention', 'disorder risk', 'eHealth', 'exome', 'genome sequencing', 'genomic data', 'genomic platform', 'improved', 'multidisciplinary', 'online resource', 'premature', 'tool', 'uptake', 'whole genome']",NHGRI,BECKMAN RESEARCH INSTITUTE/CITY OF HOPE,R35,2019,556256,-0.01644266594863978
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9751297,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Consumption', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2019,77750,-0.006322362065036301
"IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP The purpose of this contract is to provide bioinformatic support to researchers in the Divisions of National Toxicology Program (DNTP) and Intramural Research (DIR) at the National Institute of Environmental Health Sciences (NIEHS). NIEHS researchers conduct studies that produce large amounts of data, varying in size and complexity. Fields of scientific study are diverse and include toxicology, genomics, transcriptomics, high throughput screening (HTS) data and data extraction from diverse text resources. The variety and complexity of NIEHS scientific studies dictates the need for innovative analytical techniques and the development of new software tools. Bioinformatic data analyses are required to support accurate and precise interpretation of study results. Specific bioinformatics needs include data analysis, data mining, creating bioinformatics pipelines for gene expression and pathway analysis and computational support for the vast amount of data collected through studies conducted at NIEHS and NIEHS contract laboratories. n/a",IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP,9915697,73201700001C,"['Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'ChIP-seq', 'Chemical Exposure', 'Chemicals', 'Contractor', 'Contracts', 'DNA Methylation', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Epigenetic Process', 'Evaluation', 'Exons', 'Gene Expression', 'Genes', 'Genomics', 'Informatics', 'Intramural Research', 'Knowledge', 'Laboratories', 'Literature', 'Measures', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Output', 'Pathway Analysis', 'Peer Review', 'Privatization', 'Programming Languages', 'Proteomics', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scientific Evaluation', 'Scientist', 'Series', 'Software Tools', 'Specific qualifier value', 'Technology', 'Text', 'Toxicogenomics', 'Toxicology', 'analysis pipeline', 'bioinformatics tool', 'bisulfite sequencing', 'cheminformatics', 'computational intelligence', 'data integration', 'data mining', 'differential expression', 'high throughput screening', 'innovation', 'meetings', 'metabolomics', 'method development', 'next generation sequencing', 'physical property', 'programs', 'screening', 'technique development', 'transcriptomics', 'whole genome']",NIEHS,"SCIOME, LLC",N01,2019,2464037,-0.008524984520960219
"Central Sequencing Initiative Summary of ongoing projects organized by topic:   I. CLINICAL DIAGNOSTICS AND CONSULTATION  A. CLIA reports in CRIS. A central deliverable for our initiative is the clinical analysis and CLIA reporting into the CC medical record. This has been completed for approximately 600 patients since January 2019 and is a major accomplishment.  B. Operational development and refinement. Given the scale of our initiative, we dedicate significant attention to optimizing the efficiency of our workflow and anticipating potential disruptions related to policy adjustment or special circumstances. Specifically, this year we have increased automation of data entry into reporting and tracking processes, developed a new rapid turnaround time workflow for rare cases with high clinical urgency, and developed new CRIMSON workflows related to the new requirement for each patient to have a documented patient visit plan from a physician specifying sequencing prior to order activation.  C. Integration of medical geneticist in suite of consultation offerings. Recognizing room for improvement in our clinical consultation service for complex cases, we've started collaborating with a NIAID-NIAMS medical geneticist and are working concurrently with the NHGRI genetics consult service so we can continue to provide educational opportunities to genetics fellows. This has proven to be a clinically valuable service in multiple cases with complex clinical presentations or clinically significant molecular diagnoses outside the immune system.  D. Reanalysis. Clinical reanalysis of exome data is known to be an important source of new diagnoses over time. We are implementing a limited re-analysis workflow to be alerted to recent publications on variants in our database, which in rare cases provides sufficient evidence to merit a new molecular diagnosis.   E. Childrens collaboration. The CSI works with Gigi Notarangelo to recruit young patients from Children National Health System (site-PI: Mike Keller). The CSI protocol is the first protocol to be formally submitted under the new reliance agreement, paving the way for future studies and opening up a referral source of very young research participants who cannot typically be seen at the CC.   II. DISCOVERY  A. New gene disease discovery. The CSI is contributing to at least three ongoing projects with DIR investigators characterizing new gene-disease relationships. There is a significant opportunity to further exploit this data for discovery.  B. Collaboration with NHGRI on UTR and mosaic variants.  We have an ongoing collaboration with NHGRI to evaluate and develop more effective approaches for detection of clinically relevant variants in the untranslated regions of genes and clinically relevant mosaic variation.  C. Collaboration with multiple groups on computable phenotypic data. The CSI has built a highly valuable dataset of genomic data associated with detailed clinical records, manually coded with relevant phenotypic terms.  The integration of computable phenotypic and laboratory data into genomics is an area of great interest across the field.  We are working with NLM for more efficient text mining approaches, the NIAID epidemiological unit on phenotypic modeling for machine learning in large datasets from other health centers, and other extramural collaborators on tailoring phenotypic data analysis approaches for Mendelian disorders of the immune system.  D. Collaboration with The Genotype Ascertainment Cohort (TGAC), hosted by NHGRI. The CSI seeks to model genomic data sharing approaches that optimize both patient confidentiality and research productivity. In addition to the required deposition into dbGAP, CSI contributes data to TGAC. Exome and genome data from patients in contributing cohorts, including CSI, are available to view in aggregate by DIR researchers. This project is designed to enable further study of individuals via genomic ascertainment without prior knowledge of phenotype.  CSI staff has also participated in the review board for patient access requests.   III. SOCIAL AND BEHAVIORAL RESEARCH, POLICY  A. Negative results comprehension. The CSI is studying patient perceptions and understanding of the inconclusive negative exome results released in the medical record without specific counseling. The objective of this substudy is to use survey and interview data to better understand how well patients understand their negative exome sequencing results and to identify patient characteristics associated with poor understanding. Modification of our policy or specific educational interventions may follow if needed.  B. Secondary findings follow up collaboration. The CSI is collaboratively studying the clinical follow up of secondary findings with researchers at NHGRI.  There is some evidence that a minority of participants do not seek recommended follow up care for the potentially life threatening disorders which are returned on CSI as secondary findings.  The objective of this substudy is to better understand patient cognitive, emotional, and behavioral reactions to receiving a secondary finding, over time, including the motivators and barriers to clinical follow up.  C. Participating in facilitated discussions about policies on genetic research in humans at NIAID DIR. These discussions covered issues about quality of data, quality of analysis, and return of results, primarily focusing on the latter. n/a",Central Sequencing Initiative,10016049,ZICAI001244,"['Address', 'Agreement', 'Area', 'Attention', 'Automation', 'Behavioral', 'Behavioral Research', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Complex', 'Comprehension', 'Confidentiality of Patient Information', 'Consent', 'Consult', 'Consultations', 'Counseling', 'Data', 'Data Analyses', 'Data Discovery', 'Data Quality', 'Data Set', 'Databases', 'Deposition', 'Detection', 'Development', 'Diagnosis', 'Diagnostic Services', 'Disease', 'Education', 'Educational Intervention', 'Emotional', 'Enrollment', 'Epidemiology', 'Evaluation', 'Extramural Activities', 'Feedback', 'Future', 'Genes', 'Genetic', 'Genetic Counseling', 'Genetic Research', 'Genome', 'Genomics', 'Genotype', 'Health', 'Health system', 'Human', 'Immune System Diseases', 'Immune system', 'Individual', 'Infrastructure', 'Institutes', 'Interview', 'Knowledge', 'Laboratories', 'Language', 'Life', 'Machine Learning', 'Manuals', 'Medical', 'Medical Records', 'Medical center', 'Medicine', 'Mendelian disorder', 'Minority', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Diagnosis', 'Mosaicism', 'National Human Genome Research Institute', 'National Institute of Allergy and Infectious Disease', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Participant', 'Patients', 'Perception', 'Phenotype', 'Physicians', 'Policies', 'Process', 'Productivity', 'Protocols documentation', 'Publications', 'Reaction', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Robotics', 'Rotation', 'SNP array', 'Secure', 'Services', 'Site', 'Source', 'Specific qualifier value', 'Students', 'Surveys', 'Testing', 'Time', 'United States National Institutes of Health', 'Untranslated Regions', 'Variant', 'Visit', 'Work', 'clinical care', 'clinical diagnostics', 'clinically relevant', 'clinically significant', 'cohort', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'exome', 'exome sequencing', 'follow-up', 'genetic counselor', 'genomic data', 'interest', 'lectures', 'meetings', 'pediatric patients', 'phenotypic data', 'recruit', 'sample collection', 'social', 'text searching']",NIAID,NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS DISEASES,ZIC,2019,1443862,-0.03377949506740368
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9716628,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2019,763474,-0.019293968202007322
"COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data",9938885,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'Intelligence', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'commune', 'computational platform', 'computer framework', 'computing resources', 'connectome', 'cost', 'data anonymization', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'preservation', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2019,585151,0.001836297876637988
"Augmented Reality Platform for Feedback and Assessment in STEM elementary education ABSTRACT This SBIR Phase I project will build evidence-based content, challenges and assessments that promote: simulation-based learning; troubleshooting and critical-thinking; and diversity and inclusion in STEM learning. The approach will be transferable by design to teaching and measuring learner performance across scientific disciplines. Emerging digital content in virtual (VR) and augmented reality (AR) is already transforming science, technology, engineering, and math (STEM) education from the abstract and static learning models of the past to the applied and dynamic learning experiences of the future. These technologies have promise in delivering simulation environments capable of nurturing deep learning and higher-level thinking in K-12 students through practical experience, hand-on exercises and real-life applications, such as troubleshooting. Digital AR and VR educational content is beginning to address and develop these skills, but a platform has yet to be developed to effectively enable broad adoption in elementary settings. Cost efficient methods to provide formative feedback and gather summative evaluations for Next Generation Science Standards (NGSS) is also an unmet need. The successful completion of the proposed project will provide an evidence-centered content delivery and assessment framework as well as tool for addressing NGSS performance expectations that is transferable across topic areas and readily scalable for large-scale national implementation. The content will intentionally incorporate aspects of context and diversity of characters to ensure inclusion of groups that are historically underrepresented – specifically females and ethnic minorities. NARRATIVE Success in the workforce of the future will require the high-level thinking skills that the Next Generation Science Standards (NGSS) emphasize. Emerging digital content in virtual (VR) and augmented reality (AR) is transforming science and engineering education from the abstract and static learning models of the past to the applied and dynamic learning experiences of the future, through practical simulation-based experiences, but these lack the capability for performance assessment crucial to teachers and decision makers. By providing a readily scalable and flexible platform, the proposed approach promises to accelerate access to high-quality, inclusive, and evidence-centered AR content delivery and assessment of NGSS standards which in turn provides students with the skills they need for success.",Augmented Reality Platform for Feedback and Assessment in STEM elementary education,9847434,R43GM134813,"['Address', 'Adoption', 'Area', 'Augmented Reality', 'Award', 'Businesses', 'Career Choice', 'Child', 'Critical Thinking', 'Data', 'Development', 'Discipline', 'E-learning', 'Education', 'Educational process of instructing', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Exercise', 'Feedback', 'Female', 'Funding', 'Future', 'Hand', 'Indiana', 'Industry', 'K-12 student', 'Learning', 'Life', 'Longevity', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Next Generation Science Standards', 'Output', 'Perception', 'Performance', 'Phase', 'Problem Solving', 'Process', 'Reporting', 'Research', 'SECTM1 gene', 'Science', 'Science, Technology, Engineering and Mathematics', 'Science, Technology, Engineering and Mathematics Education', 'Scientist', 'Small Business Innovation Research Grant', 'Students', 'System', 'Technology', 'Testing', 'Thinking', 'Translating', 'Universities', 'Validation', 'Woman', 'Writing', 'Youth', 'base', 'cost efficient', 'deep learning', 'design', 'digital', 'digital media', 'educational atmosphere', 'engineering design', 'ethnic minority population', 'evidence base', 'expectation', 'experience', 'flexibility', 'girls', 'innovation', 'interest', 'mathematical learning', 'mobile application', 'pedagogy', 'prototype', 'simulation', 'simulation game', 'skills', 'success', 'teacher', 'theories', 'tool', 'virtual', 'virtual reality']",NIGMS,"EXPLORE INTERACTIVE, LLC",R43,2019,295622,-0.022680092646713893
"Microdata for Analysis of Early Life Conditions, Health, and Population Project Summary This project will enhance and develop the only source of data that provides core information about work, education, income, and migration for the entire U.S. population. This unique resource allows us to observe today's late-life population when they were young. This allows a prospective view of the impact of early-life environments and socioeconomic status on health and well-being in later life. The massive database describes the characteristics of all 133 million persons who resided in the United States in 1940. The data have been available for only a brief period, but are already having a profound impact on scientific research. They are the primary data source for at least 19 sponsored projects, including nine funded by NIH and seven funded by NSF. Through May 2017, 104 investigators had produced 95 papers based on the data, including 22 articles, four books, three PhD dissertations, and 66 working papers. This project will improve the quality and usability of the database by correcting transcription errors; improving the coding, editing, and allocation of key variables; and introducing new data dissemination tools designed to simplify access to the data. The project will undertake three major activities to meet these objectives. 1. Incorporate new verified census information on name, age, sex, family relationship, race, marital status,  birthplace, and residence five years ago. 2. Improve coding, editing, and allocation for geographic variables, migration, occupation, and numerically-  coded variables such as income. 3. Apply new technologies to democratize access to the data through new data access tools and a virtual data  enclave for a restricted version of the data that contains names. This project is a highly cost-effective use of scarce resources to develop shared infrastructure for research, education, and policy-making on health and aging. The proposed improvement of data quality is urgent, and will provide a resource of unprecedented power for understanding the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. Narrative This project will develop data for prospective analysis of the impact of early-life circumstances and environment on health outcomes in late life, including Alzheimer's disease. The proposed improvement of data quality is urgent, and will provide a resource of unprecedented power for understanding the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. The proposed work is directly relevant to the core mission of the Population and Social Processes branch of NIA, since the data will allow us to observe today's late-life population when they were young.","Microdata for Analysis of Early Life Conditions, Health, and Population",9750600,R01AG041831,"['Age', 'Aging', 'Alzheimer&apos', 's Disease', 'American', 'Arbitration', 'Books', 'Censuses', 'Characteristics', 'Church of Jesus Christ of Latter-day Saints', 'Code', 'County', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Demography', 'Doctor of Philosophy', 'Economics', 'Education', 'Elderly', 'Ensure', 'Environment', 'Family Relationship', 'Funding', 'Future', 'Genealogy', 'Genetic Transcription', 'Geography', 'Handwriting', 'Health', 'Income', 'Institution', 'Investments', 'Journals', 'Life', 'Life Cycle Stages', 'Link', 'Machine Learning', 'Marital Status', 'Methods', 'Mission', 'Names', 'Occupational', 'Occupations', 'Outcome', 'Paper', 'Personal Satisfaction', 'Persons', 'Policy Making', 'Population', 'Population Process', 'Privatization', 'Process', 'Public Policy', 'Publications', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Social Processes', 'Socioeconomic Status', 'Sociology', 'Source', 'Techniques', 'Technology', 'Time', 'United States', 'United States National Institutes of Health', 'Work', 'arm', 'base', 'blind', 'cost', 'cost effective', 'data access', 'design', 'experimental study', 'improved', 'migration', 'new technology', 'prospective', 'residence', 'sex', 'social', 'tool', 'usability', 'virtual']",NIA,UNIVERSITY OF MINNESOTA,R01,2019,650000,-0.02626679347660012
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9690782,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Dysregulation', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'bioinformatics tool', 'circadian', 'circadian regulation', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'informatics\xa0tool', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2019,328155,-0.026915500384189654
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,-0.03422022884679673
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive, PhysioBank, was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. PhysioToolkit, its software collection, supports exploration and quantitative analyses of PhysioBank and similar data with a wide range of well-documented, rigorously tested open-source software that can be run on any platform. PhysioNet's team of researchers leverages results of other funded projects to drive the creation and enrichment of: i) Data collections that provide increasingly comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC III (Medical Information Mart for Intensive Care) Database of critical care patients; ii) Analytic methods that lead to more timely and accurate diagnoses of major public health problems (such as life-threatening cardiac arrhythmias, infant apneas, fall risk in older individuals and those with neurologic disease, and seizures), and iii) Elucidation of dynamical changes associated with a variety of pathophysiologic processes and aging (such as cardiopulmonary interactions during sleep disordered breathing syndromes); User interfaces, reference materials and services that add value and improve accessibility to PhysioNet's data and software (such as PhysioNetWorks, a virtual laboratory for data sharing). Impact: Cited in The White House Fact Sheet on Big Data Across the Federal Government (March 29, 2012), PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are inaccessible otherwise. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world- wide, growing community of researchers, clinicians, educators, students, and medical instrument and software developers, retrieve about 380 GB of data per day. By providing free access to its unique and wide-ranging data and software collections, PhysioNet is invaluable to studies that currently result in an impressive average of nearly 250 new scholarly articles per month by academic, clinical, and industry-affiliated researchers worldwide. Over the next year we aim to sustain and enhance PhysioNet's impact with new technology and data; and complete the 2019 PhysioNet/Computing in Cardiology Challenge on sepsis. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,9993811,R01GM104987,"['Aging', 'Algorithms', 'Apnea', 'Area', 'Arrhythmia', 'Big Data', 'Biomedical Research', 'Boston', 'Bypass', 'Cardiology', 'Cardiopulmonary', 'Categories', 'Clinical', 'Clinical Data', 'Cloud Service', 'Collection', 'Communities', 'Community Outreach', 'Complex', 'Computer software', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Dedications', 'Development', 'Diagnostic radiologic examination', 'Entropy', 'FAIR principles', 'Federal Government', 'Functional disorder', 'Funding', 'Grant', 'Imagery', 'Individual', 'Industry', 'Infant', 'Infrastructure', 'Intensive Care', 'Israel', 'Journals', 'Laboratories', 'Lead', 'Licensing', 'Life', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical center', 'Methods', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase Transition', 'Physiological', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Roentgen Rays', 'Role', 'Running', 'Seizures', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Students', 'Switzerland', 'Syndrome', 'Testing', 'Thoracic Radiography', 'Time', 'United States National Institutes of Health', 'University Hospitals', 'Visit', 'accurate diagnosis', 'analytical method', 'clinical application', 'computerized data processing', 'computing resources', 'data archive', 'data sharing', 'experience', 'fall risk', 'heart rate variability', 'improved', 'innovation', 'instrument', 'instrumentation', 'interest', 'member', 'nervous system disorder', 'new technology', 'open source', 'preservation', 'repository', 'signal processing', 'software repository', 'symposium', 'time interval', 'virtual laboratory']",NIGMS,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2019,409563,-0.012583060828123488
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9765194,R44CA206782,"['Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'data warehouse', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'off-patent', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2019,238768,-0.030588393064776898
"Better Understanding and Handling of Tautomerism One motivation of our tautomerism-related work is thus to use all tools at our disposal, chemoinformatics analyses, QM computations, experimental work, and systematic extraction of results from literature, to provide a scientific footing for the recommendations how to improve handling of tautomerism in InChI V2 - instead of just holding a vote in the Working Group. While prototropic tautomerism rules are the only ones currently implemented as the standard rule set in CACTVS, and all tautomeric transformations covered by InChI (as default or by option) are prototropic, ring-chain (RC) tautomerism is well-known and widespread. Nevertheless, and somewhat surprisingly, very little in terms of RC rules was available in chemoinformatics until recently. Based on Baldwin's well-known set of rules to predict the relative facility of ring forming reactions, we developed a set of 11 rules describing RC tautomerism. The rules were encoded in SMIRKS line notation, the chemical transform extension of the chemical structure line notation SMILES, developed by Daylight Chemical Information Systems, Inc., just like the currently 20 individual rules in CACTVS for describing prototropic tautomerism are encoded. A number of modifications were applied to Baldwin's rule set, which, after all, were rules for ring-closure in general, not for RC tautomerism in specific. Foremost, ring closure and opening reactions involving a tetrahedral electrophilic carbon thus leading to breakage of a single bond would cause a loss of atoms to the molecule, violating the definition of tautomerism. Adding these new RC rules to the existing standard prototropic rules in CACTVS, we applied this combined rule set to the ""poster child"" of RC tautomerism: warfarin. This anticoagulant drug, in wide use for decades, can theoretically exist in solution in 40 distinct tautomeric forms. We investigated all these tautomers with computational approaches (relative energies calculated at the B3LYP/6-311G+ level of theory) and recorded NMR (13C and 1H) spectra. We introduced an intuitive and graphical network for tautomers and their interconversion paths, which for warfarin contained 11 tautomers and 17 tautomeric transformations between them allowed by our rules. We then applied the combined RC and prototropic rule set to an entire database: the Aldrich Market Select (AMS) database of (then) 6 million screening samples and building blocks. We found over 30,000 cases where two or more AMS products were declared by our rules to be just different tautomeric forms of the same compound. 1H and 13C NMR analysis of 166 such tautomer pairs (plus a few triplets) we purchased from the AMS were performed to determine whether the chemoinformatics transforms had accurately predicted what was the same ""stuff in the bottle"" as determined by NMR. Essentially all prototropic transforms for which examples in the AMS existed (some of the ""rarer"" types of tautomerism had no such ""conflict pairs"" in the AMS) were confirmed. Some of the RC transforms were found to be too ""aggressive"", i.e. to equate structures with one another that were different compounds according to the NMR analyses. This paper received an Editor's Choice selection in the Journal of Chemical Information and Modeling. In order to provide additional experimental data for tautomerism-related analyses and chemoinformatics work, we have created a database based on data extracted from experimental literature. This database consists of 1,873 entries which belong to n-tuples of tautomers studied in a particular set of experimental conditions (pH, solvent, temperature, technique), adding up to 3,898 records since the average of n is slightly 2. The data were extracted from 73 publications, many of them reviews, taken from a selection of 200 papers provided to the contractor company that did the initial extraction (Parthys Reverse Informatics), out of about 900 papers we identified in literature searches that might contain useful data for this purpose. Each tautomer (or tuple, as appropriate) is annotated with Structural information: SMILES, InChI, InChIKey, NCI/CADD Identifiers; ""Prevalence"" data: measured ratios, interconversion rates, relative energies etc.; Condition data: solvent, temperature, pH etc. (if given); Method data: NMR, UV spectroscopy, IR spectroscopy etc.; Reference data: Bibliographic information. To the best of our knowledge, such as tautomer database does not exist elsewhere, certainly not in the public domain. A new web service - called Tautomerizer - was created to apply and test the transforms we have compiled from the above database and literature for the Redesign of Handling of Tautomerism in InChI(Key) V.2. The set of transforms compiled in the context of this project has meanwhile grown to its final number of 81, which are also being added to the Tautomerizer. The phase of initiating and then making a decision in the IUPAC Working Group about the final set of transforms to be recommended for InChI V2 has been started. Work on a second-level analysis of tautomerism based on quantum-mechanical calculations and subsequent Deep Learning approaches has been started. Also, X-ray crystallography on a subset of the small molecules mentioned above has been performed. Several manuscripts about this project have been written and are about to be submitted. n/a",Better Understanding and Handling of Tautomerism,10014832,ZIABC011785,"['Anticoagulants', 'Appearance', 'Area', 'Azides', 'Bibliography', 'Book Chapters', 'Carbon', 'Charge', 'Chemical Structure', 'Chemicals', 'Chemistry', 'Child', 'Computer software', 'Conflict (Psychology)', 'Contractor', 'Cyclization', 'Data', 'Databases', 'Environment', 'Equilibrium', 'Eye', 'Hydrogen', 'Individual', 'Informatics', 'Intuition', 'Journals', 'Lead', 'Literature', 'Manuscripts', 'Measures', 'Mechanics', 'Methods', 'Modification', 'Molecular Weight', 'Motivation', 'Movement', 'Organic Chemistry', 'Paper', 'Phase', 'Prevalence', 'Property', 'Protons', 'Public Domains', 'Publications', 'Reaction', 'Recommendation', 'Records', 'Sampling', 'Solvents', 'Spectrum Analysis', 'Structure', 'System', 'Techniques', 'Temperature', 'Terminology', 'Testing', 'Tetrazoles', 'Triplet Multiple Birth', 'Variant', 'Voting', 'Warfarin', 'Work', 'X-Ray Crystallography', 'base', 'catalyst', 'chemical information system', 'deep learning', 'foot', 'improved', 'information model', 'migration', 'posters', 'quantum', 'quantum computing', 'screening', 'single bond', 'small molecule', 'structural biology', 'tautomer', 'theories', 'tool', 'web services', 'web site', 'working group']",NCI,DIVISION OF BASIC SCIENCES - NCI,ZIA,2019,262193,-0.005941642847114969
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",9919259,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Work', 'base', 'cell type', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,600000,-0.05020510669707224
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9762963,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Infrastructure', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2019,10000,-0.014879989670853987
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9902000,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health care facility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Infrastructure', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'implementation strategy', 'innovation', 'inpatient service', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2019,730148,-0.024532646534336722
"Biomedical Informatics Section (BIS) We interact with NIDA/IRP investigators to develop biomedical informatics applications and solutions that can access, manage, disseminate, and analyze large quantities of high-quality data. We develop, research, and/or apply computational tools to assist in the acquisition and analysis of biological, medical behavioral or health data, within a specific time frame determined to be appropriate by the NIDA. We help facilitate new system initiatives and changes to existing systems to meet legislative, regulatory, and departmental requirements within the specific time frames designated by each requirement. We conduct routine system analysis of automatic data processing resources and techniques of existing projects. We recommend, as needed, the implementation of new technologies that are efficient and timely.  We provide technical and professional support to help integrate computer systems, design or acquire computer programs, configure and support networks and streamline automated data processing within the NIDA's specified timeframe. We also integrate scientific data systems with network services and security services to foster safe and secure NIDA/IRP laboratory collaboration and for collaboration with extramural entities, when possible integrate scientific data systems with one another and ensure design for interoperable data.  We designed and deployed a mobile solution in collaboration with NIDA/IRP Archway clinic which enables studying craving and mood related to opioid and cocaine use among asymptomatic HCV+ and HCV methadone patients who have not started antiviral treatment. The smartphone-based system is capable of delivering a flexible ecological momentary assessment solution with multi-modal prompting operations as well as having enhanced integrated geolocation recording capabilities dynamically linking craving and mood to the whereabouts of the participants. Additionally, a collaborated project entitled ANCHOR: A novel Model of Hepatitis C Treatment as Anchor to Prevent HIV, Initiate Opioid Substitution Therapy, and Reduce Risky Behavior, is underway for data collection in order to evaluate a model of care for treatment of hepatitis C in people with ongoing injection drug use. These systems have been used in multiple protocols and are currently used in studies to better understand why some people are more likely to engage in HIV prevention behaviors.  We automate various aspects of clinical and non-clinical programs at the NIDA/IRP. These include but are not limited to functions related to research participant recruiting, pharmacy, nurses, physicians, and other investigators including those at the Johns Hopkins University School of Medicine and University of Maryland. n/a",Biomedical Informatics Section (BIS),10005009,ZIHDA000534,"['AIDS prevention', 'Adherence', 'Antiviral Agents', 'Archives', 'Area', 'Automatic Data Processing', 'Baltimore', 'Behavior', 'Behavioral', 'Big Data', 'Biological', 'Caring', 'Cellular Phone', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Research', 'Collaborations', 'Collection', 'Communication', 'Computational Science', 'Computer Security', 'Continuity of Patient Care', 'Data', 'Data Collection', 'Data Quality', 'Databases', 'Development', 'Ecological momentary assessment', 'Enrollment', 'Ensure', 'Environment', 'Exposure to', 'Extramural Activities', 'Fostering', 'Goals', 'Guidelines', 'HIV', 'HIV Seronegativity', 'HIV Seropositivity', 'HIV risk', 'Healthcare', 'Hepatitis C Therapy', 'Hepatitis C virus', 'Illicit Drugs', 'Informatics', 'Information Management', 'Information Resources Management', 'Information Sciences', 'Information Systems', 'Information Technology', 'Infrastructure', 'Intelligence', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Laws', 'Link', 'Machine Learning', 'Maintenance', 'Maryland', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Modeling', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Neighborhoods', 'Nurses', 'Opioid replacement therapy', 'Opioid user', 'Outcome', 'Participant', 'Patient Recruitments', 'Pharmacy facility', 'Phylogenetic Analysis', 'Physicians', 'Policies', 'Procedures', 'Process', 'Protocols documentation', 'Psychosocial Stress', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk Behaviors', 'Science', 'Secure', 'Security', 'Services', 'Site', 'Social Network', 'Specific qualifier value', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Training Support', 'Treatment Protocols', 'Universities', 'Work', 'addiction', 'base', 'biomedical informatics', 'cocaine use', 'computer program', 'computer science', 'computer system design', 'computerized tools', 'craving', 'data mining', 'design', 'disorder later incidence prevention', 'flexibility', 'handheld mobile device', 'health data', 'improved', 'injection drug use', 'interoperability', 'mHealth', 'medical schools', 'methadone patient', 'mobile computing', 'multimodality', 'new technology', 'novel', 'operation', 'opioid use', 'prevent', 'programs', 'support network', 'transmission process']",NIDA,NATIONAL INSTITUTE ON DRUG ABUSE,ZIH,2019,4183515,-0.029972491739204684
"ACTIVE: Abilities Captured Through Interactive Video Evaluation (DDT COA 000032) Project Summary/Abstract Technology has the potential to accelerate clinical research and reduce the burden of participation in rare diseases such as Duchenne Muscular Dystrophy (DMD). DMD is an x-linked genetic disorder that results in progressive muscle weakness with loss of ambulation by 10-12 years of age, progression of arm weakness resulting in difficulty with self-feeding and other self-care activities in adolescence, and death resulting from cardiopulmonary insufficiency by age 30 years. Studies in rare diseases are inherently difficult due to a small recruitment pool and a limited number of sites that possess the experience and resources to participate as a study site. An outcome measure that quantifies change in both ambulant and non-ambulant individuals with minimal evaluator training could enable more efficient data collection in multi-site clinical trials. Our upcoming submission, DDT COA 0032, Abilities Captured Through Interactive Video Evaluation (ACTIVE), has the potential to meet this need. ACTIVE is a 65-second game utilizing a skeletal-tracking algorithm to quantify workspace volume (WSV) elicited through maximal arm reaching overhead, side-to-side, and forward while also encouraging trunk lean in each direction. Our studies have shown that ACTIVE is valid and reliable in quantifying WSV in persons with DMD across the span of age and abilities. However, to increase access and portability of a tool for use across trial sites, it is critical that tool has sound scientific and technological construction. ACTIVE WSV was originally built upon the Microsoft Kinect and Kinect One for Xbox platforms. The skeletal tracking algorithm developed by Microsoft vastly exceeds all other programs as it had the full backing of the Microsoft machine. Unfortunately, the Kinect, in its additional sense, has been abandoned for more current artificial intelligence applications. The Microsoft Kinect Azure will soon be released with higher resolution and programming capabilities. Our full DDT submission has been delayed as each new camera release has required reprogramming of our software to ensure valid and reliable results. Our team has recently expanded to include software development partners, The Plan Works (thePlan), who have the technological expertise to alter our current codebase to ensure transfer of ACTIVE across camera sensor platforms is more efficient and reliable as we expect ongoing technological advances to provide opportunities for continued advances. To this end, our current application seeks support to verify the technology of the ACTIVE WSV system to 1) confirm the use of unique code that can be ported across platforms over time and 2) improving the ease of use and limit training needed at a growing number of inexperienced centers. Project Narrative Our upcoming submission, DDT COA 0032, Abilities Captured Through Interactive Video Evaluation (ACTIVE), is a 65-second outcome assessment that quantifies a person’s workspace volume and has the potential to expand the enrollment pool by measuring both ambulant and non-ambulant subjects. While the scientific construction of the tool is sound, ongoing technological advances and changes have made it challenging to port our software across camera platforms. Our proposal seeks funding to support final software updates to ensure changes in technology components (i.e. camera sensor systems) or coding instability will not interfere with clinical trial data collection and allow us to complete our final submission package.",ACTIVE: Abilities Captured Through Interactive Video Evaluation (DDT COA 000032),9989528,U01FD006883,[' '],FDA,RESEARCH INST NATIONWIDE CHILDREN'S HOSP,U01,2019,215170,-0.000981813681995928
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine No abstract available PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,10063300,U01LM012675,[' '],NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2019,375751,-0.033264637583884774
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9753389,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Autosomal dominant nocturnal frontal lobe epilepsy\xa0', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'informatics\xa0tool', 'phenotypic data', 'rare variant', 'sample collection']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2019,60925,-0.06357620097345125
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs. PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9902031,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Infrastructure', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Structure', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'data warehouse', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,WASHINGTON STATE UNIVERSITY,U54,2019,158967,-0.008257527839958572
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs.         PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.            ",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9776453,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Infrastructure', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Structure', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'data warehouse', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,WASHINGTON STATE UNIVERSITY,U54,2019,2464888,-0.008257527839958572
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),9850367,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip region structure', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2019,644134,-0.004863360062034312
"2020 Nanoscale Science and Engineering for Agriculture and Food Systems Gordon Research Conference and Gordon Research Seminar Project Summary The objectives of this GRC are: 1) To bring together experts and stakeholders to discuss nanotechnology advances, directions, and needs in food and agriculture; 2) To identify cutting-edge research and emerging opportunities to address global challenges of food security, environmental sustainability, food safety, and agricultural productivity. A major goal of this GRC is to promote exceptional early career and female investigators, as well as those of traditionally underrepresented populations, to ensure the continued growth in diversity of scholars in our vibrant community. A diverse range of invited speakers and discussion leaders will present a comprehensive vision of critical and emerging nanotechnology research advances across the field of agricultural science. The program illustrates a targeted excellent diversity of leadership (6), speakers (23), and discussion leaders (11) across the spectrum of: i) gender diversity (19 females, 21 males); ii) USA (24) /International participation (16), representing Austria, Brazil, Canada, Colombia, Denmark, Germany, India, Italy, Korea, Singapore, Spain, Switzerland, and Taiwan; and iii) industry and government (3), which includes the FDA, FAO, a founder of a blockchain startup company, and senior research scientist at a non-governmental organization. The following topics will be included in the program: Food and nutrition, bioinspired and targeted technologies, sensing and tracking, microbiomes, food contact materials and human health and disease. Discussions on the safe deployment of nanotechnology in these areas, public perception and policy, and influence on industry and entrepreneurship are integral to this GRC. The Center for Food Safety and Applied Nutrition (CFSAN), the FDA center responsible for protecting the health of US consumers, will find the following sessions of great interest, Convergence of Nanotechnology with Food & Agriculture; Advances in Nanomaterials; Environmental Nanotechnology; Nano-enabled Approaches to Improving Human & Animal Health; Translation of Nano-Based Science for Application in Food & Agriculture; Big Data, Machine Learning, AI & Modeling; and Nanotechnology's Impact on Food Safety. The significant contributions of this diverse population will provide the strong intellectual infrastructure required to develop safe and sustainable nano-enabled applications in food and agriculture in direct support of the FDA CFSAN goal of ensuring the safety, security, sanitation, and proper labeling of the U.S. food supply. Project Narrative Nanomaterials and nanotechnology are rapidly entering almost every industry around the world. The co-chairs of this GRC envision that the convergence between nanotechnology, biotechnology and information science within plant science, animal science, crop and food science/technology and environmental science will lead to revolutionary advances in improving public health. The successful achievement of this lofty vision requires the profound intellectual commitment of scientists and engineers in a highly integrated engagement across numerous disciplines. Some success examples with a focus on nanotechnology in food include: nano- biosensors for identification of pathogens, toxins and bacteria in foods; identification systems for tracking animal and plant materials from origination to consumption; development of nanotechnology-based foods with lower calories and with less fat, salt and sugar, while retaining flavor and texture; integrated systems for sensing, monitoring and active response intervention for plant and animal production; “smart field systems” to detect, locate, report and direct application of water; precision and controlled release of fertilizers and pesticides; development of plants that exhibit drought resistance and tolerance to salt and excess moisture; and nanoscale films for food packaging and contact materials that extend shelf life, retain quality, and reduce cooling requirements. Overall, nanoscale science and engineering has an important role in creating a safer and more productive agriculture and food system. Commercial advances and technological impacts have been limited somewhat due to the relative “newness” of nanotechnology in agriculture and food systems. Given that research at the nanoscale in agriculture and food systems is only into its second decade, safety of nano-enabled applications in agriculture are still up for debate. The Center for Food Safety and Applied Nutrition (CFSAN), the FDA center responsible for protecting the health of US consumers, will find the following sessions of great interest, Convergence of Nanotechnology with Food & Agriculture; Advances in Nanomaterials; Environmental Nanotechnology; Nano-enabled Approaches to Improving Human & Animal Health; Translation of Nano-Based Science for Application in Food & Agriculture; Big Data, Machine Learning, AI & Modeling; and Nanotechnology's Impact on Food Safety. In the spirit of the GRC mission, the 2020 Nano Ag & Food GRC and GRS will again bring together a diverse array of scientists and engineers, both senior and junior, postdocs, and graduate students from academia, as well as scientists and engineers from business, government, and Non-governmental organizations (NGOs); with an emphasis to attract researchers from around the world to engage in open scientific exchange. The significant contributions of this diverse population will provide the strong intellectual infrastructure required to develop safe and sustainable nano-enabled applications in food and agriculture in direct support of the FDA CFSAN goal of ensuring the safety, security, sanitation, and proper labeling of the U.S. food supply.",2020 Nanoscale Science and Engineering for Agriculture and Food Systems Gordon Research Conference and Gordon Research Seminar,9912335,R13FD006688,[' '],FDA,GORDON RESEARCH CONFERENCES,R13,2019,25000,-0.02446959630052593
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9752596,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,323659,-0.03032356316404862
"NIMH MEG Core Facility Hardware: The Magnetoencephalography (MEG) Core operates a state-of-the-art 275 channel MEG system (CTF MEG). A complete assortment of stimulus delivery and subject response equipment is interfaced to the MEG system. In FY19, we upgraded our research participant support system to better minimize movement during recordings. We also continued to obtain backup response collection equipment to insure redundancy in case of equipment failure. In addition, we obtained additional data storage hardware to enable ongoing archival storage of MEG scans collected in the MEG Core facility. Finally, a contract was awarded to obtain a BrainSight neuronavigation unit, to enable real-time co-registration of MEG localization fiducial markers and MRI scans. These units are currently in use by some laboratories utilizing the MEG Core Facility, and we are excited about expanding the reach of this technology to all labs utilizing the core, to enable more accurate co-registration.   Perhaps our most exciting hardware update is the award of a contract to acquire an initial proof-of-concept optically pumped magnetometer (OPM) system, consisting of 16 primary sensors and 3 reference sensors. A contract was also awarded to obtain a set of active compensation coils to reduce the magnetic fields surrounding the OPM array. We are working closely with the Section on Instrumentation (with George Dold) to design and construct a calibration jig. We expect delivery of the OPM sensors in early FY20, and hope to complete initial calibration measurements over several months. Our plan is to gradually increase the number of sensors in the system to hopefully obtain a system that will approach the spatial resolution of intracranial electroencephelogram (EEG), also known as electrocorticography (EcoG), without the need for neurosurgery. Once the device is fully calibrated, we will begin working with other investigators to develop applications.   Finally, in FY19 we moved forward on the installation of our 100% helium recovery and recycling unit. This will eliminate the reliance of the lab on the procurement of liquid helium, a scarce non-renewable natural resource. After an initial break-even point of several years, this system is estimated to save the US federal government approximately $100,000 per year. The planned installation date is early FY20.   Software: A variety of software for data analysis is maintained and supported by the Core, including proprietary CTF code, beamformer source reconstruction software (the SAM suite) written in-house, which interfaces with AFNI (NIMH Scientific and Statistical Computing Core) and Freesurfer, Fieldtrip, and MNE-Python. In addition, the MEG Core Facility frequently writes custom scripts to integrate stimulus and response data with the MEG dataset. The Core also actively develops new functionality within the SAM software suite, which is freely available online at our website (https://megcore.nih.gov). A release version of the software, including innovations such as a patch beamformer, automated alignment, and FreeSurfer integration will be in this new release. The MEG Core facility also assists investigators in setting up MEG software and ensures that all software is available on shared resources (The NIH High Performance Computing (HPC) center).   Education and Training: As noted earlier, the field of MEG is relatively small compared to the MRI community, and there are fewer well-established methods. The analysis of complex tasks designed to test innovative hypotheses requires unique approaches. The MEG Core Facility staff can leverage its wealth of experience to support and train investigators on these tasks. One-on-one training and support are provided upon request, and accounts for a significant portion of the scientific staffs time. In FY19, to supplement this training, we hosted a three day course teaching all aspects of the MNE-python software package. The course was attended by approximately 25 individuals, at the post-baccalaureate, post-doctoral, and staff scientist levels.   Support of the Larger MEG Community: As an effort to foster collaboration and communication across North American research and clinical MEG laboratories, the MEG Core Facility is in the planning stages of the 2019 MEG-North America Workshop to be held on the NIH campus in early FY20. The first day of the meeting will include several working groups, running concurrently with a full day hackathon. Notably, this will be the first ever MEG-focused hackathon. A collaborative white paper based upon prior work of MEG North America will be submitted by September 2019.   During FY18, an amendment was crafted to the NIMH protocol Recruitment and Characterization of Healthy Research Volunteers for NIMH Intramural studies (NCT03304665), to add an optional MEG study. We began scanning volunteers in January 2019, and to date, we have completed 36 recordings, an average of 4.5 per month. This represents the first dataset of its type, particularly given that all subjects also have both structural and functional MRI data, as well as NIH toolbox neuropsychiatric measures. We are currently in collaboration with the data science and sharing team to have this data shared broadly with the neuroimaging community.  Scientific Contribution and Collaboration:  The primary focus of the MEG Core Facility is to facilitate and enable the science of other NIMH investigators. However, based upon scientific interests, MEG Core Facility scientists frequently collaborate with other investigator to analyze data and test novel hypotheses. In FY18, core personnel have collaborated extensively with the labs of Carlos A. Zarate, Jr. (ETPB) and Karen F. Berman (CTNB) to gain insights into the pathophysiology of major depressive disorder (MDD) and schizophrenia. Details regarding these collaborations are available in the attached project bibliography and in forthcoming publications.  In order to foster discourse and collaboration between the NIH and extramural researchers, the MEG Core Facility is also hosting a short speaker series. We have scheduled three speakers for late FY19 and early FY20.   In addition, the MEG core facility has begun a unique project to demonstrate the ability of MEG to record activity from deep sources in the brain. While many investigators still believe recording from deep/subcortical sources is not feasible, through the use of a system such as the CTF system with axial gradiometers, combined with beamforming techniques, recording signals from these sources is possible. One aspect of the project involves a reward task, and we are currently in active collaboration with Dr. Argyris Stringaris to validate the task using fMRI, and demonstrate the value added from MEG data.   The final scientific contribution of the MEG Core Facility is the support of the work of NIH intramural scientists. A list of manuscripts acquired using the Core Facility resources appears below:  Bankson BB, Hebart MN, Groen, IIA, Baker CI. (2018). The Temporal Evolution of Conceptual Object Representations Revealed through Models of Behavior, Semantics and Deep Neural Networks. Neuroimage 178, 172-182.   Quentin R, King J-R, Sallard E, Fishman N, Thompson R, Buch ER, Cohen LG. (2019). Differential Brain Mechanisms of Selection and Maintenance of Information during Working Memory. J Neurosci 29(19) 3728-3740.  Bonstrup M, Iturrate I, Thompson R, Cruciani G, Censor N, Cohen LG. (2019). A Rapid Form of Offline Consolidation in Skill Learning. Curr Bio 29(8), 1346-1351.e4. n/a",NIMH MEG Core Facility,10008875,ZICMH002889,"['Amendment', 'American', 'Archives', 'Award', 'Behavior', 'Bibliography', 'Brain', 'Calibration', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Complex', 'Computer software', 'Consultations', 'Contracts', 'Core Facility', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Quality', 'Data Science', 'Data Set', 'Data Storage and Retrieval', 'Development', 'Devices', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Electrocorticogram', 'Ensure', 'Equipment', 'Equipment Failure', 'Evolution', 'Experimental Designs', 'Extramural Activities', 'Federal Government', 'Financial compensation', 'Fostering', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Goals', 'Helium', 'High Performance Computing', 'Human Resources', 'Individual', 'Knowledge', 'Laboratories', 'Learning Skill', 'Liquid substance', 'MRI Scans', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Maintenance', 'Major Depressive Disorder', 'Manuscripts', 'Measurement', 'Measures', 'Methods', 'Mission', 'Modeling', 'Movement', 'National Institute of Mental Health', 'Natural Resources', 'Neuronavigation', 'North America', 'Optics', 'Paper', 'Participant', 'Postdoctoral Fellow', 'Protocols documentation', 'Publications', 'Pump', 'Pythons', 'Recovery', 'Recycling', 'Research', 'Research Personnel', 'Resolution', 'Resource Sharing', 'Resources', 'Response to stimulus physiology', 'Rewards', 'Running', 'Scanning', 'Schedule', 'Schizophrenia', 'Science', 'Scientist', 'Semantics', 'Series', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Statistical Computing', 'Stimulus', 'Structure', 'Support System', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Training Support', 'Training and Education', 'United States National Institutes of Health', 'Update', 'Work', 'Writing', 'analytical method', 'base', 'computerized data processing', 'data acquisition', 'data sharing', 'deep neural network', 'design', 'design and construction', 'experience', 'hackathon', 'innovation', 'insight', 'instrumentation', 'interest', 'investigator training', 'magnetic field', 'meetings', 'neuroimaging', 'neurophysiology', 'neuropsychiatry', 'neurosurgery', 'novel', 'reconstruction', 'recruit', 'response', 'scientific computing', 'sensor', 'volunteer', 'web site', 'working group']",NIMH,NATIONAL INSTITUTE OF MENTAL HEALTH,ZIC,2019,2465653,-0.0036687572428094517
"HEAL: New Chemical Structures for Pain, Addiction and Overdose Targets The past year has seen the buildout of the infrastructure necessary to achieve the overall goals of the program across several key areas:  Personnel, Hardware, Information Technology (IT), Informatics, Logistics and Facilities.  1.	Personnel:  The program has started initially by taking a collection of existing NCATS personnel across a variety of disciplines ranging from chemistry, engineering, information technology, informatics and biology and having them work in a team environment to break the challenge down into manageable parts.  Over the course of the year we have moved several existing staff members into exclusively ASPIRE functions to build out the core team.  We have also hired a permanent lead of the initiative and by the end of the year will have several key chemistry and automation positions filled.  These are incredibly difficult positions to recruit capable candidates for given the multidisciplinary requirements of the program, so we are pleased with the progress of filling out several core members. 2.	Hardware:  Several key pieces of hardware have been acquired in the past year to enhance our overall chemistry capabilities with an eye towards automation and eventually autonomy.  A key factor for any piece of hardware purchased is that it must have an accessible API and the instrument must be capable of being integrated within a larger overall platform.  Key equipment acquired has given us the capability of performing programmatic and automated synthetic chemical reactions, platforms to perform reaction screening evaluation, flow and batch synthesis with a focus on the ability to scale reaction output up, autonomous mobile robotic platforms and others.  The overall focus has been to increase our automated chemistry capabilities via commercially available products and custom instrumentation projects. 3.	Information Technology:  A key area of focus for the past year was the establishment of an overall IT strategy that considers the distributed nature of the program and the requirement of secure interconnected systems acting in a synchronized fashion.  A key requirement is the integration of informatics platform to communicate and control instrumentation allowing for programmatic execution of experiments with the results being fed back to the overall system controller to help initiate the next round of experiments.  A beta system has been deployed to connect existing robotic platforms with an informatics driven controller and we have begun testing of this model. 4.	Informatics:  Aside from the afore-mentioned informatics driven experimentation a key area of focus has been on retrosynthetic analysis, both via commercial software and internally developed applications.  Time was spent with chemists and informatics to evaluate commercially available software in this space and although it was determined it was not quite ready to assist with traditional medicinal chemistry projects, it would be well suited to integrate within an automated platform to help drive novel synthetic reactions.  Work has also been done to extract information from our existing electronic laboratory notebook (ELN) which contains tens of thousands of reactions performed at NCATS over the years to create a reaction database that can is accessible via an API to perform reaction analytics to help facilitate robotic driven synthesis.  There is currently a beta application use with a project being initiated to deploy this as a production system in the IT provided cloud environment. 5.	Logistics and Facilities:  We have finally gotten preliminary access to additional space which will be required to build out the ASPIRE platform.  In the meantime, we have identified existing space that has been setup for use as the initial ASPIRE lab for prototyping and testing of new equipment and systems.  The overall progress for the year has been to advance the program in several different directions, primarily in chemistry, IT and informatics and to also begin to demonstrate how they will be integrated in an automated fashion eventually at a large scale. n/a","HEAL: New Chemical Structures for Pain, Addiction and Overdose Targets",9994052,ZIATR000344,"['3-Dimensional', 'Address', 'Animal Model', 'Area', 'Artificial Intelligence', 'Automation', 'Back', 'Biological Assay', 'Biological Testing', 'Biology', 'Cell model', 'Chemical Structure', 'Chemistry', 'Clinical Trials', 'Collection', 'Computer software', 'Custom', 'Databases', 'Development', 'Discipline', 'Engineering', 'Environment', 'Equipment', 'Evaluation', 'Eye', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Resources', 'Informatics', 'Information Technology', 'Infrastructure', 'Intelligence Tests', 'Intervention', 'Laboratories', 'Lead', 'Logistics', 'Modeling', 'Nature', 'Opiate Addiction', 'Output', 'Overdose', 'Pain', 'Patients', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Production', 'Public Health', 'Reaction', 'Research', 'Risk', 'Robotics', 'Science', 'Scientist', 'Secure', 'Synthesis Chemistry', 'System', 'Techniques', 'Testing', 'Time', 'Tissue Microarray', 'Tissues', 'Translational Research', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Work', 'addiction', 'bioprinting', 'chemical reaction', 'combat', 'design', 'drug candidate', 'efficacy study', 'experimental study', 'first-in-human', 'high throughput screening', 'human model', 'improved', 'induced pluripotent stem cell', 'innovation', 'instrument', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'novel therapeutics', 'opioid epidemic', 'opioid misuse', 'opioid use', 'preclinical efficacy', 'programs', 'prototype', 'recruit', 'safety study', 'screening']",NCATS,NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES,ZIA,2019,5842535,-0.023284861093023246
"HEAL: New Chemical Structures for Pain, Addiction and Overdose Targets The past year has seen the buildout of the infrastructure necessary to achieve the overall goals of the program across several key areas:  Personnel, Hardware, Information Technology (IT), Informatics, Logistics and Facilities.  1.	Personnel:  The program has started initially by taking a collection of existing NCATS personnel across a variety of disciplines ranging from chemistry, engineering, information technology, informatics and biology and having them work in a team environment to break the challenge down into manageable parts.  Over the course of the year we have moved several existing staff members into exclusively ASPIRE functions to build out the core team.  We have also hired a permanent lead of the initiative and by the end of the year will have several key chemistry and automation positions filled.  These are incredibly difficult positions to recruit capable candidates for given the multidisciplinary requirements of the program, so we are pleased with the progress of filling out several core members. 2.	Hardware:  Several key pieces of hardware have been acquired in the past year to enhance our overall chemistry capabilities with an eye towards automation and eventually autonomy.  A key factor for any piece of hardware purchased is that it must have an accessible API and the instrument must be capable of being integrated within a larger overall platform.  Key equipment acquired has given us the capability of performing programmatic and automated synthetic chemical reactions, platforms to perform reaction screening evaluation, flow and batch synthesis with a focus on the ability to scale reaction output up, autonomous mobile robotic platforms and others.  The overall focus has been to increase our automated chemistry capabilities via commercially available products and custom instrumentation projects. 3.	Information Technology:  A key area of focus for the past year was the establishment of an overall IT strategy that considers the distributed nature of the program and the requirement of secure interconnected systems acting in a synchronized fashion.  A key requirement is the integration of informatics platform to communicate and control instrumentation allowing for programmatic execution of experiments with the results being fed back to the overall system controller to help initiate the next round of experiments.  A beta system has been deployed to connect existing robotic platforms with an informatics driven controller and we have begun testing of this model. 4.	Informatics:  Aside from the afore-mentioned informatics driven experimentation a key area of focus has been on retrosynthetic analysis, both via commercial software and internally developed applications.  Time was spent with chemists and informatics to evaluate commercially available software in this space and although it was determined it was not quite ready to assist with traditional medicinal chemistry projects, it would be well suited to integrate within an automated platform to help drive novel synthetic reactions.  Work has also been done to extract information from our existing electronic laboratory notebook (ELN) which contains tens of thousands of reactions performed at NCATS over the years to create a reaction database that can is accessible via an API to perform reaction analytics to help facilitate robotic driven synthesis.  There is currently a beta application use with a project being initiated to deploy this as a production system in the IT provided cloud environment. 5.	Logistics and Facilities:  We have finally gotten preliminary access to additional space which will be required to build out the ASPIRE platform.  In the meantime, we have identified existing space that has been setup for use as the initial ASPIRE lab for prototyping and testing of new equipment and systems.  The overall progress for the year has been to advance the program in several different directions, primarily in chemistry, IT and informatics and to also begin to demonstrate how they will be integrated in an automated fashion eventually at a large scale. n/a","HEAL: New Chemical Structures for Pain, Addiction and Overdose Targets",9994052,ZIATR000344,"['3-Dimensional', 'Address', 'Animal Model', 'Area', 'Artificial Intelligence', 'Automation', 'Back', 'Biological Assay', 'Biological Testing', 'Biology', 'Cell model', 'Chemical Structure', 'Chemistry', 'Clinical Trials', 'Collection', 'Computer software', 'Custom', 'Databases', 'Development', 'Discipline', 'Engineering', 'Environment', 'Equipment', 'Evaluation', 'Eye', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Resources', 'Informatics', 'Information Technology', 'Infrastructure', 'Intelligence Tests', 'Intervention', 'Laboratories', 'Lead', 'Logistics', 'Modeling', 'Nature', 'Opiate Addiction', 'Output', 'Overdose', 'Pain', 'Patients', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Positioning Attribute', 'Process', 'Production', 'Public Health', 'Reaction', 'Research', 'Risk', 'Robotics', 'Science', 'Scientist', 'Secure', 'Synthesis Chemistry', 'System', 'Techniques', 'Testing', 'Time', 'Tissue Microarray', 'Tissues', 'Translational Research', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Work', 'addiction', 'bioprinting', 'chemical reaction', 'combat', 'design', 'drug candidate', 'efficacy study', 'experimental study', 'first-in-human', 'high throughput screening', 'human model', 'improved', 'induced pluripotent stem cell', 'innovation', 'instrument', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'novel therapeutics', 'opioid epidemic', 'opioid misuse', 'opioid use', 'preclinical efficacy', 'programs', 'prototype', 'recruit', 'safety study', 'screening']",NCATS,NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES,ZIA,2019,51028,-0.023284861093023246
"Physical modeling of biological systems In collaboration with a Senior Investigator in the Axon Guidance and Neural Connectivity Section, NINDS, and with his post-doctoral fellow, and with members of the Biomedical Imaging Research Services Section of the Office of Intramural Research, CIT, we are studying the mechanisms that guide neurons axons to their ultimate targets. In particular, we are analyzing experiments that record the the growth of a neuron in the wing disc extracted from Drosophila larvae. During these experiments, we monitor the dynamics of a cytoskeletal protein, actin, at sub-micrometer resolution for periods up to an 1.5 hours in the growing axon as it senses it way through the muscle tissue of the disc. We are also examining the effects on the axon growth dynamics by mutations in a kinase that modulates the actin activity. A manuscript describing this work in now under peer review.  In collaboration with a principal investigator and staff clinician in Section on Clinical Psychoneuroendocrinology and Neuropsychopharmacology, National Institute of Alcohol Abuse and Alcoholism, and with a staff scientist in the Mathematical and Statistical Laboratory, Office of Intramural Research, CIT, we are studying the factors that influence patients with alcohol use disorder (AUD) to seek treatment. This study involves analysis of hundreds of environmental, physiological, and psychiatric factors collected for each patient. A manuscript describing an alternative decision tree that shows the which factors differ between treatment-seeking and non-treatment seeking patients with AUD was published in EClinicalMedicine. In addition, we are applying methods from information theory to explore the spectrum of these factors in the AUD population.  In collaboration with a consortium of investigators consisting of (1) a principal investigator and staff clinician in Section on Clinical Psychoneuroendocrinology and Neuropsychopharmacology, National Institute of Alcohol Abuse and Alcoholism, (2) a principal investigator and staff clinician in Cognitive Neuroscience and Psychopharmacology Section, Neuroimaging Research Branch, National Institute on Drug Abuse, and (3) a staff scientist in the Mathematical and Statistical Laboratory, Office of Intramural Research, CIT, we are investigating differences in the decision making process between addicts and non-addicts. In particular, we are comparing cocaine users to a set of non-smoking, non-addicted controls. We study their decision making process using a ""trust game"" approach in which players (the subjects) decide whether or not to trust each other or a randomized computer algorithm in dispensing financial gains or penalties. Our preliminary results suggest that cocaine users respond differently from non-using controls, especially in the initial stages of the game.  From 2012 to 2017, the LI worked with a principal investigator in Section on Neuronal Connectivity, NICHD, (Chi-hon Lee) on statistical and topological analysis of retinal neurons growth in Drosophila. In FY 2018, Dr. Lee left the NIH to become the director of the Institute of Cellular and Organismic Biology, Academia Sinica, in Taipai, Taiwan. The LI and Dr. Lee, along with (1) two staff scientist in NICHD, and (2) members of the Biomedical Imaging Research Services Section of the Division of Computational Bioscience, CIT, are now finishing up their long-term collaboration. A manuscript describing this work in now under peer review.  In FY 2019 the LI started a new collaboration with a PI in the Clinical Center Nursing Department to study gut microbiome in patents. The LI is also collaborating with (1) a PI from the  Laboratory of Neurogenetics, National Institute on Alcohol Abuse and Alcoholism, and (2) a staff scientist from the Mathematical and Statistical Computing Laboratory, Center for Information Technology. Along with staff scientist Jennifer Barb from CIT, the LI presented a talk on improving estimates of population abundances at the 3rd NIH-FDA Joint Agency Microbiome Symposium held on the NIH campus. Dr Barb and the LI were invited to give a talk on the same subject at the 2019 NIST Workshop on Standards for Microbiome Measurements (to be held September 2019 at the NISt campus in Gaithersburg, MD). A manuscript describing the work on improving estimates of population abundances is now under referee review. n/a",Physical modeling of biological systems,10007467,ZIACT000276,"['Academia', 'Actins', 'Alcohol abuse', 'Alcoholism', 'Axon', 'Biological', 'Biological Sciences', 'Biology', 'Cells', 'Cellular biology', 'Clinical', 'Clinical Research', 'Cocaine Users', 'Collaborations', 'Computational algorithm', 'Cytoskeletal Proteins', 'Decision Making', 'Decision Trees', 'Discipline of Nursing', 'Drosophila genus', 'Educational workshop', 'Goals', 'Growth', 'Hour', 'Information Centers', 'Information Technology', 'Information Theory', 'Institutes', 'Intramural Research', 'Investigation', 'Joints', 'Laboratories', 'Laboratory Study', 'Larva', 'Left', 'Legal patent', 'Manuscripts', 'Mathematical Computing', 'Mathematics', 'Measurement', 'Methods', 'Monitor', 'Muscle', 'Mutation', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurons', 'Organism', 'Patients', 'Peer Review', 'Phosphotransferases', 'Physiological', 'Population', 'Postdoctoral Fellow', 'Principal Investigator', 'Process', 'Psychoneuroendocrinology', 'Psychopharmacology', 'Publishing', 'Randomized', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Services', 'Statistical Computing', 'Taiwan', 'Techniques', 'Trust', 'United States National Institutes of Health', 'Wing', 'Work', 'alcohol use disorder', 'axon growth', 'axon guidance', 'axonal guidance', 'bioimaging', 'biological systems', 'cognitive neuroscience', 'experimental study', 'gut microbiome', 'improved', 'mathematical model', 'member', 'microbiome', 'neurogenetics', 'neuroimaging', 'neuropsychopharmacology', 'non-smoking', 'off-patent', 'physical model', 'physical process', 'relating to nervous system', 'retinal neuron', 'simulation', 'symposium']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIA,2019,205110,-0.05314159143120054
"Mixed Reality System for STEM Education and the promotion of health-related careers Project Summary/Abstract Proposed is a system to combine and leverage the advantages of existing medical props with interactive media to provide engaging and cooperative group STEM learning experiences. Significance: The PowerPoint lecture style has become the standard method for teaching groups of students. Unfortunately, this style does not emphasize student-instructor or student-student instruction, and in fact seems to have made students even less engaged than before. Broad agreement exists in the field of science education that more engaging pedagogies benefit students in introductory classes. A variety of teaching aids, for example plastic medical props and mannequins are available to support more engaging learning exercises. Despite their substantial benefits, physical props are fundamentally limited as they are primarily static (e.g. fixed coloration, disease depiction), their internal structures (with limited exceptions) often bear little resemblance to actual human anatomy, and they are passive objects. Hypothesis: A system which can provide more engaging interaction with physical props will be able to improve student retention and increase interest in STEM related subjects. Specific Aims: To prove the feasibility of the proposed system in Phase I IDL will 1) Determine stakeholder requirements through round table discussions; 2) Create prototype system hardware & software to augment learning with physical props; and 3) Validate the prototype system through a pilot study. The overall Phase I effort will demonstrate the ability of the proposed system to augment learning with physical props. In the Phase II effort IDL will ready the system for commercialization by 1) Developing production-quality software, hardware, and user interfaces; 2) Developing a set of comprehensive curricula for the system; and 3) Validating the system through human subject testing. Project Narrative Passive learning methods, i.e. PowerPoint lectures, have become the standard method for teaching groups of students topics including Anatomy and Physiology in spite of broad agreement in the field of science education that more engaging pedagogies benefit students in introductory classes. A variety of teaching aids, for example plastic medical props and mannequins are available to support more engaging learning; however, these props are fundamentally limited.",Mixed Reality System for STEM Education and the promotion of health-related careers,9851024,R44GM130247,"['3-Dimensional', 'Agreement', 'Algorithmic Software', 'Anatomy', 'Biological', 'Biological Sciences', 'Collaborations', 'Color', 'Computer Vision Systems', 'Computer software', 'Computers', 'Development', 'Disease', 'Disease Progression', 'Dissection', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Environment', 'Exercise', 'Hand', 'Health', 'Health Promotion and Education', 'Hour', 'Human', 'Hybrids', 'Image', 'Instruction', 'Intervention', 'Learning', 'Location', 'Manikins', 'Medical', 'Minnesota', 'Modeling', 'Participant', 'Phase', 'Physiological', 'Physiology', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Role', 'Sampling', 'Science, Technology, Engineering and Mathematics Education', 'Scientist', 'Slide', 'Small Business Innovation Research Grant', 'Structure', 'Students', 'Support Groups', 'System', 'Teaching Method', 'Testing', 'Time', 'Training', 'Universities', 'Ursidae Family', 'animation', 'career', 'college', 'commercialization', 'design', 'digital media', 'experience', 'flexibility', 'guided inquiry', 'hands-on learning', 'human subject', 'improved', 'innovation', 'instructor', 'interactive tool', 'interest', 'learning strategy', 'lectures', 'machine vision', 'mid-career faculty', 'mixed reality', 'pedagogy', 'prototype', 'retention rate', 'science education', 'software systems']",NIGMS,"INNOVATIVE DESIGN LABS, INC.",R44,2019,782476,-0.03794394493335142
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9894990,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genetic Diseases', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health Care Costs', 'Health system', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Care Costs', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'care costs', 'clinical care', 'clinical decision support', 'clinical practice', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'education resources', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'patient portal', 'polyposis', 'practice setting', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2019,707923,-0.012064988448611277
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9731544,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'repository', 'research and development', 'software development', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,350620,-0.033854261610541045
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9789913,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Simulation', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2019,750000,-0.018815596009635928
"Development and Validation of a Collaborative Web/Cloud-Based Dosimetry System for Radiopharmaceutical Therapy. Project Summary  Radiopharmaceutical therapy (RPT) – the use of targeted radionuclides to deliver radiation specifically to cancer cells and their microenvironment – is a fundamentally different approach to cancer therapy that is growing, with a substantial number of large and small pharmaceuticals companies developing products in this area and radionuclide producers making substantial investments in scaling up production. This is especially true in the area of alpha emitters. The dosimetric evaluation of therapeutic radiopharmaceuticals is a key requirement for regulatory approval and optimal administration of RPTs, especially in combination with external beam radiation therapy. This project will provide a cloud-based dosimetry software service, delivered through a web-browser, that includes the full complement of methods needed for dosimetry in the context of obtaining regulatory approval of RPTs and, ultimately, for optimal clinical delivery. Providing this in a cloud-based system will enable a variety of models for selling the service that do not require a large up-front capital investment for clinics or radiopharmaceutical developers. It also will provide access to expert advice, customization, and dosimetry services, and allow for collaboration between developers, dosimetry experts, and clinical sites. To accomplish the goal of developing this cloud-based web-browser-delivered RPT dosimetry software service, we propose the following specific aims: (1) Design, develop and implement a web/cloud-based integrated software system for treatment planning of RPT therapy; (2) design and implement a full server-side framework for subscription, authentication, and granting collaborative privileges for the various processes and data in the dosimetry pipeline; (3) optimize and adapt the four most computationally intensive processes for a multi-processor cloud-based compute environment; (4) apply and evaluate the toolchain developed in aims 1-3 to phantom, simulated and existing patient data. Successful completion of this project will produce a cloud-based software system delivered to the user via a web browser that provides an integrated, streamlined, robust, state-of-the-art system for RPT treatment planning. This system would enable a collaborative approach to multi-center clinical trials and eventually to clinical delivery of optimally dosed RPT. Projective Narrative  Radiopharmaceutical therapy is an emerging cancer therapy modality involving the targeted delivery of radiation to tumors using tumor-targeting molecules. The dosimetric evaluation of the therapeutic radiopharmaceuticals is a key requirement for regulatory approval and optimal administration of RPTs. This project seeks to develop a cloud-based software system delivered to the user via a web browser that provides an integrated, streamlined, robust, state-of-the-art system for RPT treatment planning; and enables a collaborative approach to multi-center clinical trials and eventually to the clinical delivery of optimally dosed RPT.",Development and Validation of a Collaborative Web/Cloud-Based Dosimetry System for Radiopharmaceutical Therapy.,9909727,R44CA213782,"['3-Dimensional', 'Architecture', 'Area', 'Big Data', 'Biological', 'Businesses', 'Capital', 'Client', 'Clinic', 'Clinical', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Complement', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Collection', 'Development', 'Dose', 'Dose-Rate', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Goals', 'Grant', 'Growth', 'Health', 'Individual', 'Infrastructure', 'Internet', 'Investments', 'Licensing', 'Methods', 'Modality', 'Modeling', 'Multi-Institutional Clinical Trial', 'Online Systems', 'Pathway interactions', 'Patients', 'Pharmacologic Substance', 'Phase', 'Privacy', 'Process', 'Production', 'Radiation', 'Radioisotopes', 'Radiopharmaceuticals', 'Research Personnel', 'Running', 'Services', 'Side', 'Site', 'Small Business Innovation Research Grant', 'Software Tools', 'System', 'Systemic disease', 'Translating', 'Treatment Protocols', 'Uncertainty', 'Validation', 'Vendor', 'Work', 'analysis pipeline', 'base', 'cancer cell', 'cancer therapy', 'clinical application', 'clinical research site', 'cloud based', 'collaborative approach', 'computing resources', 'cost effective', 'data sharing', 'deep learning', 'design', 'dosimetry', 'experience', 'image reconstruction', 'image registration', 'imaging Segmentation', 'interest', 'medical specialties', 'multicore processor', 'neoplastic cell', 'precision medicine', 'prototype', 'quantitative imaging', 'radiation delivery', 'reconstruction', 'scale up', 'single photon emission computed tomography', 'software systems', 'targeted delivery', 'therapeutic evaluation', 'tool', 'treatment planning', 'treatment strategy', 'tumor', 'web services']",NCI,"RADIOPHARMACEUTICAL IMAGING AND DOSIMETRY, LLC",R44,2019,999998,-0.032948618511819364
"Longitudinal and Intergenerational Determinants of Aging and Mortality ABSTRACT  Some of the most important open questions in aging relate to the impact of longitudinal and intergenerational factors. But documenting the role of early-life and intergenerational determinants of health and aging is limited by the dearth of large-scale micro-data containing this information. This is especially true for understudied populations such as women and minority groups.  Our research objective is to add critical information on cause of death to the new large-scale data resource, the Longitudinal, Intergenerational Family Electronic Micro-database (LIFE-M). Funded by the National Science Foundation, LIFE-M links millions of vital records (birth, marriage, and death certificates) to decennial censuses over four generations and 120 years for five states. LIFE-M is a representative sample of cohorts aging and dying in the last 25 years of the 20th century and includes crucial early-life and intergenerational information. Enhancing the LIFE-M with cause of death will facilitate path-breaking research on the relationship of longevity and cause of death with demographic, socio-economic, and early-life environmental factors for family networks across four generations.  We will achieve this objective by pursuing the following specific aims:  (1) We will use new “Smart Indexing” technology to digitize and cross-validate hand-written cause-of-death information;  (2) We will link digitized causes of death to the LIFE-M infrastructure and create extensive documentation for  this new variable for public use; and  (3) We will publicly release the cause-of-death variable and documentation with the LIFE-M dataset, meta-  data, and supporting documentation on ICPSR in 2020.  The proposed project will also have broader impacts. In addition to contributing a significant new data resource that can be added to Minnesota Population Center's historical linked censuses and the Census Longitudinal Infrastructure Project (CLIP), this project's methodological innovations in script digitization will enhance on-going and future data infrastructure initiatives. Both contributions promise to transform the research frontier in population health and aging in the United States. PROJECT NARRATIVE  This project contributes to public health knowledge by adding cause-of-death information to a new intergenerational and longitudinal dataset (LIFE-M). These data will allow much more research on the long- term determinants of health and aging, including a deeper understanding the intergenerational and early-life origins of later-life diseases and mortality in today's aging population.",Longitudinal and Intergenerational Determinants of Aging and Mortality,9691090,R01AG057704,"['Address', 'Adult', 'Affect', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'American', 'Biogenesis', 'Birth', 'Birth Records', 'Cardiovascular Diseases', 'Cause of Death', 'Censuses', 'Cessation of life', 'Child', 'Childhood', 'Complement', 'Cost Savings', 'Data', 'Data Set', 'Databases', 'Death Certificates', 'Demography', 'Development', 'Disease', 'Documentation', 'Economics', 'Elderly', 'Enrollment', 'Environmental Risk Factor', 'Epidemiology', 'Exposure to', 'Family', 'Foundations', 'Funding', 'Future', 'Generations', 'Genetic Transcription', 'Hand', 'Handwriting', 'Health', 'Health Campaign', 'Health Resources', 'Health and Retirement Study', 'Hypertension', 'Image', 'Immigrant', 'Income', 'Individual', 'Inequality', 'Infant', 'Infrastructure', 'Lead', 'Life', 'Link', 'Longevity', 'Maiden Name', 'Malignant Neoplasms', 'Marriage', 'Medicare/Medicaid', 'Metadata', 'Methodology', 'Michigan', 'Minnesota', 'Minority Groups', 'Names', 'Pilot Projects', 'Population', 'Price', 'Process', 'Public Health', 'Recording of previous events', 'Records', 'Research', 'Research Infrastructure', 'Role', 'Sample Size', 'Sampling', 'Sanitation', 'Science', 'Subgroup', 'Surveys', 'Technology', 'Toxin', 'United States', 'Universities', 'Vaccines', 'Water', 'Woman', 'Women&apos', 's Group', 'aging population', 'base', 'cohort', 'convolutional neural network', 'cost', 'cost effective', 'data resource', 'early-life nutrition', 'ethnic minority population', 'frontier', 'health knowledge', 'improved', 'indexing', 'innovation', 'intergenerational', 'longitudinal dataset', 'machine learning algorithm', 'mortality', 'panel study of income dynamics', 'population health', 'population survey', 'programs', 'racial and ethnic', 'response', 'socioeconomics', 'suicide rate']",NIA,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,308553,-0.013330769282477914
"Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND) Project Summary/Abstract  This Phase III (P-III) COBRE project will extend the cores that have been successfully leveraged in our Phase I (P-I) and Phase II (P-II) COBRE projects and sustain these unique resources in New Mexico through the im- plementation of a business plan. Over the past eight years we have built up infrastructure and created a cutting edge brain imaging center, our P-II project is just over half-way through and is even more successful than our P- I was at this point in time. The Mind Research Network (MRN) houses an Elekta Neuromag 306-channel MEG System, a high density EEG lab, a 3T Siemens Trio MRI scanner, and a mobile 1.5T Siemens Avanto MRI scanner. Additional resources include a centralized neuroinformatics system, a strong IT management plan, and state-of-the-art image analysis expertise and tools. This P-III COBRE center will continue our momentum and move the cores we have developed into a position of long term sustainability. We will continue with the technical cores established during the P-II project including multimodal data acquisition (MDA), algorithm and data analy- sis (ADA), and biostatistics and neuro-informatics (BNI). These cores have begun to serve MRN and the greater community, as well as other institutions including extensive collaborations with IDeA funded projects in New Mexico and other states. We believe this P-III COBRE is extremely well-positioned to establish and sustain New Mexico as one of the premier brain imaging sites. We include an extensive pilot project program (PPP) that is built on the successful pilot programs implemented as part of the earlier COBRE phases. This includes an ex- tensive educational, mentoring, and faculty development program to carefully mentor and position faculty who use the cores to maximize their potential to successfully compete for external funding, thus fulfilling the ultimate goals of the COBRE program. 2 Narrative  This Phase III COBRE project is a natural extension of our Phase I and II COBRE projects which were cen- tered on mentoring individual researchers along with building the necessary infrastructure to support multimodal neuroimaging in mental illness. During this time, cutting-edge cores were developed that facilitated not only our local projects but also research at multiple institutions across New Mexico; the cores served as neuroimaging facilities and training centers for others to utilize. The Phase III project will ensure the sustainability of these cores as they transition to being fully funded by a broad cadre of users with various funding sources. We propose three technical cores including a multimodal data acquisition (MDA) core, an algorithm and data analysis (ADA) core, and a biostatistics and neuro-informatics (BNI) core. These cores have already shown their utility and have begun to be leveraged by users outside the COBRE. In addition, we propose a robust pilot project program (PPP) to continue to seed and enable new users of the cores to ultimately grow and sustain world class brain imaging research within our IDeA state, thus fulfilling the ultimate goals of the COBRE program. 1",Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND),9700159,P30GM122734,"['Algorithmic Analysis', 'Appointment', 'Area', 'Awareness', 'Biology', 'Biometry', 'Bipolar Depression', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Businesses', 'Centers of Research Excellence', 'Chemistry', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Core Facility', 'Data', 'Data Analyses', 'Department of Energy', 'Development', 'Devices', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Disease', 'Electroencephalography', 'Engineering', 'Ensure', 'Environment', 'Equipment', 'Faculty', 'Functional Magnetic Resonance Imaging', 'Funding', 'Funding Agency', 'Genetic', 'Goals', 'Grant', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Leadership', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Magnetoencephalography', 'Major Depressive Disorder', 'Mental Depression', 'Mental disorders', 'Mentors', 'Methods', 'Mind-Body Method', 'Mission', 'Multimodal Imaging', 'Neurobiology', 'Neurologic', 'Neurosciences', 'New Mexico', 'Paper', 'Patients', 'Peer Review', 'Phase', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Program Development', 'Psychiatry', 'Publications', 'Recording of previous events', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Schizophrenia', 'Seeds', 'Site', 'Structure', 'System', 'Teacher Professional Development', 'Time', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Vision', 'base', 'cohesion', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'deep learning', 'density', 'design', 'distinguished professor', 'improved', 'independent component analysis', 'meetings', 'multimodal data', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'neuropsychiatric disorder', 'programs', 'tool']",NIGMS,THE MIND RESEARCH NETWORK,P30,2019,1290517,-0.028673294090576732
"Research and development of an open, extensible, web-based information extraction workbench for systematic review Project Summary  1 More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence-based  2 medicine, with each review requiring, on average, between six months to one year of effort to complete. One of the most  3 time consuming and repetitive aspects of this endeavor involves extraction of detailed information from a large number  4 of scientific documents. The specific data items extracted differ among disciplines, but within a given scientific domain,  5 certain data points are extracted repeatedly for each review conducted. Research on use of natural language processing  6 (NLP) for extracting individual data elements has shown that it has the potential to greatly reduce the laborious, time  7 intensive, and repetitive nature of this step. However, there is currently no integrated, automatic data extraction platform  8 that meets the needs of the systematic review community. We propose a web-based data extraction software platform  9 specifically designed for usage in the domain of systematic review. By combining multiple state-of-the-art data extraction 10 methods utilizing NLP, text mining and machine learning, into a single, unified user interface, we will thereby empower 11 the end-user with a powerful and novel tool for automating an otherwise arduous task. 12 The research we propose encompasses three specific aims: (1) develop new data extraction models using deep learning 13 and a new technique called “data programming”; (2) develop a web-based platform to semi-automate the process; (3) 14 design protocols and standards for packaging extraction models as software components and integrating work done by 15 other research groups and vendors. In the first aim, we will contribute novel data extraction modules designed and 16 trained specifically to extract data elements of interest to those conducting systematic reviews in the domain of 17 environmental health. For this research, we will employ state-of-the-art machine learning, NLP and text mining 18 methodologies to train and evaluate several novel extraction components. In our second aim, we will develop a web- 19 based workbench which will allow users to upload scientific documents for automated data extraction. Our system will 20 also be designed to allow for integration of data extraction approaches (components) from other research groups, thus 21 enabling end users to choose from a wide variety of advanced data extraction methodologies within one unified and 22 intuitive software environment. In our third aim, we will develop new protocols to standardize the inputs and outputs 23 for data extraction components. The resulting interface, which will enable seamless integration of third party extraction 24 components into the workbench, will also facilitate the incorporation of feedback from users such that extraction 25 components can be continuously improved based on real-time data. 26 Our overarching goal is to translate emerging semi-automated extraction technologies out of the lab and into practical 27 software and to bring to market both the software itself as well as several premium data extraction components. The 28 results of the research conducted for Aims 1-3 represent the first step in this direction and will provide the foundation for 29 future developments. These result will take us one step closer to the dream of creating “living systematic reviews,” which 30 are maintained using automated or semi-automated methods and updated regularly as new evidence becomes available. Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a flexible, extensible software system that automates the crucial and resource-intensive process of extracting key data elements from scientific documents, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.","Research and development of an open, extensible, web-based information extraction workbench for systematic review",9623091,R43ES029901,"['Communities', 'Computer software', 'Consensus', 'Data', 'Data Element', 'Development', 'Discipline', 'Dreams', 'Environment', 'Environmental Health', 'Evidence Based Medicine', 'Feedback', 'Foundations', 'Future', 'Goals', 'Health', 'Individual', 'Internet', 'Intuition', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Online Systems', 'Output', 'Process', 'Protocols documentation', 'Research', 'Resources', 'Source', 'Standardization', 'Supervision', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Translating', 'Update', 'Vendor', 'Work', 'artificial neural network', 'base', 'data integration', 'deep learning', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'interest', 'learning strategy', 'model design', 'novel', 'research and development', 'software systems', 'systematic review', 'text searching', 'tool']",NIEHS,"SCIOME, LLC",R43,2018,225000,-0.02762893116043798
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9527181,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,545116,-0.032639617800900446
"Natural language processing for precision medicine and clinical and consumer health question The Repository for Informed Decision Making, Clinical Question Answering and Consumer Health Question Answering projects are addressing the above objectives by developing knowledge-based and machine learning approaches to extraction and structuring of information in biomedical literature and other types of text (such as clinical notes and registered clinical trials) for the following types of information: 1) the diseases and conditions; 2) the numbers, co-morbidities, and socio-demographic characteristics of study subjects/participants, such as species, gender, smoking status and alcohol consumption; 3) the therapeutic and diagnostic interventions; 4) the study and publication types; 5) the end-points and the outcomes of the studies; 6) drug interactions and 7) adverse drug reactions.  In FY2018, we have developed a number of approaches to facilitate understanding information requests sent to NLM customer services and long queries submitted to MedlinePlus search engine. Information requests sent to customer services are often several paragraphs long and provide the background and context that the customers believe will help understand their needs. For example, customers often describe several generations of their families affected by a disease and ask if their children will have it. The long MedlinePlus queries consist of one or two sentences and are often formed as questions. Both of these request forms are usually ungrammatical and rife with misspellings, abbreviations and informal language. We have developed a spellchecker for consumer language that is performing adequately on the misspellings important to understanding of the needs. After correcting spelling, our system employs three modules: a knowledge-based and a supervised machine learning method to understand the main points of the request, such as the disease or a drug of interest and the type of information about it. The systems extract the main points, which we found are sufficient to automatically search MedlinePlus and find authoritative and relevant pages for 65% of the requests. The third approach is to find similar questions that already have authoritative answers, e.g., provided by NIH institutes. In FY2018, the prototype consumer health question answering system became publicly available at https://chiqa.nlm.nih.gov/  Our clinical question answering system is based on the framework for asking well-formed questions developed by the evidence-based medicine experts. Their analysis showed that presenting a clinical information need as four-part question frame: patient characteristics/problem; planned intervention; comparison; and desired outcome, helps formulate search engine queries that lead to relevant results. We developed methods for automatic extraction of question frames from information requests, automatic query formulation and automatic extraction of answers from retrieval results. The LHC CQA1.0 system extracts the bottom-line advice from biomedical publications and aligns the question frames and the answers to find the best answer. The CQA 1.0 system is currently used to support development of evidence-based care plans at the NIH Clinical Center, to provide bottom-line for retrieved images in the LHC Open-i system and to provide summaries of the biomedical articles in the LHC Open Summarizer. n/a",Natural language processing for precision medicine and clinical and consumer health question,9787046,ZIALM010009,"['Abbreviations', 'Address', 'Affect', 'Alcohol consumption', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Clinical Trials', 'Comorbidity', 'Decision Making', 'Development', 'Diagnostic', 'Disease', 'Drug Interactions', 'Evidence Based Medicine', 'Family', 'Formulation', 'Gender', 'General Population', 'Generations', 'Growth', 'Health', 'Image', 'Institutes', 'Intervention', 'Language', 'Lead', 'Literature', 'Machine Learning', 'MedlinePlus', 'Methods', 'Natural Language Processing', 'Outcome', 'Outcome Study', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Publications', 'Reaction', 'Resources', 'Retrieval', 'Review Literature', 'Role', 'Services', 'Smoking Status', 'Source', 'Structure', 'Study Subject', 'Supervision', 'System', 'Text', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Update', 'base', 'clinical decision support', 'evidence base', 'interest', 'knowledge base', 'learning strategy', 'precision medicine', 'prototype', 'repository', 'search engine', 'spelling', 'study characteristics', 'systematic review']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2018,531236,-0.024198517336910124
"Clinical and Informatics Research in Medical Terminologies Terminology projects  A.	Extraction of information from drug labels using natural language processing Drug package inserts (drug labels) are a comprehensive, up-to-date and authoritative source of drug information that is publicly available. To unleash the knowledge in the drug labels, they need to be transformed into standardized data structure and encoded in standard terminologies. Only then can the knowledge be used to drive applications such as clinical decision support. This collaborative project with the FDA uses natural language processing (NLP) and machine learning to extract information from the drug labels and create mappings to standard terminologies. The output will support the FDAs drug label indexing initiative to increase the usefulness of drug labels. In collaboration with FDA, I am hosting a challenge at the Text Analysis Conference (TAC) 2018 to find the best performing NLP algorithm for extracting drug-drug interactions from drug labels.  B.	Evaluation of drug-drug interaction knowledge resources and comparison with open standards A standard, evidence-based knowledge base is a prerequisite to the effective deployment of a clinical alert system to prevent harmful drug-drug interactions. However, studies have shown significant variability between knowledge sources. Inconsistent evaluation and classification of interactions have been cited as factors contributing to excessive alerts and alert fatigue. The goal of this study is to compare systematically commercial drug-drug interaction knowledge sources against published standards. Our results show that while there are considerable differences between three commercial knowledge bases, the differences are less pronounced for the more severe interactions. There was very high coverage by all three knowledge sources of a standard list of clinically highly significant interactions.  C.	Creating maps between commonly used terminologies Mapping provides a solution to the problem caused by the use of multiple coding systems for the same kind of information. One example is the use of SNOMED CT and ICD-10-CM for coding medical diagnosis and problems. Using various computational methods supplemented by expert review, I have developed maps between SNOMED CT and the different flavors and versions of ICD codes. This will help to facilitate data re-use and data integration. I am also studying various algorithmic approaches to create mappings between SNOMED CT and ICD-10-PCS, including lexical matching, ontological alignment and indirect mapping.  D.	Facilitating adoption of terminology standards According to the Meaningful Use incentive program, SNOMED CT and RxNorm are terminologies required for the certification of electronic health records. I have studied the practical barriers of adoption of these terminologies and created useful resources to help with implementation. I studied the usage pattern of SNOMED CT terms in the problem lists of large health care providers and published a list of the most commonly used terms as the CORE Problem List Subset of SNOMED CT. The CORE subset is not only a useful resource for SNOMED CT implementers, it is also frequently used for terminology research and other purposes, and cited in multiple publications. RxTerms is another resource that I developed to overcome data entry problems with RxNorm. n/a",Clinical and Informatics Research in Medical Terminologies,9787048,ZIALM010013,"['Adoption', 'Algorithms', 'Big Data', 'Certification', 'Classification', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Code', 'Collaborations', 'Communication', 'Computing Methodologies', 'Data', 'Data Reporting', 'Data Science', 'Data Sources', 'Diagnosis', 'Drug Evaluation', 'Drug Interactions', 'Drug Labeling', 'Electronic Health Record', 'Evaluation', 'Fatigue', 'Goals', 'Health Personnel', 'ICD-10-CM', 'Information Resources', 'International Classification of Disease Codes', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Knowledge', 'Label', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Natural Language Processing', 'Ontology', 'Output', 'Pattern', 'Pharmaceutical Preparations', 'Publications', 'Publishing', 'Research', 'Resources', 'Retrieval', 'SNOMED Clinical Terms', 'Source', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'clinical decision support', 'data integration', 'data structure', 'evidence base', 'incentive program', 'indexing', 'information organization', 'insight', 'knowledge base', 'lexical', 'prevent', 'symposium', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2018,287753,-0.04729781538232884
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9523638,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2018,478806,0.007136903464686255
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9467327,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Algorithms', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Image Analysis', 'Kinetics', 'Laboratories', 'Locomotion', 'Machine Learning', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2018,510448,-0.03077357854704085
"Leveraging Twitter to monitor nicotine and tobacco-related cancer communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to monitor nicotine and tobacco-related cancer communication,9503469,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Stream', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Tobacco-Related Carcinoma', 'Work', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial and ethnic', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2018,505649,-0.01521201482146519
"A Modular Automated Platform for Large-scale Drosophila Experiments and Handling PROJECT SUMMARY / ABSTRACT Animal model systems are a powerful tool researchers use to investigate almost all aspects of biology: genetics, development, neuroscience, disease, and more. And fruit flies – Drosophila melanogaster – with their small size, easy care, and remarkable array of available genetic toolkits, occupy a sweet spot on the model organism spectrum. Over 75% of human diseases with a genetic basis have an analogue in the fly, and Drosophila have been a part of the research for six Nobel prizes. Furthermore, the advent of CRISPR/cas9 and other modern genetic tools has opened the door to modeling other diseases and pathways, leading to greater use of Drosophila for drug screens. A great deal of the work (and the majority of the budget) involved in fly experiments is tedious manual labor, and with advances in computer vision, machine learning, and other analytic techniques, the stage is set to automate many phenotypic screens. In this Phase I SBIR, we propose a robotic system – modular automated platform for large-scale experiments (MAPLE) – that can accomplish a wide variety of fly-handling tasks in Drosophila labs. This robot is the fruit fly version of a liquid handling robot, with a large, open workspace that can house a plethora of modules and several manipulators that can move small parts and animals around that workspace. Building on a collaboration between the de Bivort Lab and FlySorter completed in 2017, we will design, fabricate and validate a commercial system that can collect virgin flies, run behavioral assays, conduct drug screens, and adapt to the needs of fly labs through easy-to-code Python scripts. By strategically combining modules and instructions to the robot, MAPLE can perform a wide variety of tasks in a fly lab, saving experimentalists from repetitive chores, cutting labor costs, and increasing scientific output. Just as pipette robots have become standard equipment in wet labs, we envision our fly handling robot will be the engine that powers Drosophila labs in academia and pharma, enabling new kinds of experiments and freeing researchers from the drudgery of fly pushing. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are a powerful model organism used in the study of disease, neuroscience, development, genetics, and recently in drug screens, too, largely through phenotypic screening. This labor-intensive work is time consuming and expensive, and ripe for automation. We propose a fly-handling robot – analogous to a liquid pipetting robot in a wet lab – that can perform a variety of tasks in Drosophila labs, free researchers from the drudgery of fly pushing, and enable a broader spectrum of experiments that will increase scientific knowledge.",A Modular Automated Platform for Large-scale Drosophila Experiments and Handling,9623017,R43MH119092,"['Academia', 'Address', 'Affect', 'Air', 'Anesthesia procedures', 'Animal Model', 'Animals', 'Architecture', 'Automation', 'Basic Science', 'Behavior', 'Behavioral Assay', 'Biological Models', 'Biology', 'Budgets', 'CRISPR/Cas technology', 'Carbon Dioxide', 'Caring', 'Code', 'Collaborations', 'Computer Vision Systems', 'Computer software', 'Computers', 'Custom', 'Data Collection', 'Deposition', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Screening', 'Drug usage', 'Ensure', 'Equipment', 'Feedback', 'Genetic', 'Genetic Screening', 'Genetic study', 'Grant', 'Hand', 'Human', 'Instruction', 'Knowledge', 'Libraries', 'Liquid substance', 'Machine Learning', 'Manuals', 'Modeling', 'Modernization', 'Neurosciences', 'Nobel Prize', 'Organism', 'Output', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Protocols documentation', 'Pythons', 'Reagent', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Running', 'Savings', 'Scanning', 'Small Business Innovation Research Grant', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Transgenic Organisms', 'Travel', 'Universities', 'Update', 'Vacuum', 'Work', 'analog', 'bone', 'cost', 'design', 'drug discovery', 'experimental study', 'flexibility', 'fly', 'graduate student', 'health science research', 'human disease', 'improved', 'operation', 'programs', 'repository', 'robot control', 'screening', 'tool', 'touchscreen']",NIMH,"FLYSORTER, LLC",R43,2018,348007,-0.022545442297247784
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,-0.02244327918909253
"Informatics, Machine Learning & Biomedical Data Science Over the past year, we have been active in: (1) developing computationally efficient methods and algorithms to solve known problems in the analysis of biomedical and clinical data and study complex interactions in biological systems; (2) developing knowledge-based data management systems for the discovery and curation of biomedical knowledge, including distributed annotation systems and laboratory information management systems;  (3) applying predictive-analytic models to scientific and administrative domains; and (4) consulting with NIH leadership to provide evidence-based solutions to improve the grant application and review process.  Specifically, in 2018, collaborative efforts in support of these goals included the following:  - In a partnership with Dr. John Tsang of the NIAID Laboratory of Systems Biology, HPCIO is conducting a multifaceted project to profile the immune system using the latest high-throughput, multiplexed technologies and systems approaches. One of the goals of this collaboration is to develop novel computational methodologies that can exploit inter-subject heterogeneity and measurements at various scales to assess the roles of the immune system in health and disease.  Currently, we are developing a multi-level linear model utilizing ATAC-seq and RNA-seq data from monogenic disease patients to dissect the gene regulatory network.  The results will provide insights on the impacts specific genetic lesions have on downstream genes and how they are manifested at the clinical phenotypical level.   - HPCIO has entered into a wide-ranging collaboration to provide data science support and innovation to Dr. Leorey Saligan and the Symptom Management Branch of NINR.   An existing effort to identify radiotherapy-related fatigue genes from a PCR assay of oxidative stress using machine-learning methods has largely been completed.   In addition, a number of new efforts were initiated in 2018.   To provide guidance for future scRNA experiments to identify clusters of cells and their marker genes associated with wound healing in mice, an analysis of a similar experiment deposited to the Sequence Read Archive was conducted.  We also initiated a machine-learning analysis of mouse proteomic data to identify targets or pathways that may be involved in both the generation of radiation-related fatigue and rescue through taltirelin.    - As a pilot for a potential future service offering across NIH, HPCIO is working with Dr. John Tsang of NIAID to develop a laboratory information management system (LIMS) that can more effectively capture data associated with a large number of assays performed at the Center for Human Immunology (CHI). The CHI-LIMS will facilitate the standardization and management of procedures and workflows for various types of high-throughput assays, to enhance tracking of relevant information in each step and to improve reproducibility of results and identification of operational issues.  Modules will be developed to ensure interoperability with other NIH research information systems.  We envision that this system can serve as a model and be readily customized for other labs with similar needs.  - In collaboration with NINDS Spinal Circuits and Plasticity Unit, HPCIO is helping develop novel methods of analyzing single-cell RNAseq data collected from mice undergoing treatment for spinal injuries. We are working with NINDS researchers to help understand the pathways involved during spinal cord injury and healing.   - HPCIO is working with NCI Occupational & Environmental Epidemiology Branch to develop methodologies to incorporate occupational risk factors into epidemiological models. We are enlarging the training data to improve our novel classifiers for coding free text job descriptions into the 840 codes of the 2010 U.S. Standard Occupational Classification System.  Our classifier is being embedded within the data collection software of the NCI Agriculture Health Study, a large scale epidemiological study, to automatically code job descriptions as they are entered by study participants.  - In collaboration with the Membrane Transport Biophysics Section of NINDS, HPCIO is 1) developing  a computational tool to accurately identify the boundaries of the lysosomes in fluorescence microscopy and 2) using the fluorescence ration to measure lysosomal pH within each organelle for better understanding of the lysosomal pH regulation.    - A freely available plasmid database that is inter-operable with popular freeware is currently being developed for the NIDA Optogenetics and Transgenic Technology Core. The Plasmid Manager offers a versatile yet simple platform for scientists to store and analyze their plasmid data. Motivated by the need for a more comprehensive approach to archiving plasmid data, the database platform is enriched with numerous components beyond the repository, serving as an informatics platform designed to enhance the efficiency and analytic capabilities of scientists.   - The Human Salivary Proteome Wiki is a community-driven Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. Many features and external contents have been incorporated over the last few years to make it easier for users to extract different kinds of information from the wiki.  We are actively adding new data to the system to allow users to discern the origin of proteins found in saliva, whether they may come from the different salivary glands or from blood plasma, for instance.  Current efforts also include working with major stakeholders to engage with the research community and to gather feedback from them.  - In collaboration with CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process. A Web-based automated referral tool, called ART, was developed and deployed to help PIs and SROs to identify the most relevant study section(s) or special emphasis panel(s) based on the scientific content of an application. In addition, HPCIO is analyzing text from quick feedback surveys on peer review. HPCIO has developed a system to capture the sentiment of reviewer comments in quick feedback surveys and classify these comments with sentiment score into broad categories.   In 2018, HPCIO continued to maintain ART and retrain the machine-learning models to reflect new and changing study sections.  In support of the ARGO initiative, we represented study sections through Word2Vec and generated distance metrics as well as diagrams, allowing leadership to analyze the relationship between study sections.  We developed an interactive tool that allows users to curate grant applications for measures of scientific rigor n/a","Informatics, Machine Learning & Biomedical Data Science",9787090,ZIHCT000200,"['ATAC-seq', 'Agricultural Health Study', 'Algorithms', 'Applications Grants', 'Archives', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Biophysics', 'Categories', 'Cells', 'Classification', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computational Linguistics', 'Computer software', 'Computing Methodologies', 'Consult', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Database Management Systems', 'Databases', 'Deposition', 'Disease', 'Ensure', 'Environmental Epidemiology', 'Evaluation', 'Fatigue', 'Feedback', 'Fluorescence', 'Fluorescence Microscopy', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Grant Review Process', 'Harvest', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Immune system', 'Immunology', 'Informatics', 'Information Systems', 'Intramural Research Program', 'Job Description', 'Knowledge', 'Laboratories', 'Leadership', 'Lesion', 'Linear Models', 'Lysosomes', 'Machine Learning', 'Management Information Systems', 'Measurement', 'Measures', 'Mendelian disorder', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Mus', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Nature', 'Occupational', 'Occupational Epidemiology', 'Online Systems', 'Organelles', 'Oxidative Stress', 'Participant', 'Pathway interactions', 'Patients', 'Peer Review', 'Plasma', 'Plasmids', 'Predictive Analytics', 'Procedures', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Published Comment', 'Radiation', 'Radiation therapy', 'Regulation', 'Regulator Genes', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Research Support', 'Risk Factors', 'Role', 'Saliva', 'Salivary', 'Salivary Glands', 'Scientist', 'Services', 'Spinal', 'Spinal Injuries', 'Spinal cord injury', 'Standardization', 'Strategic Planning', 'Study Section', 'Surveys', 'System', 'Systems Biology', 'Technology', 'Text', 'Training', 'Transgenic Organisms', 'Translational Research', 'Transmembrane Transport', 'United States National Institutes of Health', 'Vision', 'Visualization software', 'Work', 'Wound Healing', 'annotation  system', 'base', 'biological systems', 'biomedical informatics', 'clinical phenotype', 'computerized tools', 'data mining', 'design', 'epidemiological model', 'epidemiology study', 'evidence base', 'experimental study', 'healing', 'high throughput screening', 'improved', 'information organization', 'innovation', 'insight', 'interactive tool', 'interdisciplinary approach', 'interoperability', 'knowledge base', 'learning strategy', 'novel', 'optogenetics', 'programs', 'repository', 'research and development', 'single cell analysis', 'software development', 'symptom management', 'text searching', 'tool', 'transcriptome sequencing', 'web portal', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2018,2624735,-0.019688119797095283
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,-0.019937461886953247
"Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM Project Summary Efforts to integrate contact network data with molecular surveillance data provide enormous promise for HIV tracking and intervention. However, the lack of tools to facilitate integrated molecular-social surveillance remains a substantial barrier to progress. For example, most contact network data only contains information on the immediate sexual and drug use partners of a single individual. Yet, the same partners can appear across the contact networks of multiple individuals. Therefore, partners must be matched across contact networks - a process called entity resolution (ER) - in order to provide an accurate view of the overall contact network structure. The process of ER currently requires either substantial resources to manually match individuals or considerable technological expertise in programming to more efficiently match individuals using probabilistic models. Accordingly, this project will 1) develop a machine learning algorithm to match individuals across personal contact networks and validate it using a large existing dataset of young men who have sex with men, and 2) create a graphical user interface to implement the algorithm as an add-on package to an existing tool for network data capture and processing (Network Canvas). The results of this project will provide an open- source and freely available tool that can drastically reduce barriers to matching individuals across contact networks, thereby providing researchers and public health officials with unencumbered access to the underlying structure of drug use and sexual networks, and a potent tool for integrating contact network data with molecular surveillance. Project Narrative Developing an accurate picture of the drug use and sexual contact networks of men who have sex with men (MSM) and other high risk populations can revolutionize the way in which HIV spread is tracked and intervened upon by public health practitioners. However, these data are usually limited to personal networks, which only include immediate sex and drug use partners. The current project will employ state-of-the-art methods in machine learning to improve the accuracy of matching individuals across personal network data and will integrate this tool into a user-friendly graphical user interface and data management tool, thereby substantially reducing barriers to studying contact networks and providing a potent tool for HIV surveillance and interventions.",Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM,9563322,R21LM012578,"['AIDS prevention', 'Age', 'Algorithms', 'Area', 'Behavioral', 'Big Data', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Drug usage', 'Epidemiology', 'Foundations', 'Funding', 'Future', 'Goals', 'Graph', 'HIV', 'Human', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Intervention Studies', 'Interview', 'Intuition', 'Investigation', 'Knowledge', 'Link', 'Location', 'Logic', 'Machine Learning', 'Manuals', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Epidemiology', 'Names', 'National Institute of Drug Abuse', 'Nature', 'Output', 'Pathway Analysis', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Prevention Research', 'Process', 'Public Health', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Structure', 'Subgroup', 'Surveys', 'System', 'Testing', 'Time', 'Training', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'computer human interaction', 'computer science', 'data management', 'disease transmission', 'drug structure', 'epidemiologic data', 'experience', 'graphical user interface', 'high risk population', 'improved', 'innovation', 'men who have sex with men', 'novel', 'open source', 'outreach', 'response', 'sex', 'social', 'success', 'surveillance data', 'tool', 'touchscreen', 'transmission process', 'usability', 'user-friendly', 'young men who have sex with men']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2018,98987,-0.04944957280192253
"Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM Project Summary Efforts to integrate contact network data with molecular surveillance data provide enormous promise for HIV tracking and intervention. However, the lack of tools to facilitate integrated molecular-social surveillance remains a substantial barrier to progress. For example, most contact network data only contains information on the immediate sexual and drug use partners of a single individual. Yet, the same partners can appear across the contact networks of multiple individuals. Therefore, partners must be matched across contact networks - a process called entity resolution (ER) - in order to provide an accurate view of the overall contact network structure. The process of ER currently requires either substantial resources to manually match individuals or considerable technological expertise in programming to more efficiently match individuals using probabilistic models. Accordingly, this project will 1) develop a machine learning algorithm to match individuals across personal contact networks and validate it using a large existing dataset of young men who have sex with men, and 2) create a graphical user interface to implement the algorithm as an add-on package to an existing tool for network data capture and processing (Network Canvas). The results of this project will provide an open- source and freely available tool that can drastically reduce barriers to matching individuals across contact networks, thereby providing researchers and public health officials with unencumbered access to the underlying structure of drug use and sexual networks, and a potent tool for integrating contact network data with molecular surveillance. Project Narrative Developing an accurate picture of the drug use and sexual contact networks of men who have sex with men (MSM) and other high risk populations can revolutionize the way in which HIV spread is tracked and intervened upon by public health practitioners. However, these data are usually limited to personal networks, which only include immediate sex and drug use partners. The current project will employ state-of-the-art methods in machine learning to improve the accuracy of matching individuals across personal network data and will integrate this tool into a user-friendly graphical user interface and data management tool, thereby substantially reducing barriers to studying contact networks and providing a potent tool for HIV surveillance and interventions.",Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM,9563322,R21LM012578,"['AIDS prevention', 'Age', 'Algorithms', 'Area', 'Behavioral', 'Big Data', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Drug usage', 'Epidemiology', 'Foundations', 'Funding', 'Future', 'Goals', 'Graph', 'HIV', 'Human', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Intervention Studies', 'Interview', 'Intuition', 'Investigation', 'Knowledge', 'Link', 'Location', 'Logic', 'Machine Learning', 'Manuals', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Epidemiology', 'Names', 'National Institute of Drug Abuse', 'Nature', 'Output', 'Pathway Analysis', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Prevention Research', 'Process', 'Public Health', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Structure', 'Subgroup', 'Surveys', 'System', 'Testing', 'Time', 'Training', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'computer human interaction', 'computer science', 'data management', 'disease transmission', 'drug structure', 'epidemiologic data', 'experience', 'graphical user interface', 'high risk population', 'improved', 'innovation', 'men who have sex with men', 'novel', 'open source', 'outreach', 'response', 'sex', 'social', 'success', 'surveillance data', 'tool', 'touchscreen', 'transmission process', 'usability', 'user-friendly', 'young men who have sex with men']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2018,74791,-0.04944957280192253
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9525704,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,599947,0.006469984356060583
"Data Science and Sharing Team This is the second annual report for the Data Science and Sharing Team (DSST), our first year with our full staff in place. 2018 also marked the arrival of Francisco Pereira, the director of our sister group, the Machine Learning Team. Follows is a list of our noteworthy projects.  Intramural Data Sharing  Over the course of 2018, our team has worked with the authors of several different projects to make their data publicly available. They include: Gonzalez-Castillo et al., 2012, https://doi.org/10.15154/1464517 Gonzalez-Castillo et al., 2015, https://doi.org/10.15154/1464520 Shaw et al., 2018, https://doi.org/10.15154/1463004 Thurm et al., 2018, https://doi.org/10.15154/1464602 Power et al., 2018, https://openneuro.org/datasets/ds000258/  These datasets will be or are available through both the NIMH Data Archive (NDA) as well as our internal repository at http://nido.nimh.nih.gov which is derived from Stanford Center for Reproducible Neuroscience's (CRN) OpenNeuro project. In 2018 we were granted supplementary funding to expand NIDO's capabilities to execute analyses directly on the NIH HPC. We are currently working closely with the CRN to implement this functionality for both NIDO and OpenNeuro.  Acquisition and Maintenance of Shared Dataset on the NIH HPC  Our team also makes it easy for intramural researchers to access and use data from other institutions by streamlining the authorization process. We download and organize each dataset on shared disk accessible from the NIH High Performance Computing (HPC) cluster for easy and efficient analysis. Datasets that we have download and are currently maintaining include the Human Connectome Project, the OpenNeuro Archive, and the UK Biobank Imaging Extension.  Exploring Cross Scanner Harmonization  We were approached by several member of the NIMH extramural staff to consult with them on how effective current inter-scanner homogenization techniques are. In order to test these techniques, we used the initial dataset recently released by the Adolescent Brain Cognitive Development (ABCD) project (https://abcdstudy.org/). This work is now a manuscript which is available as a preprint (https://doi.org/10.1101/309260) and is currently under peer review.  Aggregating Worldwide MRI Scanner Quality Metrics  MRIQC is a package for assessing the quality of structural and functional MRI data. We are supporting a centralized database for aggregating anonymized quality control statistics on over 100,000 scans collected around the world. This database allows investigators to compare the data they have collected with a vast trove of similar data, to assess the quality of their scans and help guide changes in collection protocols and decide which scans need to be manually reviewed.  Research Volunteers Protocol  Our team continues to collaborate with Joyce Chung and NIMH Clinical Director's Office on the Research Volunteers Protocol that was approved in October of 2017. Over 45 participants have been scanned to date using a standardized MRI protocol. The data from these participants will soon be publicly available on the NIMH Data Archive. These participants are also being actively referred to other NIMH protocols for which they qualify. For more details, see the annual report for the Clinical Director's Office.  Machine Learning from Distributed Datasets  Helping to make data widely available is part of our core mission, but sometimes privacy issues make it impossible for data to be shared. We are collaborating with the Machine Learning Team on a project to build a ""parallel weight consolidation"" tool. This technique will allow researchers to train a neural network and share anonymized data from that training with a central hub that aggregates across many sites to improve its performance and prediction. See the Machine Learning Team's annual report and this preprint (https://arxiv.org/abs/1805.10863, currently under review) for more details on this project.  Exemplifying Open Science  Our team has served as an example of open science practices. All of our papers have been released to the public as preprints on bioRxiv prior to submission, along with code and data. We have pre-registered two large data analysis efforts with the Open Science Foundation (http://doi.org/10.17605/OSF.IO/3YTQJ, http://doi.org/10.17605/OSF.IO/GCD7Z). The vast majority of the code for our own projects is available in open source repositories on github.com, where we have committed over 400 code changes. We have additionally contributed over 75 code changes to 18 open source projects across the community via github pull requests.  Crowdsourcing Imaging Science  Accurate volumetric measurements of the brain, derived from MR image segmentation, are fundamental for understanding structure-function relationships in the healthy brain, and in psychiatric and neurological disorders; however, obtaining these measurements in large datasets remains a challenge, especially in many patient populations. We sought to integrate the expertise of neuroanatomists, citizen science through crowd-sourcing, and recent advances in deep-learning algorithms for image-processing. We worked with Drs. Anisha Keshavan, Ariel Rokem, and Jason Yeatman from the University of Washington to create Medulina, an open source, web-browser based platform for biomedical image segmentation.  Medulina is available online at medulina.com, and we have IRB approval from the Univ. of Washington to begin collecting citizen contributed medical image annotations. One of the available projects is to segment high resolution hippocampal images that will aid in the analysis of data recently collected and published in collaboration with the Child Psychiatry Branch (Zhou et al., 2018).  Measuring and Incentivizing Open Science  We are working with the non-profit group ImpactStory to create an online tool for tracking how many publications from the NIMH IRP are publicly available, have open code, and have open data. ImpactStory is uniquely suited to work on this project because they have created and maintain a database of which publications are and are not publicly accessible and they have created websites with similar functionality (impactstory.org). This tool will provide the foundation for other institutions to measure and incentivize open science.   Accessible, reproducible pipelines in AFNI for standardized data  The Data Science and Sharing Team works closely with the Scientific and Statistical Computing Core. The SSCC is responsible for AFNI -- the NIMH IRP's flagship neuroimaging analysis software which is used in research on many conditions, including autism, anxiety disorders, drug addiction, major depression, and epilepsy. We collaborated with the AFNI team on a BRAIN Initiative grant for funding to make it practicable to use AFNI on large data collections stored in the cloud, or on local supercomputing clusters, accelerating research into diverse brain-based conditions in sizable groups. 	 	 	 	 						 Our grant brings AFNI's widely acknowledged analytical and visualization capabilities to cloud-based environments in order to better support BRAIN Initiative projects. Upon completing the proposed work, a user will be able to complete an entire analysis workflow including preprocessing, analysis, quality control, and visualization of results in the cloud. n/a",Data Science and Sharing Team,9790857,ZICMH002960,"['Adolescent', 'Algorithms', 'Annual Reports', 'Anxiety Disorders', 'Archives', 'Authorization documentation', 'Autistic Disorder', 'BRAIN initiative', 'Base of the Brain', 'Biological Neural Networks', 'Brain', 'Child Psychiatry', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Consult', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Drug Addiction', 'Environment', 'Epilepsy', 'Extramural Activities', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Grant', 'High Performance Computing', 'Hippocampus (Brain)', 'Human', 'Image', 'Imagery', 'Incentives', 'Institution', 'Institutional Review Boards', 'Internet', 'Intramural Research Program', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Major Depressive Disorder', 'Manuals', 'Manuscripts', 'Measurement', 'Measures', 'Medical Imaging', 'Mental disorders', 'Methods', 'Mission', 'National Institute of Mental Health', 'Neurosciences', 'Paper', 'Participant', 'Peer Review', 'Performance', 'Privacy', 'Process', 'Protocols documentation', 'Publications', 'Publishing', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Scanning', 'Science', 'Scientist', 'Sister', 'Site', 'Standardization', 'Statistical Computing', 'Structure-Activity Relationship', 'Supercomputing', 'Techniques', 'Testing', 'Training', 'United States National Institutes of Health', 'Universities', 'Washington', 'Weight', 'Work', 'base', 'biobank', 'bioimaging', 'citizen science', 'cloud based', 'cluster computing', 'cognitive development', 'connectome', 'crowdsourcing', 'data archive', 'data sharing', 'data warehouse', 'deep learning', 'image processing', 'imaging Segmentation', 'improved', 'member', 'nervous system disorder', 'neuroimaging', 'open data', 'open source', 'patient population', 'repository', 'scientific computing', 'statistics', 'tool', 'volunteer', 'web site']",NIMH,NATIONAL INSTITUTE OF MENTAL HEALTH,ZIC,2018,671292,-0.01951876527026325
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9510096,R21GM128020,"['Address', 'Algorithms', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Learning', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2018,239527,-0.00861472693343973
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9749413,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,155743,-0.008547886947396952
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9560825,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,674602,-0.008547886947396952
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9545836,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'deep learning', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2018,569784,-0.029474712843861408
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9577591,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Supervision', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'forest', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'tool', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2018,243750,-0.01134404325773589
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or “position” on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9687220,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Research Infrastructure', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2018,330000,-0.03067863578012151
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9451318,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2018,1354554,-0.02546562253978306
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9490340,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2018,1536744,-0.034130186709346345
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9764035,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2018,74880,-0.034130186709346345
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9764029,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2018,114880,-0.034130186709346345
NIST Assistance with NTP SR Automation NIEHS seeks advice from NIST in the areas of human language technology and natural language processing component evaluations that support the measurement of systems that automatically extract toxicology information from publications to support the complex human task of systematic review of literature. NIST is positioned to assist NIEHS building upon existing test and evaluation infrastructure through its Text Analysis Conference (TAC) program. NIST is coordinating the 2019 Systematic Review Information Extraction evaluation (SRIE 2019) task for NIEHS as part of the Retrieval Group’s Text Analysis Conference (TAC) program. This coordination includes advising NIEHS on developing annotation guidelines; advising NIEHS on dataset construction and distribution; writing guidelines for the evaluation task; developing scoring methods and supporting software; including the evaluation task as part of the TAC program and call for participation; accepting participant submissions in the evaluation; evaluating those submissions; and reporting results of the evaluation. NIST and NIH will design an evaluation task in this domain. n/a,NIST Assistance with NTP SR Automation,9794240,ES18001002,"['Advertisements', 'Area', 'Automation', 'Complex', 'Computer software', 'Data Set', 'Development', 'Evaluation', 'Guidelines', 'Human', 'Language', 'Measurement', 'National Institute of Environmental Health Sciences', 'Natural Language Processing', 'Participant', 'Positioning Attribute', 'Preparation', 'Publications', 'Reporting', 'Research', 'Research Infrastructure', 'Retrieval', 'Review Literature', 'Scoring Method', 'System', 'Technology', 'Testing', 'Text', 'Toxicology', 'United States National Institutes of Health', 'Writing', 'biomedical informatics', 'design', 'programs', 'symposium', 'systematic review']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,200000,-0.03075139578424812
"Device for real-time streaming of preclinical research data into a central cloud-based platform Project Summary BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. Traditionally researchers collect data from many disparate instruments and store data on PCs running single license software. Raw data is stored across several locations, analysis is restricted to the PC used to collect the data, and opportunities for collaboration, remote-participation, and data sharing are limited. BehaviorCloud is leveraging cloud data streaming and storage to overcome these barriers to discovery. In 2017 BehaviorCloud released a first version of the underlying cloud platform as well as BehaviorCloud Camera, an open-source “reference implementation” that demonstrates automated video tracking of animal behavior on the BehaviorCloud platform using a consumer-grade smartphone. This tool and the underlying platform are both in active use across academic and pharmaceutical labs. The aim of this Phase I SBIR application is to develop patent-pending “Bridge” technology that allows data streaming from third-party instrumentation into the central web platform. Researchers will bypass the original software and PCs associated with their instruments to control trials through their BehaviorCloud account and receive data back in real-time. BehaviorCloud will provide a public repository to aggregate all of these data and accelerate discovery by providing computational tools for large-scale meta-analysis and machine learning based predictive analytics. Project Narrative BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. The goal of this Phase I SBIR application is to develop the technology to enable streaming of data from all kinds of behavioral and phenotyping instrumentation into the BehaviorCloud platform. BehaviorCloud will aggregate these data into a repository and accelerate discovery by providing tools for collaboration and meta-analysis.",Device for real-time streaming of preclinical research data into a central cloud-based platform,9621228,R43OD025448,"['Adoption', 'Animal Behavior', 'Animal Experimentation', 'Animal Model', 'Area', 'Back', 'Behavioral', 'Bypass', 'Carbon Dioxide', 'Cellular Phone', 'Collaborations', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Devices', 'Goals', 'Heart Rate', 'Information Systems', 'Internet', 'Intervention', 'Legal patent', 'Licensing', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Meta-Analysis', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Process', 'Research', 'Research Contracts', 'Research Personnel', 'Resources', 'Running', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Standardization', 'Stimulus', 'Stream', 'System', 'Technology', 'Temperature', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'cloud based', 'cloud platform', 'computerized tools', 'control trial', 'data management', 'data sharing', 'data warehouse', 'design', 'experimental study', 'instrument', 'instrumentation', 'laptop', 'open source', 'phenotypic data', 'pre-clinical', 'pre-clinical research', 'prototype', 'repository', 'tool', 'wasting', 'web interface']",OD,"BEHAVIORCLOUD, LLC",R43,2018,220420,-0.0237196670112147
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9770622,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,380000,-0.008646994991683917
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,9589711,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'Standardization', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2018,439996,-0.009631316623124933
"NLM Scrubber: NLM's Software Application to De-identify Clinical Text Documents Narrative clinical reports contain a rich set of clinical knowledge that could be invaluable for clinical research. However, they may also contain personally identifiable information (PII) that make those clinical reports classified as PHI, which is associated with use restrictions and risks to privacy. Computational de-identification seeks to remove all instances of PII in such narrative text in order to produce de-identified documents, which would no longer be classified as PHI and can be used in clinical research with fewer constraints and with almost no risk to privacy. Computational de-identification uses artificial intelligence methods including pattern recognition and computational linguistic techniques to recognize words and other alphanumeric tokens denoting PII (e.g., names, addresses, and telephone and social security numbers) in the text, and replace them with labels such as NAME and ADDRESS. In this way, both patient privacy is protected and clinical knowledge is preserved.  After exploring existing de-identification tools, the U.S. National Library of Medicine (NLM) began developing a new software application called NLM Scrubber, which is capable of de-identifying many types of clinical reports with high accuracy. The software design is based on both deterministic and probabilistic artificial intelligence methods utilizing large dictionaries of personal names, addresses, and organizations. The application accepts narrative reports in plain text or in HL7 format. When the input reports are formatted as HL7 messages, the application software leverages patient information embedded in HL7 segments to find such information in the text portion of the HL7 message.  NLM Scrubber has been downloaded by a number of organizations for testing and use, including IBM, Google, Fred Hutch Cancer Research Center, Harvard Medical School, Florida International University, University of Bristol, University College Dublin, and Oak Ridge National Laboratory. National Cancer Institute (NCI) along with the state cancer registries working with NCI have also been voiced their interests in using NLM Scrubber to de-identify narrative pathology reports in Surveillance, Epidemiology and End Results (SEER) database. Our current focus is making NLM-Scrubber answer various needs of clinical scientists and clinical data managers without a deep understanding of the underlying technology. While NLM-Scrubber can be used for de-identifying all clinical reports repository-wide, it can also be used in various modes tailored to the user and their context, including on-demand cohort-specific de-identification and de-identification with patient and provider identifiers. NLM-Scrubber enables clinical scientists, the users of de-identified data, to be part of the process and shape the de-identification output based on their needs. n/a",NLM Scrubber: NLM's Software Application to De-identify Clinical Text Documents,9787041,ZIALM010002,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Clinical', 'Clinical Data', 'Clinical Research', 'Computational Linguistics', 'Computer software', 'Data', 'Databases', 'Dictionary', 'Florida', 'Goals', 'Guidelines', 'Health', 'International', 'Knowledge', 'Label', 'Laboratories', 'Laws', 'Methods', 'NCI Center for Cancer Research', 'Names', 'National Cancer Institute', 'Output', 'Pathology Report', 'Patients', 'Pattern Recognition', 'Personally Identifiable Information', 'Policies', 'Privacy', 'Process', 'Provider', 'Regulation', 'Reporting', 'Risk', 'SEER Program', 'Scientist', 'Shapes', 'Social Security Number', 'Software Design', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Text', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Voice', 'base', 'cohort', 'college', 'interest', 'medical schools', 'neoplasm registry', 'patient privacy', 'repository', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2018,830057,-0.051949069712797225
"Exploring the evolving relationship between tobacco, marijuana and e-cigarettes Abstract The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana products (respectively). In order to understand this changing landscape we need new, ﬂexible, and responsive research methods capable of rapidly providing insights into product initiation patterns, use patterns, and cessation strategies. Social media — here deﬁned as including internet discussion forums — provides a ready-made source of abundant, naturalistic, longitudinal, publicly accessible, ﬁrst-person narratives with which to understand health behaviours and attitudes. We propose to use a combination of qualitative methods and automated natural language processing techniques to investigate online discussion forums devoted to tobacco, marijuana, and e-cigarettes in order to understand user trajectories through the three product categories. PROJECT NARRATIVE The relationship between marijuana, tobacco, and e-cigarettes is both rapidly changing and poorly understood, particularly in the light of recent federal and state-level regulatory changes governing the availability of e- cigarettes and marijuana (respectively). In order to make sense of this rapidly changing landscape, we need new, ﬂexible, and responsive research methods capable of providing insights into tobacco, marijuana, and e- cigarette product use patterns. We propose to use a combination of qualitative and automated natural language processing techniques to investigate online discussion forums related to tobacco, marijuana, and e-cigarettes in order to better understand user trajectories through these different product classes.","Exploring the evolving relationship between tobacco, marijuana and e-cigarettes",9530020,R21DA043775,"['Adolescent and Young Adult', 'Adult', 'Age', 'Algorithms', 'Attitude to Health', 'Categories', 'Chronic Bronchitis', 'Code', 'Data', 'Data Science', 'Devices', 'Educational Status', 'Electronic cigarette', 'Health', 'Health behavior', 'High School Student', 'Individual', 'Internet', 'Manuals', 'Marijuana', 'Modeling', 'Multiple Marriages', 'Natural Language Processing', 'Pattern', 'Persons', 'Population', 'Qualitative Methods', 'Reporting', 'Research', 'Research Methodology', 'Resources', 'Role', 'Sampling', 'Smoking', 'Source', 'Surgeon', 'Techniques', 'Therapeutic', 'Tobacco', 'Tobacco use', 'Training', 'Work', 'base', 'cigarette smoking', 'combustible cigarette', 'electronic cigarette use', 'flexibility', 'high school', 'innovation', 'insight', 'man', 'marijuana use', 'nicotine replacement', 'smoking cessation', 'social media']",NIDA,UNIVERSITY OF UTAH,R21,2018,201771,-0.030786117319816866
"The next generation of RNA-Seq simulators for benchmarking analyses Abstract: RNA-Sequencing (RNA-Seq) has established itself as the primary method for studying transcription in basic research, with an emerging role in the clinic – currently upwards of 5,000 publications using the technology are indexed in PubMed. However, the interpretation of RNA-Seq requires several complex operations including alignment, quantification, normalization and statistical analyses of various types. Since its inception a large number of algorithms have appeared for each step, creating a very confusing landscape for investigators. In order to determine the best analysis practices, numerous benchmarking studies have emerged which leverage real RNA-Seq data made from well-studied RNA samples, such as the Genetic European Variation in Health and Disease (GEUVADIS) consortium data. These valuable RNA-Seq datasets contain the biases and errors introduced by sequencing biochemistry—factors that any analysis method must account for and overcome. However, the utility of such datasets for benchmarking analysis methods is limited by the fact that we do not know the underlying truth (e.g. the true number of RNA molecules from each transcript in the original sample). Therefore researchers tend to rely heavily on simulated data, since we know everything about the true composition of these samples. There are dozens of DNA simulators aimed at benchmarking applications such as variant calling. And while the need for simulators is just as strong in RNA analysis, there are only a scant few RNA-Seq simulators available. Furthermore, the available RNA- Seq simulators are based on simplifying assumptions that greatly restrict their utility for benchmarking anything but the most upstream steps in the analysis pipeline (e.g. alignment). The further downstream the analysis method is, the more accurately the true nature of real data and its technical biases need to be modeled in order to draw meaningful conclusions. For example, no simulator generates data from a diploid genome, which would be necessary to evaluate allele specific quantification. Given our extensive experience with RNA-Seq analysis and transcriptomics in general, and our success at building the BEERS simulator, and our track record of authorship on all comprehensive RNA-Seq aligner benchmarking studies published to date, we are ideally situated to develop the next generation of open-source RNA-Seq simulator which aims to model all sources of technical variability. Furthermore, the simulator will model biological variability with an empirical approach based on using real data to configure the simulator’s parameters, which is a natural problem for machine learning. There are eleven steps in RNA-Seq library preparation which introduce bias, all of which will be modeled by the software in an object-oriented modular framework. Project Narrative: There have been many algorithms developed for every step of the RNA-Seq analysis pipeline with no easy way to compare between them. Simulated data are useful for this purpose, but to date there are very few RNA-Seq simulators available and all make too many simplifying assumptions to be used for anything but the most upstream steps in the pipeline, e.g. alignment. We propose to develop the next generation of open-source RNA-Seq simulator, which will capture all of the biochemical processes in a modular fashion and model all of the sources of technical variation.",The next generation of RNA-Seq simulators for benchmarking analyses,9600808,R21LM012763,"['Affect', 'Algorithms', 'Alleles', 'Alternative Splicing', 'Authorship', 'Basic Science', 'Benchmarking', 'Biochemical', 'Biochemical Process', 'Biochemical Reaction', 'Biological', 'Biological Models', 'Clinic', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'DNA', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Development', 'Diploidy', 'Disease', 'Enzymes', 'European', 'Genetic', 'Genetic Transcription', 'Genome', 'Goals', 'Guanine + Cytosine Composition', 'Health', 'In Vitro', 'Libraries', 'Machine Learning', 'Methods', 'Modeling', 'Morphologic artifacts', 'Nature', 'Output', 'Preparation', 'Process', 'Protein Isoforms', 'Protocols documentation', 'PubMed', 'Public Domains', 'Publications', 'Publishing', 'RNA', 'RNA Splicing', 'RNA analysis', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Sequencing Biochemistry', 'Signal Transduction', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Transcript', 'Variant', 'Work', 'analog', 'base', 'biochemical model', 'design', 'digital', 'experience', 'experimental study', 'flexibility', 'indexing', 'next generation', 'open source', 'operation', 'power analysis', 'success', 'tool', 'transcriptome sequencing', 'transcriptomics']",NLM,UNIVERSITY OF PENNSYLVANIA,R21,2018,217350,-0.017343075760630326
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9453640,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2018,305500,-0.01829767591492799
"Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research PROJECT SUMMARY This Phase II project aims to continue development of a commercial quality, innovative cloud hosted information management system, called Climb 2.0™ that will increase laboratory efficiency and provide improved capabilities for research laboratories. Climb is designed to offer integrated laboratory process management modules that include mobile communications tools data monitoring and alert systems, and integrated access to Microsoft Azure Cloud™ Machine Learning and Stream Analytics services. Initially, Climb 2.0 will target animal model research laboratories; however, the core of the platform is designed to be adaptable to nearly any research type or related industry. Current research information management systems are primarily designed as record-keeping tools with little or no direct focus on laboratory efficiency or in enhancing value of the research data. They also do not leverage emergent mobile device technologies, social media frameworks, and data analysis and storage capabilities of cloud computing. Many laboratories still use paper as their primary recording system. Paper data logging is then followed by secondary data entry into a laboratory database. These systems are error prone, time consuming and lead to laboratory databases with significant time lags between data acquisition and data entry. Moreover, they do not recognized cumulative data relationships, which may identify important trends, and researchers often miss windows of opportunity to take action on time-sensitive events. In Phase I, RockStep Solutions demonstrated feasibility of an innovative Cloud Information Management Bundle system, Climb, which will increase efficiency and improve capabilities in animal model data management. During Phase I, a beta version of Climb was successfully developed and tested against strict performance metrics as a proof of concept. We successfully built a prototype with working interfaces that integrates real-time communication technologies with media capabilities of mobile devices. Phase II proposes four specific Aims: 1) Develop the technology infrastructure to support the secure and scalable Software as a Service (SaaS) deployment of Climb for enterprise commercial release; 2) Develop and extend the Phase-I prototype Data Monitoring and Messaging System (DMMS) into a platform ready for production use; 3) Extend Climb’s DMMS adding a Stream Analytics engine to support Internet of Things (IoT) devices and streaming media; 4) Deploy a beta release of Climb at partner research labs, test and refine the product for final commercialization. To ensure Climb is developed with functionality and tools relevant to research organizations, RockStep Solutions has established collaborations with key beta sites to test all of the major functionality developed in this proposal. IMPACT: By leveraging emergent technologies and cloud computing, Climb offers several advantages: 1) enables real-time communications using familiar tools among members of research groups; 2) reduces the risk of experimental setbacks, and 3) enables complex experiments to be conducted efficiently. PROJECT NARRATIVE PUBLIC HEALTH RELEVANCE: The NIH Invests approximately $12 billion each year in animal model research that is central to both understanding basic biological processes and for developing applications directly related to improving human health. More cost-effective husbandry and colony management techniques, equipment, and new approaches to improve laboratory animal welfare and assure efficient and appropriate research use (e.g., through cost savings and reduced animal burden) are high priorities for NIH. RockStep Solutions proposes to meet this need by integrating existing and emergent software and communication technologies to create a novel solution, Climb 2.0, for research animal data management. Climb 2.0 extracts immediate value of data in animal research laboratories and extends the database into a multimedia communication network, thus enabling science that would be impractical to conduct with traditional methods.",Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research,9741597,R44GM112206,"['Address', 'Algorithms', 'Animal Experimentation', 'Animal Model', 'Animal Welfare', 'Animals', 'Biological Process', 'Biomedical Research', 'Clinical Laboratory Information Systems', 'Cloud Computing', 'Collaborations', 'Communication', 'Communication Tools', 'Communications Media', 'Complex', 'Computer software', 'Computers', 'Consumption', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Security', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Devices', 'Emerging Technologies', 'Ensure', 'Equipment', 'Equipment and supply inventories', 'Event', 'Feedback', 'Future', 'Goals', 'Health', 'Health system', 'Human', 'Industry', 'Information Management', 'Internet', 'Internet of Things', 'Investments', 'Laboratories', 'Laboratory Animals', 'Laboratory Research', 'Lead', 'Machine Learning', 'Management Information Systems', 'Methods', 'Modernization', 'Monitor', 'Motion', 'Multimedia', 'Notification', 'Paper', 'Performance', 'Phase', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Science', 'Secure', 'Services', 'Site', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'The Jackson Laboratory', 'Time', 'Transact', 'United States National Institutes of Health', 'analytical tool', 'base', 'cloud based', 'commercialization', 'computerized data processing', 'cost', 'cost effective', 'data acquisition', 'data management', 'design', 'encryption', 'experimental study', 'good laboratory practice', 'graphical user interface', 'handheld mobile device', 'improved', 'innovation', 'laptop', 'member', 'novel', 'novel strategies', 'predictive modeling', 'programs', 'prototype', 'public health relevance', 'service learning', 'social media', 'software as a service', 'success', 'time interval', 'tool', 'trend', 'usability']",NIGMS,"ROCKSTEP SOLUTIONS, INC.",R44,2018,99999,-0.013618331988025388
"Academy of Aphasia Research and Training Symposium PROJECT SUMMARY/ABSTRACT The annual Academy of Aphasia meeting is the premier conference for researchers in the field of language processing and aphasia. Since the first meeting in 1963, this international meeting has brought together an interdisciplinary group of linguists, psychologists, neurologists, and speech-language pathologists to discuss the latest research in the field of aphasia, including theoretical, clinical, and rehabilitation aspects of this language disorder. The topics at the conference range widely but almost always cover all aspects of language processing including phonological processing, lexical-semantic processing, syntactic processing, orthographic processing, bilingualism, computational modeling, non-invasive and invasive brain imaging, language recovery, neuroplasticity, and rehabilitation. In this proposal, we aim to include two special initiatives that will take place during the annual academy of aphasia conference. The first initiative involves a formal mentoring program for young investigators entering the field of aphasia research. In this program, selected student/post-doctoral fellows from interdisciplinary backgrounds who are first authors at the conference are paired with a mentor. This mentor will provide specific feedback about the fellow's presentation and general mentorship to the fellow about research and academic careers. This program is currently occurring as part of the conference and has been growing at the annual meeting with very positive feedback. Additionally, a formal mentoring meeting will allow a structured format for discussion about a career in aphasia research. The second initiative will be a three hour seminar (New Frontiers in Aphasia Research) that covers the background and approach of a state of the art methodology (e.g., fNIRS, graph theoretical metric, machine learning approaches) that has an application to the study of aphasia. These workshops will be recorded and, consequently, uploaded to the academy website/youtube channel for dissemination to aphasia researchers and the public. This workshop will allow conference attendees to understand the conceptual and methodological aspect of a particular scientific approach that can be implemented in their study of aphasia. Given the highly interdisciplinary nature of aphasia research, these workshops will bridge the communication between aphasia researchers and scientists and experts who have developed new approaches to study the brain. This meeting already allows a valuable opportunity for cross-pollination of research ideas and will now provide a platform for the training the next generation of scientists interested in pursuing the nature of aphasia and associated language disorders in adults. PROJECT NARRATIVE Approximately 100,000 individuals suffer from aphasia each year. The academy of aphasia is an organization of clinicians, scientists and practitioners who study this communication disorder and develop interventions to treat individuals with aphasia. The members comprise a very interdisciplinary group of linguists, psychologists, speech and language clinicians, neurologists and neuroscientists. The annual academy of aphasia meeting is the premier venue to share state of the art methodologies for application in the study of aphasia to improve the research in the development of diagnosis and treatment approaches to alleviate aphasia. This conference is also the ideal venue to educate and train the next generation of aphasia researchers who are well trained from a theoretical, technical and clinical standpoint and are committed to expand the impact of research in aphasia.",Academy of Aphasia Research and Training Symposium,9612777,R13DC017375,"['Academy', 'Adult', 'Aphasia', 'Brain', 'Brain imaging', 'Chicago', 'Clinical', 'Collaborations', 'Committee Membership', 'Communication', 'Communication impairment', 'Computer Simulation', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Discipline', 'Educational workshop', 'Feedback', 'Fellowship', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Governing Board', 'Grant', 'Graph', 'Hour', 'Image Analysis', 'Individual', 'International', 'Intervention', 'Language', 'Language Disorders', 'Learning', 'Lesion', 'Linguistics', 'Machine Learning', 'Manuscripts', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neurologic', 'Neurologist', 'Neuronal Plasticity', 'Neurosciences', 'Orthography', 'Outcome', 'Pathologist', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychologist', 'Psychology', 'Publishing', 'Reading', 'Recovery', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Support', 'Research Training', 'Rest', 'Retrieval', 'Scientist', 'Secure', 'Speech', 'Speech Pathologist', 'Speech Perception', 'Stroke', 'Structure', 'Students', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Training and Education', 'Transcranial magnetic stimulation', 'Travel', 'Videotape', 'Work', 'Writing', 'base', 'bilingualism', 'career', 'career networking', 'experience', 'frontier', 'improved', 'improved outcome', 'interest', 'language processing', 'lexical', 'meetings', 'member', 'neuroimaging', 'next generation', 'novel', 'novel strategies', 'outcome forecast', 'phonology', 'programs', 'relating to nervous system', 'semantic processing', 'success', 'symposium', 'syntax', 'tenure track', 'web site']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R13,2018,39939,-0.04948843815967116
"Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!) PROJECT SUMMARY/ABSTRACT Our society faces significant challenges in providing quality health care that is accessible by each person and is sensitive to each person's individual lifestyle and individual health needs. Due to recent advances in sensing technologies that have improved in accuracy, increased in throughput, and reduced in cost, it has become relatively easy to gather high resolution behavioral and individualized health data at scale. The resulting big datasets can be analyzed to understand the link between behavior and health and to design healthy behavior interventions. In this emerging area, however, very few courses are currently available for teaching researchers and practitioners about the foundational principles and best practices behind collecting, storing, analyzing, and using behavior- based sensor data. Teaching these skills can help the next generation of students thrive in the increasingly digital world.  The goal of this application is to design online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to WSU faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.  This contribution is significant because not only large research groups but even individual investigators can create large data sets that provide valuable, in-the-moment information about human behavior. They need to be able to handle the challenges that arise when working with sensor- based behavior data. Because students will receive hands-on training with actual sensor datasets and analysis tools, they will know how to get the best results from available tools and will be able to interpret the significance of analysis results.  Our proposed online course program, called AHA!, builds on the investigators' extensive experience and ongoing collaboration at Washington State University on the development of smart home and mobile health app design, activity recognition, scalable biological data mining, and the use of these technologies for clinical applications. Our approach will be to design online course modules to train individuals in the analysis of behavior-based sensor data using clinical case studies (Aim 1). We will design an educational program that involves students from diverse backgrounds and that is findable, accessible, interoperable, and reusable (Aim 2). Finally, we will conduct a thorough evaluation to monitor success and incrementally improve the program (Aim 3). All of the materials will be designed for continued use beyond the funding period of the program. PROJECT NARRATIVE  This program focuses on the development and dissemination of online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to Washington State University faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.",Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!),9482420,R25EB024327,"['Address', 'Aging', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological', 'Case Study', 'Charge', 'Chronic Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Development', 'Discipline', 'E-learning', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Evaluation', 'FAIR principles', 'Face', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'General Population', 'Goals', 'Health', 'Home environment', 'Human', 'Immersion Investigative Technique', 'Individual', 'Interdisciplinary Study', 'Life Style', 'Link', 'Longevity', 'Machine Learning', 'Methods', 'Mobile Health Application', 'Monitor', 'Performance', 'Persons', 'Precision Medicine Initiative', 'Pythons', 'Rehabilitation Nursing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Site', 'Societies', 'Structure', 'Students', 'Suggestion', 'Techniques', 'Technology', 'Training', 'Universities', 'Washington', 'Work', 'base', 'behavior influence', 'behavioral health', 'biocomputing', 'career networking', 'clinical application', 'cognitive rehabilitation', 'cost', 'course development', 'course module', 'data mining', 'design', 'digital', 'experience', 'health care quality', 'health data', 'improved', 'innovation', 'learning materials', 'learning strategy', 'mHealth', 'next generation', 'online course', 'programs', 'recruit', 'responsible research conduct', 'scale up', 'sensor', 'sensor technology', 'skills', 'statistics', 'success', 'synergism', 'tool', 'web page']",NIBIB,WASHINGTON STATE UNIVERSITY,R25,2018,169419,-0.051842477577493405
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9672008,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,347221,-0.0154187073057739
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9559873,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,300000,-0.0154187073057739
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9621771,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2018,748748,-0.02038040755009343
"Accelerating Multi-modal Biomarker Discovery in Translational Research with Cloud Data Integration Project Summary/Abstract Cytobank is the leading cloud-based platform for analysis and storage of single cell flow and mass cytometry data, technologies that are essential for investigating the interplay between the immune system and disease conditions including cancer. There are numerous data analysis steps between raw data and insight especially for many single-cell technologies, where the data analysis is complex, highly expert-driven and/or reliant on novel computational methodologies. Cytobank already makes major contributions (1) centralizing single-cell cytometry data, (2) providing data analysis traceability that removes knowledge sharing complexities, and (3) establishing a platform that increases access to cutting edge algorithms and makes complex machine learning methods easy for biologists to use. However, as the amount, complexity, and different types of single cell data and other associated data increases and the number of workflows and single-cell algorithms to analyze the data also increases, the need for open and easy access to existing and new tools and secure, complete storage of the workflows and the resulting data has increased to the point of being critical for supporting basic and translational research collaborations and enabling them to efficiently achieve their objectives including biomarker discovery and development. The proposed project significantly extends the capabilities of the Cytobank platform. This will benefit the community by (1) enabling scalable and secure access to a number of new single-cell data analysis tools that will result in new automated workflows, and (2) enable more efficient cross platform knowledge generation with increased meta-analysis capabilities across experiments and data types. The potential of this project is that thousands of scientists around the world will be able to more easily leverage additional single-cell cytometry, transcript, and other data in their translational research data analysis including automating analysis that has primarily been dominated by expert-driven annotation, thus providing a central repository and knowledge management framework that will accelerate biomarker discovery and precision medicine. Project Narrative Single-cell biology and Immunotherapy are exploding and generating larger and more complex datasets in combination clinical trials. To take full advantage of these revolutions, the iteration and dissemination of advanced single-cell data analysis algorithms (many of whose development was funded by the NIH) needs to scale at the same rate as single-cell data generation technologies are scaling, and multi-omics data analysis and visualization must be integrated and automated. This project will greatly accelerate scientific research, transparency, and reproducibility by significantly lowering the barrier to perform complex data analysis of multiple types of high-dimensional data, providing the biomedical research community with access to powerful tools needed in immuno-oncology, autoimmunity and other high-impact disease areas.",Accelerating Multi-modal Biomarker Discovery in Translational Research with Cloud Data Integration,9464486,R44GM117914,"['Acute Lymphocytic Leukemia', 'Algorithmic Analysis', 'Algorithms', 'Area', 'Autoimmunity', 'B-Lymphocytes', 'Basic Science', 'Biological Markers', 'Biomedical Research', 'Cells', 'Cellular biology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Follicular Lymphoma', 'Foundations', 'Funding', 'Generations', 'Immune System Diseases', 'Immune system', 'Immunologic Monitoring', 'Immunology', 'Immunooncology', 'Immunotherapy', 'Information Resources Management', 'Knowledge', 'Label', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Meta-Analysis', 'Modality', 'Modeling', 'Outcome', 'Patients', 'Phase', 'Population', 'Positioning Attribute', 'Regimen', 'Relapse', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Scientist', 'Secure', 'System', 'Target Populations', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Toxic effect', 'Transcript', 'Translational Research', 'Treatment Efficacy', 'United States National Institutes of Health', 'Work', 'anti-cancer', 'base', 'biomarker development', 'biomarker discovery', 'biomarker validation', 'clinically actionable', 'cloud based', 'cost effective', 'cytokine', 'data integration', 'data management', 'data visualization', 'experimental study', 'high dimensionality', 'immunotherapy trials', 'improved', 'insight', 'learning strategy', 'multiple omics', 'novel', 'outcome prediction', 'personalized medicine', 'population based', 'precision medicine', 'predict clinical outcome', 'predictive marker', 'predictive modeling', 'relapse prediction', 'repository', 'response', 'single cell analysis', 'single cell technology', 'single-cell RNA sequencing', 'synergism', 'tool', 'transcriptome sequencing', 'transcriptomics', 'tumor']",NIGMS,"CYTOBANK, INC.",R44,2018,613841,-0.019634740858293677
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9454246,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease Surveillance', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2018,461012,-0.04011880366589098
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9534738,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'ontology development', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2018,431097,-0.026721412452100295
"Development Of Theoretical Methods For Studying Biological Macromolecules New theoretical techniques are being developed and characterized. These efforts are usually coupled with software development, and involve the systematic testing and evaluation of new ideas.  Methods for evaluating long range interactions Long range electrostatic and dispersion interactions have important consequences for accurate modeling of macromolecular structures, particularly in highly anisotropic systems. After preliminary results showed that long range effects are crucial for accurately modeling lipid models, a large scale collaboration isunderway to update the CHARMM force fields, particularly those forlipids, supplanting distance based truncation methods with the particle mesh Ewald(PME)method. Becausevery few programshave PME capabilities for dispersion, we have developed a standalone library to enable these calculationsto be carried out by any program and have made it freely available to all researchers. The code uses state-of-the-art techniques to parallelize the calculationacross supercomputers and ongoing work is extending themultithreading capabilities to use graphics cards toaccelerate the calculations, as well as efficiently leveraging the large vector capabilities of modern processors. We have also developed a fresh approach for PME that can be more efficiently parallelized across nodes within a supercomputer. The bottleneck in such computations is due to the need to communicate large quantities of information between nodes. Our novel approach introduces a strategy to a priori compress the information, which should greatly reduce the communication bottleneck. This new algorithm will also be able to utilize the latest features of modern graphics cards and chips that were introduced to aid the machine learning community; work is underway to write PME code exploiting these features. We are also making these developments available to the community through our open source PME library.  Polarization approches for multiscale modeling The AMOEBA polarizable force field has an elaborate functional form that permits very high accuracy descriptions of the complex environments introduced at interfaces and surrounding highly charged moieties. Our preliminary work has revealed that the polarization groups, introduced by AMOEBAs creators to ensure transferability, introduce problems related to efficiency and they prevent the simple formation of a variational quantum mechanical framework, which is a long standing goal of ours. We have tested a number of schemes for eliminating these polarization groups and are working with the creators of AMOEBA to incorporate these schemes into the emerging next-generation polarizable models.  Obtaining QM/MM free energies via reparameterization of classical force fields with force matching We demonstrate that obtaining free energies at a high (e.g., QM/MM) level of theory by computing free energy corrections to classical results can be greatly facilitated through the use of force matching on classical generated ensembles. In particular, by adjusting only bonded type parameters (e.g., bond, angle, and dihedral terms), the configurational overlap of the classical model with the desired high level of theory is increased and thus improving the calculation of free energy corrections between levels of theory.  Interaction Energy Hypothesis:  Failure of the interaction energy hypothesis The need to achieve free energy calculations at highly accurate (e.g., QM/MM) levels of theory have resulted in various tricks and approximations to subvert the need for properly sampling conformationally relevant regions of the desired high level of theory. One such approach that has gained popularity is the so-called Interaction Energy Hypothesis'' (IEH). Initially grounded in rigorous implementation, the approach focuses on the substitution of total potential energy differences between classical MM and high-level QM in rigorous free energy simulation approaches (e.g, free energy perturbation and Bennett's acceptance ratio) with changes in environment-solute interactions between MM and QM/MM, which has substantially smaller fluctuations and therefore improving calculation convergence. Our results demonstrate that IEH is not only rigorously incorrect as a substitute for the change in total potential energy between high and low levels of theory but that IEH can systematically provide the wrong result. Specifically, using IEH on two different low levels of theory to obtain a density functional tight binding solvation free energy of bis-2-chloro-diethyl ether produced two almost identical results (e.g., differing only by 0.16 kcal/mol), but both deviating from rigorous calculations by about 1.5 kcal/mol. This result is particularly concerning as it deludes practitioner into a false confidence garnered by consistency in findings.  MPID FF Efforts: Development of the multipole induced dipole forcefield One of the biggest pushes in the development of modern force fields is the need to incorporate polarizability effects. However, present efforts to incorporate polarization are often incompatible with refinement approaches (e.g., correcting a fixed charge model to a drude based force field) or are somewhat underwhelming in performance (in results and computational expediency). This motivated us to start work on the creation of our own polarizable force field, the Multipole Induced Dipole (MPID) force field. Based on the generation of higher-order multipole moments from heavy atoms of a molecule, initial parameters were derived by direct mapping of the CHARMM drude force field onto MPID. However, this need not (and quite possibly should not) be the case, as the best parameterization might deviate from the initial drude based setup. Thus, we are working towards generating a parameter set derived independently of the drude force field, with a dedicated MPID water model validated through various phases of bulk water in conjunction with the computation of solvation free energies.  Other projects (without description due to annual report character count limits) Methods for Predicting Physicochemical Properties Evaluation of Enhanced Sampling Methods for Accelerating Peptide Insertion in Membranes Application of novel binding free energy calculation protocols to the host-guest blind challenges Reservoir pH replica exchange (R-pH-REM) for constant pH simulations Hybrid QM and MM method for pKa prediction in explicit solvent Using VMMS to explain the role of hydronium ions nearby acidic residues as an important reason behind the high apparent dielectric constant inside proteins n/a",Development Of Theoretical Methods For Studying Biological Macromolecules,9787865,ZIAHL001051,"['Active Sites', 'Algorithms', 'Amber', 'Annual Reports', 'Basic Science', 'Behavior', 'Binding', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Process', 'Biophysics', 'Catalysis', 'Cells', 'Charge', 'Chemicals', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Complex', 'Computational Biology', 'Computational Technique', 'Computer Assisted', 'Computers', 'Computing Methodologies', 'Coupled', 'Development', 'Drug Design', 'Electron Microscopy', 'Electrostatics', 'Ensure', 'Environment', 'Enzymes', 'Ethyl Ether', 'Evaluation', 'Failure', 'Free Energy', 'Gene Expression Profiling', 'Generations', 'Goals', 'Grain', 'Hybrids', 'Image Analysis', 'Laboratories', 'Libraries', 'Lipids', 'Machine Learning', 'Maps', 'Mechanics', 'Membrane', 'Membrane Proteins', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Molecular Conformation', 'Molecular Structure', 'National Heart, Lung, and Blood Institute', 'Peptides', 'Performance', 'Pharmacotherapy', 'Phase', 'Play', 'Potential Energy', 'Property', 'Proteins', 'Protocols documentation', 'Psychological Techniques', 'Quantum Mechanics', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Sampling', 'Scheme', 'Science', 'Scientist', 'Solvents', 'Structure', 'System', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Update', 'Variant', 'Water', 'Work', 'Writing', 'base', 'biological systems', 'blind', 'computing resources', 'density', 'design', 'experimental study', 'gene function', 'human disease', 'hydronium ion', 'improved', 'interest', 'learning community', 'macromolecule', 'models and simulation', 'molecular dynamics', 'molecular mechanics', 'molecular modeling', 'multi-scale modeling', 'next generation', 'novel', 'novel strategies', 'open source', 'particle', 'prevent', 'programs', 'quantum', 'simulation', 'small molecule', 'software development', 'solute', 'supercomputer', 'theories', 'therapeutic lead compound', 'therapy design', 'tool', 'vector']",NHLBI,"NATIONAL HEART, LUNG, AND BLOOD INSTITUTE",ZIA,2018,1028860,-0.015000013201283682
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9583854,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2018,74515,-0.006322362065036301
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9531327,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2018,773018,-0.019293968202007322
"How Does Automated Record Linkage Affect Inferences about Population Health? ABSTRACT  Our broad research objective is to create the Longitudinal Intergenerational Family Electronic Micro-dataset (LIFE-M) spanning the late 19th and 20th century United States. Using automated record linkage technology, the LIFE-M project combines millions of vital records to reconstruct how and why individuals' health has changed across time. This multi-generational, longitudinal micro-database aims to transform research on health and longevity, on childbearing and family structure, and on the long-run health effects of early-life circumstances and exposures.  In creating LIFE-M, however, we have encountered serious deficits in knowledge about the performance of automated record linkage technology. The proposed project seeks to evaluate the performance of the most popular and cutting-edge automated linking techniques for the purposes of creating longitudinal health data. Our specific aims are to (1) produce systematic evidence regarding the performance of automated record linking algorithms in terms of match rates, representativeness of the linked sample, erroneous matches (type I errors), and systematic measurement error; (2) examine how phonetic name-cleaning methods affect quality metrics; and (3) examine how record quality metrics vary for different underrepresented subgroups (including women, racial/ethnic minorities, and immigrants) and to determine how linking methods affect representativeness and inferences. To achieve these aims, we have developed new partnerships with record linking experts allowing us to incorporate the most cutting-edge methods in record linking. We will also rely on new “ground truth” generated by LIFE-M project's independent, double-blind human review process.  This project will contribute significantly to existing knowledge about the use of automated linking methods for creating longitudinal and intergenerational health data. It will also increase knowledge about potential sources of bias in health studies. Both contributions should greatly enhance the quality of descriptive and causal inferences about population health and aging and disparities in these outcomes. PROJECT NARRATIVE  This project contributes to public health knowledge by advancing record linking methodology for creating longitudinal and intergenerational health datasets. It will also increase knowledge about potential sources of bias in public health and aging studies using linked records. Both contributions should significantly improve the quality of inferences about public and population health and health disparities.",How Does Automated Record Linkage Affect Inferences about Population Health?,9565480,R21AG056912,"['Affect', 'Aging', 'Algorithms', 'American', 'Benchmarking', 'Big Data', 'Birth', 'Birth Certificates', 'Birth Records', 'Censuses', 'Child', 'Computers', 'Data', 'Data Linkages', 'Data Set', 'Databases', 'Double-Blind Method', 'Economics', 'Family', 'Foundations', 'Four-dimensional', 'Funding', 'Genealogy', 'Generations', 'Genetic Transcription', 'Goals', 'Graph', 'Hand', 'Health', 'Heterogeneity', 'Human', 'Immigrant', 'Incidence', 'Individual', 'Infant', 'Joints', 'Knowledge', 'Life', 'Link', 'Longevity', 'Machine Learning', 'Maiden Name', 'Marriage', 'Measurement', 'Measures', 'Medicare', 'Methodology', 'Methods', 'Minnesota', 'Minor', 'Names', 'Outcome', 'Performance', 'Pilot Projects', 'Politics', 'Population', 'Process', 'Public Health', 'Records', 'Research', 'Research Infrastructure', 'Running', 'Sample Size', 'Sampling', 'Science', 'Scientist', 'Source', 'Speed', 'Subgroup', 'Techniques', 'Technology', 'Time', 'United States', 'United States National Institutes of Health', 'Universities', 'Variant', 'Veterans', 'Woman', 'aging population', 'child bearing', 'cost effective', 'ethnic minority population', 'family structure', 'health data', 'health disparity', 'health knowledge', 'improved', 'innovation', 'intergenerational', 'longitudinal database', 'population health', 'racial and ethnic', 'repository', 'social', 'vector']",NIA,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R21,2018,194895,-0.019854605459037857
"COINSTAC: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community be- comes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2). The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates a dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informat- ics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an inde- pendent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented stor- age vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and pri- vacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix fac- torization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. 4 Project Narrative  Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don’t have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this ‘missing data’ and allow for pooling of both open and ‘closed’ repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed compu- tational solution for a large toolkit of widely used algorithms. 3","COINSTAC: decentralized, scalable analysis of loosely coupled data",9717051,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,282320,0.0030251391982790998
"A Multigenerational Longitudinal Panel for Aging Research Summary/Abstract This project will construct a Multigenerational Longitudinal Panel (IPUMS-MLP) of unprecedented scale and scope. Using cutting-edge automatic record linkage technology and drawing on complete count U.S. census data available from IPUMS for the period 1850 to 1940, the project will construct millions of individual life histories and trace millions of families over multiple generations. This infrastructure will provide the most comprehensive view of long-run changes in life-course dynamics available for any place in the world and will transform our understanding of processes of population aging. The work will require significant innovation and new technical infrastructure to accommodate the massive scale of the database. These data will allow investigators to directly observe changes in aging processes and life-course transitions during the period in which U.S. society was being transformed by industrialization, urbanization, immigration, demographic transition, and economic collapse. Investigators will be able to follow individuals over time to evaluate the impact of early-life conditions on later outcomes, trace life-course transitions into adulthood and old age, and observe family change over multiple generations. IPUMS-MLP will enrich existing aging surveys by providing data on multiple generations of forebears of survey respondents; likewise, it will enrich existing historical databases by enabling them to connect with descendants across multiple generations. Leveraging billions of dollars of federal investments in census data and transactional records from a variety of administrative sources, this project is a highly cost-effective use of scarce resources to develop shared infrastructure for research, education, and policy-making on health and aging. Project Narrative The proposed work is directly relevant to the core mission of the Population and Social Processes branch of NIA: the new data will advance fundamental knowledge about the causes and consequences of changes in health and well-being of the older population and will support research on the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. For example, the data will enable examinations of the impact of lead exposure to late onset Alzheimer’s disease, the socioeconomic and health effects of early-life income support, intergenerational transmission of health and wellbeing over multiple generations, and the impact of early-life cognitive capacity on later-life health and economic outcomes.",A Multigenerational Longitudinal Panel for Aging Research,9594032,R01AG057679,"['Adult', 'Age', 'Aging', 'Aging-Related Process', 'Algorithms', 'Big Data', 'Censuses', 'Characteristics', 'Communities', 'Custom', 'Data', 'Data Quality', 'Data Security', 'Databases', 'Demographic Transitions', 'Economics', 'Education', 'Elderly', 'Exposure to', 'Family', 'Family member', 'Future', 'Genealogy', 'Generations', 'Health', 'Household', 'Immigration', 'Income', 'Individual', 'Industrialization', 'Institution', 'Investments', 'Knowledge', 'Late Onset Alzheimer Disease', 'Life', 'Life Cycle Stages', 'Link', 'Machine Learning', 'Metadata', 'Methods', 'Military Personnel', 'Mission', 'Names', 'Neighborhoods', 'Older Population', 'Outcome', 'Personal Satisfaction', 'Persons', 'Policy Making', 'Population', 'Population Process', 'Process', 'Public Policy', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Resources', 'Respondent', 'Running', 'Sampling', 'Selection Bias', 'Social Processes', 'Social Security', 'Societies', 'Source', 'Structure', 'Surveys', 'Technology', 'Time', 'Urbanization', 'War', 'Weight', 'Woman', 'Work', 'aging population', 'base', 'cognitive capacity', 'cost effective', 'data integration', 'economic outcome', 'experience', 'health economics', 'improved', 'innovation', 'intergenerational', 'lead exposure', 'learning strategy', 'life history', 'longitudinal dataset', 'novel', 'parallel processing', 'social', 'socioeconomics', 'tool', 'transmission process']",NIA,UNIVERSITY OF MINNESOTA,R01,2018,695826,-0.025441118310533112
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9473021,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,649098,0.0016230118842699283
"Development of a secure, cloud-based platform to improve record linkage & cross-agency collaboration for the public sector: using deep learning & scalable data integrations to combat the opioid crisis Project Summary/Abstract: This SBIR Phase I proposal aims to fund research and development for a new, multitenant secure cloud-based platform specifically tailored to provide local governmental agencies with tools to share datasets and link them accurately, at high quality and low cost. The OpenLattice platform will focus on reducing drug overdoses and making drug treatment less fractured. Individual-level datasets linked across medical providers and law enforcement can support analyses of prescribing pathways and treatment trajectories that precede opioid overdose, entry into treatment, disruption, and recovery. However, linking data at the individual level has proven to be a difficult and resource-intensive endeavor compared to use of aggregate-level data, with issues with deduplication plaguing many institutional databases. With 91 American deaths recorded daily from opioid overdoses and systems of care spread across multiple institutions, the need for greater and high-quality data sharing is undeniable. Our test partner for assessing the efficacy of proposed innovations is the Greater Portland Addiction Collaborative (GPAC) in Maine, a partnership of hospitals, a police department, jail, detox treatment centers and halfway houses already working together to reduce drug overdoses. This proposal aims to demonstrate proof of concept for (i) scaling high-quality data integrations across multiple governmental domains via a standardized entity data model, and (ii) improving record linkage using neural networks. Firstly, OpenLattice is developing an open source ontology and integration scripts to standardize integration of datasets into OpenLattice's database. As the individual customization requirements decline for onboarding customers and integrating new data into the platform, costs will be greatly slashed, removing a significant barrier to data solutions for smaller counties and cities across the country, who have historically faced custom integrations, system updates, data storage fees and add-ons at high cost. The OpenLattice platform also enables use of existing ETL tools and seamless integration with police dispatch systems, emergency medical calls, healthcare records, and online prescription systems across partners who have committed to data sharing and collaboration. Secondly, OpenLattice is developing a new, proprietary algorithm for record linkage that employs a promising but as-yet commercially untested technique: a multilayer perceptron neural network, more commonly known as deep learning. In pilot research, the linking algorithm has already demonstrated success rivaling—and sometimes exceeding—current state of the art linking technologies. In Phase I, OpenLattice will continue to improve ontologies, integration tools, and the deep learning neural network, and test on publicly available datasets with dissimilar data types and formats, with manual confirmation of results. When successful, these innovations will address critical barriers to improving clinical practice in treating opioid addiction by enabling a more comprehensive continuum of care for those in treatment. Project Narrative: Large-scale and coordinated responses to several of the US’s hot-button public health and criminal justice issues, such as the opioid epidemic and mass incarceration, are complicated by poor resource sharing and the US government’s highly fractured jurisdictional authority. This Small Business Innovation Research Phase I project aims to develop an efficient, scalable, cloud- based platform for hosting and linking highly sensitive state and local government databases at low cost, using (i) innovative data integration scripts and ontologies that standardize and scale capacity and (ii) technical advances in record de-duplication for linking databases. Data solutions would have tremendous societal impact on understandings of public health and the opioid epidemic by making drug treatment less fractured, saving lives and dramatically broadening contextual information, once data is broken out of silos.","Development of a secure, cloud-based platform to improve record linkage & cross-agency collaboration for the public sector: using deep learning & scalable data integrations to combat the opioid crisis",9622726,R43CE002937,[' '],NCIPC,"OPENLATTICE, INC.",R43,2018,225000,-0.028467254631409075
"Microdata for Analysis of Early Life Conditions, Health, and Population Project Summary This project will enhance and develop the only source of data that provides core information about work, education, income, and migration for the entire U.S. population. This unique resource allows us to observe today's late-life population when they were young. This allows a prospective view of the impact of early-life environments and socioeconomic status on health and well-being in later life. The massive database describes the characteristics of all 133 million persons who resided in the United States in 1940. The data have been available for only a brief period, but are already having a profound impact on scientific research. They are the primary data source for at least 19 sponsored projects, including nine funded by NIH and seven funded by NSF. Through May 2017, 104 investigators had produced 95 papers based on the data, including 22 articles, four books, three PhD dissertations, and 66 working papers. This project will improve the quality and usability of the database by correcting transcription errors; improving the coding, editing, and allocation of key variables; and introducing new data dissemination tools designed to simplify access to the data. The project will undertake three major activities to meet these objectives. 1. Incorporate new verified census information on name, age, sex, family relationship, race, marital status,  birthplace, and residence five years ago. 2. Improve coding, editing, and allocation for geographic variables, migration, occupation, and numerically-  coded variables such as income. 3. Apply new technologies to democratize access to the data through new data access tools and a virtual data  enclave for a restricted version of the data that contains names. This project is a highly cost-effective use of scarce resources to develop shared infrastructure for research, education, and policy-making on health and aging. The proposed improvement of data quality is urgent, and will provide a resource of unprecedented power for understanding the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. Narrative This project will develop data for prospective analysis of the impact of early-life circumstances and environment on health outcomes in late life, including Alzheimer's disease. The proposed improvement of data quality is urgent, and will provide a resource of unprecedented power for understanding the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. The proposed work is directly relevant to the core mission of the Population and Social Processes branch of NIA, since the data will allow us to observe today's late-life population when they were young.","Microdata for Analysis of Early Life Conditions, Health, and Population",9522599,R01AG041831,"['Age', 'Aging', 'Alzheimer&apos', 's Disease', 'American', 'Arbitration', 'Books', 'Censuses', 'Characteristics', 'Church of Jesus Christ of Latter-day Saints', 'Code', 'County', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Demography', 'Doctor of Philosophy', 'Economics', 'Education', 'Elderly', 'Ensure', 'Environment', 'Family Relationship', 'Funding', 'Future', 'Genealogy', 'Genetic Transcription', 'Geography', 'Handwriting', 'Health', 'Income', 'Institution', 'Investments', 'Journals', 'Life', 'Life Cycle Stages', 'Link', 'Machine Learning', 'Marital Status', 'Methods', 'Mission', 'Names', 'Occupational', 'Occupations', 'Outcome', 'Paper', 'Personal Satisfaction', 'Persons', 'Policy Making', 'Population', 'Population Process', 'Privatization', 'Process', 'Public Policy', 'Publications', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Social Processes', 'Socioeconomic Status', 'Sociology', 'Source', 'Techniques', 'Technology', 'Time', 'United States', 'United States National Institutes of Health', 'Work', 'arm', 'base', 'blind', 'cost', 'cost effective', 'data access', 'design', 'experimental study', 'improved', 'migration', 'new technology', 'prospective', 'residence', 'sex', 'social', 'tool', 'usability', 'virtual']",NIA,UNIVERSITY OF MINNESOTA,R01,2018,663500,-0.02626679347660012
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9699855,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,75000,-0.026915500384189654
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9537614,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,329257,-0.026915500384189654
"Query Log Analysis for Improving User Access to NCBI Web Services Over the last decade, the online search for biological information has progressed rapidly and has become an integral part of any scientific discovery process. Today, it is virtually impossible to conduct R&D in biomedicine without relying on the kind of Web resources developed and maintained by the NCBI. Indeed, each day millions of users search for biological information via NCBIs online Entrez system. However, finding data relevant to a users information need is not always easy in Entrez. Improving our understanding of the growing population of Entrez users, their information needs and the way in which they meet these needs opens opportunities to improve information services and information access provided by NCBI.   Among all Entrez databases, PubMed is the most used one and often serves as an entry point for people to access related data in other Entrez databases. One source for understanding and characterizing patrons of search engines is the transaction logs. Our previous investigation of PubMed query logs has led us to develop and deploy several useful applications in assisting user searches and retrieval such as the query formulation in PubMed, namely Related Queries, Query Autocomplete and Author Name Disambiguation. Inspired by its success, we have continued using log analysis to identify research problems which are closely related to NCBI operations.    Many users only look at the first page of search results. With the growing usage of mobile devices, even fewer search results may be seen. While PubMeds traditional inverse date-order results meets the needs of many, improving Best Match results provides quick access to relevant results. In addition to search terms in the title and abstract, these results are based on the length of the query, the journal, the publication date, and whether other users clicked on the article.  A first step in providing relevant results is understanding the query. To better understand queries, we developed a Field Sensor to completely identify the portions and aims of a query. In other words, we identify which part of the query is an author name, a journal title, a date, or key phrases describing a knowledge the searcher would like to uncover. One use for this tool is reminding those looking for information, not specific articles, about our improved Best Match searching.  It would seem like queries to obtain a particular article would be the easiest to process. But when one considers the bewildering array of reference formats, and a users partial and incomplete memory, properly handling a single citation query is more difficult than one might think. Our approach was to match portions of a query to the bibliographic material of a clicked article. From these patterns, artificial queries could be constructed following patterns seen in actual queries. Since any number of these artificial queries could be constructed, there was plenty of data to inform and train an algorithm.  Helping the user enter the query they really want is also valuable. Query auto completion is a commonly available feature in search engines that helps users enter queries quickly, efficiently, accurately, and prompts for useful detail the searcher may not have not originally thought to include. The usual algorithm for these completions is the most popular completion, because popular queries are popular. However, this is not as useful in a scientific context. Scientists instead are often looking for the novel, the unknown, and the unseen. Using a personalized language model, time-sensitive data, neural nets, and a beam search for diversity we were able to better predict actual future queries. This could let to better query auto completion.  Some phrases carry more meaning than is obvious from the words in the phrase. Identifying these phrases in queries and documents helps identify the most relevant documents. But these phrases cannot be obtained by simply collecting known lists of good phrases. Creative researchers coin and use new phrases all the time. A way of identifying these phrases by comparing an articles title and body has been developed. The value of this approach has been confirmed by the large number of known phrases identified. But it also identifies phrases not yet recognized by curated lists. When any of these phrases are recognized in a query and then used in the search, the results are better than merely searching for the words individually.  Deep Learning and Neural Networks have shown their value in image processing for some time. Now their application in text processing is becoming more important. Several of our projects use word embeddings, or even neural nets directly. One challenge is having models efficient enough to scale for use in a high capacity search engine. By focusing on the differences between vectors describing a query and vectors describing a document, we have developed a system that could contribute to a relevance ranking algorithm.  Another approach directly compares the vectors for words in the query and words in the documents. Using an algorithm motivated by Word Movers Distance, it provides results better than traditional information retrieval methods. Moreover, when combined with those traditional approaches, the combined approach is superior to either single method.  Now that the full text of more and more articles is available, we want to use the full text to improve search. That is harder than might be expected because authors usually do a good job of summarizing their work in the abstract. A major hurdle is the lack of humanly annotated relevance data. One project showed MeSH terms which overlap with queries can be used as a proxy for direct human relevance judgements. n/a",Query Log Analysis for Improving User Access to NCBI Web Services,9796755,ZIALM000001,"['Algorithms', 'Bibliography', 'Biological', 'Biological Neural Networks', 'Coin', 'Data', 'Databases', 'Formulation', 'Future', 'Goals', 'Human', 'Individual', 'Information Retrieval', 'Information Services', 'Internet', 'Investigation', 'Journals', 'Knowledge', 'Language', 'Length', 'Link', 'MeSH Thesaurus', 'Memory', 'Methods', 'Modeling', 'Molecular Biology', 'Names', 'Occupations', 'Pattern', 'Population', 'Process', 'Proxy', 'PubMed', 'Publications', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Source', 'System', 'Text', 'Time', 'Training', 'Transact', 'Work', 'base', 'deep learning', 'handheld mobile device', 'image processing', 'improved', 'learning network', 'novel', 'online resource', 'operation', 'phrases', 'relating to nervous system', 'research and development', 'search engine', 'sensor', 'success', 'tool', 'vector', 'virtual', 'web services']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2018,1691145,-0.0035303678776225167
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9612854,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2018,10000,-0.014879989670853987
"Longitudinal and Intergenerational Determinants of Aging and Mortality ABSTRACT  Some of the most important open questions in aging relate to the impact of longitudinal and intergenerational factors. But documenting the role of early-life and intergenerational determinants of health and aging is limited by the dearth of large-scale micro-data containing this information. This is especially true for understudied populations such as women and minority groups.  Our research objective is to add critical information on cause of death to the new large-scale data resource, the Longitudinal, Intergenerational Family Electronic Micro-database (LIFE-M). Funded by the National Science Foundation, LIFE-M links millions of vital records (birth, marriage, and death certificates) to decennial censuses over four generations and 120 years for five states. LIFE-M is a representative sample of cohorts aging and dying in the last 25 years of the 20th century and includes crucial early-life and intergenerational information. Enhancing the LIFE-M with cause of death will facilitate path-breaking research on the relationship of longevity and cause of death with demographic, socio-economic, and early-life environmental factors for family networks across four generations.  We will achieve this objective by pursuing the following specific aims:  (1) We will use new “Smart Indexing” technology to digitize and cross-validate hand-written cause-of-death information;  (2) We will link digitized causes of death to the LIFE-M infrastructure and create extensive documentation for  this new variable for public use; and  (3) We will publicly release the cause-of-death variable and documentation with the LIFE-M dataset, meta-  data, and supporting documentation on ICPSR in 2020.  The proposed project will also have broader impacts. In addition to contributing a significant new data resource that can be added to Minnesota Population Center's historical linked censuses and the Census Longitudinal Infrastructure Project (CLIP), this project's methodological innovations in script digitization will enhance on-going and future data infrastructure initiatives. Both contributions promise to transform the research frontier in population health and aging in the United States. PROJECT NARRATIVE  This project contributes to public health knowledge by adding cause-of-death information to a new intergenerational and longitudinal dataset (LIFE-M). These data will allow much more research on the long- term determinants of health and aging, including a deeper understanding the intergenerational and early-life origins of later-life diseases and mortality in today's aging population.",Longitudinal and Intergenerational Determinants of Aging and Mortality,9423915,R01AG057704,"['Address', 'Adult', 'Affect', 'Age', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'American', 'Biogenesis', 'Biological Neural Networks', 'Birth', 'Birth Records', 'Cardiovascular Diseases', 'Cause of Death', 'Censuses', 'Cessation of life', 'Child', 'Childhood', 'Complement', 'Cost Savings', 'Data', 'Data Set', 'Databases', 'Death Certificates', 'Demography', 'Development', 'Disease', 'Documentation', 'Economics', 'Elderly', 'Enrollment', 'Environmental Risk Factor', 'Epidemiology', 'Exposure to', 'Family', 'Foundations', 'Funding', 'Future', 'Generations', 'Genetic Transcription', 'Hand', 'Handwriting', 'Health', 'Health Campaign', 'Health Resources', 'Health and Retirement Study', 'Hypertension', 'Image', 'Immigrant', 'Income', 'Individual', 'Inequality', 'Infant', 'Lead', 'Life', 'Link', 'Longevity', 'Machine Learning', 'Maiden Name', 'Malignant Neoplasms', 'Marriage', 'Medicare/Medicaid', 'Metadata', 'Methodology', 'Michigan', 'Minnesota', 'Minority Groups', 'Names', 'Pilot Projects', 'Population', 'Price', 'Process', 'Public Health', 'Recording of previous events', 'Records', 'Research', 'Research Infrastructure', 'Role', 'Sample Size', 'Sampling', 'Sanitation', 'Science', 'Subgroup', 'Surveys', 'Technology', 'Toxin', 'United States', 'Universities', 'Vaccines', 'Water', 'Woman', 'Women&apos', 's Group', 'aging population', 'base', 'cohort', 'cost', 'cost effective', 'data resource', 'early-life nutrition', 'ethnic minority population', 'frontier', 'health knowledge', 'improved', 'indexing', 'innovation', 'intergenerational', 'longitudinal dataset', 'mortality', 'panel study of income dynamics', 'population health', 'population survey', 'programs', 'racial and ethnic', 'response', 'socioeconomics', 'suicide rate']",NIA,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2018,325845,-0.013330769282477914
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9515974,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health care facility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'implementation strategy', 'innovation', 'inpatient service', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2018,871212,-0.024532646534336722
"Biomedical Informatics Section (BIS) We interact with NIDA/IRP investigators to develop biomedical informatics applications and solutions that can access, manage, disseminate, and analyze large quantities of high quality data. We develop, research, and/or apply computational tools to assist in the acquisition and analysis of biological, medical behavioral or health data, within a specific time frame determined to be appropriate by the NIDA. We help facilitate new system initiatives and changes to existing systems to meet legislative, regulatory, and departmental requirements within the specific time frames designated by each requirement. We conduct routine system analysis of automatic data processing resources and techniques of existing projects. We recommend, as needed, the implementation of new technologies that are efficient and timely.  We provide technical and professional support to help integrate computer systems, design or acquire computer programs, configure and support networks and streamline automated data processing within the NIDA's specified timeframe. We also integrate scientific data systems with network services and security services to foster safe and secure NIDA/IRP laboratory collaboration and for collaboration with extramural entities, when possible integrate scientific data systems with one another and ensure design for interoperable data.  We designed and deployed a mobile solution in collaboration with IRP Archway clinic which enables studying craving and mood related to opioid and cocaine use among asymptomatic HCV+ and HCV methadone patients who have not started antiviral treatment. The smartphone-based system is capable of delivering a flexible ecological momentary assessment solution with multi-modal prompting operations as well as having enhanced integrated geolocation recording capabilities dynamically linking craving and mood to the whereabouts of the participants. We also collaborated on a mobile HIV risk reduction intervention to individuals and designed and deployed a video-based smartphone-delivered system combining the capabilities of our mPAL solution with enhanced interactive video-quizzing as an integrated part of our automata-based mobile solution. These systems have been used in multiple protocols and are currently used in studies to better understand why some people are more likely to engage in HIV prevention behaviors.  We automate various aspects of clinical and non-clinical programs at the NIDA/IRP. These include but are not limited to functions related to research participant recruiting, pharmacy, nurses, physicians, and other investigators including those at the Johns Hopkins University School of Medicine. n/a",Biomedical Informatics Section (BIS),9776126,ZIHDA000534,"['AIDS prevention', 'Adherence', 'Antiviral Agents', 'Archives', 'Area', 'Automatic Data Processing', 'Baltimore', 'Behavior', 'Behavioral', 'Big Data', 'Biological', 'Cellular Phone', 'Chemicals', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Research', 'Collaborations', 'Collection', 'Communication', 'Computational Science', 'Computer Security', 'Continuity of Patient Care', 'Data', 'Data Collection', 'Data Quality', 'Databases', 'Development', 'Ecological momentary assessment', 'Enrollment', 'Ensure', 'Environment', 'Exposure to', 'Extramural Activities', 'Fostering', 'Goals', 'Guidelines', 'HIV', 'HIV Seropositivity', 'HIV risk', 'Healthcare', 'Hepatitis C Therapy', 'Hepatitis C virus', 'Illicit Drugs', 'Individual', 'Informatics', 'Information Management', 'Information Resources Management', 'Information Sciences', 'Information Systems', 'Information Technology', 'Intervention', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Laws', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Modality', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Neighborhoods', 'Nurses', 'Opioid user', 'Outcome', 'Participant', 'Patient Recruitments', 'Pharmacy facility', 'Phylogenetic Analysis', 'Physicians', 'Policies', 'Procedures', 'Process', 'Protocols documentation', 'Psychosocial Stress', 'Regulation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Retrieval', 'Risk Behaviors', 'Risk Reduction', 'Science', 'Secure', 'Security', 'Services', 'Site', 'Social Network', 'Specific qualifier value', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Training Support', 'Treatment Protocols', 'Universities', 'Work', 'addiction', 'base', 'biomedical informatics', 'cocaine use', 'computer program', 'computer science', 'computer system design', 'computerized tools', 'craving', 'data mining', 'design', 'disorder later incidence prevention', 'flexibility', 'handheld mobile device', 'health data', 'improved', 'interoperability', 'mHealth', 'medical schools', 'methadone patient', 'mobile computing', 'new technology', 'operation', 'opioid use', 'programs', 'support network', 'transmission process']",NIDA,NATIONAL INSTITUTE ON DRUG ABUSE,ZIH,2018,4173911,-0.02416078032787598
"Innovative text-mining tools to accelerate citation screening: comparative efficiency and impact on conclusions Project Summary Systematic reviews (SRs) synthesize and critically assess bodies of evidence to produce a comprehensive unbiased assessment of what is known. As such, SRs are vital for evidence-based decision-making. However, given the pace of new research, the process of developing an SR remains too slow. One particularly time-consuming step in the process is citation screening, which requires manual review of thousands of abstracts to identify only a small number of relevant studies. Screening such large numbers of studies is necessary because systematic reviewers place a high priority on identifying all relevant studies to avoid bias. Innovative citation screening tools, which utilize text-mining and new sophisticated machine learning methods, represent one potential solution. Abstrackr (Brown University) and EPPI-Reviewer (University College London) are off- the-shelf, web-based citation screening tools designed to improve screening efficiency. Both programs utilize machine- learning techniques to semi-automate the screening process by modeling the probability that each citation will meet criteria for inclusion. This allows efficiency gains through screening prioritization and screening truncation. With screening prioritization, citations are organized for screening from highest to lowest likelihood of inclusion. This allows earlier retrieval of full-text articles and facilitates workflow planning. Organizing citations by likelihood of inclusion also allows reviewers the option of truncating the screening process when remaining citations fall below a certain threshold. While promising, existing studies have predominantly been performed by computer scientists testing individual tools or comparing different modeling algorithms (e.g., various classifiers). To date, no studies have performed a direct comparison of citation screening tools. Similarly, although automatically excluding citations that fall below particular thresholds could substantively improve efficiency, adoption has been low due to concerns that relevant studies could be missed. However, how often studies would be missed and how important such omissions would be remains unknown. To address these knowledge gaps, this project will (1) Compare screening efficiency for two citation-screening tools, Abstrackr and EPPI-reviewer, and (2) Characterize the potential impact of using thresholds to exclude low probability studies automatically. To address aim 1, using citations from 3 large and 6 small completed evidence reports, we will compare Abstrackr to EPPI-Reviewer for citation screening. Using screening prioritization, we will assess what proportion of articles must be screened to identify all included studies (e.g., to achieve 100% sensitivity). For Aim 2, we will explore the potential impact of excluding all citations that fall below particular thresholds during the screening process. We will also assess to what extent missing these studies would alter report conclusions. By characterizing potential efficiency gains from new, innovative, and widely accessible tools, this project can facilitate wider adoption by evidence based practice centers seeking to speed systematic review production. Project Narrative Systematic reviews provide a comprehensive, unbiased assessment of a body of literature and are vital for timely, evidence-based decision-making. However, development of systematic reviews remains too slow, with one particularly time-consuming step being citation screening, in which thousands of research abstracts are manually reviewed to identify a small number of relevant studies. By testing potential gains in citation screening efficiency offered by two innovative, widely-accessible machine- learning tools (Abstrackr and EPPI–Reviewer), and determining if automatically excluding studies to improve speed compromises report conclusions, we hope to enable more rapid production of systematic reviews to inform clinicians and policy-makers and promote high quality, evidence-based patient care.",Innovative text-mining tools to accelerate citation screening: comparative efficiency and impact on conclusions,9435231,R03HS025859,[' '],AHRQ,ECRI INSTITUTE,R03,2018,95324,-0.010699794853434058
"Computational Tools For Bioinformatics And Genome Analysis A crucial component to the recent major advances in genomic research has been the uniting of advances in biology with those in computers, informatics and networking. As technologies have advanced allowing high throughput, Genomics scale data collection, the technological burden has shifted to analysis and informatics. This project was established to ensure that necessary computational tools and resources are available to the NIH intramural community.   OIR's long-term collaboration with Dr. Louis Staudt (Distinguished Investigator, NCI Center for Cancer Research and Director, NCI Center for Cancer Genomics) has yielded significant findings and discoveries that have led to improvements in the treatment of lymphoma.   By providing comprehensive computational expertise, resources, and support, Dr. Staudt's lab has been able to perform sophisticated analyses on large-scale, high-dimensional data which have in turn been instrumental to achieving a number of highly significant findings.  In 2018, OIR made significant contributions to the identification of genetic subtypes of Diffuse Large B-cell Lymphoma (DLBCL) based on patterns of occurrence of mutations and other genetic aberrations.  Predictions from the novel GenClass iterative method were validated using a random forest model.  To distinguish somatic from germline mutations, a random forest model was trained on toxicity assessors (Annovar annotations such as GERP++ and MutationTaster)  on 46 cancer-normal pairs from The Cancer Genome Atlas (TCGA).   This model achieved perfect sensitivity at 90% PPV and has shown applicability to other cancers.   To identify aberrant somatic hypermutations based on the AID sequence motif, we trained two classifiers on a set of 44 genes previously reported to be hypermutated in DLBCL using a set of features deemed characteristic of somatic hypermutations as well as the flanking nucleotide patterns of the mutations.    OIR provides comprehensive computational support to Dr. Staudt's laboratory.  This support entails maintaining databases of genomic data, providing computational servers with custom software for running a variety of analyses, and developing and maintaining public and local-access Web sites.    These supported resources include the following:  - LLMPP/SPECS: The Lymphoma/Leukemia Molecular Profiling Project/Strategic Partnering to Evaluate Cancer Signature (SPECS) program is a multi-institution grant for translational cancer research funded by National Cancer Institute. This website is designed for entering/managing clinical data for cases associated with samples included in the SPECS study. The LLMPP/SPECS project is using microarrays and other high throughput whole genome technologies to define the molecular profiles of all types of human lymphoid malignancies. One primary goal of this project is to redefine the classification of human lymphoid malignancies in molecular terms. A second major goal is to define molecular correlates of clinical parameters that can be used in prognosis and in the selection of appropriate therapy for these patients. As members of the international LLMPP/SPECS consortium, we provide the informatics development and support critical to the success of this project. A database and tools have been implemented to facilitate integrating and analyzing clinical parameters with genomic/genetic data from high throughput technologies. The consortium involves 12 participating centers in 7 countries. Data for 3,000 clinical cases have been uploaded into the system.  - LYMPHCX: A Web site that allows researchers to predict DLBCL subtypes based on samples processed with a Nanostring protocol.  Determination of these subtypes can be critical in deciding appropriate therapy since some subtypes are more aggressive than others.  - LymphoDB: An interactive Web site and database for researchers to search and compare over 1.5 million lymphoma mutations that have been reported in 57 prominent publications.  All mutations have been validated and stored along with relevant annotations and metrics to enable comparative quantitative analyses.  - Signature database: A Web-site companion to Shaffer AL et al. A library of gene expression signatures to illuminate normal and pathological lymphoid biology, Immunol Rev. 2006 Apr;210:67-85.  - Staudt lab analytical test bed: Web site to support quick turn-around of test analytical methods and rapidly allow lab members to more easily explore their own data with new algorithms.  - Database support: OIR maintains information on more than 10 million mutations across over 3,000 clinical samples. Information on digital expression is also stored.   The mAdb (microArray database, https://madb.nci.nih.gov) system provides a secure data management system for gathering, storing, and managing experimental information and expression array data. A variety of Web accessible tools has been implemented to support the multiple analytical approaches needed to decipher array data in a more meaningful way. Important to the mAdb system design is compatibility with any platform (Unix, Windows or Macintosh) capable of running an Internet browser.  A natural extension of mAdb has been the inclusion of additional data resources. This includes supporting information from various data sources (e.g. Gene Ontology, GenBank, Entrez Gene, UniGene, BioSystems Pathways, Biocarta Pathways, COSMIC, and 1000 Genomes) to enable drilling down into the rapidly expanding biological knowledge-base. In order to have effective use of the informational resource developed to support microArray analysis, ongoing user training and support is provided through CIT facilities for this collaborative effort. While ongoing development of new and improved analysis tools continues, the mAdb system is in routine service, having supported over 1900 NIH researchers and collaborators and containing over 111,000 microArray experiments. A critical design element for the mAdb system was to accommodate scalability to allow expansion to support other ICDs. The design allows us to support separate web servers serving different user communities from a single code base. The mAdb system has been set up on separate Web servers to support users of the NIAID microArray core facility. In addition to user-specific, Web-based analyses, our group has facilitated the submission of over 7,000 samples to the NCBI Gene Expression Omnibus (GEO) public repository for required sharing of data associated with publications.    In collaboration with Dr. Timothy Myers of NIAID, OIR also provides comprehensive computational support the Genomic Technologies Section (GTS) of NIAID.   Since GTS provides state-of-the-art bioinformatics support to the   entire NIAID intramural research program, we effectively support all the users of the GTS facility.  In addition to maintaining GTS computational servers and databases, OIR maintains a number of commercial software packages for GTS, including CLC-Genomics and SAS Visual Analyzer.     In 2018, OIR contributed to the development of an improved De Novo assembly procedure for early detection of minor populations of drug-resistant HIV strains.   We developed a statistical test for the identification of variant pairs with unexpectedly high co-occurrence frequencies. n/a",Computational Tools For Bioinformatics And Genome Analysis,9787091,ZIHCT000260,"['Algorithms', 'Beds', 'Bioinformatics', 'Biological', 'Biology', 'CLC Gene', 'Cancer Center', 'Characteristics', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Management', 'Code', 'Collaborations', 'Communities', 'Companions', 'Computer software', 'Computers', 'Core Facility', 'Country', 'Custom', 'Data', 'Data Collection', 'Data Sources', 'Databases', 'Development', 'Discipline', 'Early Diagnosis', 'Elements', 'Ensure', 'Frequencies', 'Funding', 'Genbank', 'Gene Chips', 'Gene Expression', 'Gene Expression Profile', 'Gene Library', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Germ-Line Mutation', 'Goals', 'Grant', 'HIV drug resistance', 'Human', 'Immunoglobulin Somatic Hypermutation', 'Informatics', 'Institution', 'Interdisciplinary Study', 'International', 'Internet', 'Intramural Research Program', 'Laboratories', 'Lymphoid', 'Lymphoma', 'Malignant Neoplasms', 'Malignant lymphoid neoplasm', 'Methods', 'Microarray Analysis', 'Minor', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mutation', 'NCI Center for Cancer Research', 'National Cancer Institute', 'National Institute of Allergy and Infectious Disease', 'Nucleotides', 'Online Systems', 'Ontology', 'Other Genetics', 'Pathologic', 'Pathway interactions', 'Patients', 'Pattern', 'Population', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Science', 'Secure', 'Services', 'Strategic Planning', 'System', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Toxic effect', 'Training', 'Training Support', 'United States National Institutes of Health', 'Variant', 'Visual', 'analytical method', 'base', 'cancer genomics', 'clinical practice', 'comparative', 'computerized tools', 'computing resources', 'data management', 'data resource', 'data sharing', 'design', 'digital', 'experimental study', 'forest', 'genome analysis', 'genomic data', 'high dimensionality', 'high throughput technology', 'improved', 'innovation', 'knowledge base', 'large cell Diffuse non-Hodgkin&apos', 's lymphoma', 'leukemia/lymphoma', 'member', 'nano-string', 'novel', 'outcome forecast', 'programs', 'repository', 'success', 'tool', 'translational cancer research', 'web site', 'web-accessible', 'whole genome']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2018,507974,-0.026556652338462692
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9584612,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Frontal Lobe Epilepsy', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'Informatics', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'phenotypic data', 'rare variant', 'sample collection', 'tool']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2018,75911,-0.06357620097345125
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs.         PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.            ",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9567502,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'data warehouse', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,WASHINGTON STATE UNIVERSITY,U54,2018,1776346,-0.008257527839958572
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs. PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9592655,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'data warehouse', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,WASHINGTON STATE UNIVERSITY,U54,2018,151564,-0.008257527839958572
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs.         PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.            ",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9746827,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'data warehouse', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,WASHINGTON STATE UNIVERSITY,U54,2018,888473,-0.008257527839958572
"Physical modeling of biological systems In collaboration with a Senior Investigator in the Axon Guidance and Neural Connectivity Section, NINDS, and with his post-doctoral fellow, and with members of the Biomedical Imaging Research Services Section of the Office of Intramural Research, CIT, we are studying the mechanisms that guide neurons axons to their ultimate targets. In particular, we are analyzing experiments that record the the growth of a neuron in the wing disc extracted from Drosophlia larvae. During these experiments, we monitor the dynamics of a cytoskeletal protein, actin, at sub-micrometer resolution for periods up to an 1.5 hours in the growing axon as it senses it way through the muscle tissue of the disc. We are also examining the effects on the axon growth dynamics by mutations in a kinase that modulates the actin activity. An article is in preparation for submission to peer review. Furthermore, we are studying new techniques to increase the imaging sampling rate cellular cytoskeletal dynamics while reducing the damage to the cells being observed.  In collaboration with a principal investigator and staff clinician in Section on Clinical Psychoneuroendocrinology and Neuropsychopharmacology, National Institute of Alcohol Abuse and Alcoholism, and with a staff scientist in the Mathematical and Statistical Laboratory, Office of Intramural Research, CIT, we are studying the factors that influence patients with alcohol use disorder (AUD) to seek treatment. This study involves analysis of hundreds of environmental, physiological, and psychiatric factors collected for each patient. A manuscript describing an alternative decision tree for predicting the propensity of AUD patients to seek or not seek treatment (and that could be use by the AUD treatment providers in the field) has been submitted for peer-review. In addition, we are applying methods from information theory to explore the spectrum of these factors in the AUD population.  In collaboration with a consortium of investigators consisting of (1) a principal investigator and staff clinician in Section on Clinical Psychoneuroendocrinology and Neuropsychopharmacology, National Institute of Alcohol Abuse and Alcoholism, (2) a principal investigator and staff clinician in Cognitive Neuroscience and Psychopharmacology Section, Neuroimaging Research Branch, National Institute on Drug Abuse, and (3) a staff scientist in the Mathematical and Statistical Laboratory, Office of Intramural Research, CIT, we are investigating differences in the decision making process between addicts and non-addicts. In particular, we are comparing cigarette smokers, cocaine users, and a set of non-smoking, non-addicted controls to each other. We study their decision making process using a ""trust game"" approach in which players (the subjects) decide whether or not to trust each other or a randomized computer algorithm in dispensing financial gains or penalties. Our preliminary results suggest that cocaine users respond differently from non-using controls and cigarette smokers, especially in the initial stages of the game.  From 2012 to 2017, the LI worked with a principal investigator in Section on Neuronal Connectivity, NICHD, (Chi-hon Lee) on statistical and topological analysis of retinal neurons growth in Drosophila. In FY 2018, Dr. Lee left the NIH to become the director of the Institute of Cellular and Organismic Biology, Academia Sinica, in Taipai, Taiwan. The LI and Dr. Lee, along with (1) a staff scientist in NICHD, and (2) members of the Biomedical Imaging Research Services Section of the Division of Computational Bioscience, CIT, are now finishing up their long-term collaboration. Two papers are being prepared for peer-review. n/a",Physical modeling of biological systems,9773527,ZIACT000276,"['Academia', 'Actins', 'Alcohol abuse', 'Alcohol consumption', 'Alcoholism', 'Axon', 'Biological', 'Biological Sciences', 'Biology', 'Cells', 'Cellular biology', 'Cigarette Smoker', 'Clinical', 'Clinical Research', 'Cocaine Users', 'Collaborations', 'Computational algorithm', 'Cytoskeletal Proteins', 'Decision Making', 'Decision Trees', 'Drosophila genus', 'Goals', 'Growth', 'Health Personnel', 'Hour', 'Image', 'Information Theory', 'Institutes', 'Intramural Research', 'Investigation', 'Laboratories', 'Laboratory Study', 'Larva', 'Left', 'Manuscripts', 'Mathematics', 'Methods', 'Monitor', 'Muscle', 'Mutation', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Neurons', 'Organism', 'Paper', 'Patients', 'Peer Review', 'Phosphotransferases', 'Physiological', 'Population', 'Postdoctoral Fellow', 'Preparation', 'Principal Investigator', 'Process', 'Psychoneuroendocrinology', 'Psychopharmacology', 'Randomized', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Services', 'Taiwan', 'Techniques', 'Trust', 'United States National Institutes of Health', 'Wing', 'Work', 'alcohol use disorder', 'axon growth', 'axon guidance', 'bioimaging', 'biological systems', 'cognitive neuroscience', 'experimental study', 'mathematical model', 'member', 'neuroimaging', 'neuropsychopharmacology', 'non-smoking', 'physical model', 'physical process', 'relating to nervous system', 'retinal neuron', 'simulation']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIA,2018,204471,-0.05338385561415424
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9535429,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,315184,-0.03032356316404862
"Mixed Reality System for STEM Education and the promotion of health-related careers Project Summary/Abstract Proposed is a system to combine and leverage the advantages of existing medical props with interactive media to provide engaging and cooperative group STEM learning experiences. Significance: The PowerPoint lecture style has become the standard method for teaching groups of students. Unfortunately, this style does not emphasize student-instructor or student-student instruction, and in fact seems to have made students even less engaged than before. Broad agreement exists in the field of science education that more engaging pedagogies benefit students in introductory classes. A variety of teaching aids, for example plastic medical props and mannequins are available to support more engaging learning exercises. Despite their substantial benefits, physical props are fundamentally limited as they are primarily static (e.g. fixed coloration, disease depiction), their internal structures (with limited exceptions) often bear little resemblance to actual human anatomy, and they are passive objects. Hypothesis: A system which can provide more engaging interaction with physical props will be able to improve student retention and increase interest in STEM related subjects. Specific Aims: To prove the feasibility of the proposed system in Phase I IDL will 1) Determine stakeholder requirements through round table discussions; 2) Create prototype system hardware & software to augment learning with physical props; and 3) Validate the prototype system through a pilot study. The overall Phase I effort will demonstrate the ability of the proposed system to augment learning with physical props. In the Phase II effort IDL will ready the system for commercialization by 1) Developing production-quality software, hardware, and user interfaces; 2) Developing a set of comprehensive curricula for the system; and 3) Validating the system through human subject testing. Project Narrative Passive learning methods, i.e. PowerPoint lectures, have become the standard method for teaching groups of students topics including Anatomy and Physiology in spite of broad agreement in the field of science education that more engaging pedagogies benefit students in introductory classes. A variety of teaching aids, for example plastic medical props and mannequins are available to support more engaging learning; however, these props are fundamentally limited.",Mixed Reality System for STEM Education and the promotion of health-related careers,9622753,R44GM130247,"['Agreement', 'Algorithmic Software', 'Anatomy', 'Biological', 'Biological Sciences', 'Collaborations', 'Color', 'Computer Vision Systems', 'Computer software', 'Computers', 'Development', 'Disease', 'Disease Progression', 'Dissection', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Environment', 'Exercise', 'Hand', 'Health', 'Health Promotion and Education', 'Hour', 'Human', 'Hybrids', 'Image', 'Instruction', 'Intervention', 'Learning', 'Location', 'Manikins', 'Medical', 'Minnesota', 'Modeling', 'Participant', 'Phase', 'Physiological', 'Physiology', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Role', 'Sampling', 'Science, Technology, Engineering and Mathematics Education', 'Scientist', 'Slide', 'Small Business Innovation Research Grant', 'Structure', 'Students', 'Support Groups', 'System', 'Teaching Method', 'Testing', 'Time', 'Training', 'Universities', 'Ursidae Family', 'Vision', 'animation', 'career', 'college', 'commercialization', 'design', 'digital media', 'experience', 'flexibility', 'guided inquiry', 'hands-on learning', 'human subject', 'improved', 'innovation', 'instructor', 'interactive tool', 'interest', 'learning strategy', 'lectures', 'mid-career faculty', 'pedagogy', 'prototype', 'retention rate', 'science education', 'software systems', 'tool']",NIGMS,"INNOVATIVE DESIGN LABS, INC.",R44,2018,224984,-0.03794394493335142
"Deep Fractions Learning: A Core Curriculum of Games, Inquiry, and Collaboration Project​ ​Summary/Abstract  There​ ​is​ ​an​ ​enormous​ ​deficit​ ​in​ ​students’​ ​understanding​ ​of​ ​fractions​ ​in​ ​the​ ​United​ ​States. Fifth​ ​grade​ ​fraction​ ​knowledge​ ​predicts​ ​high​ ​school​ ​math​ ​performance,​ ​even​ ​when​ ​controlling for​ ​working​ ​memory,​ ​whole​ ​number​ ​knowledge,​ ​IQ,​ ​reading​ ​ability,​ ​and​ ​demographic​ ​factors (Siegler​ ​et​ ​al.,​ ​2012).​ ​Therefore,​ ​addressing​ ​this​ ​deficit​ ​is​ ​a​ ​particularly​ ​important​ ​area​ ​for​ ​early intervention.​ ​With​ ​this​ ​Fast-Track​ ​grant,​ ​​Deep​ ​Fractions​ ​Learning​,​ ​we​ ​propose​ ​to​ ​transform​ ​the way​ ​in​ ​which​ ​students​ ​learn​ ​core​ ​math​ ​curriculum​ ​so​ ​that​ ​materials​ ​are​ ​more​ ​interactive​ ​and engaging,​ ​promote​ ​deeper​ ​learning​ ​of​ ​content,​ ​and​ ​are​ ​aligned​ ​with​ ​the​ ​Common​ ​Core.​ ​More specifically,​ ​we​ ​will​ ​develop​ ​and​ ​evaluate​ ​a​ ​digital​ ​curriculum​ ​for​ ​grades​ ​3-5​ ​covering​ ​the fractions​ ​domain​ ​that​ ​combines​ ​games,​ ​collaboration,​ ​and​ ​an​ ​inquiry​ ​approach.​ ​We​ ​propose​ ​to develop​ ​an​ ​innovative​ ​technology​ ​infrastructure​ ​that​ ​will​ ​integrate​ ​Teachley​ ​learning​ ​games, Success​ ​for​ ​All’s​ ​(SFA)​ ​cooperative​ ​learning​ ​framework,​ ​and​ ​rigorous​ ​lesson​ ​content.​ ​We​ ​will integrate​ ​research​ ​into​ ​the​ ​design​ ​process​ ​and​ ​work​ ​with​ ​Johns​ ​Hopkins​ ​University​ ​to​ ​evaluate the​ ​efficacy​ ​of​ ​the​ ​intervention.  Outcomes.​ ​​The​ ​intervention​ ​will​ ​encourage​ ​four​ ​direct​ ​outcomes​ ​for​ ​students,​ ​namely improved:​ ​1)​ ​conceptual​ ​understanding​ ​of​ ​fractions,​ ​2)​ ​procedural​ ​fluency​ ​with​ ​fractions operations,​ ​3)​ ​mathematical​ ​justification,​ ​and​ ​4)​ ​motivation.​ ​First,​ ​the​ ​curriculum​ ​will​ ​build​ ​both conceptual​ ​understanding​ ​and​ ​procedural​ ​fluency,​ ​providing​ ​strong​ ​visual​ ​models​ ​within engaging​ ​games​ ​that​ ​motivate​ ​students​ ​to​ ​practice.​ ​The​ ​collaborative​ ​learning​ ​model​ ​and​ ​inquiry approach​​ ​​will​ ​improve​ ​students’​ ​mathematical​ ​justification.​ ​Finally,​ ​we​ ​encourage​ ​these outcomes​ ​within​ ​a​ ​motivational​ ​support​ ​structure​ ​designed​ ​to​ ​foster​ ​engagement​ ​and self-efficacy.  Improving​ ​students’​ ​academic​ ​outcomes​ ​and​ ​self-efficacy​ ​in​ ​the​ ​area​ ​of​ ​fractions​ ​during elementary​ ​school​ ​will​ ​promote​ ​later​ ​success​ ​in​ ​high​ ​school​ ​mathematics.​ ​Since​ ​each​ ​additional math​ ​class​ ​students​ ​complete​ ​in​ ​high​ ​school​ ​more​ ​than​ ​doubles​ ​the​ ​odds​ ​of​ ​college​ ​completion (Adelman,​ ​2006),​ ​the​ ​intervention​ ​has​ ​the​ ​potential​ ​to​ ​make​ ​a​ ​real​ ​difference​ ​in​ ​whether students​ ​achieve​ ​sustainable​ ​careers​ ​versus​ ​being​ ​stuck​ ​in​ ​low-wage​ ​jobs. Project​ ​Narrative  Fractions​ ​knowledge​ ​in​ ​the​ ​fifth​ ​grade​ ​strongly​ ​predicts​ ​high​ ​school​ ​math​ ​performance, even​ ​when​ ​controlling​ ​for​ ​working​ ​memory,​ ​whole​ ​number​ ​knowledge,​ ​IQ,​ ​reading​ ​ability,​ ​and demographic​ ​factors​ ​(Siegler​ ​et​ ​al.,​ ​2012).​ ​Intervention​ ​in​ ​this​ ​essential​ ​content​ ​area​ ​will improve​ ​students’​ ​math​ ​ability​ ​in​ ​the​ ​short​ ​and​ ​long​ ​term,​ ​which​ ​in​ ​turn​ ​will​ ​lead​ ​to​ ​several positive​ ​distal​ ​outcomes,​ ​such​ ​as​ ​greater​ ​high​ ​school​ ​graduation​ ​rates​ ​and​ ​college​ ​attendance.","Deep Fractions Learning: A Core Curriculum of Games, Inquiry, and Collaboration",9617983,R44GM130162,"['Active Learning', 'Address', 'Area', 'Behavior', 'Child', 'Childhood Cancer Survivor Study', 'Collaborations', 'Common Core', 'Control Groups', 'Demographic Factors', 'Distal', 'Early Intervention', 'Educational Curriculum', 'Ethnic Origin', 'Fostering', 'Goals', 'Graduation Rates', 'Grant', 'High School Student', 'Instruction', 'Intervention', 'Investments', 'Knowledge', 'Learning', 'Maps', 'Mathematics', 'Mathematics Curriculum', 'Measures', 'Modeling', 'Motivation', 'Occupations', 'Online Systems', 'Outcome', 'Performance', 'Phase', 'Privatization', 'Process', 'Research', 'Research Infrastructure', 'Sampling', 'Self Efficacy', 'Services', 'Short-Term Memory', 'Structure', 'Students', 'Treatment Efficacy', 'United States', 'Universities', 'Visual', 'Wages', 'Work', 'boys', 'career', 'college', 'dashboard', 'deep learning', 'design', 'digital', 'elementary school', 'fifth grade', 'fourth grade', 'girls', 'high school', 'improved', 'innovation', 'innovative technologies', 'mathematical ability', 'operation', 'prototype', 'reading ability', 'success', 'teacher', 'third grade', 'usability']",NIGMS,"TEACHLEY, LLC",R44,2018,149650,-0.0022662369792503627
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9491866,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genetic Diseases', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health Care Costs', 'Health system', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Care Costs', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'care costs', 'clinical care', 'clinical decision support', 'clinical practice', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'patient portal', 'polyposis', 'practice setting', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2018,840228,-0.012064988448611277
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9593059,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Simulation', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data management', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2018,750000,-0.018815596009635928
"Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND) Project Summary/Abstract  This Phase III (P-III) COBRE project will extend the cores that have been successfully leveraged in our Phase I (P-I) and Phase II (P-II) COBRE projects and sustain these unique resources in New Mexico through the im- plementation of a business plan. Over the past eight years we have built up infrastructure and created a cutting edge brain imaging center, our P-II project is just over half-way through and is even more successful than our P- I was at this point in time. The Mind Research Network (MRN) houses an Elekta Neuromag 306-channel MEG System, a high density EEG lab, a 3T Siemens Trio MRI scanner, and a mobile 1.5T Siemens Avanto MRI scanner. Additional resources include a centralized neuroinformatics system, a strong IT management plan, and state-of-the-art image analysis expertise and tools. This P-III COBRE center will continue our momentum and move the cores we have developed into a position of long term sustainability. We will continue with the technical cores established during the P-II project including multimodal data acquisition (MDA), algorithm and data analy- sis (ADA), and biostatistics and neuro-informatics (BNI). These cores have begun to serve MRN and the greater community, as well as other institutions including extensive collaborations with IDeA funded projects in New Mexico and other states. We believe this P-III COBRE is extremely well-positioned to establish and sustain New Mexico as one of the premier brain imaging sites. We include an extensive pilot project program (PPP) that is built on the successful pilot programs implemented as part of the earlier COBRE phases. This includes an ex- tensive educational, mentoring, and faculty development program to carefully mentor and position faculty who use the cores to maximize their potential to successfully compete for external funding, thus fulfilling the ultimate goals of the COBRE program. 2 Narrative  This Phase III COBRE project is a natural extension of our Phase I and II COBRE projects which were cen- tered on mentoring individual researchers along with building the necessary infrastructure to support multimodal neuroimaging in mental illness. During this time, cutting-edge cores were developed that facilitated not only our local projects but also research at multiple institutions across New Mexico; the cores served as neuroimaging facilities and training centers for others to utilize. The Phase III project will ensure the sustainability of these cores as they transition to being fully funded by a broad cadre of users with various funding sources. We propose three technical cores including a multimodal data acquisition (MDA) core, an algorithm and data analysis (ADA) core, and a biostatistics and neuro-informatics (BNI) core. These cores have already shown their utility and have begun to be leveraged by users outside the COBRE. In addition, we propose a robust pilot project program (PPP) to continue to seed and enable new users of the cores to ultimately grow and sustain world class brain imaging research within our IDeA state, thus fulfilling the ultimate goals of the COBRE program. 1",Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND),9281577,P30GM122734,"['Algorithmic Analysis', 'Appointment', 'Area', 'Awareness', 'Biology', 'Biometry', 'Bipolar Depression', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Businesses', 'Centers of Research Excellence', 'Chemistry', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Core Facility', 'Data', 'Data Analyses', 'Department of Energy', 'Development', 'Devices', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Disease', 'Electroencephalography', 'Engineering', 'Ensure', 'Environment', 'Equipment', 'Faculty', 'Functional Magnetic Resonance Imaging', 'Funding', 'Funding Agency', 'Genetic', 'Goals', 'Grant', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Institution', 'Interdisciplinary Study', 'Leadership', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Magnetoencephalography', 'Major Depressive Disorder', 'Mental Depression', 'Mental disorders', 'Mentors', 'Methods', 'Mind-Body Method', 'Mission', 'Multimodal Imaging', 'Neurobiology', 'Neurologic', 'Neurosciences', 'New Mexico', 'Paper', 'Patients', 'Peer Review', 'Phase', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Program Development', 'Psychiatry', 'Publications', 'Recording of previous events', 'Research', 'Research Domain Criteria', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Schizophrenia', 'Seeds', 'Site', 'Structure', 'System', 'Teacher Professional Development', 'Time', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Vision', 'base', 'cohesion', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'deep learning', 'density', 'design', 'distinguished professor', 'improved', 'independent component analysis', 'meetings', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'neuropsychiatric disorder', 'programs', 'tool']",NIGMS,THE MIND RESEARCH NETWORK,P30,2018,1320387,-0.028673294090576732
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9403171,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Learning', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,548068,-0.032639617800900446
"Natural language processing for precision medicine and clinical and consumer health question The Repository for Informed Decision Making, Clinical Question Answering and Consumer Health Question Answering projects are addressing the above objectives by developing knowledge-based and machine learning approaches to extraction and structuring of information in biomedical literature and other types of text (such as clinical notes and registered clinical trials) for the following types of information: 1) the diseases and conditions; 2) the numbers, co-morbidities, and socio-demographic characteristics of study subjects/participants, such as species, gender, smoking status and alcohol consumption; 3) the therapeutic and diagnostic interventions; 4) the study and publication types; 5) the end-points and the outcomes of the studies; 6) drug interactions and 7) adverse drug reactions.  In FY2017, we have developed a number of approaches to facilitate understanding information requests sent to NLM customer services and long queries submitted to MedlinePlus search engine. Information requests sent to customer services are often several paragraphs long and provide the background and context that the customers believe will help understand their needs. For example, customers often describe several generations of their families affected by a disease and ask if their children will have it. The long MedlinePlus queries consist of one or two sentences and are often formed as questions. Both of these request forms are usually ungrammatical and rife with misspellings, abbreviations and informal language. We have developed a spellchecker for consumer language that is performing adequately on the misspellings important to understanding of the needs. After correcting spelling, our system employs three modules: a knowledge-based and a supervised machine learning method to understand the main points of the request, such as the disease or a drug of interest and the type of information about it. The systems extract the main points, which we found are sufficient to automatically search MedlinePlus and find authoritative and relevant pages for 65% of the requests. The third approach is to find similar questions that already have authoritative answers, e.g., provided by NIH institutes.  Our clinical question answering system is based on the framework for asking well-formed questions developed by the evidence-based medicine experts. Their analysis showed that presenting a clinical information need as four-part question frame: patient characteristics/problem; planned intervention; comparison; and desired outcome, helps formulate search engine queries that lead to relevant results. We developed methods for automatic extraction of question frames from information requests, automatic query formulation and automatic extraction of answers from retrieval results. The LHC CQA1.0 system extracts the bottom-line advice from biomedical publications and aligns the question frames and the answers to find the best answer. The CQA 1.0 system is currently used to support development of evidence-based care plans at the NIH Clinical Center, to provide bottom-line for retrieved images in the LHC Open-i system and to provide summaries of the biomedical articles in the LHC Open Summarizer. n/a",Natural language processing for precision medicine and clinical and consumer health question,9554461,ZIALM010009,"['Abbreviations', 'Address', 'Affect', 'Alcohol consumption', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Clinical Trials', 'Comorbidity', 'Decision Making', 'Development', 'Diagnostic', 'Disease', 'Drug Interactions', 'Evidence Based Medicine', 'Family', 'Formulation', 'Gender', 'General Population', 'Generations', 'Growth', 'Health', 'Image', 'Institutes', 'Intervention', 'Language', 'Lead', 'Literature', 'Machine Learning', 'MedlinePlus', 'Methods', 'Natural Language Processing', 'Outcome', 'Outcome Study', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Publications', 'Reaction', 'Resources', 'Retrieval', 'Review Literature', 'Role', 'Services', 'Smoking Status', 'Source', 'Structure', 'Study Subject', 'Supervision', 'System', 'Text', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Update', 'base', 'evidence base', 'interest', 'knowledge base', 'learning strategy', 'precision medicine', 'repository', 'search engine', 'spelling', 'study characteristics', 'systematic review']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2017,436325,-0.025345014035338634
"Deep learning for representation of codes used for SEER-Medicare claims research ﻿    DESCRIPTION (provided by applicant):  We propose developing an algorithm and user-friendly software to better identify treatments using Medicare claims data. We will validate our approach using procedures listed in the Surveillance, Epidemiology, and End Results (SEER) database as a gold standard. In this way, we hope to better match procedures identified using Medicare claims data with SEER listed procedures.  The focus of this research is observational (i.e. non-randomized) data. Well-run randomized clinical trials can provide the best level of evidence of treatment effects. However, randomized trials in the United States have suffered from poor accrual for many interventions. Despite the fact that well-designed randomized clinical trials should be the gold standard, well-designed observational studies might be the only method of obtaining inferences concerning comparative effectiveness for some cancer interventions.  In cancer research, one of the most commonly used databases for observational research is the linked SEER-Medicare database. SEER-Medicare data has provided useful measurements of the effectiveness of a number of cancer therapies. Algorithms for identifying relevant treatment and diagnosis codes using Medicare data are often based on clinical reasoning and scientific evidence. One group of researchers, for example, developed an algorithm for identifying laparoscopic surgery among kidney cancer cases before claims codes for laparoscopic surgery were well developed. While such algorithms are useful for others pursuing similar investigations, there may still be substantial mismatch between treatment identified by the SEER cancer registry and treatment identified through Medicare claims. In this work, we propose developing a rigorous machine learning algorithm that can help researchers in better identifying treatments in Medicare claims data. Specifically, we will design a neural language modeling algorithm and implement a software system that finds vector representations of diagnosis and procedure codes.  We plan on using the neural language modeling algorithm to learn vector representations from SEER- Medicare claims data where related procedure and diagnosis codes are ""neighbors"" (i.e. closely related). We will investigate whether the codes we identify within neighborhoods correspond to the procedure codes used for published SEER-Medicare studies. We will then design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. Finally, we will investigate the sensitivity and specificity of the algorithm by comparing procedures identified using Medicare claims with procedures listed in the SEER database. We will replicate analyses from a published SEER-Medicare paper to investigate if estimated treatment effects differ when using our novel algorithm compared to using the algorithm in the published paper. PUBLIC HEALTH RELEVANCE: In cancer research, one of the most commonly used databases for observational research is the linked Surveillance, Epidemiology, and End Results (SEER)-Medicare database. To improve the identification of procedures when using Medicare claims data, we will design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. This should improve the identification of procedures when using Medicare claims data, and make conclusions drawn from analyses using the database more reliable and consistent.",Deep learning for representation of codes used for SEER-Medicare claims research,9188540,R21CA202130,"['Algorithms', 'Cancer Intervention', 'Clinical', 'Code', 'Computer software', 'Data', 'Databases', 'Diagnosis', 'Effectiveness', 'Ethical Issues', 'Funding', 'Future', 'Gold', 'International Classification of Diseases', 'Intervention', 'Investigation', 'Investigational Therapies', 'Language', 'Laparoscopic Surgical Procedures', 'Learning', 'Level of Evidence', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Medical Records', 'Medicare', 'Medicare claim', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhoods', 'Observational Study', 'Outcome', 'Paper', 'Patients', 'Procedures', 'Process', 'Proxy', 'Publishing', 'Randomized Clinical Trials', 'Records', 'Renal carcinoma', 'Research', 'Research Personnel', 'Running', 'SEER Program', 'Seeds', 'Sensitivity and Specificity', 'Software Tools', 'Statistical Study', 'Terminology', 'Testing', 'Time', 'United States', 'Update', 'Work', 'anticancer research', 'base', 'cancer therapy', 'comparative effectiveness', 'design', 'health disparity', 'improved', 'interest', 'malignant breast neoplasm', 'neoplasm registry', 'novel', 'public health relevance', 'randomized trial', 'relating to nervous system', 'software systems', 'treatment effect', 'usability', 'user friendly software', 'vector', 'volunteer']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R21,2017,219753,-0.023167642618510127
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,-0.02244327918909253
"Informatics, Machine Learning & Biomedical Data Science Over the past year, we have been active in: (1) developing computationally efficient methods and algorithms to solve known problems in the analysis of biomedical and clinical data and study complex interactions in biological systems; (2) developing knowledge-based data management systems for the discovery and curation of biomedical knowledge, including distributed annotation systems and clinical information management systems;  (3) applying predictive-analytic models to scientific and administrative domains; and (4) consulting with NIH leadership to provide evidence-based solutions to improve the grant application and review process.  Specifically, in 2017, collaborative efforts in support of these goals included the following:  -In a partnership with Dr. John Tsang of the NIAID Laboratory of Systems Biology, HPCIO is conducting a multifaceted project to profile the immune system using the latest high-throughput, multiplexed technologies and systems approaches. One of the goals of this collaboration is to develop novel computational methodologies that can exploit inter-subject heterogeneity and measurements at various scales to assess the roles of the immune system in health and disease. We have collected samples from a large cohort of patients with immune-mediated monogenic diseases and are the in process of deeply phenotyping blood samples of these patients.  By studying the immune system of multiple monogenic, immune-mediated diseases, we will have the opportunities to infer cellular and molecular networks of the human immune system.  HPCIO is actively involved in the development of a database to record clinical information of patient visits and in the bioinformatics analyses of data generated from the project.  - HPCIO is working with NCI Occupational & Environmental Epidemiology Branch to develop methodologies to incorporate occupational risk factors into epidemiological models. We are enlarging the training data to improve our novel classifiers for coding free text job descriptions into the 840 codes of the 2010 U.S. Standard Occupational Classification System. Agreement between our classification system and expert coders is measured using SOC code agreement and exposure prediction from CANJEM, a job-exposure matrix of over 250 exposure agents developed by Jerome Lavoue at the University of Montreal.  We are also working with NCI to develop a two-stage mixed generalized linear model to predict lifetime occupation exposures to lead.  - In collaboration with the Membrane Transport Biophysics Section of NINDS, HPCIO is 1) developing  a computational tool to accurately identify the boundaries of the lysosomes in fluorescence microscopy and 2) using the fluorescence ration to measure lysosomal pH within each organelle for better understanding of the lysosomal pH regulation.    - A freely available plasmid database that is inter-operable with popular freeware is currently being developed for the NIDA Optogenetics and Transgenic Technology Core. The Plasmid Manager offers a versatile yet simple platform for scientists to store and analyze their plasmid data. Motivated by the need for a more comprehensive approach to archiving plasmid data, the database platform is enriched with numerous components beyond the repository, serving as an informatics platform designed to enhance the efficiency and analytic capabilities of scientists.   - As high-throughput next-generation sequencing (NGS) technology plays an important role in systematically identifying novel cancer driver mutations in genome-wide surveys, NGS data generation is rapidly increasing, currently accumulating at a rate of several terabytes of data every month at the Lymphoid Malignancies Section of NCI. In collaboration with the Louis Staudt Laboratory, a bioinformatics website is being developed containing useful tools for the analysis of the laboratory's Diffuse Large B-Cell Lymphoma data. This website enables users with very little computer expertise to run their own analyses, as opposed to having a specialist run the analyses for them. Methodologies in parallelization and text searching have also been incorporated for returning the analysis results much more quickly and efficiently than before.  In 2017, a new dimension to this collaboration was the development of machine learning methods to identify somatic from germline mutations from NGS sequencing data.  Machine learning models have also been tested to identify subtypes of diffuse large B-cell lymphoma, based on their features of gene aberrations.  - In collaboration with NIA and NCI, we are applying machine learning and visualization techniques on large biological datasets to discover novel patterns of functional gene or protein interactions as related to aging. In this collaboration, we are developing a machine learning method that models the temporal nature of the longitudinal clinical data to predict the progression of Amyotrophic lateral sclerosis. Such machine learning method may also work well in prediction of high-dimensional time-series genomic data.  - The Human Salivary Proteome Wiki is a community-driven Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. Many features and external contents have been incorporated over the last few years to make it easier for users to extract different kinds of information from the wiki.  One of the latest enhancements is the integration of RNA-seq transcriptional and protein immunohistochemistry data from the Human Protein Atlas.  This affords users the ability to weigh evidence generated by different, independent modalities, in addition to the original mass-spectrometry-based data, to assess the status of a protein.    - In collaboration with CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process. A Web-based automated referral tool, called ART, was developed and deployed to help PIs and SROs to identify the most relevant study section(s) or special emphasis panel(s) based on the scientific content of an application. In addition, HPCIO is analyzing text from quick feedback surveys on peer review. HPCIO has developed a system to capture the sentiment of reviewer comments in quick feedback surveys and classify these comments with sentiment score into broad categories.   Progress has been made to identify needs and suggestions offered by the reviewers and to assign topic labels for these needs and suggestions.  In 2017, HPCIO began to explore appropriate topological network mapping diagrams of CSR study sections, superimposed with measures of scientific productivity for those study sections.  - In collaboration with the Office of Data Analysis Tools and Systems, NIH Office of Extramural Research, HPCIO has been developing a standard database update pipeline for NIH Topic Maps, originally developed by Dr. Ned Talley of NINDS.   This effort was concluded in 2017.  - In collaboration with NIAID, HPCIO has supported its release HT JoinSolver(R), a new application capable of analyzing V(D)J recombination in thousands of immunoglobulin gene sequences produced by high throughput sequencing. n/a","Informatics, Machine Learning & Biomedical Data Science",9550738,ZIHCT000200,"['Aging', 'Agreement', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Applications Grants', 'Archives', 'Atlases', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biophysics', 'Blood specimen', 'Categories', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Linguistics', 'Computers', 'Computing Methodologies', 'Consult', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Database Management Systems', 'Databases', 'Development', 'Dimensions', 'Disease', 'Environmental Epidemiology', 'Evaluation', 'Exposure to', 'Extramural Activities', 'Feedback', 'Fluorescence', 'Fluorescence Microscopy', 'Fostering', 'Funding', 'Generations', 'Genes', 'Genetic Transcription', 'Genomics', 'Germ-Line Mutation', 'Goals', 'Grant Review Process', 'Harvest', 'Health', 'Heterogeneity', 'High Performance Computing', 'High-Throughput Nucleotide Sequencing', 'Human', 'Imagery', 'Immune', 'Immune system', 'Immunoglobulin Genes', 'Immunohistochemistry', 'Informatics', 'Interdisciplinary Study', 'Intramural Research Program', 'Job Description', 'Knowledge', 'Label', 'Laboratories', 'Lead', 'Leadership', 'Linear Models', 'Lysosomes', 'Machine Learning', 'Malignant Neoplasms', 'Malignant lymphoid neoplasm', 'Management Information Systems', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Mediating', 'Mendelian disorder', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Molecular', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Nature', 'Occupational', 'Occupational Epidemiology', 'Occupations', 'Online Systems', 'Organelles', 'Patients', 'Pattern', 'Peer Review', 'Phenotype', 'Plasmids', 'Play', 'Predictive Analytics', 'Process', 'Productivity', 'Proteins', 'Proteome', 'Proteomics', 'Published Comment', 'Regulation', 'Research', 'Research Personnel', 'Research Support', 'Risk Factors', 'Role', 'Running', 'Salivary', 'Sampling', 'Scientist', 'Series', 'Specialist', 'Strategic Planning', 'Study Section', 'Suggestion', 'Surveys', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Transgenic Organisms', 'Translational Research', 'Transmembrane Transport', 'United States National Institutes of Health', 'Universities', 'Update', 'V(D)J Recombination', 'Visit', 'Visualization software', 'Work', 'actionable mutation', 'annotation  system', 'base', 'bioimaging', 'biological systems', 'biomedical informatics', 'cohort', 'computerized tools', 'data mining', 'design', 'epidemiological model', 'evidence base', 'genome-wide', 'genomic data', 'high dimensionality', 'human data', 'improved', 'information organization', 'innovation', 'interdisciplinary approach', 'interoperability', 'knowledge base', 'large cell Diffuse non-Hodgkin&apos', 's lymphoma', 'learning strategy', 'next generation sequencing', 'novel', 'optogenetics', 'parallelization', 'programs', 'repository', 'research and development', 'software development', 'terabyte', 'text searching', 'tool', 'transcriptome sequencing', 'web portal', 'web site', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2017,2619784,-0.02139363202420552
"IGF::OT::IGF Semantic Bibliometric System for Improving Healthcare Research (Topic 162) (Period of Performance: September 15, 2017 - March 14, 2018). BASE AWARD N43DA-17-1217 BCL will expand the current bibliometric methods by developing a Semantic Bibliometric System using machine learning that will examine research publications, rank the publications by quality, and identify research-productive scientific teams. Used in conjunction with current methods this Semantic Bibliometric System will have the dual use of improving the impact of Government Research and improving semantic search on the web and in ecommerce. n/a","IGF::OT::IGF Semantic Bibliometric System for Improving Healthcare Research (Topic 162) (Period of Performance: September 15, 2017 - March 14, 2018). BASE AWARD N43DA-17-1217",9576638,71201700054C,"['Award', 'BCL1 Oncogene', 'Bibliometrics', 'Data', 'Effectiveness', 'Evaluation', 'Feasibility Studies', 'Government', 'Health Care Research', 'Internet', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Performance', 'Procedures', 'Publications', 'Research', 'Semantics', 'System', 'Text', 'improved']",NIDA,"BCL TECHNOLOGIES, INC.",N43,2017,225000,-0.013693341152748187
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,-0.019937461886953247
"From genomics to natural language processing: A protected environment for research computing in the health science NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Health sciences researchers are often required to manage, mine, and analyze restricted patient data (Protected Health Information, PHI) to facilitate and advance their research aims. They are often required to do this without access to central information technology expertise or resources to facilitate their research aims. These researchers are often left to their own devices to “solve” their research compute and data needs and are challenged due to lack of available resources, barriers from central IT, and/or lack of knowledge of available resources. A further challenge is that “small” data sets— data that researchers could formerly handle on office resources—have morphed and grown into the big data domain through the explosion of technical advances and significant expansion in various research directions. Examples include: genomics research, image analysis, simulation, natural language processing, and mining of EMRs. Therefore, the need exists to develop a framework for managing and processing this data securely and reliably. This S10 equipment proposal is to replace the “protected environment” (PE) prototype the University of Utah’s Center for High Performance Computing (CHPC) and Department of Biomedical Informatics built six years ago and has operated since. The PE consists of both high performance computing and virtual machine (VM) components and associated storage sufficient to manage, protect and analyze HIPAA protected health information. This environment has been very successful and has grown significantly in scope. CHPC isolated this protected environment in the secured University of Utah Downtown Data Center and setup a network protected logical partition that provided research groups specific access to individual data sets. As the environment and technology developed, CHPC added additional security features such as two-factor authentication for entry and audit/monitoring. Unfortunately, the prototype has reached the point where demand is surpassing capability and all the hardware is aged and off-warranty. To give an idea of users of the virtual machine farm component, the Biomedical Informatics Core (BMIC) REDCap (Research Electronic Data Capture) environment for data collection has over 2,500 users in 1,500 projects supporting over $25M in NIH funding at the University of Utah, including support for more than 25 active NIH R-01 grants. Moreover, the HIPAA compliant protected environment was a key factor that aided passing the recent University of Utah HIPAA audit. The “protected environment” also helped the University of Utah Health Sciences Center and the BMIC justify the NCATS Center Clinical and Translational Science award (1ULTR001067). NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Project Narrative: The proposed “Protected Environment” instrument will provide research computing and data management capabilities for health sciences researchers to properly manage, secure, and analyze HIPAA regulated protected health information. The technology will not only support a large number of clinical trials, but also enable research in Human Genetics and Natural Language Processing of electronic health records.",From genomics to natural language processing: A protected environment for research computing in the health science,9274445,S10OD021644,"['Award', 'Big Data', 'Clinical Sciences', 'Data', 'Data Collection', 'Data Set', 'Devices', 'Environment', 'Equipment', 'Explosion', 'Farming environment', 'Funding', 'Genomics', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'High Performance Computing', 'Image Analysis', 'Individual', 'Information Technology', 'Knowledge', 'Left', 'Mining', 'Monitor', 'Natural Language Processing', 'Patients', 'Research', 'Research Personnel', 'Resources', 'Secure', 'Security', 'Technology', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Utah', 'aged', 'biomedical informatics', 'computerized data processing', 'electronic data', 'prototype', 'simulation', 'virtual']",OD,UNIVERSITY OF UTAH,S10,2017,493595,-0.024868565903730648
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9282279,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Custom', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Source', 'Standardization', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2017,455519,-0.02484614453262159
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9310440,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2017,599995,0.006469984356060583
"Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM Project Summary Efforts to integrate contact network data with molecular surveillance data provide enormous promise for HIV tracking and intervention. However, the lack of tools to facilitate integrated molecular-social surveillance remains a substantial barrier to progress. For example, most contact network data only contains information on the immediate sexual and drug use partners of a single individual. Yet, the same partners can appear across the contact networks of multiple individuals. Therefore, partners must be matched across contact networks - a process called entity resolution (ER) - in order to provide an accurate view of the overall contact network structure. The process of ER currently requires either substantial resources to manually match individuals or considerable technological expertise in programming to more efficiently match individuals using probabilistic models. Accordingly, this project will 1) develop a machine learning algorithm to match individuals across personal contact networks and validate it using a large existing dataset of young men who have sex with men, and 2) create a graphical user interface to implement the algorithm as an add-on package to an existing tool for network data capture and processing (Network Canvas). The results of this project will provide an open- source and freely available tool that can drastically reduce barriers to matching individuals across contact networks, thereby providing researchers and public health officials with unencumbered access to the underlying structure of drug use and sexual networks, and a potent tool for integrating contact network data with molecular surveillance. Project Narrative Developing an accurate picture of the drug use and sexual contact networks of men who have sex with men (MSM) and other high risk populations can revolutionize the way in which HIV spread is tracked and intervened upon by public health practitioners. However, these data are usually limited to personal networks, which only include immediate sex and drug use partners. The current project will employ state-of-the-art methods in machine learning to improve the accuracy of matching individuals across personal network data and will integrate this tool into a user-friendly graphical user interface and data management tool, thereby substantially reducing barriers to studying contact networks and providing a potent tool for HIV surveillance and interventions.",Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM,9348928,R21LM012578,"['AIDS prevention', 'Age', 'Algorithms', 'Area', 'Behavioral', 'Big Data', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Drug usage', 'Epidemiology', 'Foundations', 'Funding', 'Future', 'Goals', 'Graph', 'HIV', 'Human', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Intervention Studies', 'Interview', 'Intuition', 'Investigation', 'Knowledge', 'Link', 'Location', 'Logic', 'Machine Learning', 'Manuals', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Epidemiology', 'Names', 'National Institute of Drug Abuse', 'Nature', 'Output', 'Pathway Analysis', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Prevention Research', 'Process', 'Public Health', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Structure', 'Subgroup', 'Surveys', 'System', 'Testing', 'Time', 'Training', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'computer human interaction', 'computer science', 'data management', 'disease transmission', 'drug structure', 'epidemiologic data', 'experience', 'graphical user interface', 'high risk population', 'improved', 'innovation', 'men who have sex with men', 'novel', 'open source', 'outreach', 'response', 'sex', 'social', 'success', 'surveillance data', 'tool', 'touchscreen', 'transmission process', 'usability', 'user-friendly', 'young men who have sex with men']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2017,118784,-0.04944957280192253
"Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM Project Summary Efforts to integrate contact network data with molecular surveillance data provide enormous promise for HIV tracking and intervention. However, the lack of tools to facilitate integrated molecular-social surveillance remains a substantial barrier to progress. For example, most contact network data only contains information on the immediate sexual and drug use partners of a single individual. Yet, the same partners can appear across the contact networks of multiple individuals. Therefore, partners must be matched across contact networks - a process called entity resolution (ER) - in order to provide an accurate view of the overall contact network structure. The process of ER currently requires either substantial resources to manually match individuals or considerable technological expertise in programming to more efficiently match individuals using probabilistic models. Accordingly, this project will 1) develop a machine learning algorithm to match individuals across personal contact networks and validate it using a large existing dataset of young men who have sex with men, and 2) create a graphical user interface to implement the algorithm as an add-on package to an existing tool for network data capture and processing (Network Canvas). The results of this project will provide an open- source and freely available tool that can drastically reduce barriers to matching individuals across contact networks, thereby providing researchers and public health officials with unencumbered access to the underlying structure of drug use and sexual networks, and a potent tool for integrating contact network data with molecular surveillance. Project Narrative Developing an accurate picture of the drug use and sexual contact networks of men who have sex with men (MSM) and other high risk populations can revolutionize the way in which HIV spread is tracked and intervened upon by public health practitioners. However, these data are usually limited to personal networks, which only include immediate sex and drug use partners. The current project will employ state-of-the-art methods in machine learning to improve the accuracy of matching individuals across personal network data and will integrate this tool into a user-friendly graphical user interface and data management tool, thereby substantially reducing barriers to studying contact networks and providing a potent tool for HIV surveillance and interventions.",Implementing and Evaluating a Machine Learning Tool for Entity Resolution in Drug Use and Sexual Contact Networks of YMSM,9348928,R21LM012578,"['AIDS prevention', 'Age', 'Algorithms', 'Area', 'Behavioral', 'Big Data', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Development', 'Drug usage', 'Epidemiology', 'Foundations', 'Funding', 'Future', 'Goals', 'Graph', 'HIV', 'Human', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Intervention Studies', 'Interview', 'Intuition', 'Investigation', 'Knowledge', 'Link', 'Location', 'Logic', 'Machine Learning', 'Manuals', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Epidemiology', 'Names', 'National Institute of Drug Abuse', 'Nature', 'Output', 'Pathway Analysis', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Prevention Research', 'Process', 'Public Health', 'Race', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Structure', 'Subgroup', 'Surveys', 'System', 'Testing', 'Time', 'Training', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'computer human interaction', 'computer science', 'data management', 'disease transmission', 'drug structure', 'epidemiologic data', 'experience', 'graphical user interface', 'high risk population', 'improved', 'innovation', 'men who have sex with men', 'novel', 'open source', 'outreach', 'response', 'sex', 'social', 'success', 'surveillance data', 'tool', 'touchscreen', 'transmission process', 'usability', 'user-friendly', 'young men who have sex with men']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2017,100318,-0.04944957280192253
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility. PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9270528,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'Supervision', 'System', 'Time', 'Training', 'clinical predictors', 'cohort', 'computer science', 'crowdsourcing', 'data sharing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2017,316000,-0.016153673937278564
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9384200,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2017,685783,-0.008547886947396952
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,0.006154026068678719
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9209155,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2017,1354554,-0.02546562253978306
"Policy Surveillance: the Future of Scientific Legal Mapping. “Policy surveillance” is the “ongoing, systematic collection, analysis, interpretation and dissemination of information about a given body of public health law and policy” (Chriqui, O'Connor, and Chaloupka 2011). Policy surveillance produces reliable longitudinal data for evaluating the health effects of laws; it can also increase public and policymaker awareness of trends in the law, and support diffusion of promising and evidence-based policy innovations. The National Institutes of Health (NIH) supported the development of policy surveillance programs, through NIAAA’s Alcohol Policy Information System (APIS) and the National Cancer Institute’s CLASS. More recently, NIDA has funded the launch of two policy surveillance programs, the Prescription Drug Abuse Policy System (PDAPS) and the Drug Abuse Policy System (DAPS). CDC has been increasing its support for policy surveillance activities, and the Robert Wood Johnson Foundation has funded a Policy Surveillance Program to provide methodological and technical assistance, and maintain LawAtlas.org, a general health policy surveillance portal. As more and more researchers and consumers enter the field — from the state, federal and international levels and in private industry — harmonization of terms, methodological standards, and consensus on any needed governance mechanisms is valuable to ensure continued growth and adoption of this important scientific process and its valuable products. To meet these needs, we propose to convene one conference of 30-35 invited attendees who represent the practice’s primary stakeholders from across research, technology and practice; develop, publish and disseminate at least one report that summarizes the discussion and consensus reached among attendees; and establish a one-year plan of collaborative work towards further cooperation and cohesion. Project Narrative Policy surveillance produces reliable longitudinal data for evaluating the health effects of laws, can increase public and policymaker awareness of trends in the law, and support diffusion of promising and evidence-based policy innovations to improve health. A conference funded by NIH will harmonize terms and methodological standards, and will build consensus on field leadership and governance needed to ensure continued growth and adoption of this important scientific process and its valuable products.",Policy Surveillance: the Future of Scientific Legal Mapping.,9331919,R13DA043979,"['Adoption', 'Artificial Intelligence', 'Awareness', 'Categories', 'Centers for Disease Control and Prevention (U.S.)', 'Code', 'Collection', 'Consensus', 'Data', 'Development', 'Diffusion', 'Drug abuse', 'Emerging Technologies', 'Ensure', 'Foundations', 'Funding', 'Future', 'Goals', 'Growth', 'Health', 'Health Policy', 'Industry', 'Industry Standard', 'Information Dissemination', 'Information Technology', 'International', 'Laws', 'Leadership', 'Legal', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'National Cancer Institute', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Outcome', 'Policies', 'Privatization', 'Process', 'Public Health', 'Public Health Schools', 'Publishing', 'Quality Control', 'Reporting', 'Research', 'Research Personnel', 'Site', 'Statutes and Laws', 'Sum', 'Surveillance Program', 'System', 'Technology', 'Text', 'United States National Institutes of Health', 'Variant', 'Wood material', 'Work', 'alcohol policy information system', 'cohesion', 'data access', 'data structure', 'development policy', 'evidence base', 'improved', 'innovation', 'innovative technologies', 'prescription drug abuse', 'programs', 'public health intervention', 'symposium', 'tool', 'trend', 'uptake']",NIDA,TEMPLE UNIV OF THE COMMONWEALTH,R13,2017,24772,-0.01224361839437096
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9270925,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2017,1573499,-0.034130186709346345
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9565097,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2017,93567,-0.034130186709346345
"IGF::OT::IGF Base Award. Creation of an Accurate Model of the Topical Structure of PubMed and Associated Indicators. POP: 09/01/17 - 02/28/18. N43DA-17-1215. The Contractor will develop advanced and sophisticated analytical models, tools and metrics to enhance the professional evaluation and decision making in life sciences management and administration.  The intended result is a novel set of metrics that can be used by NGOs/disease foundations, advocacy groups, research funders, policy makers and by academic institutional bodies. n/a",IGF::OT::IGF Base Award. Creation of an Accurate Model of the Topical Structure of PubMed and Associated Indicators. POP: 09/01/17 - 02/28/18. N43DA-17-1215.,9583616,71201700041C,"['Advocacy', 'Award', 'Biological Sciences', 'Complement', 'Contractor', 'Data', 'Databases', 'Decision Making', 'Disease', 'Evaluation', 'Foundations', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Modeling', 'Policy Maker', 'PubMed', 'Quality Indicator', 'Records', 'Reproducibility', 'Research', 'Structure', 'Testing', 'Text', 'Translations', 'base', 'economic impact', 'novel', 'tool']",NIDA,"SCITECH STRATEGIES, INC.",N43,2017,225000,-0.0007005162180545344
"Data Science and Sharing Team This is the first annual report for the Data Science and Sharing Team (DSST). In this past year, the team has been busy hiring staff and establishing infrastructure. Significant progress has also been made towards the goals and mission of the group.   The first data scientist for the group, John Lee, was hired in June of 2016. The second data scientist, Dylan Nielson, was hired in April of 2017. Nino Migineishvili joined the team as a summer student in June of 2017. Adam Thomas  has been leading the team since its creation.  Building an Intramural Neuroimaging Data Repository 	 The Functional Magnetic Resonance Imaging  Facility (FMRIF) maintains five MRI scanners that generate over 150 scans per week. All of this data is archived, but it is not currently standardized, organized, or consented such that investigators can aggregate across the participants in the many ongoing studies within the NIH IRP. The DSST is working to change that on three fronts.   First, we have consulted with the scientists responsible for the Alzheimer's Disease Neuroimaging Initiative project who have established standard MRI protocols to use on large, diverse groups of participants across different scanner platforms. The protocols are now available on the FMRIF scanners and will soon replace the clinical scans currently collected on all participants on a yearly basis. Second, we have deployed a data sharing repository using the software developed at the Stanford Center  for Reproducible Neuroscience. This repository went online in June of 2017 and is currently accessible to intramural researchers on the NIH campus. It will be available to researchers throughout the world before the end of the year. Third, we have worked closely with the NIMH Office of the Clinical Director to help develop a protocol for recruiting and scanning healthy volunteers. This protocol will provide a pool of volunteers that can be streamlined into other studies within the IRP while also building a normative database of phenotypic, genetic, and imaging data.  Facilitating Access to Shared Data  The DSST is also working to facilitate and streamline access to existing public data repository for NIMH intramural researchers by creating a local copy on the NIH High Performance Computing System, also known as the Biowulf. Once the local copy is in place, the DSST guides researchers through the necessary data use agreements and then grants them access to the data. This has already been accomplished for the Human Connectome Project which offers comprehensive phenotypic, genetic and imaging data on 1200 participants. We are in the process of making data available from the LIFE Project (2,377 participants), the Adolescent Brain Cognitive Development (ABCD)  project (10,000 participants), the Child Mind Institute's Healthy Brain Network, and the UK Biobank (100,000 participants).   Delivering Education and Training  Conducting reproducible science on large datasets requires skillsets that many researchers do not possess. Since May of 2016 the DSST has conducted four hands-on courses and hosted invited talks from internationally recognized experts in the field of open and reproducible science. All slides and materials are available at https://cmn.nimh.nih.gov/dsst   - On May 26 & 27, 2016 the DSST taught a course on Data Carpentry in the NIH library.  - On June 9 & 10 2016 the DSST taught a course on Software Carpentry for the NIH Graduate Partnership program.  - On Nov 2, 2016  Adam Thomas gave a presentation on reproducible methods to the MEG North America Meeting. - On Dec 9, 2016 the DSST hosted Dr. Matthew Brett who provided an education lecture on how to improve reproducibility in neuroimaging research. - On Jan 25, 2017 John Lee taught in the Introduction to R: Data Wrangling & Statistical Analysis course at the NIH Library.  - On March 13-17, 2017 the DSST held a four day course on reproducible neuroscience . - On April 25th, 2017 John Lee taught a course on using R for reproducible analyses in the NIH Library. - On May 18, 2017 the DSST held a workshop on reproducible research as part of the NIH Pi Day celebration. - On August 1-2, 2017 the DSST held a two-day course on reproducible neuroscience with guest lectures from Dr. Satra Ghosh of MIT and Dr. Yarik Halchenko of Dartmouth University. - On August 14th & 17th, 2017 the DSST hosted talks from Dr. Anisha Keshavan on Crowdsourcing in Neuroscience and Dr. Regina Nuzzo on common statistical pitfalls in neuroscience.  Future Directions and Applications   One of the motivations for building large, standardized repositories of data is so that we may more easily employ data-hungry techniques such as machine learning. The Data Science and Sharing Team was conceived of and designed to work closely with the Machine Learning Team which is still forming. In August of 2017, Charles Zheng, the first member of that team arrived and we are actively planning future projects. Two more members of that team are expected to arrive in the next few months.  Nino Migineishvili, a summer student from UCLA who has been working with the DSST since June 2017, has also been applying machine learning to several large datasets: one acquired here by Dr. Philip Shaw from the Social and Behavioral Research Branch at NHGRI (442 participants) and others acquired outside NIH such as the Nathan Kline Institute (888 participants). Nino's project of predicting brain age from structural images has been pre-registered at Open Science Foundation and is currently being prepared for publication (DOI: 10.13140/RG.2.2.25126.63047).  Jong Hwan Lee, a visiting professor from Korea University working with the DSST and Dr. Peter Bandettini, has also been applying machine learning methods to predict fMRI time course using the HCP dataset on the NIH HPC. Several similar, data-intensive projects are planned for the coming year. n/a",Data Science and Sharing Team,9589768,ZICMH002960,"['Adolescent', 'Age', 'Agreement', 'Alzheimer&apos', 's Disease', 'Annual Reports', 'Archives', 'Behavioral Research', 'Brain', 'Child', 'Clinical', 'Code', 'Communities', 'Computer Systems', 'Computer software', 'Consent', 'Consult', 'Data', 'Data Science', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Education', 'Educational workshop', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Future', 'Genetic', 'Goals', 'Grant', 'High Performance Computing', 'Human', 'Image', 'Institutes', 'International', 'Intramural Research Program', 'Korea', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Mind', 'Mission', 'Motivation', 'National Human Genome Research Institute', 'National Institute of Mental Health', 'Neurosciences', 'North America', 'Participant', 'Phenotype', 'Process', 'Protocols documentation', 'Publications', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scanning', 'Science', 'Scientist', 'Slide', 'Standardization', 'Statistical Data Interpretation', 'Students', 'Techniques', 'Time', 'Training', 'Training and Education', 'United States National Institutes of Health', 'Universities', 'Visit', 'Work', 'biobank', 'cognitive development', 'connectome', 'crowdsourcing', 'data sharing', 'data wrangling', 'design', 'direct application', 'healthy volunteer', 'improved', 'learning strategy', 'lectures', 'meetings', 'member', 'neuroimaging', 'open data', 'professor', 'programs', 'repository', 'social', 'software development', 'tool', 'volunteer']",NIMH,NATIONAL INSTITUTE OF MENTAL HEALTH,ZIC,2017,548228,-0.03295527394928799
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  As part of its activities, the National Toxicology Program (NTP) at NIEHS conducts literature-based evaluations to identify the state of the science, evaluate hazards, and determine the effectiveness of NTP’s research. NTP lacks an intelligent automated approach to assist with this work. NIEHS has requested assistant from the Oak Ridge National Laboratory (ORNL), Department of Energy to assist with the research and development of publication and web mining tools for use in NTP’s evaluations. This assistance extends to the Division of Extramual Research and Training to automate the approach for capturing outcomes and impacts from NIEHS-funded research noted in scientific publications and grantees’ progress reports. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9492481,ES16002001,"['Applications Grants', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Funding', 'Imagery', 'Internet', 'Laboratories', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Outcome', 'Program Effectiveness', 'Program Evaluation', 'Progress Reports', 'Publications', 'Research', 'Research Training', 'Science', 'Techniques', 'Work', 'base', 'hazard', 'learning strategy', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2017,446000,-0.02959572086302919
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9285168,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2017,267313,-0.01829767591492799
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9486059,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2017,1408263,-0.01868199687959115
"Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research ﻿    DESCRIPTION (provided by applicant): This Phase II project aims to continue development of a commercial quality, innovative cloud hosted information management system, called Climb 2.0(tm) that will increase laboratory efficiency and provide improved capabilities for research laboratories. Climb is designed to offer integrated laboratory process management modules that include mobile communications tools data monitoring and alert systems, and integrated access to Microsoft Azure Cloud(tm) Machine Learning and Stream Analytics services. Initially, Climb 2.0 will target animal model research laboratories; however, the core of the platform is designed to be adaptable to nearly any research type or related industry. Current research information management systems are primarily designed as record-keeping tools with little or no direct focus on laboratory efficiency or in enhancing value of the research data. They also do not leverage emergent mobile device technologies, social media frameworks, and data analysis and storage capabilities of cloud computing. Many laboratories still use paper as their primary recording system. Paper data logging is then followed by secondary data entry into a laboratory database. These systems are error prone, time consuming and lead to laboratory databases with significant time lags between data acquisition and data entry. Moreover, they do not recognized cumulative data relationships, which may identify important trends, and researchers often miss windows of opportunity to take action on time-sensitive events. In Phase I, RockStep Solutions demonstrated feasibility of an innovative Cloud Information Management Bundle system, Climb, which will increase efficiency and improve capabilities in animal model data management. During Phase I, a beta version of Climb was successfully developed and tested against strict performance metrics as a proof of concept. We successfully built a prototype with working interfaces that integrates real-time communication technologies with media capabilities of mobile devices. Phase II proposes four specific Aims: 1) Develop the technology infrastructure to support the secure and scalable Software as a Service (SaaS) deployment of Climb for enterprise commercial release; 2) Develop and extend the Phase-I prototype Data Monitoring and Messaging System (DMMS) into a platform ready for production use; 3) Extend Climb's DMMS adding a Stream Analytics engine to support Internet of Things (IoT) devices and streaming media; 4) Deploy a beta release of Climb at partner research labs, test and refine the product for final commercialization. To ensure Climb is developed with functionality and tools relevant to research organizations, RockStep Solutions has established collaborations with key beta sites to test all of the major functionality developed in this proposal. IMPACT: By leveraging emergent technologies and cloud computing, Climb offers several advantages: 1) enables real-time communications using familiar tools among members of research groups; 2) reduces the risk of experimental setbacks, and 3) enables complex experiments to be conducted efficiently. PUBLIC HEALTH RELEVANCE: The NIH Invests approximately $12 billion each year in animal model research that is central to both understanding basic biological processes and for developing applications directly related to improving human health. More cost-effective husbandry and colony management techniques, equipment, and new approaches to improve laboratory animal welfare and assure efficient and appropriate research use (e.g., through cost savings and reduced animal burden) are high priorities for NIH. RockStep Solutions proposes to meet this need by integrating existing and emergent software and communication technologies to create a novel solution, Climb 2.0, for research animal data management. Climb 2.0 extracts immediate value of data in animal research laboratories and extends the database into a multimedia communication network, thus enabling science that would be impractical to conduct with traditional methods.",Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research,9354497,R44GM112206,"['Address', 'Algorithms', 'Animal Experimentation', 'Animal Model', 'Animal Welfare', 'Animals', 'Biological Process', 'Biomedical Research', 'Clinical Laboratory Information Systems', 'Cloud Computing', 'Collaborations', 'Communication', 'Communication Tools', 'Communications Media', 'Complex', 'Computer software', 'Computers', 'Consumption', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Security', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Devices', 'Emerging Technologies', 'Ensure', 'Equipment', 'Equipment and supply inventories', 'Event', 'Feedback', 'Future', 'Goals', 'Health', 'Health system', 'Human', 'Industry', 'Information Management', 'Internet', 'Internet of Things', 'Investments', 'Laboratories', 'Laboratory Animals', 'Laboratory Research', 'Lead', 'Machine Learning', 'Management Information Systems', 'Methods', 'Modernization', 'Monitor', 'Motion', 'Multimedia', 'Notification', 'Paper', 'Performance', 'Phase', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Science', 'Secure', 'Services', 'Site', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'The Jackson Laboratory', 'Time', 'Transact', 'United States National Institutes of Health', 'analytical tool', 'base', 'cloud based', 'commercialization', 'computerized data processing', 'cost', 'cost effective', 'data acquisition', 'data management', 'design', 'encryption', 'experimental study', 'good laboratory practice', 'graphical user interface', 'handheld mobile device', 'improved', 'innovation', 'laptop', 'member', 'novel', 'novel strategies', 'predictive modeling', 'programs', 'prototype', 'public health relevance', 'service learning', 'social media', 'software as a service', 'success', 'time interval', 'tool', 'trend', 'usability']",NIGMS,"ROCKSTEP SOLUTIONS, INC.",R44,2017,739402,-0.012801666846909972
"Computational Tools For Bioinformatics And Genome Analysis A crucial component to the recent major advances in genomic research has been the uniting of advances in biology with those in computers, informatics and networking. As technologies have advanced allowing high throughput, Genomics scale data collection, the technological burden has shifted to analysis and informatics. This project was established to ensure that necessary computational tools and resources are available to the NIH intramural community.   OIRs long-term collaboration with Dr. Louis Staudt (Distinguished Investigator, NCI Center for Cancer Research and Director, NCI Center for Cancer Genomics) has yielded significant findings and discoveries that have led to improvements in the treatment of lymphoma.   By providing comprehensive computational expertise, resources, and support, Dr. Staudts lab has been able to perform sophisticated analyses on large-scale, high-dimensional data which have in turn been instrumental to achieving a number of highly significant findings.  OIR provides comprehensive computational support to Dr. Staudts laboratory.  This support entails maintaining databases of genomic data, providing computational servers with custom software for running a variety of analyses, and developing and maintaining public and local-access Web sites.    These supported resources include the following:  - LLMPP/SPECS: The Lymphoma/Leukemia Molecular Profiling Project/Strategic Partnering to Evaluate Cancer Signature (SPECS) program is a multi-institution grant for translational cancer research funded by National Cancer Institute. This website is designed for entering/managing clinical data for cases associated with samples included in the SPECS study. The LLMPP/SPECS project is using microarrays and other high throughput whole genome technologies to define the molecular profiles of all types of human lymphoid malignancies. One primary goal of this project is to redefine the classification of human lymphoid malignancies in molecular terms. A second major goal is to define molecular correlates of clinical parameters that can be used in prognosis and in the selection of appropriate therapy for these patients. As members of the international LLMPP/SPECS consortium, we provide the informatics development and support critical to the success of this project. A database and tools have been implemented to facilitate integrating and analyzing clinical parameters with genomic/genetic data from high throughput technologies. The consortium involves 12 participating centers in 7 countries.  Data for 3,000 clinical cases have been uploaded into the system.  - LYMPHCX: A Web site that allows researchers to predict DLBCL (Diffuse Large B-Cell Lymphoma) subtypes based on samples based on samples processed with a Nanostring protocol.  Determination of these subtypes can be critical in deciding appropriate therapy since some subtypes are more aggressive than others.  - VDB (Variation Database): An interactive web site for lab researchers to search and compare mutations from various tumor types. An analysis pipeline was developed and implemented for processing next generation sequence data generated from RNAseq libraries. Variation results derived from more than 500 lymphoma, pancreatic and prostrate RNA samples have been stored in a database, classified and integrated with relevant external annotations.   - Signature database: A Web-site companion to Shaffer AL et al. A library of gene expression signatures to illuminate normal and pathological lymphoid biology, Immunol Rev. 2006 Apr;210:67-85.  - Staudt lab analytical test bed: Web site to support quick turn-around of test analytical methods and rapidly allow lab members to more easily explore their own data with new algorithms.  - Database support: OIR maintains information on more than 10 million mutations across over 3,000 clinical samples. Information on digital expression is also stored.   - Machine learning:  Development of machine learning methods to identify somatic from germline mutations in NGS sequencing data. Machine learning models have also been tested to identify subtypes of diffuse large B-cell lymphoma, based on their features of gene aberrations.  The mAdb (microArray database, https://madb.nci.nih.gov) system provides a secure data management system for gathering, storing, and managing experimental information and expression array data. A variety of web accessible tools have been implemented to support the multiple analytical approaches needed to decipher array data in a more meaningful way. Important to the mAdb system design is compatibility with any platform (Unix, Windows or Macintosh) capable of running an Internet browser.  A natural extension of mAdb has been the inclusion of additional data resources. This includes supporting information from various data sources (e.g. Gene Ontology, GenBank, Entrez Gene, UniGene, BioSystems Pathways, Biocarta Pathways, COSMIC, and 1000 Genomes) to enable drilling down into the rapidly expanding biological knowledgebase. In order to have effective use of the informational resource developed to support microArray analysis, ongoing user training and support is provided through CIT facilities for this collaborative effort. While ongoing development of new and improved analysis tools continues, the mAdb system is in routine service, having supported over 1900 NIH researchers and collaborators and containing over 111,000 microArray experiments. A critical design element for the mAdb system was to accommodate scalability to allow expansion to support other ICDs. The design allows us to support separate web servers serving different user communities from a single code base. The mAdb system has been set up on separate web servers to support users of the NIAID microArray core facility. In addition to user specific web based analysis, out group has facilitated the submission of over 7,000 samples to the NCBI Gene Expression Omnibus (GEO) public repository for required sharing of data associated with publications.  Over the past year, we have developed next-generation sequencing extensions to mAdb, allowing it to accept RNA-Seq digital expression data and perform edgeR normalization and differential expression analysis.   We have also developed nimble JavaScript implementations of several data visualization features.  In collaboration with Dr. Timothy Meyers of NIAID, CIT/OIR also provides comprehensive computational support the Genomic Technologies Section (GTS) of NIAID.   Since GTS provides state-of-the-art bioinformatics support to the   entire NIAID intramural research program, we effectively support all the users of the GTS facility.  In addition to maintaining GTS computational servers and databases, OIR maintains a number of commercial software packages for GTS, including Partel Flow, CLC-Bio n/a",Computational Tools For Bioinformatics And Genome Analysis,9550740,ZIHCT000260,"['Algorithms', 'Beds', 'Bioinformatics', 'Biological', 'Biology', 'CLC Gene', 'Cancer Center', 'Classification', 'Clinical', 'Clinical Data', 'Code', 'Collaborations', 'Communities', 'Companions', 'Computer software', 'Computers', 'Core Facility', 'Country', 'Custom', 'Data', 'Data Collection', 'Data Sources', 'Databases', 'Development', 'Discipline', 'Elements', 'Ensure', 'Expression Library', 'Funding', 'Genbank', 'Gene Chips', 'Gene Expression', 'Gene Library', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Germ-Line Mutation', 'Goals', 'Grant', 'Human', 'Informatics', 'Institution', 'Interdisciplinary Study', 'International', 'Internet', 'Intramural Research Program', 'Laboratories', 'Libraries', 'Lymphoid', 'Lymphoma', 'Machine Learning', 'Malignant Neoplasms', 'Malignant lymphoid neoplasm', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mutation', 'NCI Center for Cancer Research', 'National Cancer Institute', 'National Institute of Allergy and Infectious Disease', 'Online Systems', 'Ontology', 'Pancreas', 'Pathologic', 'Pathway interactions', 'Patients', 'Process', 'Protocols documentation', 'Publications', 'RNA', 'Research', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Science', 'Secure', 'Services', 'Strategic Planning', 'System', 'Technology', 'Testing', 'Training Support', 'United States National Institutes of Health', 'Variant', 'analytical method', 'base', 'cancer genomics', 'clinical practice', 'computerized tools', 'computing resources', 'data management', 'data resource', 'data sharing', 'data visualization', 'design', 'differential expression', 'digital', 'experimental study', 'genome analysis', 'genomic data', 'high dimensionality', 'high throughput technology', 'improved', 'innovation', 'knowledge base', 'large cell Diffuse non-Hodgkin&apos', 's lymphoma', 'learning strategy', 'leukemia/lymphoma', 'member', 'nano-string', 'next generation', 'next generation sequencing', 'outcome forecast', 'programs', 'repository', 'success', 'tool', 'transcriptome sequencing', 'translational cancer research', 'tumor', 'web site', 'web-accessible', 'whole genome']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2017,503805,-0.016248973732074515
"Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!) PROJECT SUMMARY/ABSTRACT Our society faces significant challenges in providing quality health care that is accessible by each person and is sensitive to each person's individual lifestyle and individual health needs. Due to recent advances in sensing technologies that have improved in accuracy, increased in throughput, and reduced in cost, it has become relatively easy to gather high resolution behavioral and individualized health data at scale. The resulting big datasets can be analyzed to understand the link between behavior and health and to design healthy behavior interventions. In this emerging area, however, very few courses are currently available for teaching researchers and practitioners about the foundational principles and best practices behind collecting, storing, analyzing, and using behavior- based sensor data. Teaching these skills can help the next generation of students thrive in the increasingly digital world.  The goal of this application is to design online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to WSU faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.  This contribution is significant because not only large research groups but even individual investigators can create large data sets that provide valuable, in-the-moment information about human behavior. They need to be able to handle the challenges that arise when working with sensor- based behavior data. Because students will receive hands-on training with actual sensor datasets and analysis tools, they will know how to get the best results from available tools and will be able to interpret the significance of analysis results.  Our proposed online course program, called AHA!, builds on the investigators' extensive experience and ongoing collaboration at Washington State University on the development of smart home and mobile health app design, activity recognition, scalable biological data mining, and the use of these technologies for clinical applications. Our approach will be to design online course modules to train individuals in the analysis of behavior-based sensor data using clinical case studies (Aim 1). We will design an educational program that involves students from diverse backgrounds and that is findable, accessible, interoperable, and reusable (Aim 2). Finally, we will conduct a thorough evaluation to monitor success and incrementally improve the program (Aim 3). All of the materials will be designed for continued use beyond the funding period of the program. PROJECT NARRATIVE  This program focuses on the development and dissemination of online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to Washington State University faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.",Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!),9313495,R25EB024327,"['Address', 'Aging', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological', 'Case Study', 'Charge', 'Chronic Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Development', 'Discipline', 'E-learning', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Evaluation', 'FAIR principles', 'Face', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'General Population', 'Goals', 'Health', 'Home environment', 'Human', 'Immersion Investigative Technique', 'Individual', 'Interdisciplinary Study', 'Life Style', 'Link', 'Longevity', 'Machine Learning', 'Methods', 'Monitor', 'Performance', 'Persons', 'Precision Medicine Initiative', 'Pythons', 'Recruitment Activity', 'Rehabilitation Nursing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Site', 'Societies', 'Structure', 'Students', 'Suggestion', 'Techniques', 'Technology', 'Training', 'Universities', 'Washington', 'Work', 'base', 'behavior influence', 'behavioral health', 'biocomputing', 'career networking', 'clinical application', 'cognitive rehabilitation', 'cost', 'course development', 'course module', 'data mining', 'design', 'digital', 'experience', 'health care quality', 'health data', 'improved', 'innovation', 'learning materials', 'learning strategy', 'mHealth', 'next generation', 'online course', 'programs', 'responsible research conduct', 'scale up', 'sensor', 'skills', 'statistics', 'success', 'synergism', 'tool', 'web page']",NIBIB,WASHINGTON STATE UNIVERSITY,R25,2017,186245,-0.051842477577493405
"All of Us, Wisconsin Our application for OT-PM-16-003, “One in a Million – Precision Medicine Initiative Wisconsin” represents the collaborative efforts of three fully integrated regional healthcare systems to form a virtual state-wide integrated delivery network. The application originates at the Marshfield Clinic Health System (MCHS), one of the most productive and earliest adopters of what has become Precision Medicine. The work of the MCHS has been cited by NIH Director Dr. Francis Collins as a model for the Precision Medicine Initiative (PMI). To ensure comprehensive nearly state-wide coverage of participant health care, MCHS has partnered with the other academic integrated delivery networks in Wisconsin, (University of Wisconsin (UW) and Medical College of Wisconsin (MCW)), that together include 173 clinics, 13 hospitals, insurer partners and 5 Federally Qualified Health Centers (FQHCs). The Blood Center of Wisconsin (BCW) also partners to operate state-wide mobile units of staff and equipment for blood collection that may be purposed for this effort. Through an independent agreement with Aurora Health, the electronic health records of participants recruited by academic and FQHC sites will be made available, ensuring nearly complete coverage of health care records across an entire state, with special emphasis on those populations that are traditionally the most underserved and understudied. The academic sites and FQHC partners have a long history of research cooperation and robust community engagement demonstrated by Clinical and Translational Science Awards (CTSA) ties, the Wisconsin Genome Initiative, reciprocal IRB arrangements, multiple joint grants and other collaborative studies. We have pioneered and implemented data sharing platforms and have demonstrated their usefulness with numerous high impact publications over many years. All three academic centers are leaders in the details of community engagement, recruitment, tracking, return of results and data sharing across various platforms in national and international consortia using large cohorts (e.g., MCHS’s 20,000 participant Personalized Medicine Research Project). A major innovation of this application is that it aligns the major healthcare systems of an entire state to more fully capture each participant’s data wherever participants get their health care. Indeed, 80% of all health care in Wisconsin is captured by our footprint. The State-wide catchment area also includes the oldest, sickest and poorest regions in the US and represent rural and urban areas, including African Americans, Hispanics and Native Americans, all represented as community champions. Through the FQHC partners, the recruiting institutions will achieve the 40% African American, Hispanic and Native American participant rate, with long standing community engagement and history of successful recruitment, including Wisconsin’s 12 Native American Tribes. Lending active support are Community Champions and Community Advisory Boards. A second innovative approach will employ Machine Learning (ML) techniques to optimize recruitment and retention processes. Our scientists have long collaborated on efforts to enhance the breadth and reliability of information extracted from the electronic health record (EHR), using a range of data to identify elements including family pedigrees or the occurrence of clinical events that are susceptible to unreliable coding. These efforts employed state-of-the-art methods including random forests, support vector machines and statistical relational learning, among others. These advanced computational methods will be used to predict those who are most likely to participate and which methods of approach are most effective. A third innovative approach is the planned use of mobile recruitment labs, previously successful with the Survey of the Health of Wisconsin (SHOW). SHOW, began in 2008 provides a novel infrastructure for population health research recruitment, enabling engagement across the state, longitudinal follow-up of participants and community-specific studies. Key assets of SHOW are two mobile units staffed by research and healthcare professionals to facilitate on-site health studies in randomly selected or community-specific participants. SHOW is predicated on community engagement and the response rate has been outstanding (in one study more than 90% of ~4,000 participants consented to follow-up, DNA testing, or blood work). This is true across all racial/ethnic groups, including non-Hispanic Whites, African Americans, Hispanics and Native Americans. The fourth innovative approach is to work with our FQHC partners to recruit traditionally underserved populations. MCHS, UW and MCW will work with FQHC partners (Marshfield Family Health Center, Access Community Health Center, Milwaukee Health Services, Progressive Health Center and 16th Street Community Health Center). Importantly, we are ready to start. We have already informed and engaged our communities through press releases and newsletters. Processes are in place. Volunteers have already asked us to contact them. All the required personnel are trained and SOPs are in place to begin recruitment. We have a productive history of working with partners already funded for PMI cohort recruitment. For several years, we have worked closely (and published) with our colleagues at Northwestern and Columbia through the eMERGE Network and with the University of Pittsburgh through CTSA. Project Director Murray Brilliant has recently served as an advisor to the University of Arizona’s Precision Medicine Program. Thus, we are team-players who are ready, willing and able to be valuable collaborative partners in the effort to build a national engine to transform healthcare under PMI. n/a","All of Us, Wisconsin",9512099,OT2OD025286,"['African American', 'Agreement', 'Arizona', 'Award', 'Blood', 'Catchment Area', 'Clinic', 'Clinical', 'Clinical Sciences', 'Code', 'Collection', 'Communities', 'Community Health Centers', 'Computing Methodologies', 'Consent', 'DNA', 'Data', 'Electronic Health Record', 'Elements', 'Ensure', 'Equipment', 'Ethnic group', 'Event', 'Family', 'Family health status', 'Federally Qualified Health Center', 'Funding', 'Genome', 'Grant', 'Health', 'Health Professional', 'Health Services', 'Health Surveys', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hispanic Americans', 'Hospitals', 'Human Resources', 'Institution', 'Insurance Carriers', 'International', 'Joints', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Native Americans', 'Newsletter', 'Not Hispanic or Latino', 'PMI cohort', 'Participant', 'Population', 'Precision Medicine Initiative', 'Press Releases', 'Process', 'Publications', 'Publishing', 'Recording of previous events', 'Records', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Scientist', 'Site', 'Techniques', 'Testing', 'Training', 'Translational Research', 'Tribes', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Wisconsin', 'Work', 'cohort', 'data sharing', 'follow-up', 'forest', 'genetic pedigree', 'innovation', 'medical schools', 'novel', 'personalized medicine', 'population health', 'precision medicine', 'programs', 'racial and ethnic', 'response', 'rural area', 'urban area', 'virtual', 'volunteer']",OD,MARSHFIELD CLINIC RESEARCH FOUNDATION,OT2,2017,5360833,-0.015191549263700643
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9357656,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2017,572778,-0.029474712843861408
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9360131,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'permissiveness', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2017,456710,-0.026721412452100295
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk. PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9249484,R01AI117011,"['Animals', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Manuals', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'Zoonoses', 'improved', 'information model', 'interest', 'journal article', 'molecular sequence database', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2017,461012,-0.04011880366589098
"Development of Deep Learning Models for Biomarker Identification and Classification Nearly half of all biomedical publications ever written have been published in the last 15 years— the era since the completion of the human genome project. The rate of increase is exponential, shows no signs of slowing down, and can be seen in all of the information sources relevant to precision medicine development. This constitutes a nearly insurmountable burden for the drug-development and diagnostics professionals who develop precision medicines. Next-generation automated evaluation of this data that will enable rapid, supportable, and innovative product-development decisions would lead to the development and approval of many more precision medicines, resulting in improved public health, decreased precision medicine time-to-market, and increased efficiency and profitability for drug-development and diagnostics companies. Developing precision medicines is exceedingly difficult due to an underlying chicken-and-egg problem: patient populations for whom a drug will be effective cannot be identified without a test, but test development is notworth justifying without a drug that is demonstrated to be effective in that population. No system for the identification of actionable precision medicine opportunities exists. The ideal approach would 1) examine multiple systems of record that impact marketplace decisions; 2) be aware of the identify of individual molecular biomarkers mentioned therein, as well as bring in technical details such as measurability and measurement method; and 3) be accessible to drug-development companies looking to define a patient population and diagnostics companies looking to develop a test to provide that definition. Therefore, the overall goal of this multi-phase SBIR project is to capitalize on our preliminary success in building a biomarker database (BiomarkerBase™) that supports this critical interface of precision medicine development decisions. Amplion's highly qualified data science R&D team will collaborate with Dr. Parag Mallick of Stanford University to pursue three Aims: 1) train a Deep Learning Model for biomarker identification in clinical trials, 2) extend Model application and prove performance for classification of biomarker usage intent, and 3) prove that Model-identified and classified biomarkers match and expand expert opinions. Showing that we can identify biomarkers and their usage classifications in clinical trials and publications will establish the predictive potential of our unique algorithms within the limited scope of this Phase I feasibility project and will set the stage for a larger Phase II demonstration. Phase I will provide data that can be incorporated directly into Amplion's BiomarkerBase™ product and will allow us to assess how well our Model meets user requirements for this data. Phase II work will allow us to expand/extend this Model to cover additional sources. Phase III work (with Industry partners) will allow us to integrate this commercial service directly into customer work flows. Our next-generation capabilities will provide a critical linkage at the challenging interface between the diagnostics and drug-development efforts and will accelerate the development of novel precision medicines. The development of precision medicines requires the synthesis of information from multiple sources to identify the opportunities that are technically feasible and likely to succeed in the marketplace. Current methods for analyzing this information are too slow, too non-specific, or too focused on only a small fraction of the information required to make a good decision. The overall goal of this project is to develop, validate, and commercialize Amplion's proprietary algorithms for identifying precision medicine development opportunities from publicly available sources.",Development of Deep Learning Models for Biomarker Identification and Classification,9348166,R43GM123851,"['Address', 'Algorithms', 'Area', 'Awareness', 'Biological', 'Biological Markers', 'Chickens', 'Classification', 'Clinical Trials', 'Consult', 'Data', 'Data Analyses', 'Data Science', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Evaluation', 'Expert Opinion', 'Explosion', 'Feedback', 'Goals', 'Grant', 'Human Genome Project', 'Individual', 'Industry', 'Knowledge', 'Label', 'Lead', 'Learning', 'Legal patent', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Names', 'New Drug Approvals', 'Output', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Positioning Attribute', 'Public Health', 'Publications', 'Publishing', 'Report (document)', 'Research', 'Research Personnel', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Techniques', 'Testing', 'Time', 'TimeLine', 'Training', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'biomarker identification', 'commercialization', 'companion diagnostics', 'drug development', 'egg', 'experience', 'improved', 'industry partner', 'innovation', 'insight', 'journal article', 'molecular marker', 'next generation', 'novel', 'patient population', 'precision medicine', 'product development', 'research and development', 'search engine', 'success']",NIGMS,"AMPLION, INC.",R43,2017,150000,-0.0025694363893955857
"Sci-Score, a tool to support rigor and transparency guidelines Project Summary While  standards  in  reporting  of  scientific  methods  are  absolutely  critical  to  producing  reproducible  science,  meeting  such  standards  is  difficult.  Checklists  and  instructions  are  tough  to  follow  often  resulting  in  low  and inconsistent  compliance.  Scientific  journals  and  societies  as  well  as  the  National  Institutes  of  Health  are  now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods  (e.g.,  http://www.cell.com/star-­methods),  but  the  trickier  part  will  be  to  train  the  biomedical  community  to  use these standards to effectively improve how scientific methods are communicated. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency of  Key Biological Resources, we propose to build Sci-­Score a text mining based tool suite to help authors meet the  standard.  Sci-­Score  will  provide  an  automated  check  on  compliance  with  the  RRID  standard  already  implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. The innovation behind Sci-­ score is the provision of a score, which can be obtained by individual investigators, which reflects a numerical  validation of the quality of their methods reporting. We posit that the score will serve as a tool that investigators  and journals can use to compete with themselves and each other, or in the very least allow them to see how  close they are to the average in meeting quality requirements.   Recently, our group has developed a text mining algorithm that has now been successfully been used to detect software tools and databases from the SciCrunch Registry in published papers. Digital tools are one of four resource types that the RRID standard identifies. We propose to extend this approach to the other types of entities: antibodies, cell lines and model organisms. Resource identification along with other quality metrics twill be used to train an algorithm to score the overall quality of the methods document. If successful, the tool could be used by editors, reviewers, and investigators to improve the number of RRIDs, therefore the quality of descriptors of key biological resources in published papers. This SBIR project will build a set of algorithms similar to the resource finding pipeline and develop it into an industrial robust and reconfigurable software system. Our Phase I specific aims include to 1) creating gold sets of data for each resource type and training a set of algorithms for each resource type; 2) designing and evaluating the scoring system; 3) designing and evaluating a report generating system based on the previous aims. In Phase II, we will develop a scalable backend infrastructure to serve the needs of scientific publishers and research community. Standards for scientific methods reporting are absolutely critical to producing reproducible science, but meeting  such standards is difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent  compliance. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency, we propose to build Sci-Score text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. Sci-Score will provide a score rating the quality of  methods reporting in submitted articles, which provides feedback to authors, reviewers and editors on how to improvecompliance with RRIDs and other standards. ","Sci-Score, a tool to support rigor and transparency guidelines",9345707,R43OD024432,"['Address', 'Agreement', 'Algorithms', 'Animal Model', 'Antibodies', 'Area', 'Big Data', 'Biological', 'California', 'Cell Line', 'Cell model', 'Cells', 'Communities', 'Data Set', 'Databases', 'Descriptor', 'Elements', 'Ensure', 'Evaluation', 'Feedback', 'Funding', 'Glare', 'Gold', 'Guidelines', 'Habits', 'Human', 'Individual', 'Industrialization', 'Instruction', 'Journals', 'Learning', 'Literature', 'Machine Learning', 'Manuscripts', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neurosciences', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reading', 'Reagent', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sales', 'Science', 'Scoring Method', 'Services', 'Small Business Innovation Research Grant', 'Societies', 'Software Tools', 'System', 'Technology', 'Text', 'To specify', 'Training', 'United States National Institutes of Health', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'biological systems', 'computerized tools', 'design', 'digital', 'improved', 'innovation', 'interest', 'meetings', 'prototype', 'software systems', 'sound', 'text searching', 'tool', 'vigilance', 'web site']",OD,"SCICRUNCH, INC.",R43,2017,221865,-0.014129677687538044
"How Does Automated Record Linkage Affect Inferences about Population Health? ABSTRACT  Our broad research objective is to create the Longitudinal Intergenerational Family Electronic Micro-dataset (LIFE-M) spanning the late 19th and 20th century United States. Using automated record linkage technology, the LIFE-M project combines millions of vital records to reconstruct how and why individuals' health has changed across time. This multi-generational, longitudinal micro-database aims to transform research on health and longevity, on childbearing and family structure, and on the long-run health effects of early-life circumstances and exposures.  In creating LIFE-M, however, we have encountered serious deficits in knowledge about the performance of automated record linkage technology. The proposed project seeks to evaluate the performance of the most popular and cutting-edge automated linking techniques for the purposes of creating longitudinal health data. Our specific aims are to (1) produce systematic evidence regarding the performance of automated record linking algorithms in terms of match rates, representativeness of the linked sample, erroneous matches (type I errors), and systematic measurement error; (2) examine how phonetic name-cleaning methods affect quality metrics; and (3) examine how record quality metrics vary for different underrepresented subgroups (including women, racial/ethnic minorities, and immigrants) and to determine how linking methods affect representativeness and inferences. To achieve these aims, we have developed new partnerships with record linking experts allowing us to incorporate the most cutting-edge methods in record linking. We will also rely on new “ground truth” generated by LIFE-M project's independent, double-blind human review process.  This project will contribute significantly to existing knowledge about the use of automated linking methods for creating longitudinal and intergenerational health data. It will also increase knowledge about potential sources of bias in health studies. Both contributions should greatly enhance the quality of descriptive and causal inferences about population health and aging and disparities in these outcomes. PROJECT NARRATIVE  This project contributes to public health knowledge by advancing record linking methodology for creating longitudinal and intergenerational health datasets. It will also increase knowledge about potential sources of bias in public health and aging studies using linked records. Both contributions should significantly improve the quality of inferences about public and population health and health disparities.",How Does Automated Record Linkage Affect Inferences about Population Health?,9372797,R21AG056912,"['Affect', 'Aging', 'Algorithms', 'American', 'Benchmarking', 'Big Data', 'Birth', 'Birth Certificates', 'Birth Records', 'Censuses', 'Child', 'Computers', 'Data', 'Data Linkages', 'Data Set', 'Databases', 'Double-Blind Method', 'Economics', 'Family', 'Foundations', 'Four-dimensional', 'Funding', 'Genealogy', 'Generations', 'Genetic Transcription', 'Goals', 'Graph', 'Hand', 'Health', 'Heterogeneity', 'Human', 'Immigrant', 'Incidence', 'Individual', 'Infant', 'Joints', 'Knowledge', 'Life', 'Link', 'Longevity', 'Machine Learning', 'Maiden Name', 'Marriage', 'Measurement', 'Measures', 'Medicare', 'Methodology', 'Methods', 'Minnesota', 'Minor', 'Names', 'Outcome', 'Performance', 'Pilot Projects', 'Politics', 'Population', 'Process', 'Public Health', 'Records', 'Research', 'Research Infrastructure', 'Running', 'Sample Size', 'Sampling', 'Science', 'Scientist', 'Source', 'Speed', 'Subgroup', 'Techniques', 'Technology', 'Time', 'United States', 'United States National Institutes of Health', 'Universities', 'Variant', 'Veterans', 'Woman', 'aging population', 'child bearing', 'cost effective', 'ethnic minority population', 'family structure', 'health data', 'health disparity', 'health knowledge', 'improved', 'innovation', 'intergenerational', 'longitudinal database', 'population health', 'racial and ethnic', 'repository', 'social', 'vector']",NIA,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R21,2017,232500,-0.019854605459037857
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9268713,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Data Sources', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2017,655080,0.0016230118842699283
"Biomedical Informatics Section (BIS) Interacts with NIDA/IRP investigators to develop biomedical informatics applications and solutions that can access, manage, disseminate, and analyze large quantities of high quality data. Develops, researches, and/or applies computational tools to assist in the acquisition and analysis of biological, medical behavioral or health data, within a specific time frame determined to be appropriate by the NIDA. Helps facilitate new system initiatives and changes to existing systems to meet legislative, regulatory, and departmental requirements within the specific time frames designated by each requirement. Conducts routine system analysis of automatic data processing resources and techniques of existing projects. Recommends, as needed, the implementation of new technologies that are efficient and timely.  	Provides technical and professional support to help integrate computer systems, design or acquire computer programs, configure and support networks and streamline automated data processing within the NIDA's specified timeframe. Integrates scientific data systems with network services and security services to foster safe and secure NIDA/IRP laboratory collaboration and for collaboration with extramural entities, when possible integrate scientific data systems with one another and ensure design for interoperable data.  	Automates various aspects of clinical and non-clinical programs at the NIDA/IRP. These include but are not limited to functions related to research participant recruiting, pharmacy, nurses, physicians, and other investigators including those at the Maryland Psychiatric Research Center. n/a",Biomedical Informatics Section (BIS),9550744,ZIHDA000534,"['Area', 'Automatic Data Processing', 'Baltimore', 'Behavioral', 'Biological', 'Chemicals', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Research', 'Collaborations', 'Computational Science', 'Computer Security', 'Data', 'Data Quality', 'Databases', 'Ensure', 'Exposure to', 'Extramural Activities', 'Fostering', 'Goals', 'Guidelines', 'HIV', 'Illicit Drugs', 'Informatics', 'Information Sciences', 'Information Systems', 'Information Technology', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Laws', 'Machine Learning', 'Maryland', 'Medical', 'Medical Informatics', 'Methods', 'National Institute of Drug Abuse', 'Nurses', 'Participant', 'Pharmacy facility', 'Phylogenetic Analysis', 'Physicians', 'Policies', 'Procedures', 'Psychosocial Stress', 'Recruitment Activity', 'Regulation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Secure', 'Security', 'Services', 'Site', 'Specific qualifier value', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Training Support', 'Work', 'addiction', 'biomedical informatics', 'computer program', 'computer system design', 'computerized tools', 'data mining', 'design', 'disorder later incidence prevention', 'health data', 'interoperability', 'mHealth', 'new technology', 'programs', 'support network', 'transmission process']",NIDA,NATIONAL INSTITUTE ON DRUG ABUSE,ZIH,2017,4163003,-0.02979436022349623
"OMOP information model for eMERGE phenotyping ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",OMOP information model for eMERGE phenotyping,9481767,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'information model', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2017,100000,-0.02552361369431552
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9285815,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2017,871212,-0.024532646534336722
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9443736,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,50000,-0.030588393064776898
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9536289,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,589741,-0.030588393064776898
"Center for Environmental Genetics This application requests a 5-year renewal of the Center for Environmental Genetics. The unified theme of  the proposal is to elucidate how gene and environment interaction through epigenetics influences disease  risks and health outcome. Ohioans are exposed to exceptionally bad air quality due to dated power plants  and the fact that it is situated in the nation's main artery of the north-south transportation route. These unique  geographical/economic features put the residents in the state at high risk of the following diseases: (a)  endocrine disruption and cancer; (b) immune and allergic diseases; (c) cardiovascular and lipid disorders;  and (d) neurology and behavior disorders. The CEG has 20-years' of accumulative success in environmental  health sciences (EHS) research, our aspiration is to help mitigate these environmental diseases, first in Ohio,  then extend our success to similar populations in the nation and around the globe. Here we propose to attain  our goal by (1) expanding EHS research human capital through the continued recruitment and support of  investigators of all levels to EHS research, (2) innovative ideas and collaborative work from new and  established investigators will be supported by the Pilot Project Program and Director's Fund, (3) junior  investigators will be mentored by a Career Development Program and strong mentor-mentee relationships;  (4) an Integrative Technology Support Core combined with a Bioinformatics Core will provide investigators  with leading-edge technologies and data analysis to attain a new level of system-biology approach to EHS;  (5) enhancement activities such as workshops, symposia and seminar series will be used to stimulate  collaboration and innovation; (6) an Integrative Health Sciences Facility Core will provide the toolbox for  translation of EHS to the clinical and human studies; and (7) a Community Outreach and Engagement Core  will serve to translate EHS to public health and health education for healthier living and disease prevention.  The center is led by an outstanding team of directors with visionary scientific and administrative experiences  and insights. The team will be assisted by Internal and External Advisory Boards to provide checks and  balances. CEG, through cooperation with other centers will strive to expand EHS capacity and beyond. Our environmental health center focuses on understanding how genes and environment affect our health,  serving the residents, communities of Ohio, the nation and the globe by generating knowledge that is  applicable to the general public and policymakers for healthier living.",Center for Environmental Genetics,9565110,P30ES006096,"['Acute', 'Affect', 'Air', 'Allergic Disease', 'Area', 'Arteries', 'Attention', 'Barker Hypothesis', 'Behavior Disorders', 'Biogenesis', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cells', 'Chronic', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Community Outreach', 'Complex', 'Core Facility', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Dose', 'Economics', 'Educational workshop', 'Endocrine disruption', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Epigenetic Process', 'Equilibrium', 'Equipment', 'Evolution', 'Exposure to', 'Funding', 'General Population', 'Generations', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Health', 'Health Sciences', 'Human', 'Human Genome Project', 'Immune System Diseases', 'Individual', 'Informatics', 'Integrative Medicine', 'Knowledge', 'Lead', 'Life', 'Life Style', 'Link', 'Machine Learning', 'Medicine', 'Mentors', 'Mission', 'Monozygotic twins', 'Nature', 'Neurology', 'Ohio', 'Organ', 'Outcome', 'Phenotype', 'Pilot Projects', 'Population', 'Power Plants', 'Predisposition', 'Program Development', 'Public Health', 'Public Health Education', 'Recruitment Activity', 'Request for Applications', 'Research', 'Research Personnel', 'Route', 'Series', 'Systems Biology', 'Technology', 'Time', 'Translating', 'Translations', 'Transportation', 'Update', 'Vision', 'Work', 'base', 'career development', 'clinical practice', 'disorder prevention', 'disorder risk', 'environmental agent', 'epigenome', 'experience', 'gene environment interaction', 'genome wide association study', 'high risk', 'human capital', 'innovation', 'insight', 'lipid disorder', 'malignant endocrine gland neoplasm', 'member', 'multidisciplinary', 'next generation sequencing', 'programs', 'response', 'success', 'symposium', 'translation to humans']",NIEHS,UNIVERSITY OF CINCINNATI,P30,2017,106418,-0.02536687118953597
"Center for Environmental Genetics This application requests a 5-year renewal of the Center for Environmental Genetics. The unified theme of  the proposal is to elucidate how gene and environment interaction through epigenetics influences disease  risks and health outcome. Ohioans are exposed to exceptionally bad air quality due to dated power plants  and the fact that it is situated in the nation's main artery of the north-south transportation route. These unique  geographical/economic features put the residents in the state at high risk of the following diseases: (a)  endocrine disruption and cancer; (b) immune and allergic diseases; (c) cardiovascular and lipid disorders;  and (d) neurology and behavior disorders. The CEG has 20-years' of accumulative success in environmental  health sciences (EHS) research, our aspiration is to help mitigate these environmental diseases, first in Ohio,  then extend our success to similar populations in the nation and around the globe. Here we propose to attain  our goal by (1) expanding EHS research human capital through the continued recruitment and support of  investigators of all levels to EHS research, (2) innovative ideas and collaborative work from new and  established investigators will be supported by the Pilot Project Program and Director's Fund, (3) junior  investigators will be mentored by a Career Development Program and strong mentor-mentee relationships;  (4) an Integrative Technology Support Core combined with a Bioinformatics Core will provide investigators  with leading-edge technologies and data analysis to attain a new level of system-biology approach to EHS;  (5) enhancement activities such as workshops, symposia and seminar series will be used to stimulate  collaboration and innovation; (6) an Integrative Health Sciences Facility Core will provide the toolbox for  translation of EHS to the clinical and human studies; and (7) a Community Outreach and Engagement Core  will serve to translate EHS to public health and health education for healthier living and disease prevention.  The center is led by an outstanding team of directors with visionary scientific and administrative experiences  and insights. The team will be assisted by Internal and External Advisory Boards to provide checks and  balances. CEG, through cooperation with other centers will strive to expand EHS capacity and beyond. Our environmental health center focuses on understanding how genes and environment affect our health,  serving the residents, communities of Ohio, the nation and the globe by generating knowledge that is  applicable to the general public and policymakers for healthier living.",Center for Environmental Genetics,9565118,P30ES006096,"['Acute', 'Affect', 'Air', 'Allergic Disease', 'Area', 'Arteries', 'Attention', 'Barker Hypothesis', 'Behavior Disorders', 'Biogenesis', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cells', 'Chronic', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Community Outreach', 'Complex', 'Core Facility', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Dose', 'Economics', 'Educational workshop', 'Endocrine disruption', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Epigenetic Process', 'Equilibrium', 'Equipment', 'Evolution', 'Exposure to', 'Funding', 'General Population', 'Generations', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Health', 'Health Sciences', 'Human', 'Human Genome Project', 'Immune System Diseases', 'Individual', 'Informatics', 'Integrative Medicine', 'Knowledge', 'Lead', 'Life', 'Life Style', 'Link', 'Machine Learning', 'Medicine', 'Mentors', 'Mission', 'Monozygotic twins', 'Nature', 'Neurology', 'Ohio', 'Organ', 'Outcome', 'Phenotype', 'Pilot Projects', 'Population', 'Power Plants', 'Predisposition', 'Program Development', 'Public Health', 'Public Health Education', 'Recruitment Activity', 'Request for Applications', 'Research', 'Research Personnel', 'Route', 'Series', 'Systems Biology', 'Technology', 'Time', 'Translating', 'Translations', 'Transportation', 'Update', 'Vision', 'Work', 'base', 'career development', 'clinical practice', 'disorder prevention', 'disorder risk', 'environmental agent', 'epigenome', 'experience', 'gene environment interaction', 'genome wide association study', 'high risk', 'human capital', 'innovation', 'insight', 'lipid disorder', 'malignant endocrine gland neoplasm', 'member', 'multidisciplinary', 'next generation sequencing', 'programs', 'response', 'success', 'symposium', 'translation to humans']",NIEHS,UNIVERSITY OF CINCINNATI,P30,2017,52102,-0.02536687118953597
"Center for Environmental Genetics This application requests a 5-year renewal of the Center for Environmental Genetics. The unified theme of  the proposal is to elucidate how gene and environment interaction through epigenetics influences disease  risks and health outcome. Ohioans are exposed to exceptionally bad air quality due to dated power plants  and the fact that it is situated in the nation's main artery of the north-south transportation route. These unique  geographical/economic features put the residents in the state at high risk of the following diseases: (a)  endocrine disruption and cancer; (b) immune and allergic diseases; (c) cardiovascular and lipid disorders;  and (d) neurology and behavior disorders. The CEG has 20-years' of accumulative success in environmental  health sciences (EHS) research, our aspiration is to help mitigate these environmental diseases, first in Ohio,  then extend our success to similar populations in the nation and around the globe. Here we propose to attain  our goal by (1) expanding EHS research human capital through the continued recruitment and support of  investigators of all levels to EHS research, (2) innovative ideas and collaborative work from new and  established investigators will be supported by the Pilot Project Program and Director's Fund, (3) junior  investigators will be mentored by a Career Development Program and strong mentor-mentee relationships;  (4) an Integrative Technology Support Core combined with a Bioinformatics Core will provide investigators  with leading-edge technologies and data analysis to attain a new level of system-biology approach to EHS;  (5) enhancement activities such as workshops, symposia and seminar series will be used to stimulate  collaboration and innovation; (6) an Integrative Health Sciences Facility Core will provide the toolbox for  translation of EHS to the clinical and human studies; and (7) a Community Outreach and Engagement Core  will serve to translate EHS to public health and health education for healthier living and disease prevention.  The center is led by an outstanding team of directors with visionary scientific and administrative experiences  and insights. The team will be assisted by Internal and External Advisory Boards to provide checks and  balances. CEG, through cooperation with other centers will strive to expand EHS capacity and beyond. Our environmental health center focuses on understanding how genes and environment affect our health,  serving the residents, communities of Ohio, the nation and the globe by generating knowledge that is  applicable to the general public and policymakers for healthier living.",Center for Environmental Genetics,9565126,P30ES006096,"['Acute', 'Affect', 'Air', 'Allergic Disease', 'Area', 'Arteries', 'Attention', 'Barker Hypothesis', 'Behavior Disorders', 'Biogenesis', 'Bioinformatics', 'Biological', 'Cardiovascular Diseases', 'Cells', 'Chronic', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Community Outreach', 'Complex', 'Core Facility', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Dose', 'Economics', 'Educational workshop', 'Endocrine disruption', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Epigenetic Process', 'Equilibrium', 'Equipment', 'Evolution', 'Exposure to', 'Funding', 'General Population', 'Generations', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Geography', 'Goals', 'Health', 'Health Sciences', 'Human', 'Human Genome Project', 'Immune System Diseases', 'Individual', 'Informatics', 'Integrative Medicine', 'Knowledge', 'Lead', 'Life', 'Life Style', 'Link', 'Machine Learning', 'Medicine', 'Mentors', 'Mission', 'Monozygotic twins', 'Nature', 'Neurology', 'Ohio', 'Organ', 'Outcome', 'Phenotype', 'Pilot Projects', 'Population', 'Power Plants', 'Predisposition', 'Program Development', 'Public Health', 'Public Health Education', 'Recruitment Activity', 'Request for Applications', 'Research', 'Research Personnel', 'Route', 'Series', 'Systems Biology', 'Technology', 'Time', 'Translating', 'Translations', 'Transportation', 'Update', 'Vision', 'Work', 'base', 'career development', 'clinical practice', 'disorder prevention', 'disorder risk', 'environmental agent', 'epigenome', 'experience', 'gene environment interaction', 'genome wide association study', 'high risk', 'human capital', 'innovation', 'insight', 'lipid disorder', 'malignant endocrine gland neoplasm', 'member', 'multidisciplinary', 'next generation sequencing', 'programs', 'response', 'success', 'symposium', 'translation to humans']",NIEHS,UNIVERSITY OF CINCINNATI,P30,2017,39875,-0.02536687118953597
An Integrated Biomedical Dataset Discovery System for Immunological Research Databases  The The Contractor will develop a prototype of an integrated biomedical dataset discovery system for Immunological Research Databases (BIRD) to facilitate integrated search for data/knowledge/tools of interest from DAIT bioinformatics resources. n/a,An Integrated Biomedical Dataset Discovery System for Immunological Research Databases ,9574416,72201700019C,"['Bioinformatics', 'Contractor', 'Data', 'Data Discovery', 'Data Reporting', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Immune system', 'Immunology', 'Ingestion', 'Knowledge', 'Maps', 'Metadata', 'Modeling', 'Natural Language Processing', 'Ontology', 'Research', 'Resources', 'System', 'Techniques', 'Technology', 'Terminology', 'base', 'data integration', 'indexing', 'information organization', 'interest', 'prototype', 'tool', 'user-friendly']",NIAID,"TECHWAVE INTERNATIONAL, INC.",N43,2017,224960,-0.03709437384298008
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9360448,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biological Assay', 'Biology', 'Biometry', 'Cells', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Quality', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Imagery', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2017,906404,-0.019293968202007322
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9352296,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2017,249995,-0.0609193445769507
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs.         PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.            ",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9346658,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,WASHINGTON STATE UNIVERSITY,U54,2017,1832235,-0.008257527839958572
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs. PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9386165,U54AT008909,"['Address', 'Archives', 'Clinical', 'Clinical Management', 'Clinical assessments', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Drug usage', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Physiological', 'Procedures', 'Process', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinical risk', 'clinically relevant', 'data archive', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository', 'web portal']",NCCIH,UNIVERSITY OF WASHINGTON,U54,2017,284531,-0.008257527839958572
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,9307936,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Databases', 'Disease', 'Drug Exposure', 'Drug Modelings', 'Drug toxicity', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'cost', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'genomic data', 'improved', 'longitudinal dataset', 'novel', 'open source', 'personalized medicine', 'phenotypic data', 'public health relevance', 'rapid growth', 'rare variant', 'response', 'study population', 'success', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2017,600474,-0.03116650450820731
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9325275,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Plasticizers', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2017,324508,-0.026915500384189654
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9564312,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Medical Libraries', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'polyposis', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2017,54232,-0.012064988448611277
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9551116,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Medical Libraries', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'polyposis', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2017,842722,-0.012064988448611277
"Query Log Analysis for Improving User Access to NCBI Web Services Over the last decade, the online search for biological information has progressed rapidly and has become an integral part of any scientific discovery process. Today, it is virtually impossible to conduct R&D in biomedicine without relying on the kind of Web resources developed and maintained by the NCBI. Indeed, each day millions of users search for biological information via NCBIs online Entrez system. However, finding data relevant to a users information need is not always easy in Entrez. Improving our understanding of the growing population of Entrez users, their information needs and the way in which they meet these needs opens opportunities to improve information services and information access provided by NCBI.   Among all Entrez databases, PubMed is the most used and often serves as an entry point for people to access related data in other databases.One resource for understanding and characterizing patrons of PubMed search engines is its transaction logs. Our previous investigation of PubMed search logs has led us to develop and deploy several useful applications in assisting user searches and retrieval such as the query formulation in PubMed, namely Related Queries, Query Autocomplete and Author Name Disambiguation.   Inspired by past success, we have continued using log analysis to improve access to NCBI resources. For example, we have used user clicks to identify articles that the user considered relevant to their own query. In 2016-2017, we have used deep learning models to understand the relationship between the query and the content of potentially relevant articles. This approach is robust and outperforms both traditional IR algorithms as well as related shallow and deep models based on continuous representations of text, with better results on the under-specified query and term mismatch problems.  Of course, there are multiple factors that indicate whether an article is relevant to the searcher. These include the connection between the query and the content, how recent the article is, whether other people found the article relevant, etc. PubMeds new Best Match sort order (using a Learning to Rank algorithm) combines a number of different scores and sources of information to identify the most relevant queries. This has significantly improved the results of our relevance rankings since Spring 2017.   We are continuing the effort begun by our work on TermVariants. When a term is used in a query, usually documents using equivalent terms are also desired. A seeming trivial example is singular and plural terms. But care must be taken to avoid irrelevant articles. For example, navely applying plural rules to abbreviations is often not helpful. Guidelines are being developed to show where these expansions will be helpful.  To better understand queries, we developed a Field Sensor to completely identify the portions and aims of a query. In other words, we identify which part of the query is an author name, a journal title, a date, or key phrases describing a knowledge the searcher would like to uncover. One practical use for this tool is reminding those looking for information, not specific articles, about our improved relevance searching.  We continue to improve our handling and understanding of author names in PubMed articles. Principle Investigators on NIH-funded grants make a particularly important subset of PubMed authors. Additional information about these authors is available from their grants. Information about published papers in grants allows us to do a better job connecting papers and authors. These authors can be more reliably identified between different institutional affiliations, across changes in research focus and even connect different names for the same author. n/a",Query Log Analysis for Improving User Access to NCBI Web Services,9564626,ZIALM000001,"['Abbreviations', 'Algorithms', 'Biological', 'Caring', 'Data', 'Databases', 'Formulation', 'Funding', 'Goals', 'Grant', 'Guidelines', 'Improve Access', 'Information Services', 'Internet', 'Investigation', 'Journals', 'Knowledge', 'Learning', 'Link', 'Modeling', 'Molecular Biology', 'Names', 'Occupations', 'Paper', 'Population', 'Process', 'PubMed', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Source', 'Specific qualifier value', 'System', 'Text', 'Transact', 'United States National Institutes of Health', 'Work', 'base', 'improved', 'online resource', 'phrases', 'research and development', 'search engine', 'sensor', 'success', 'tool', 'virtual', 'web services']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2017,1606260,-0.004699938564209831
"Development Of Theoretical Methods For Studying Biological Macromolecules New theoretical techniques are being developed and characterized. These efforts are usually coupled with software development, and involve the systematic testing and evaluation of new ideas.  Ongoing development of the Action-CSA method We developed a new computational approach to find multiple conformational transition pathways of biological macromolecules with fixed initial and final states via global optimization of Onsager-Machlup (OM) action. For efficient global optimization of OM action, we used the conformational space annealing (CSA) algorithm and the modified classical action. This approach successfully samples not only the most dominant pathway but also other possible ones without initial guesses of reaction pathways. It was demonstrated that the rank order of actions and transition time distribution of multiple pathways identified by the new approach are in good agreement with those of molecular dynamics simulations. The new method successfully finds multiple folding pathways of the small protein FDS-1. The lowest action folding pathway obtained with Action-CSA shows that the helix of FSD-1 folds earlier than the -sheet part. This pathway is consistent with the experimentally identified folding mechanism of FSD-1. In addition, we are currently developing an algorithm to perform direct optimization of OM action using the Hessian matrix of a potential. Currently, we are investigating pathways of alpha-beta transition caused by nine-residue small peptide whose sequence is YQNPDGSQA.  Benchmark of polarizable force field models using QM/MM polarization energy We compare different polarizable force field models for estimating the polarization energy in hybrid QM/MM calculations. Two models (PAC and PAD) have been formulated based on QM response kernels, and closely resemble the fluctuating charge polarizable model and the induced dipole polarizable model respectively. An empricially parametrized force field, the Drude polarizable force field, is also tested. Our results suggests that polarization effects, including both local charge distortion and intramolecular charge transfer, can be well captured by induced dipole type models with proper parametrization.  Development of the MPID polarizable force field We developed a new polarizable force field based on mapping the electrostatic model optimized in the context of the Drude force field onto a multipole and induced dipole (MPID) model. Condensed phase simulations on water and 15 small model compounds show that without any reparametrization, the MPID model yields properties similar to the Drude force field with both models yielding satisfactory reproduction of a range of experimental data. We subsequently show that the MPID model also works for biomacromolecules including proteins and nucleic acids. With the new MPID force field, more than 15 years of development of the Drude polarizable force field can now be used without the need for dual-thermostat integrators nor self-consistent iterations.  Constraint free energy calculation A good way to increase the accuracy and precision of free energy calculations is to use constraints and to calculate the free energy costs of constraints during post-processing. A new functionality in CHARMM has been implemented to compute the free energy when applying or removing constraints on arbitrary degree of freedom, which will be done as additional explicit steps in the free energy cycle. With this constraints for free energy methods employed, the phase space overlap between ensembles can be highly increased, which is required for accuracy and convergence. The new techniques focus on hard degrees of freedom and use both gradients and Hessian Estimation. As Hessian matrix computation is the most time-consuming step, we introduced an approximation which saves much machine time. As a byproduct for debugging this functionality, we also implemented an optimizer for the geometry of any specific set of molecular fragments.  An Eighth-Shell(ES) implementation of P21 periodic boundary condition in CHARMM Spatial domain decomposition methods, for evaluating pairwise particle interactions, function by assigning a specific region of space to each processor. For moderate sized clusters (with 150 processors), eighth-shell method has been shown to provide the best performance. The parallel engine of CHARMM, DOMDEC, however, implements only the P1 periodic boundary condition. In this work, we extend the implementation to include P21 periodic boundary condition. P21 space group is represented by (-X,Y+,-Z) i.e. it has a half-screw symmetry. We modify the import region of DOMDEC while keeping the import volume unchanged and hence obtain parallelization efficiency similar to Eighth-Shell method. This method is useful for membrane simulations as it allows the movement of lipids between the bilayers, thus balancing the chemical potential between them.   Free energy calculations using neural-network based potentials The extremely high computational cost of QM energy calculations limits their use on large biological systems of interest. Semi-empirical methods and classical MM force fields trade accuracy for speed and can give approximate results. However, deep learning of QM potential using neural networks has recently been shown to have similar accuracy to DFT while taking up to six orders of magnitude lesser time. In this work, we are developing a method to perform free energy calculations using neural networks based potential. We represent the molecule using vectors describing the radial and angular environment. An additional per-atom lambda variable scales the contribution of the atom to the evaluated potential energy. Lambda values of 0 and 1 represent the end states with intermediary values representing the alchemical path of transformation. The method is being implemented in OpenMM and uses Tensorflow to learn the model.    Improving sampling in constant pH simulations through use of reservoirs Determining accurate pKa values with constant pH simulations requires extensive conformational sampling. At this point constant pH simulations are still much slower than regular MD simulations with fixed charges. We have thus developed a method to use reservoirs of structures generated with fixed charge MD simulations to improve sampling in constant pH simulations. The reservoir structures are then fed into the constant pH simulations in a statistically correct way. We have tested this method on small systems and show excellent agreement in calculated pKa values and sampled conformations between the new method and the standard constant pH method. For larger systems where conformational sampling is an issue, this method should greatly increase the accuracy of pKa calculations.  Other ongoing efforts include: A virtual mixture simulation approach for constant pH simulation in explicit water Improving sampling in constant pH simulations through use of reservoirs A generalized self-guided Langevin dynamics simulation method n/a",Development Of Theoretical Methods For Studying Biological Macromolecules,9572272,ZIAHL001051,"['Active Sites', 'Agreement', 'Algorithms', 'Amber', 'Basic Science', 'Behavior', 'Benchmarking', 'Biochemical Pathway', 'Biochemical Reaction', 'Biological', 'Biological Neural Networks', 'Biological Process', 'Biophysics', 'Catalysis', 'Cell physiology', 'Cereals', 'Charge', 'Chemicals', 'Complement', 'Computational Biology', 'Computational Technique', 'Computer Assisted', 'Computers', 'Computing Methodologies', 'Coupled', 'Data', 'Development', 'Drug Design', 'Electron Microscopy', 'Electrostatics', 'Environment', 'Enzymes', 'Equilibrium', 'Evaluation', 'Free Energy', 'Freedom', 'Gene Expression Profiling', 'Geometry', 'Goals', 'Hybrids', 'Image Analysis', 'Laboratories', 'Lead', 'Learning', 'Lipids', 'Maps', 'Mechanics', 'Membrane', 'Membrane Proteins', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Molecular Models', 'Movement', 'National Heart, Lung, and Blood Institute', 'Network-based', 'Nucleic Acids', 'Pathway interactions', 'Peptides', 'Performance', 'Periodicity', 'Phase', 'Play', 'Potential Energy', 'Property', 'Proteins', 'Psychological Techniques', 'Quantum Mechanics', 'Radial', 'Reaction', 'Reproduction', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Scientist', 'Speed', 'Structure', 'System', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Water', 'Work', 'base', 'biological systems', 'biomacromolecule', 'computing resources', 'conformational conversion', 'cost', 'design', 'experimental study', 'gene function', 'human disease', 'improved', 'interest', 'macromolecule', 'models and simulation', 'molecular dynamics', 'molecular mechanics', 'molecular modeling', 'multi-scale modeling', 'novel', 'novel strategies', 'parallelization', 'particle', 'programs', 'quantum', 'response', 'simulation', 'small molecule', 'software development', 'therapy design', 'tool', 'vector', 'virtual']",NHLBI,"NATIONAL HEART, LUNG, AND BLOOD INSTITUTE",ZIA,2017,955244,-0.023485645628098673
"Informatics, Machine Learning & Biomedical Data Science Over the past year, we have been active in: (1) developing computationally efficient methods and algorithms to solve known problems in the analysis of biomedical and clinical data and study complex interactions in biological systems; (2) developing knowledge-based data management systems for the discovery and curation of biomedical knowledge, including distributed annotation systems and clinical information management systems;  (3) applying predictive-analytic models to scientific and administrative domains; and (4) consulting with NIH leadership to provide evidence-based solutions to improve the grant application and review process.  Specifically, in 2016, collaborative efforts in support of these goals included the following:  - In a partnership with Dr. John Tsang of the NIAID Laboratory of Systems Biology, HPCIO is conducting a multifaceted project to profile the immune system using the latest high-throughput, multiplexed technologies and systems approaches. One of the goals of this collaboration is to develop novel computational methodologies that can exploit inter-subject heterogeneity and measurements at various scales to assess the roles of the immune system in health and disease.  In a cohort of more than 1,000 patients who went through gastric bypass surgeries, we have developed an approach to deconvolve individual populations of immune cell subsets in complex tissues based on tissue gene expression data.  Associations between the immune cell populations and clinical traits were measured to understand the roles these immune cells play in response to obesity-driven signals. In another aspect of the collaboration, we have compared the chromatin accessibility of fresh and cryopreserved samples of four immune cell types from health donors.  The results from ATAC-seq analyses indicate subtle but interesting changes after cells have been frozen, and may have implication to the practicality of using cryopreserved samples in assessing the regulatory landscape of gene expression in the cells.  - HPCIO is working with NCI Occupational & Environmental Epidemiology Branch to develop methodologies to incorporate occupational risk factors into epidemiological models. We are enlarging the training data to improve our novel classifiers for coding free text job descriptions into the 840 codes of the 2010 U.S. Standard Occupational Classification System. Agreement between our classification system and expert coders is measured using SOC code agreement and exposure prediction from CANJEM, a job-exposure matrix of over 250 exposure agents developed by Jerome Lavoue at the University of Montreal.  We are also working with NCI to develop a two-stage mixed generalized linear model to predict lifetime occupation exposures to lead.  - In collaboration with the Membrane Transport Biophysics Section of NINDS, HPCIO is 1) developing  a computational tool to accurately identify the boundaries of the lysosomes in fluorescence microscopy and 2) using the fluorescence ration to measure lysosomal pH within each organelle for better understanding of the lysosomal pH regulation.    - A freely available plasmid database that is inter-operable with popular freeware is currently being developed for the NIDA Optogenetics and Transgenic Technology Core. The Plasmid Manager offers a versatile yet simple platform for scientists to store and analyze their plasmid data. Motivated by the need for a more comprehensive approach to archiving plasmid data, the database platform is enriched with numerous components beyond the repository, serving as an informatics platform designed to enhance the efficiency and analytic capabilities of scientists.   - As high-throughput next-generation sequencing (NGS) technology plays an important role in systematically identifying novel cancer driver mutations in genome-wide surveys, NGS data generation is rapidly increasing, currently accumulating at a rate of several terabytes of data every month at the Lymphoid Malignancies Section of NCI. In collaboration with the Louis Staudt Laboratory, a bioinformatics website is being developed containing useful tools for the analysis of the laboratorys Diffuse Large B-Cell Lymphoma data. This website enables users with very little computer expertise to run their own analyses, as opposed to having a specialist run the analyses for them. Methodologies in parallelization and text searching have also been incorporated for returning the analysis results much more quickly and efficiently than before.  - In collaboration with NIA and NCI, we are applying machine learning and visualization techniques on large biological datasets to discover novel patterns of functional gene or protein interactions as related to aging. In this collaboration, we are developing a machine learning method that models the temporal nature of the longitudinal clinical data to predict the progression of Amyotrophic lateral sclerosis. Such machine learning method may also work well in prediction of high-dimensional time-series genomic data.  - The Human Salivary Proteome Wiki is a community-driven Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. Many features and external contents have been incorporated over the last few years to make it easier for users to extract different kinds of information from the wiki.  One of the latest enhancements is the integration of RNA-seq transcriptional and protein immunohistochemistry data from the Human Protein Atlas.  This affords users the ability to weigh evidence generated by different, independent modalities, in addition to the original mass-spectrometry-based data, to assess the status of a protein.    - In collaboration with CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process. A Web-based automated referral tool, called ART, is being developed to help PIs and SROs to identify the most relevant study section(s) or special emphasis panel(s) based on the scientific content of an application. In addition, HPCIO is analyzing text from quick feedback surveys on peer review. This effort includes evaluating a pilot study to evaluate the feasibility of analyzing free text from peer reviewers on their perception of the study section quality. If successful, the pilot results will be used to as initial input for a full-scale implementation.  - In collaboration with the Office of Data Analysis Tools and Systems, NIH Office of Extramural Research, HPCIO has been developing a standard database update pipeline for NIH Topic Maps, originally developed by Dr. Ned Talley of NINDS.   We are preparing this pipeline for incorporation into a stable hosted instance and hope to go live in late 2016.  - In collaboration with NIAID, HPCIO has released HT JoinSolver(R), a new application capable of analyzing V(D)J recombination in thousands of immunoglobulin gene sequences produced by high throughput sequencing.  - Based on its experience in building novel models for classifying research grants and projects, HPCIO has collaborated with several ICs to develop the Portfolio Learning Tool, a comprehensive classification workflow system that will allow users to select from multiple classification algorithms, feature spaces, and training regimes, to build and run their own classifiers. HPCIO has developed an augmented support vector machine (SVM) that augments a training set by sampling from a corpus of unknowns and runs a large ensemble on various samples of this augmented space. The results obtained from this classifier suggest that, when coupled with an effective annotation strategy, such a classifier can be quite effective at categorizing a research portfolio. n/a","Informatics, Machine Learning & Biomedical Data Science",9344251,ZIHCT000200,"['ATAC-seq', 'Aging', 'Agreement', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Applications Grants', 'Archives', 'Atlases', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biophysics', 'Cells', 'Chromatin', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Linguistics', 'Computers', 'Computing Methodologies', 'Consult', 'Coupled', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Databases', 'Disease', 'Environmental Epidemiology', 'Evaluation', 'Expert Systems', 'Exposure to', 'Extramural Activities', 'Feedback', 'Fluorescence', 'Fluorescence Microscopy', 'Fostering', 'Freezing', 'Funding', 'Gene Expression', 'Generations', 'Genes', 'Genomics', 'Goals', 'Grant Review Process', 'Harvest', 'Health', 'Heterogeneity', 'High Performance Computing', 'High-Throughput Nucleotide Sequencing', 'Human', 'Imagery', 'Immune', 'Immune system', 'Immunoglobulin Genes', 'Immunohistochemistry', 'Individual', 'Informatics', 'Interdisciplinary Study', 'Intramural Research Program', 'Job Description', 'Knowledge', 'Laboratories', 'Lead', 'Leadership', 'Learning', 'Life', 'Linear Models', 'Lysosomes', 'Machine Learning', 'Malignant Neoplasms', 'Malignant lymphoid neoplasm', 'Management Information Systems', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Nature', 'Obesity', 'Occupational', 'Occupational Epidemiology', 'Occupations', 'Online Systems', 'Organelles', 'Patients', 'Pattern', 'Peer Review', 'Perception', 'Pilot Projects', 'Plasmids', 'Play', 'Population', 'Predictive Analytics', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Research Project Grants', 'Risk Factors', 'Role', 'Running', 'Salivary', 'Sampling', 'Scientist', 'Series', 'Signal Transduction', 'Specialist', 'Staging', 'Strategic Planning', 'Study Section', 'Surveys', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Time', 'Tissues', 'Training', 'Transgenic Organisms', 'Translational Research', 'Transmembrane Transport', 'United States National Institutes of Health', 'Universities', 'Update', 'V(D)J Recombination', 'Work', 'actionable mutation', 'bariatric surgery', 'base', 'bioimaging', 'biological systems', 'biomedical informatics', 'cell type', 'cohort', 'computerized tools', 'data management', 'data mining', 'data sharing', 'design', 'epidemiological model', 'evidence base', 'experience', 'genome-wide', 'genomic data', 'human data', 'improved', 'information organization', 'innovation', 'interdisciplinary approach', 'interest', 'knowledge base', 'large cell Diffuse non-Hodgkin&apos', 's lymphoma', 'learning strategy', 'next generation sequencing', 'novel', 'optogenetics', 'parallelization', 'peer', 'programs', 'repository', 'research and development', 'response', 'software development', 'terabyte', 'text searching', 'tool', 'trait', 'transcriptome sequencing', 'web portal', 'web site', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2016,2034266,-0.02320320151504024
"Deep learning for representation of codes used for SEER-Medicare claims research ﻿    DESCRIPTION (provided by applicant):  We propose developing an algorithm and user-friendly software to better identify treatments using Medicare claims data. We will validate our approach using procedures listed in the Surveillance, Epidemiology, and End Results (SEER) database as a gold standard. In this way, we hope to better match procedures identified using Medicare claims data with SEER listed procedures.  The focus of this research is observational (i.e. non-randomized) data. Well-run randomized clinical trials can provide the best level of evidence of treatment effects. However, randomized trials in the United States have suffered from poor accrual for many interventions. Despite the fact that well-designed randomized clinical trials should be the gold standard, well-designed observational studies might be the only method of obtaining inferences concerning comparative effectiveness for some cancer interventions.  In cancer research, one of the most commonly used databases for observational research is the linked SEER-Medicare database. SEER-Medicare data has provided useful measurements of the effectiveness of a number of cancer therapies. Algorithms for identifying relevant treatment and diagnosis codes using Medicare data are often based on clinical reasoning and scientific evidence. One group of researchers, for example, developed an algorithm for identifying laparoscopic surgery among kidney cancer cases before claims codes for laparoscopic surgery were well developed. While such algorithms are useful for others pursuing similar investigations, there may still be substantial mismatch between treatment identified by the SEER cancer registry and treatment identified through Medicare claims. In this work, we propose developing a rigorous machine learning algorithm that can help researchers in better identifying treatments in Medicare claims data. Specifically, we will design a neural language modeling algorithm and implement a software system that finds vector representations of diagnosis and procedure codes.  We plan on using the neural language modeling algorithm to learn vector representations from SEER- Medicare claims data where related procedure and diagnosis codes are ""neighbors"" (i.e. closely related). We will investigate whether the codes we identify within neighborhoods correspond to the procedure codes used for published SEER-Medicare studies. We will then design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. Finally, we will investigate the sensitivity and specificity of the algorithm by comparing procedures identified using Medicare claims with procedures listed in the SEER database. We will replicate analyses from a published SEER-Medicare paper to investigate if estimated treatment effects differ when using our novel algorithm compared to using the algorithm in the published paper.         PUBLIC HEALTH RELEVANCE: In cancer research, one of the most commonly used databases for observational research is the linked Surveillance, Epidemiology, and End Results (SEER)-Medicare database. To improve the identification of procedures when using Medicare claims data, we will design a software assistant interface that will allow an investigator to explore which codes are related to a given seed of diagnosis or procedure codes. This should improve the identification of procedures when using Medicare claims data, and make conclusions drawn from analyses using the database more reliable and consistent.            ",Deep learning for representation of codes used for SEER-Medicare claims research,9023921,R21CA202130,"['Algorithms', 'Cancer Intervention', 'Clinical', 'Code', 'Computer software', 'Data', 'Databases', 'Diagnosis', 'Effectiveness', 'Ethical Issues', 'Funding', 'Future', 'Gold', 'International Classification of Diseases', 'Intervention', 'Investigation', 'Language', 'Laparoscopic Surgical Procedures', 'Learning', 'Level of Evidence', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Medical Records', 'Medical Surveillance', 'Medicare', 'Medicare claim', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhoods', 'Observational Study', 'Outcome', 'Paper', 'Patients', 'Procedures', 'Process', 'Proxy', 'Publishing', 'Randomized', 'Randomized Clinical Trials', 'Records', 'Renal carcinoma', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sensitivity and Specificity', 'Software Tools', 'Statistical Study', 'Terminology', 'Testing', 'Time', 'United States', 'Update', 'Work', 'abstracting', 'anticancer research', 'base', 'cancer therapy', 'comparative effectiveness', 'design', 'health disparity', 'improved', 'interest', 'malignant breast neoplasm', 'neoplasm registry', 'novel', 'public health relevance', 'randomized trial', 'relating to nervous system', 'software systems', 'treatment effect', 'usability', 'user friendly software', 'vector', 'volunteer']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R21,2016,178072,-0.023167642618510127
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9146381,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,457075,-0.02484614453262159
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9243496,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,150865,-0.02484614453262159
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9176354,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Staging', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2016,599995,0.006469984356060583
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,-0.02244327918909253
"Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research ﻿    DESCRIPTION (provided by applicant): Supervised machine learning is a popular method that uses labeled training examples to predict future outcomes.  Unfortunately, supervised machine learning for biomedical research is often limited by a lack of labeled data.  Current methods to produce labeled data involve manual chart reviews that are laborious and do not scale with data creation rates.  This project aims to develop a framework to crowd source labeled data sets from electronic medical records by forming a crowd of clinical personnel labelers.  The construction of these labeled data sets will allow for new biomedical research studies that were previously infeasible to conduct.  There are numerous practical and theoretical challenges of developing a crowd sourcing platform for clinical data.  First, popular, public crowd sourcing platforms such as Amazon's Mechanical Turk are not suitable for medical record labeling as HIPAA makes clinical data sharing risky.  Second, the types of clinical questions that are amenable for crowd sourcing are not well understood.  Third, it is unclear if the clinical crowd can produce labels quickly and accurately.  Each of these challenges will be addressed in a separate Aim. As the first Aim of this project, the team will evaluate different clinical crowd sourcing architectures.  The architecture must leverage the scale of the crowd, while minimizing patient information exposure.  De-identification tools will be considered to scrub clinical notes t reduce information leakage.  Using this design, the team will extend a popular open source crowd sourcing tool, Pybossa, and release it to the public.  As the second Aim, the team will study the type, structure, topic and specificity of clinical prediction questions, and how these characteristics impact labeler quality.  Lastly, the team will evaluate the quality and accuracy of collected clinical crowd sourced data on two existing chart review problems to determine the platform's utility.         PUBLIC HEALTH RELEVANCE: Traditionally, clinical prediction models rely on supervised machine learning algorithms to probabilistically predict clinical events using labeled medical records.  When data sets are small, manual chart reviews performed by clinical staff are sufficient to label each outcome; however, as data sets have scaled up and researchers aim to study larger cohorts, current manual approaches become intractable.  The goal of this proposal is to develop a framework to crowd source labeled data sets from electronic medical records to support prediction model development.        ",Crowd Sourcing Labels From Electronic Medical Records to Enable Biomedical Research,9076555,UH2CA203708,"['Accident and Emergency department', 'Address', 'Algorithms', 'Architecture', 'Area', 'Asthma', 'Biomedical Research', 'Characteristics', 'Childhood', 'Clinical', 'Clinical Data', 'Collection', 'Computerized Medical Record', 'Crowding', 'Data', 'Data Set', 'Development', 'Disclosure', 'Ensure', 'Evaluation', 'Event', 'Extravasation', 'Future', 'Goals', 'Health', 'Health Insurance Portability and Accountability Act', 'Human Resources', 'Incentives', 'Interview', 'Label', 'Machine Learning', 'Management Audit', 'Manuals', 'Measures', 'Mechanics', 'Medical Records', 'Medical Research', 'Medical Students', 'Medical center', 'Methods', 'Modeling', 'Nurses', 'Outcome', 'Patients', 'Privacy', 'Productivity', 'Receiver Operating Characteristics', 'Relapse', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Role', 'Security', 'Specificity', 'Structure', 'System', 'Time', 'Training', 'cohort', 'computer science', 'crowdsourcing', 'design', 'member', 'model development', 'open source', 'public health relevance', 'research study', 'response', 'scale up', 'tool']",NCI,VANDERBILT UNIVERSITY MEDICAL CENTER,UH2,2016,316000,-0.016153673937278564
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,9005867,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'mobile computing', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2016,1271107,-0.007816873091463922
"IGF::OT::IGF ARTIFICIAL INTELLIGENCE IN MEDICINE INC: HHSN261201500033I- TASK ORDER 2; POP 9/26/2016-9/25/2017 The purpose of this acquisition is to 1) maintain and update the existing Surveillance Epidemiology and End Results (SEER) ePath network; 2) expand the SEER network to additional pathology laboratories; 3) expand electronic data capture to include electronic reports from diagnostic imaging, and 4) install cancer data forwarding module in previously installed ePath laboratories n/a",IGF::OT::IGF ARTIFICIAL INTELLIGENCE IN MEDICINE INC: HHSN261201500033I- TASK ORDER 2; POP 9/26/2016-9/25/2017,9361217,61201500033I,"['Artificial Intelligence', 'Data', 'Diagnostic Imaging', 'Electronics', 'Laboratories', 'Malignant Neoplasms', 'Medical Surveillance', 'Medicine', 'Pathology', 'Reporting', 'Update', 'electronic data']",NCI,"ARTIFICIAL INTELLIGENCE IN MEDICINE, INC",N03,2016,1999910,-0.0194189468965733
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338977,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'improved', 'innovation', 'inter-individual variation', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2016,1071976,-0.01868199687959115
"Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research ﻿    DESCRIPTION (provided by applicant): This Phase II project aims to continue development of a commercial quality, innovative cloud hosted information management system, called Climb 2.0(tm) that will increase laboratory efficiency and provide improved capabilities for research laboratories. Climb is designed to offer integrated laboratory process management modules that include mobile communications tools data monitoring and alert systems, and integrated access to Microsoft Azure Cloud(tm) Machine Learning and Stream Analytics services. Initially, Climb 2.0 will target animal model research laboratories; however, the core of the platform is designed to be adaptable to nearly any research type or related industry. Current research information management systems are primarily designed as record-keeping tools with little or no direct focus on laboratory efficiency or in enhancing value of the research data. They also do not leverage emergent mobile device technologies, social media frameworks, and data analysis and storage capabilities of cloud computing. Many laboratories still use paper as their primary recording system. Paper data logging is then followed by secondary data entry into a laboratory database. These systems are error prone, time consuming and lead to laboratory databases with significant time lags between data acquisition and data entry. Moreover, they do not recognized cumulative data relationships, which may identify important trends, and researchers often miss windows of opportunity to take action on time-sensitive events. In Phase I, RockStep Solutions demonstrated feasibility of an innovative Cloud Information Management Bundle system, Climb, which will increase efficiency and improve capabilities in animal model data management. During Phase I, a beta version of Climb was successfully developed and tested against strict performance metrics as a proof of concept. We successfully built a prototype with working interfaces that integrates real-time communication technologies with media capabilities of mobile devices. Phase II proposes four specific Aims: 1) Develop the technology infrastructure to support the secure and scalable Software as a Service (SaaS) deployment of Climb for enterprise commercial release; 2) Develop and extend the Phase-I prototype Data Monitoring and Messaging System (DMMS) into a platform ready for production use; 3) Extend Climb's DMMS adding a Stream Analytics engine to support Internet of Things (IoT) devices and streaming media; 4) Deploy a beta release of Climb at partner research labs, test and refine the product for final commercialization. To ensure Climb is developed with functionality and tools relevant to research organizations, RockStep Solutions has established collaborations with key beta sites to test all of the major functionality developed in this proposal. IMPACT: By leveraging emergent technologies and cloud computing, Climb offers several advantages: 1) enables real-time communications using familiar tools among members of research groups; 2) reduces the risk of experimental setbacks, and 3) enables complex experiments to be conducted efficiently.         PUBLIC HEALTH RELEVANCE: The NIH Invests approximately $12 billion each year in animal model research that is central to both understanding basic biological processes and for developing applications directly related to improving human health. More cost-effective husbandry and colony management techniques, equipment, and new approaches to improve laboratory animal welfare and assure efficient and appropriate research use (e.g., through cost savings and reduced animal burden) are high priorities for NIH. RockStep Solutions proposes to meet this need by integrating existing and emergent software and communication technologies to create a novel solution, Climb 2.0, for research animal data management. Climb 2.0 extracts immediate value of data in animal research laboratories and extends the database into a multimedia communication network, thus enabling science that would be impractical to conduct with traditional methods.        ",Novel Use of Emergent Technologies to Improve Efficiency of Animal Model Research,9138555,R44GM112206,"['Address', 'Algorithms', 'Animal Experimentation', 'Animal Model', 'Animal Welfare', 'Animals', 'Biological Process', 'Biomedical Research', 'Clinical Laboratory Information Systems', 'Cloud Computing', 'Collaborations', 'Communication', 'Communication Tools', 'Complex', 'Computer software', 'Computers', 'Consumption', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Security', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Devices', 'Emerging Technologies', 'Ensure', 'Equipment', 'Equipment and supply inventories', 'Event', 'Feedback', 'Future', 'Goals', 'Health', 'Health system', 'Human', 'Industry', 'Information Management', 'Internet', 'Investments', 'Laboratories', 'Laboratory Animals', 'Laboratory Research', 'Lead', 'Machine Learning', 'Management Information Systems', 'Methods', 'Monitor', 'Motion', 'Multimedia', 'Notification', 'Paper', 'Performance', 'Phase', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rest', 'Risk', 'Science', 'Secure', 'Services', 'Site', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'The Jackson Laboratory', 'Time', 'United States National Institutes of Health', 'Work', 'analytical tool', 'animal data', 'base', 'cloud based', 'commercialization', 'computerized data processing', 'cost', 'cost effective', 'data acquisition', 'data management', 'design', 'encryption', 'good laboratory practice', 'graphical user interface', 'handheld mobile device', 'improved', 'innovation', 'laptop', 'meetings', 'member', 'novel', 'novel strategies', 'predictive modeling', 'programs', 'prototype', 'public health relevance', 'research study', 'social media', 'software as a service', 'success', 'time interval', 'tool', 'trend', 'usability']",NIGMS,"ROCKSTEP SOLUTIONS, INC.",R44,2016,750063,-0.012801666846909972
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9161233,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'Work', 'base', 'dashboard', 'empowered', 'genome sequencing', 'improved', 'insight', 'meetings', 'microbial community', 'peer', 'repository', 'social', 'tool', 'transcriptomics', 'web portal']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2016,575532,-0.029474712843861408
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,0.006106827093949083
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9161167,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Maps', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'information processing', 'knowledge base', 'novel', 'personalized medicine', 'repository', 'response', 'stem', 'tool']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2016,471848,-0.026721412452100295
"Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error ﻿    DESCRIPTION (provided by applicant): Tracking evolutionary changes in viral genomes and their spread often requires the use of data deposited in public databases such as GenBank, the Influenza Research Database (IRD), or the Virus Pathogen Resource (ViPR). GenBank provides an abundance of available viral sequence data for phylogeography. Sequences and their metadata can be downloaded and imported into software applications that generate phylogeographic trees and models for surveillance. IRD and ViPR are NIH/NIAID funded programs that import data from GenBank but contain additional data sources, visualization, and search tools for their users. Tracking evolutionary changes and spread also requires the geospatial assignment of taxa, which is often obtained from GenBank metadata. Unfortunately, geospatial metadata such as host location is often uncertain in GenBank entries, with only 36% containing a precise location such as a county, town, or region within a state. For example, information such as China or USA was indicated instead of Beijing or Bedford, NH. While town or county might be included in the corresponding journal article, this valuable information is not available for immediate use unless it is extracted and then linked back to the appropriate sequence. The goal of our work is to enable health agencies and other researchers to automatically generate phylogeographic models that incorporate enhanced geospatial data for better estimates of virus spread. This proposal focuses on developing and applying information extraction and statistical phylogeography approaches to enhance models that track evolutionary changes in viral genomes and their spread. We propose a framework that uses natural language processing (NLP) for the automatic extraction of relevant geospatial data from the literature, and assigns a confidence between such geospatial mentions and the GenBank record. We will then use these locations and the estimates as observation error in the creation of phylogeographic models of zoonotic virus spread. We hypothesize that a combined NLP-phylogeography infrastructure that produces models that include observation error in the geospatial assignment of taxa will be closer to a gold standard than phylogeographic models that do not include them. Our research will extend phylogeography and zoonotic surveillance by: creating a NLP infrastructure that will improve the level of detail of geospatial data for phylogeography of zoonotic viruses (Aim 1), develop phylogeographic models using the estimates from Aim 1 as observation error (Aim 2), and evaluating our approach by comparing the models it produces to models that do not account for observation error in the geospatial assignment of taxa (Aim 3). We will allow users to generate enhanced models and view results on a web portal accessible via a LinkOut feature from GenBank, IRD, and ViPR. The addition of more precise geospatial information in building such models could enable health agencies to better target areas that represent the greatest public health risk.         PUBLIC HEALTH RELEVANCE: We will develop and evaluate an infrastructure that uses Natural Language Processing (NLP) to identify more precise geographic information for modeling spread of zoonotic viruses. These new models could enable public health agencies to identify the most at-risk areas. In addition, by improving geospatial information in popular sequence databases such as GenBank, we will enrich other sciences that utilize this information such as molecular epidemiology, population genetics, and environmental health.                ",Tracking Evolution and Spread of Viral Genomes by Geospatial Observation Error,9065021,R01AI117011,"['Accounting', 'Animals', 'Applied Research', 'Area', 'Award', 'Back', 'China', 'Computer software', 'County', 'Data', 'Data Sources', 'Databases', 'Deposition', 'Development', 'Diffusion', 'Disease', 'Environmental Health', 'Evaluation', 'Evolution', 'Funding', 'Genbank', 'Genetic Variation', 'Genome', 'Goals', 'Gold', 'Hantavirus', 'Health', 'Human', 'Imagery', 'Influenza', 'Knowledge', 'Link', 'Literature', 'Location', 'Metadata', 'Methods', 'Modeling', 'Molecular Epidemiology', 'National Institute of Allergy and Infectious Disease', 'Natural Language Processing', 'Nucleotides', 'Population Genetics', 'Public Health', 'Publications', 'RNA Viruses', 'Rabies', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Source', 'Surveillance Modeling', 'System', 'Taxon', 'Time', 'Trees', 'United States National Institutes of Health', 'Viral', 'Viral Genome', 'Virus', 'Work', 'improved', 'information model', 'interest', 'journal article', 'pathogen', 'population health', 'programs', 'public health relevance', 'simulation', 'surveillance data', 'tool', 'web portal']",NIAID,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2016,479735,-0.04011880366589098
"Semi-supervised learning with electronic medical records Project Summary/Abstract The implementation of electronic medical record (EMR) systems in routine healthcare has resulted in a rich and inexpensive source of data for translational research. When linked with specimen biobanks, these extensive databases offer a unique opportunity to accelerate the goals of disease genomics as they contain large amounts of detailed clinical and genetic data collected for the purposes of medical care [4; 6; 7; 8; 9]. However, the statistical methods to analyze EMR data are limited and thus the focus of this proposal.  In particular, extracting accurate disease phenotype information is a major challenge impeding EMR-based research [10]. Currently, ICD9 codes are used to conﬁrm presence or absence of a disease in cohorts derived from EMRs. These codes are extremely variable and therefore have a signiﬁcant impact on the statistical power of genetic studies [11; 12]. An alternative approach is to develop a highly accurate algorithm to classify disease status. But due to the laborious medical record review required to obtain validated phenotype information for classiﬁer estimation, phenotypes are only available for a small training set nested in a large cohort. In contrast, predictors of phenotype are available for all observations. To improve accuracy and efﬁciency in model estimation and evaluation, it is therefore of interest to develop semi-supervised learning (SSL) methods that utilize the so- called unlabeled data or observations without conﬁrmed phenotype status in addition to the labeled training set.  Although a great body of literature on SSL exists, nearly all methods concern estimation of classiﬁers or prediction rules when the labeled training set is a simple random sample from the large unlabeled data set [13; 14; 15; 16; 17; 18; 19; 20; 21]. Despite the practical importance of evaluating the prediction performance of an estimated model, no SSL procedures currently exist to improve the estimation of model performance parame- ters. Additionally, the simple random sampling assumption is restrictive and the development of semi-supervised (SS) methods that accommodate more ﬂexible sampling schemes in the context of both model estimation and evaluation is needed.  In this proposal, these limitations are addressed through formulation of an efﬁcient method to estimate various prediction performance measures including the ROC curve within the traditional SS framework of simple random sampling. The stratiﬁed random sampling design in the SS setting is also considered and methods to estimate a classiﬁer and its accuracy are developed. These procedures will be applied to EMR-based studies of bipolar disorder and depression. The success of this work will thus improve efﬁciency in analyzing EMR data and expedite the use of EMRs in clinical and genetic research in neuropsychiatry. Project Narrative  The use of electronic medical records (EMRs) in routine healthcare has generated a rich source of data for in-depth study of disease risk factors. However, EMR data typically consists of a very small number of expensive observations with information on disease status and a large amount of automatically extracted data concerning risk factors such as laboratory results and previous health history. Statistical methods that accommodate this data structure are limited and thus the focus of this proposal.",Semi-supervised learning with electronic medical records,9192096,F31GM119263,"['Address', 'Algorithms', 'Bipolar Depression', 'Bipolar Disorder', 'Caring', 'Clinical', 'Clinical Research', 'Code', 'Computerized Medical Record', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Evaluation', 'Formulation', 'Genes', 'Genetic Research', 'Genetic study', 'Genomics', 'Goals', 'Health', 'Healthcare', 'International', 'Label', 'Laboratories', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Genetics', 'Medical Records', 'Methodology', 'Methods', 'Modeling', 'Patients', 'Performance', 'Phenotype', 'Procedures', 'ROC Curve', 'Recording of previous events', 'Research', 'Rest', 'Risk Factors', 'Sampling', 'Sampling Biases', 'Scheme', 'Specimen', 'Statistical Methods', 'System', 'Time', 'Training', 'Translational Research', 'Work', 'abstracting', 'base', 'biobank', 'clinical care', 'clinical practice', 'clinically relevant', 'cohort', 'cost effective', 'data structure', 'design', 'disease phenotype', 'disorder risk', 'experience', 'improved', 'interest', 'learning strategy', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'patient population', 'phenome', 'repository', 'success']",NIGMS,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2016,34098,-0.01894347534706771
"Integrated Public Use Microdata Series: North Atlantic Population Project    DESCRIPTION (provided by applicant): Research teams in the United States, Britain, Canada, Iceland, Norway, and Sweden have worked in close coordination to create the North Atlantic Population Project (NAPP), a massive integrated cross-national microdatabase that provides a baseline for studies of demographic change and opens fresh paths for spatiotemporal data analysis. We now propose improvements that will multiply the power of the NAPP infrastructure. We have three major aims: (1) Triple the size of the database to approximately 365 million records, adding 40 new datasets for the period 1787 to 1930 from Albania, Britain, Canada, Denmark, Egypt, Iceland, Ireland, Germany, Norway, Mexico, Sweden, and the United States. (2) Leverage our innovative record-linkage technology to create linked national panels that will allow expanded longitudinal analyses. (3) Connect the past to the present by merging NAPP with the Integrated Public Use Microdata Series (IPUMS), simplifying analysis of long-run change and ensuring long-run preservation and maintenance of the database. The landscape of scientific research on the human population is shifting. It is no longer sufficient just to study the relationships among variables at a particular moment in time. Researchers around the world now recognize that to understand the large-scale processes that are transforming society, we must investigate long-term change. The goal of this project is to provide the infrastructure to make such analysis possible. NAPP will make a strategic contribution to demographic infrastructure by providing a baseline for the study of changes in the demography and health of European and North American populations. In each country, NAPP provides the earliest census microdata available. Models and descriptions based on historical experience underlie both theories of past change and projections into the future. The NAPP data provide a unique laboratory for the study of economic and demographic processes; this kind of empirical foundation is essential for testing social and economic theory. The proposed work will be carried out by a team of highly-skilled researchers with unparalleled expertise and experience in data integration and record linkage. Collaborators include leading researchers from the University of Minnesota and the Max Planck Institute for Demographic Research, and local experts from each of the participating countries. Centralized support for international collaboration will leverage the investments of each country and allow us to create an extraordinary resource for comparative social and economic research.        The North Atlantic Population Project (NAPP) provides fundamental infrastructure for scientific research, education, and policy-making and will allow social scientists to make comparisons across Northern Europe, the Mediterranean, and North America during three centuries of transformative change. The proposed work is directly relevant to the central mission of the NIH as the steward of medical and behavioral research for the nation: the new data will advance fundamental knowledge about the nature and behavior of human population dynamics. NAPP will unlock access to some of the largest and longest-running cross-sectional and longitudinal data sources in the world, and stimulate health-related research on population growth and movement, fertility, mortality, nuptiality, and family change, as well as the economic and social correlates of demographic behavior.         ",Integrated Public Use Microdata Series: North Atlantic Population Project,9117993,R01HD052110,"['Albania', 'American', 'Behavior', 'Behavioral Research', 'Biological Preservation', 'Canada', 'Censuses', 'Code', 'Collaborations', 'Collection', 'Communication', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Demographic Transitions', 'Demography', 'Denmark', 'Documentation', 'Economic Development', 'Economics', 'Egypt', 'England', 'Ensure', 'European', 'Family', 'Fertility', 'Foundations', 'Funding', 'Future', 'Germany', 'Goals', 'Health', 'Human', 'Iceland', 'Immigration', 'Individual', 'Institutes', 'Institution', 'International', 'Investigation', 'Investments', 'Ireland', 'Knowledge', 'Laboratory Study', 'Link', 'Machine Learning', 'Maintenance', 'Mediation', 'Medical Research', 'Metadata', 'Mexico', 'Minnesota', 'Mission', 'Modeling', 'Nature', 'North America', 'Northern Europe', 'Norway', 'Nuptiality', 'Online Systems', 'Policy Making', 'Population', 'Population Characteristics', 'Population Dynamics', 'Population Growth', 'Private Sector', 'Process', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Scientist', 'Scotland', 'Series', 'Social Mobility', 'Societies', 'Statistical Data Interpretation', 'Subgroup', 'Sum', 'Sweden', 'Technology', 'Testing', 'Time', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'comparative', 'critical period', 'data integration', 'data mining', 'education research', 'experience', 'improved', 'innovation', 'mortality', 'population movement', 'social', 'spatiotemporal', 'success', 'theories', 'tool']",NICHD,UNIVERSITY OF MINNESOTA,R01,2016,630244,-0.03358946573101898
"An Open Source Precision Medicine Platform for Cloud Operating Systems ﻿    DESCRIPTION (provided by applicant):  Rapid improvements in DNA sequencing and synthesis have the potential to usher in a new era of precision medicine. To realize this vision, however, we must re-imagine the computational and storage infrastructure used to manage and extract actionable results from the massive data sets made possible by widely available advances in DNA sequencing and synthetic biology. In conjunction with the Global Alliance for Genomics and Health (GA4GH), we propose to build the Arvados platform so that a new ecosystem of clinical decision support applications will be able to navigate petabytes of global biomedical data and search millions of genomes in real-time (seconds). Our team has a proven track record of commercial success and high impact scientific research. Commercialization of this free and open-source software (FOSS) platform, which will be greatly accelerated by this grant, will permit organizations to seamlessly span on-premise & hosted cloud- operating systems and vastly simplify data-management & computation, all while facilitating compliance with institutional policies and regulatory requirements.         PUBLIC HEALTH RELEVANCE:  The delivery of healthcare based on molecular data specific to an individual patient (i.e. precision medicine) will require the creation of a new ecosystem of Clinical Decision Support (CDS) applications. This work will provide a platform that will make the development of such applications faster, easier, and less expensive.        ",An Open Source Precision Medicine Platform for Cloud Operating Systems,9140741,R44GM109737,"['Address', 'Adopted', 'Big Data', 'Bioinformatics', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Collaborations', 'Communities', 'Computer software', 'Contractor', 'DNA Sequence', 'DNA biosynthesis', 'Data', 'Data Set', 'Databases', 'Development', 'Distributed Systems', 'Ecosystem', 'Feedback', 'Fostering', 'Funding', 'Galaxy', 'Genome', 'Genomics', 'Grant', 'Health', 'Healthcare', 'Human', 'Industry', 'Information Technology', 'Institutional Policy', 'International', 'Internet', 'Language', 'Length', 'Letters', 'Machine Learning', 'Maintenance', 'Manuscripts', 'Measures', 'Medicine', 'Memory', 'Molecular', 'Operating System', 'Phase', 'Policies', 'Production', 'Publications', 'Reproducibility', 'Research', 'Research Infrastructure', 'Resources', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Source Code', 'System', 'Technology', 'Time', 'Training Support', 'Vision', 'Work', 'base', 'big biomedical data', 'cloud platform', 'commercialization', 'data management', 'genome sequencing', 'genomic data', 'health care delivery', 'individual patient', 'meetings', 'new technology', 'next generation sequencing', 'open source', 'operation', 'petabyte', 'portability', 'precision medicine', 'public health relevance', 'repository', 'screening', 'success', 'symposium', 'synthetic biology', 'terabyte', 'web services', 'whole genome']",NIGMS,"CUROVERSE, INC.",R44,2016,985339,-0.006733219400945039
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9100683,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2016,680020,0.0016230118842699283
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9200905,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Decision Modeling', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Immigration', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Legal', 'Legal patent', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2016,221175,-0.030588393064776898
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9248725,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2016,98294,-0.024532646534336722
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9134825,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2016,873141,-0.024532646534336722
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005). PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9146893,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2016,249994,-0.0609193445769507
"Scalable EEG interpretation using Deep Learning and Schema Descriptors ﻿    DESCRIPTION (provided by applicant): Electronic medical records (EMRs) collected at every hospital in the country collectively contain a staggering wealth of biomedical knowledge. EMRs can include unstructured text, temporally constrained measurements (e.g., vital signs), multichannel signal data (e.g., EEGs), and image data (e.g., MRIs). This information could be transformative if properly harnessed. Information about patient medical problems, treatments, and clinical course is essential for conducting comparative effectiveness research. Uncovering clinical knowledge that enables comparative research is the primary goal of this proposal. We will focus on the automatic interpretation of clinical EEGs collected over 12 years at Temple University Hospital (over 25,000 sessions and 15,000 patients). Clinicians will be able to retrieve relevant EEG signals and EEG reports using standard queries (e.g. ""Young patients with focal cerebral dysfunction who were treated with Topamax""). In Aim 1 we will automatically annotate EEG events that contribute to a diagnosis. We will develop automated techniques to discover and time-align the underlying EEG events using semi-supervised learning. In Aim 2 we will process the text from the EEG reports using state-of-the-art clinical language processing techniques. Clinical concepts, their type, polarity and modality shall be discovered automatically, as well as spatial and temporal information. In addition, we shall extract the medical concepts describing the clinical picture of patients from the EEG reports. In Aim 3, we will develop a patient cohort retrieval system that will operate on the clinical knowledge extracted in Aims 1 and 2. In addition we shall organize this knowledge in a unified representation: the Qualified Medical Knowledge Graph (QMKG), which will be built using BigData solutions through MapReduce. The QMKG will be able to be searched by biomedical researchers as well as practicing clinicians. The QMKG will also provide a characterization of the way in which events in an EEG are narrated by physicians and the validation of these across a BigData resource. The EMKG represents an important contribution to basic science. In Aim 4 we will validate the usefulness of the patient cohort identification system by collecting feedback from clinicians and medical students who will participate in a rigorous evaluation protocol. Inclusion and exclusion criteria for the queries shall be designed and experts will provide relevance judgments for the results. For each query, medical experts shall examine the top-ranked cohorts for common precision errors (false positives) and the bottom five ranked common recall errors (false negatives). User validation testing will be performed using live clinical data and the feedback wil enhance the quality of the cohort identification system. The existence of an annotated BigData archive of EEGs will greatly increase accessibility for non- experts in neuroscience, bioengineering and medical informatics who would like to study EEG data. The creation of this resource through the development of efficient automated data wrangling techniques will demonstrate that a much wider range of BigData bioengineering applications are now tractable. PUBLIC HEALTH RELEVANCE: The primary goal of this proposal is to enable comparative research by automatically uncovering clinical knowledge from a vast BigData archive of clinical EEG signals and EEG reports collected over the past 12 years at Temple University Hospital. In the proposed project, we will develop a proof-of-concept based on the discovery of patient cohorts and provide an annotated BigData archive as well as the software that enabled the annotations and the generation of the patient cohort retrieval system. This resource will be accompanied by a novel medical knowledge representation generated with MapReduce, greatly increasing accessibility for non- experts in neuroscience, bioengineering and medical informatics, and demonstrating the transformative potential of mining the staggering wealth of biomedical knowledge available in hospital medical records.",Scalable EEG interpretation using Deep Learning and Schema Descriptors,9243724,U01HG008468,"['Accident and Emergency department', 'Archives', 'Area', 'Basic Science', 'Big Data', 'Bilateral', 'Biomedical Engineering', 'Blinking', 'Cerebrum', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Treatment', 'Code', 'Comparative Study', 'Computer software', 'Computerized Medical Record', 'Country', 'Data', 'Descriptor', 'Development', 'Diagnosis', 'Diffuse', 'Discharge from eye', 'Electroencephalography', 'Epilepsy', 'Evaluation', 'Event', 'Exclusion Criteria', 'Feedback', 'Frequencies', 'Functional disorder', 'Generations', 'Goals', 'Graph', 'Health', 'Hospitals', 'Image', 'Judgment', 'Knowledge', 'Language', 'Learning', 'Life', 'Link', 'Measurement', 'Medical', 'Medical Informatics', 'Medical Records', 'Medical Students', 'Mining', 'Modality', 'Modeling', 'Morphologic artifacts', 'Multimedia', 'Nature', 'Neurosciences', 'Outcome', 'Patients', 'Pattern', 'Physicians', 'Process', 'Protocols documentation', 'Qualifying', 'Reporting', 'Research', 'Research Personnel', 'Research Support', 'Resources', 'Retrieval', 'Role', 'Signal Transduction', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'University Hospitals', 'Validation', 'base', 'career', 'cohort', 'comparative', 'comparative effectiveness', 'data archive', 'data wrangling', 'design', 'effectiveness research', 'inclusion criteria', 'information organization', 'language processing', 'novel', 'repository']",NHGRI,TEMPLE UNIV OF THE COMMONWEALTH,U01,2016,374745,-0.02599157090364378
"Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr) EMPIRICAL TESTING AND ENHANCEMENT OF WEB-BASED ABSTRACT SCREENING TOOL (ABSTRACKR)  In this year-long project, we aim to empirically assess the performance and efficiency of state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine and stakeholder-driven comparative effectiveness reviews. We have developed AbstrackrTM (hereon, Abstrackr), a human-guided computerized abstract screening tool that aims to reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. Abstrackr makes use of machine learning techniques, and is offered as a free web-based tool that enables management of the screening process.  We also aim to revise the web-interface of Abstrackr to make it more intuitive, user friendly, and add documentation and functionalities requested by users; and to revise Abstrackr’s back-end, which includes the way the software parses and analyses citations, fits machine learning models, and makes computations, to make it more efficient. These revisions will ensure that the tool becomes more robust, and that it remains usable for larger projects and for many teams.  The proposed work will be carried out by the developers of Abstrackr, comprising a highly experienced team of systematic review investigators and computer scientists at Brown University and the University of Texas at Austin, who have been working together for at least seven years. We will pursue dissemination of the findings of this assessment and of the revised tool through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its wider adoption by the Agency for Healthcare Research and Quality Evidence-based Practice Center Program, Cochrane Collaboration, and other groups conducting systematic reviews. We will also continue to make all code available online. Our aims are to: Aim 1. Empirically measure the efficiency and accuracy of the prediction algorithms in Abstrackr in the computer-assisted semi-automated screening of citations for eligibility in systematic reviews. Aim 2. Improve and add to the functionality of the Web-based Abstrackr software, based in part on enhancements suggested by a panel of identified heavy users. Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making and systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to assess the performance and efficiency of a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care, and to augment the functionality of its public implementation.",Empiric Testing and enhancement of web-based abstract screening tool(Abstrackr),9168247,R03HS024812,[' '],AHRQ,BROWN UNIVERSITY,R03,2016,99999,-0.012732313238971333
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs.         PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.            ",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,9135193,U54AT008909,"['Address', 'Archives', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Infrastructure Activities', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Procedures', 'Process', 'Qualifying', 'Quality Control', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinically relevant', 'data archive', 'data mining', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'online repository', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository']",NCCIH,UNIVERSITY OF WASHINGTON,U54,2016,1928492,-0.008257527839958572
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,9068953,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Health', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'genomic data', 'improved', 'large-scale database', 'novel', 'open source', 'personalized medicine', 'rapid growth', 'rare variant', 'response', 'study population', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2016,536892,-0.03116650450820731
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,-0.008054561021840557
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9134844,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronics', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Laboratories', 'Leadership', 'Libraries', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'college', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'outreach', 'polyposis', 'prevent', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2016,996374,-0.012064988448611277
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9358802,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronics', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Laboratories', 'Leadership', 'Libraries', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'college', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'outreach', 'polyposis', 'prevent', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2016,72000,-0.012064988448611277
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in several subprojects which use natural language processing techniques:  1) We have developed a machine learning algorithm for abbreviation definition identification in text which makes use of what we term naturally labeled data. Positive training examples are naturally occurring potential abbreviation-definition pairs in text. Negative training examples are generated by randomly mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Medstract corpora. Our system demonstrated results that compare favourably to the existing Ab3P and BIOADI systems. We achieve an F-measure of 91.36% on Ab3P corpus, and an F-measure of 87.13% on BIOADI corpus which are superior to the results reported by Ab3P and BIOADI systems. Moreover, we outperform these systems in terms of recall, which is one of our goals. 2) We are studying paraphrases in MEDLINE abstracts. These come about because an author is describing some entity of interest and uses a phrase like ""drug abuse"" and then needing to describe the same entity again a sentence or two latter does not wish to use exactly the same wording again and may use a variant of the phrase such as ""drug use"" which in the context of ""drug abuse"" has substantially the same meaning.  3) An author disambiguation algorithm has been developed which relies on machine learning based on the assumption that if an author name is infrequent in the data it probably represents the same person in all documents where it is found. This gives us positive instances. Negative instances are sampled from pairs of documents that have no author in common. Such positive and negative data allows us to do machine learning on all aspects of the document other than the name in question. This allows us to learn how to weight this data for best performance in distinguishing the positive and negative instances from each other. This learning is then applied in individual name cases or spaces to determine which author document pairs represent the same author. 4) We are using results of dependency parsers and syntactic parsers to create features for improved machine learning and also to automatically find good titles for document clusters. The BioC environment and the BioC tools including the Natural language processing pipelines have facilitated this work. n/a",Natural Language Processing Techniques To Enhance Information Access.,9160915,ZIALM000090,"['Abbreviations', 'Algorithms', 'Automated Abstracting', 'Data', 'Dependency', 'Drug abuse', 'Drug usage', 'Environment', 'Goals', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'MEDLINE', 'Machine Learning', 'Measures', 'Names', 'Natural Language Processing', 'Performance', 'Persons', 'Reporting', 'Sampling', 'System', 'Techniques', 'Text', 'Training', 'Variant', 'Weight', 'Work', 'abstracting', 'base', 'improved', 'indexing', 'interest', 'phrases', 'spelling', 'syntax', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2015,224965,-0.03465997054849153
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method we are currently studying is to perform machine learning with an SVM or other classifier and score the documents based on this learning. Then PAV can be applied to the resulting scores and this score function can be descretized without the loss of significant information. This allows us to make use of the results as features which can be individually weighted in another classifier. 4) We have developed a new algorithm called the periodic random orbiter algorithm (PROBE) which is applicable to minimize any convex loss function. We have applied it to the MeSH classification problem and it seems to work very well and better than the alternatives on such a large problem. 5) Stochastic Gradient Descent (SGD) has gained popularity for solving large scale supervised machine learning problems. It provides a rapid method for minimizing a number of loss functions and is applicable to Support Vector Machine (SVM) and Logistic optimizations. However SGD does not provide a convenient stopping criterion. Generally an optimal number of iterations over the data may be determined using held out data. We have compared stopping predictions based on held out data with simply stopping at a fixed number of iterations and found that the latter works as well as the former for a number of commonly studied text classification problems. In particular fixed stopping works well for MeSH predictions on PubMed records. We also surveyed the published algorithms for SVM learning on large data sets, and chose three for comparison: PROBE, SVMperf, and Liblinear and compared them with SGD with a fixed number of iterations. We find SGD with a fixed number of iterations performs as well as these alternative methods and is much faster to compute. As an application we have made SGD-SVM predictions for all MeSH terms and used the Pool Adjacent Violators (PAV) algorithm to convert these predictions to probabilities. Such probabilistic predictions lead to ranked MeSH term predictions superior to previously published results on two test sets 6) We are also investigating methods to create features for machine learning using dependency parses and syntactic parse trees. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,9160914,ZIALM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Classification', 'Data', 'Data Set', 'Dependency', 'Exclusion', 'Lead', 'Learning', 'Literature', 'Logistics', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Probability', 'Procedures', 'PubMed', 'Publishing', 'Records', 'Retrieval', 'Surveys', 'Testing', 'Text', 'Trees', 'Weight', 'Work', 'base', 'design', 'improved', 'loss of function', 'rapid technique', 'syntax', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2015,224965,0.0005676797634013046
"Informatics, Machine Learning & Biomedical Data Science The Informatics, Machine Learning, and Biomedical Data Science, which operates within the High Performance Computing and Informatics Office (HPCIO), Division of Computational Bioscience of CIT, is collaborating with NIH investigators to build a critical mass in text and numerical analytics that is envisioned to encompass a number of pertinent and related disciplines in biomedical research including semantic interoperability, computational linguistics, text and data mining, natural language processing, machine learning, longitudinal analysis, and visualization.  The program is intended to foster advances in critical domains at NIH including biomedical and clinical informatics, translational research, genomics, proteomics, systems biology, ""big data"" analysis, and portfolio analysis.  In 2015, collaborative efforts in support of these goals included the following:  -In collaboration with NIA, we are applying machine learning and visualization techniques on large biological datasets to discover novel patterns of functional gene or protein interactions as related to aging. In this collaboration, we are developing a machine learning method that models the temporal nature of the longitudinal clinical data to predict the progression of Amyotrophic lateral sclerosis. Such machine learning method may also work well in prediction of high-dimensional time-series genomic data.  - In collaboration with NIAID, HPCIO has released HT JoinSolver(R), a new application capable of analyzing V(D)J recombination in thousands of immunoglobulin gene sequences produced by high throughput sequencing.  - HPCIO is working with NCI to develop methodologies to incorporate occupational risk factors into epidemiological models. Novel classifiers are being developed to classify free text job descriptions into the 840 codes of the 2010 U.S. Standard Occupational Classification System.  Agreement between our classification system and expert coders is measured using SOC code agreement and exposure agreement after applying CANJEM, a job-exposure matrix of over 250 exposure agents developed by Jrme Lavou at the University of Montreal.  - In collaboration with the Membrane Transport Biophysics Section NINDS, HPCIO is 1) developing a tool to accurately identify the boundaries of the lysosomes in fluorescence microscopy and 2) using the fluorescence ration to measure lysosomal pH within each organelle for better understanding of the lysosomal pH regulation.  - HPCIO is collaborating with NIAID to study immune cell infiltration in various tissue samples from patients with metabolic diseases. Using systems-based approaches, we examine gene expression and genotyping data to understand the roles and interactions of different immune cells in response to metabolic disease signals and their associations to intervention outcomes and other phenotypes.    - A freely available plasmid database that is interoperable with popular freeware is currently being developed for the NIDA Optogenetics and Transgenic Technology Core. The Plasmid Manager offers a versatile yet simple platform for scientists to store and analyze their plasmid data. Motivated by the need for a more comprehensive approach to archiving plasmid data, the database platform is enriched with numerous components beyond the repository, serving as an informatics platform designed to enhance the efficiency and analytic capabilities of scientists.   - In collaboration with CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process. A Web-based automated referral tool, called ART, is being developed to help PIs and SROs to identify the most relevant study section(s) or special emphasis panel(s) based on the scientific content of an application. In addition, HPCIO is analyzing text from quick feedback surveys on peer review. This effort includes evaluatinng a pilot study to evaluate the feasibility of analyzing free text from peer reviewers on their perception of the study section quality. If successful, the pilot results will be used to as initial input for a full-scale implementation.  - The Human Salivary Protein wiki has been made available online on a community-based Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. This is a major step towards the discovery and use of saliva biomarkers to diagnose oral and systemic diseases.   - In collaboration with the Office of Data Analysis Tools and Systems, NIH Office of the Director, HPCIO has been developing a standard database update pipeline for NIH Topic Maps, originally developed by Dr. Ned Talley of NINDS.   We are evaluating whether this pipeline can be incorporated into a stable hosted instance.  - As high-throughput next-generation sequencing (NGS) technology plays an important role in systematically identifying novel cancer driver mutations in genome-wide surveys, NGS data generation is rapidly increasing, currently accumulating at a rate of several terabytes of data every month at the Lymphoid Malignancies Section of NCI. We need to enhance database platforms in anticipation of even more growth in the near future. The recent emergence of Hadoop/NoSQL systems (e.g., Hbase) has provided an alternative platform for querying large-scale genomic data. In addition, relational database providers have been enhancing their offerings to include products for explicitly distributing data across multiple nodes (e.g., Postgres XL). We have sought to integrate these technologies with current relational database systems (e.g., Postgres) to improve performance in a parallel or distributed manner. The goal of our effort has been to investigate the potential of these distributed platforms in storing and querying the large volumes of data that NCI accumulates, thereby augmenting their current analytical capabilities.  - Based on its experience in building novel models for classifying research grants and projects, HPCIO has collaborated with DPCPSI/OD and other ICs to develop the Portfolio Learning Tool, a comprehensive classification workflow system that will allow users to select from multiple classification algorithms, feature spaces, and training regimes, to build and run their own classifiers.  HPCIO has developed an augmented support vector machine (SVM) that augments a training set by sampling from a corpus of unknowns and runs a large ensemble on various samples of this augmented space. The results obtained from this classifier suggest that, when coupled with an effective annotation strategy, such a classifier can be quite effective at categorizing a research portfolio. n/a","Informatics, Machine Learning & Biomedical Data Science",9146134,ZIHCT000200,"['Address', 'Aging', 'Agreement', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Archives', 'Big Data', 'Biological', 'Biological Markers', 'Biomedical Research', 'Biophysics', 'Cells', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Linguistics', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Diagnosis', 'Discipline', 'Evaluation', 'Expert Systems', 'Feedback', 'Fluorescence', 'Fluorescence Microscopy', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Grant Review Process', 'Growth', 'High Performance Computing', 'High-Throughput Nucleotide Sequencing', 'Human', 'Imagery', 'Immune', 'Immunoglobulin Genes', 'Infiltration', 'Informatics', 'Information Resources', 'Intervention', 'Job Description', 'Knowledge', 'Leadership', 'Learning', 'Lysosomes', 'Machine Learning', 'Malignant Neoplasms', 'Malignant lymphoid neoplasm', 'Management Information Systems', 'Maps', 'Measures', 'Metabolic Diseases', 'Methodology', 'Methods', 'Modeling', 'Mouth Diseases', 'Mutation', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Nature', 'Occupational', 'Occupations', 'Online Systems', 'Organelles', 'Outcome', 'Patients', 'Pattern', 'Peer Review', 'Perception', 'Performance', 'Phenotype', 'Pilot Projects', 'Plasmids', 'Play', 'Proteins', 'Proteomics', 'Provider', 'Regulation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Sharing', 'Risk Factors', 'Role', 'Running', 'Saliva', 'Salivary Proteins', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Series', 'Signal Transduction', 'Software Engineering', 'Study Section', 'Surveys', 'System', 'Systemic disease', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Time', 'Tissue Sample', 'Training', 'Transgenic Organisms', 'Translational Research', 'Transmembrane Transport', 'United States National Institutes of Health', 'Universities', 'Update', 'V(D)J Recombination', 'Work', 'base', 'biological systems', 'biomedical informatics', 'cluster computing', 'computing resources', 'data management', 'data mining', 'data sharing', 'data visualization', 'design', 'distributed data', 'epidemiological model', 'evidence base', 'experience', 'genome-wide', 'improved', 'information organization', 'interoperability', 'knowledge base', 'longitudinal analysis', 'next generation sequencing', 'novel', 'optogenetics', 'peer', 'programs', 'prototype', 'relational database', 'repository', 'response', 'text searching', 'tool', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2015,1941638,-0.035195333411756034
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.   1)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 2)	The PubMed database presents a unique challenge because of its very large size of over 25 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  3) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with the results of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. 4) We are currently working on a problem which arises when several different kinds of documents appear in a dataset and one wants to compute neighboring documents for each document. A simple application of the same approach used to find related citations in PubMed does not produce good results. Analysis of the problem shows that there are many records with words in them that are not keyed to the actual focus of the record and that these words mislead the neighboring process. In some cases this is due to a common author of records who users certain word forms frequently in their writing even on very different subjects. In other cases the problem seems to appear when two different drugs have sections on side effects that are quite generic and have a large overlap, etc. We have found our best results with a completely automatic approach which examines how related each word in the body of a record is to words in the records title. This is achieved by removing all words related below a certain low threshold. 5) Some of our latest work uses concepts that appear in multiple article titles to produce document clusters. These are then analyzed using naive Bayesian classification methods to ascertain their significance. Those that are significant are extended using the same Bayesian technique. The result is a set of concepts each represented by a document cluster. This proves to be an effective way to produce significant clusters of relatively small data sets that are difficult to cluster by more standard methods. 6) We have implemented a distributional semantics approach modeled somewhat after the work of Lin and Pantel and have found this useful in finding synonyms for terms. However the method does not produce a quality that can be effectively used for most purposes without human review. We believe the model could be improved if p-values could be computed in addition to scores and are working on an approach to assign such values. n/a",Automatic Bayesian Methods In Text Retrieval,9160905,ZIALM000021,"['Accounting', 'Active Learning', 'Adverse effects', 'Bayesian Method', 'Bayesian Modeling', 'Classification', 'Data', 'Data Set', 'Databases', 'Exhibits', 'Frequencies', 'Generic Drugs', 'Genomics', 'Goals', 'Human', 'Label', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Modeling', 'Performance', 'Peripheral', 'Pharmaceutical Preparations', 'Poisson Distribution', 'Probability', 'Process', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'Writing', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2015,490834,0.0002576142409843058
"Automatic Analysis and Annotation of Document Keywords in Biomedical Literature 1) Electronic Textbook and PubMed Central Indexing Current processing of the electronic textbook material involves a number of steps designed to produce the most meaningful phrases in the text to be used as reference points. The first task is to identify grammatically reasonable phrases. We use a version of the Brill transformation based tagger, rewritten in C++, for part-of- speech tagging. This forms the basis for determining grammatically reasonable phrases. There is a significant post processing step that removes phrases that involve inappropriate references to context (e.g., different cells, final mutation). After finding grammatically reasonable phrases we attempt to eliminate those that are too common or generic to be useful (e.g., significant result, short time).  The next step is to compare a phrase with previously rated phrases that have been collected over the life of the project. The final stage is to estimate the importance of a phrase in the passage where it is found in a textbook. Such an estimate is based on the frequency of the phrase and the size of the passage compared with the frequency of the phrase throughout the book and the overall size of the book. In order to improve such an estimate we attempt to take account of the phrase or any phrase that represents the same concept. For this purpose we use the UMLS Metathesaurus and also stemming and combine these two approaches into a consistent picture of the concept as it occurs in the text.  The result of this processing is a scored list of phrase-book section pairs for each textbook. These are used to guide the response of general searching in the books. When a user types in a phrase that is on our curated list the first results given are the highly rated book sections for that phrase. We are now applying a similar indexing scheme to the text of articles in PMCentral. This allows us to give a list of highly rated phrases for each article as an enhanced reference point for searchers.    2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text.   3) We have studied how good phrases can be recognized by their characteristics, such as frequency, tendency to be repeated in documents where they occur, and other numerical properties. These features allow one to predict which phrases are of high quality. We have found such predictions to be useful in studying different kinds of terms that may appear in text and how an ontoloogy might be extracted from text.  4) We have found stochastic gradient descent (SGD) with regularization by early stopping to be a very efficient method for training a Support Vector Machine (SVM) for MeSH term assignment. We have discovered that the early stopping can be implemented as stopping after a constant number of iterations and the results are as good as stopping based on held out data and also as good as more conventional methods of training an SVM on large data sets. The SGD approach is much faster and allows one to readily train classifiers for all 27,000 MeSH terms. Results are superior to previously published methods. The approach could be the basis of indexing suggestions for PubMed records or for an automatic concept assignment system similar to MeSH. n/a",Automatic Analysis and Annotation of Document Keywords in Biomedical Literature,9160928,ZIALM091711,"['Accounting', 'Address', 'Books', 'Cells', 'Characteristics', 'Collection', 'Data', 'Data Set', 'Electronics', 'Evaluation', 'Frequencies', 'Generic Drugs', 'Goals', 'Life', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Mutation', 'Outcome', 'Process', 'Property', 'PubMed', 'Publishing', 'Records', 'Retrieval', 'Scheme', 'Speech', 'Staging', 'Suggestion', 'System', 'Techniques', 'Text', 'Textbooks', 'Time', 'Training', 'Training Support', 'UMLS Metathesaurus', 'base', 'design', 'improved', 'indexing', 'insight', 'phrases', 'response', 'stem']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2015,122708,-0.02722842826635289
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims.         PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.            ",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,8874546,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Knowledge', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2015,478724,-0.02484614453262159
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) Code has been multithreaded and memory mapping capabilities added to speed up processing. 6) Most recently the code has been updated to work in a 64 bit environment.   The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system is currently proving useful in testing different retrieval parameters and methods on the PubMedHealth records.  We have recently developed a software system called DStor that allows us to store all of PubMed in a manner which is easily updateable and allows fast access. This system is now being used to maintain and update five different versions of the PubMed data twice a week. This system has greatly improved our access to PubMed data in various useful forms and we anctipate that its use will continue to grow. In addition we have developed software to maintain and update a list of strings where each string is associated with some fixed vector of integers. We currently maintain a list of all multi-word phrases without stop words or punctuation and with each is associated a vector of six integers representing counts of different types associated with each phrase where counts are computed over all PubMed records having abstracts. We also maintain a list of all one and two word phrases and MeSH terms in various forms (with & without stars and subheadings) and two counts with each consisting of the document frequency and the total frequency counting all occurrences in each document over all of PubMed. n/a",A Document Processing System,9160906,ZIALM000022,"['Code', 'Data', 'Databases', 'Environment', 'Frequencies', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'MeSH Thesaurus', 'Memory', 'Methods', 'Process', 'Programming Languages', 'PubMed', 'Records', 'Research', 'Retrieval', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Update', 'Work', 'abstracting', 'base', 'computerized data processing', 'improved', 'phrases', 'repository', 'software development', 'software systems', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2015,439705,-0.0019994026437594697
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,-0.02244327918909253
"Automatically Creating and Updating Meta-Studies of Randomized Controlled Trials ﻿    DESCRIPTION (provided by applicant):  A ""meta-study"" (or ""meta-analysis"") collects and analyzes many studies on the same topic to understand if there is a meaningful, overall result. Meta-studies can support (or refute) interventions, spur new investigations, and lead to novel clinical guidelines. However, constructing meta-studies is a time intensive process of searching the literature, compiling the results, and performing the statistical analysis. Due to the time commitment that is required, many topics are unexplored, and many meta-studies are not kept up-to-date with the latest published results. Finally, a number of (unknown) biases, via subjective choices during the meta-study, may influence the results. Our long-term goal is to automate, as much as possible, the meta-study process. This should decrease subjective bias; increase the dissemination of evidence, especially for diseases and interventions that receive less attention; and allow for the automatic updating of meta-studies as new results are published. We propose a computer system that uses statistical machine learning to gather and group studies focused on similar interventions and outcomes; extract the necessary results from the text; and analyze the results using standard meta-analysis techniques. The final output will be presented in a spreadsheet-like Web-interface where users can explore and even change the data and meta-analyses. Our team uniquely blends technical expertise in machine learning with leadership in publishing meta-studies about Inflammatory Bowel Disease (IBD), our disease of focus for our Phase I feasibility study. We are therefore qualified technically and able to ensure that the techniques generate valid and accurate meta-studies. Our Phase I results will define the current state-of-the-art for this novel task. Further, although we will initially focus n IBD, our Phase I results will demonstrate that our approach can generalize to other diseases, eventually applying to any intervention and any disease. The feasibility shown by our Phase I results will motivate our Phase II effort where we will focus on dramatically improving the approach, yielding broad coverage of all medical literature and generating human-quality meta-studies. We note that by the end of Phase I we should have a viable end-to-end prototype, focused on IBD, which we can begin taking to market. The final product should significantly benefit our target markets given the Phase II emphasis to improve the technology, user experience, and scope of covered diseases. PUBLIC HEALTH RELEVANCE:  A meta-analysis collects and analyzes the results from multiple studies that are all focused on the same topic, and it can confirm (or refute) the overall effect across the studies, lead to changes in clinical guidelines, or spur new directions for research. However, generating a meta-analysis is an extremely time-consuming process, so many diseases are not covered, and most meta-analyses are not updated to reflect the latest published studies. This work begins to automate the process of creating meta-analyses, overcoming these difficulties in order to make the results published in the medical literature more accessible.",Automatically Creating and Updating Meta-Studies of Randomized Controlled Trials,8977531,R43LM012210,"['Adverse event', 'Algorithms', 'Attention', 'Clinical', 'Computer Systems', 'Computer software', 'Data', 'Data Aggregation', 'Diabetes Mellitus', 'Disease', 'Disease remission', 'Dourine', 'Ensure', 'Evidence Based Medicine', 'Feasibility Studies', 'Goals', 'Grouping', 'Guidelines', 'Hand', 'Health', 'Human', 'Inflammatory Bowel Diseases', 'Intervention', 'Intervention Studies', 'Investigation', 'Lead', 'Leadership', 'Literature', 'Lupus', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Meta-Analysis', 'Modeling', 'Odds Ratio', 'Outcome', 'Outcome Study', 'Output', 'Pattern', 'Performance', 'Phase', 'Placebos', 'Population Sizes', 'Process', 'Publishing', 'Qualifying', 'Randomized Controlled Trials', 'Research', 'Research Personnel', 'System', 'Technical Expertise', 'Techniques', 'Technology', 'Text', 'Time', 'Update', 'Work', 'abstracting', 'base', 'experience', 'falls', 'improved', 'novel', 'primary outcome', 'programs', 'prototype', 'software development', 'text searching', 'web interface']",NLM,INFERLINK CORPORATION,R43,2015,150000,-0.007772621675146826
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,8840984,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2015,1243799,-0.007816873091463922
"Free Text Gene Name Recognition 1) I have been a co-organizer of the BioCreative Workshops since 2005 and have taken part in BioCreative II (2007), BioCreative III (2010), BioCreative-2012 Workshop (2012), and BioCreative IV (2013) and my group is taking part in BioCreative V (2015) which has not yet taken place. The overall goal of the BioCreative Workshops is to promote the development of text mining and text processing tools which are useful to the communities of researchers and database curators in the biological sciences.  2) We are currently working to develop more general methods of finding high value articles for PPI based on their abstracts. This effort involves not only more powerful ranking methods, but also ways to display evidence to the user for a users quick evaluation. 3) We are also investigating an approach to named entity recognition for a large number of biologically important entity types. We have found certain general patterns that can be used to find genes and other entity types with a higher reliability than can be done with a general CRF. This is ongoing research with a promise for more useful general patterns.  4) We have begun a project called BioC which is an effort to create a general XML format defined by a DTD and software to read and write this format. Currently this approach has been implemented in C++, Java, Python, Pearl, Ruby, and GO. The idea is to use this common currency to make software modules that are useful for natural language processing  more interoperable. The project is in its early stages, but already we have software to read and write in the languages mentioned as well as significant NLP processing modules using this approach and over 25 gold standard NLP annotated data sets available in the format. The approach was featured in the BioCreative IV Workshop and the approach has formed the basis of the BioC Collaborative Track at BioCreative V which will take place in a short time. This track has received contributions from eight teams besides our own and has built a user interface which displays annotated articles to Biogrid curators to assist them in their work. n/a",Free Text Gene Name Recognition,9160916,ZIALM000093,"['Biological Sciences', 'Biology', 'Communities', 'Computer software', 'Data Set', 'Databases', 'Development', 'Educational workshop', 'Evaluation', 'Extensible Markup Language', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Gold', 'Java', 'Language', 'Literature', 'Methods', 'Names', 'Natural Language Processing', 'Pattern', 'Process', 'Proteins', 'Proteomics', 'Pythons', 'Reading', 'Research', 'Research Personnel', 'Staging', 'Techniques', 'Text', 'Time', 'Variant', 'Work', 'Writing', 'abstracting', 'base', 'improved', 'text searching', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2015,143160,-0.06767228907712776
"Protect Privacy of Healthcare Data in the Cloud DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier. Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8925916,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Laplacian', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'privacy protection', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2015,192613,-0.004615484000509634
"Integrated Public Use Microdata Series: North Atlantic Population Project    DESCRIPTION (provided by applicant): Research teams in the United States, Britain, Canada, Iceland, Norway, and Sweden have worked in close coordination to create the North Atlantic Population Project (NAPP), a massive integrated cross-national microdatabase that provides a baseline for studies of demographic change and opens fresh paths for spatiotemporal data analysis. We now propose improvements that will multiply the power of the NAPP infrastructure. We have three major aims: (1) Triple the size of the database to approximately 365 million records, adding 40 new datasets for the period 1787 to 1930 from Albania, Britain, Canada, Denmark, Egypt, Iceland, Ireland, Germany, Norway, Mexico, Sweden, and the United States. (2) Leverage our innovative record-linkage technology to create linked national panels that will allow expanded longitudinal analyses. (3) Connect the past to the present by merging NAPP with the Integrated Public Use Microdata Series (IPUMS), simplifying analysis of long-run change and ensuring long-run preservation and maintenance of the database. The landscape of scientific research on the human population is shifting. It is no longer sufficient just to study the relationships among variables at a particular moment in time. Researchers around the world now recognize that to understand the large-scale processes that are transforming society, we must investigate long-term change. The goal of this project is to provide the infrastructure to make such analysis possible. NAPP will make a strategic contribution to demographic infrastructure by providing a baseline for the study of changes in the demography and health of European and North American populations. In each country, NAPP provides the earliest census microdata available. Models and descriptions based on historical experience underlie both theories of past change and projections into the future. The NAPP data provide a unique laboratory for the study of economic and demographic processes; this kind of empirical foundation is essential for testing social and economic theory. The proposed work will be carried out by a team of highly-skilled researchers with unparalleled expertise and experience in data integration and record linkage. Collaborators include leading researchers from the University of Minnesota and the Max Planck Institute for Demographic Research, and local experts from each of the participating countries. Centralized support for international collaboration will leverage the investments of each country and allow us to create an extraordinary resource for comparative social and economic research.        The North Atlantic Population Project (NAPP) provides fundamental infrastructure for scientific research, education, and policy-making and will allow social scientists to make comparisons across Northern Europe, the Mediterranean, and North America during three centuries of transformative change. The proposed work is directly relevant to the central mission of the NIH as the steward of medical and behavioral research for the nation: the new data will advance fundamental knowledge about the nature and behavior of human population dynamics. NAPP will unlock access to some of the largest and longest-running cross-sectional and longitudinal data sources in the world, and stimulate health-related research on population growth and movement, fertility, mortality, nuptiality, and family change, as well as the economic and social correlates of demographic behavior.         ",Integrated Public Use Microdata Series: North Atlantic Population Project,8704343,R01HD052110,"['Albania', 'American', 'Behavior', 'Behavioral Research', 'Biological Preservation', 'Canada', 'Censuses', 'Code', 'Collaborations', 'Collection', 'Communication', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Demographic Transitions', 'Demography', 'Denmark', 'Documentation', 'Economic Development', 'Economics', 'Education', 'Egypt', 'England', 'Ensure', 'European', 'Family', 'Fertility', 'Foundations', 'Funding', 'Future', 'Germany', 'Goals', 'Health', 'Human', 'Iceland', 'Immigration', 'Individual', 'Institutes', 'Institution', 'International', 'Investigation', 'Investments', 'Ireland', 'Knowledge', 'Laboratory Study', 'Link', 'Machine Learning', 'Maintenance', 'Mediation', 'Medical Research', 'Metadata', 'Mexico', 'Minnesota', 'Mission', 'Modeling', 'Nature', 'North America', 'Northern Europe', 'Norway', 'Nuptiality', 'Online Systems', 'Policy Making', 'Population', 'Population Characteristics', 'Population Growth', 'Private Sector', 'Process', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Scientist', 'Scotland', 'Series', 'Social Mobility', 'Societies', 'Subgroup', 'Sum', 'Sweden', 'Technology', 'Testing', 'Time', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'comparative', 'critical period', 'data integration', 'data mining', 'experience', 'human population dynamics', 'improved', 'innovation', 'mortality', 'population movement', 'social', 'spatiotemporal', 'success', 'theories', 'tool']",NICHD,UNIVERSITY OF MINNESOTA,R01,2015,597800,-0.03358946573101898
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities.         PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.                ","COINSTAC: decentralized, scalable analysis of loosely coupled data",8975906,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Solutions', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2015,727692,0.0016230118842699283
IGF::OT::IGF TASK ORDER 1 - CORE & ADMINISTRATION; MAINTENANCE & INSTALLATION FOR THE SURVEILLANCE EPIDEMIOLOGY AND END RESULTS (SEER) ELECTRONIC DATA CAPTURE SOFTWARE SUPPORT AND INSTALLATIONS. Surveillance Epidemiology and End Results (SEER) Electronic Data Capture Software Support and Installations. n/a,IGF::OT::IGF TASK ORDER 1 - CORE & ADMINISTRATION; MAINTENANCE & INSTALLATION FOR THE SURVEILLANCE EPIDEMIOLOGY AND END RESULTS (SEER) ELECTRONIC DATA CAPTURE SOFTWARE SUPPORT AND INSTALLATIONS.,9162089,61201500033I,"['Artificial Intelligence', 'Award', 'Computer software', 'Contracts', 'Data', 'Diagnostic Imaging', 'Electronics', 'Hour', 'Laboratories', 'Maintenance', 'Malignant Neoplasms', 'Medical Surveillance', 'Medicine', 'Pathology', 'Reporting', 'Update', 'electronic data']",NCI,"ARTIFICIAL INTELLIGENCE IN MEDICINE, INC",N03,2015,1135265,-0.011371079084528715
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality.         PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results                ",EMR-Linked Biobank for Translational Genomics,8968014,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2015,899776,-0.024532646534336722
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG. RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8786588,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cardiovascular visualization', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2015,2177431,-0.011198060072204042
"NLP to Improve Accuracy and Quality of Dictated Medical Documents ﻿    DESCRIPTION (provided by applicant):  Errors in medical documents represent a critical issue that can adversely affect healthcare quality and safety. Physician use of speech recognition (SR) technology has risen in recent years due to its ease of use and efficiency at the point of care. However, high error rates, upwards of 10-23%, have been observed in SR-generated medical documents. Error correction and content editing can be time consuming for clinicians. A solution to this problem is to improve accuracy through automated error detection using natural language processing (NLP). In this study, we will provide solutions to these challenges by addressing the following specific aims: 1) build a large corpus of clinical documents dictated via SR across different healthcare institutions and clinical settings; 2) conduct error analysis to estimate the prevalence and severity of SR errors; 3) develop innovative methods based on NLP for automated error detection and correction and create a comprehensive knowledge base that contains confusion sets, error frequencies and other error patterns; 4) evaluate the performance of the proposed methods and tool; and 5) distribute our methods and findings to make them available to other researchers.  We believe this application aligns with AHRQ's HIT and Patient Safety portfolios as well as AHRQ's Special Emphasis Notice to support projects to generate new evidence on health IT system safety (NOT- HS-15-005).         PUBLIC HEALTH RELEVANCE    Public Health Relevance Statement  Errors in medical documents are dangerous for patients. Physician use of speech recognition technology, a computerized form of medical transcription, has risen in recent years due to its ease of use and efficiency. However, high error rates, upwards of 10-23%, have been observed. The goal of this study is two-fold: 1) to study the nature of such errors and how they may affect the quality of care and 2) to develop innovative methods based on computerized natural language processing to automatically detect these errors in clinical documents so that physicians can correct the documents before entering them into the patient's medical record.            ",NLP to Improve Accuracy and Quality of Dictated Medical Documents,9004939,R01HS024264,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R01,2015,250000,-0.0609193445769507
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,-0.004100438388811052
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,-0.004100438388811052
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,-0.004100438388811052
"Natural Product-Drug Interaction Research: The Roadmap to Best Practices ﻿    DESCRIPTION (provided by applicant): As natural products (NP) sales increase, the risk of adverse NP-drug interactions (NPDIs) increases, yet the pharmacokinetic (PK) elucidation and clinical relevance of putative NPDIs remain elusive. Assessing the risk of NPDIs is more challenging than that of drug-drug interactions (DDIs), often due to relatively scant PK knowledge of individual NP constituents that perpetrate these interactions. The proposed U54 Center will develop a roadmap for NPDI research through a series of well-designed human in vitro and in vivo studies (termed Interaction Projects) on 4-6 priority NPs that pose a potential risk for clinically relevant NPDIs. A repository will be developed for the data generated from the Interaction Projects, and results will be communicated to various target audiences through a public portal. This U54 Center is composed of an accomplished team of investigators, including pharmacokineticists with expertise in NPDIs, complemented by expertise in NP chemistry, DDIs, and health information communication. In collaboration with the Steering Committee, an innovative strategy that combines a mechanistic approach with practical considerations (e.g., popularity of the NPs) will be used to select and prioritize 4-6 NPs for further investigation in te Interaction Projects. Mechanistic aspects include curated clinical NDPI data from the widely used Drug Interaction Database (DIDB) of the Informatics Core, structural alerts, and compelling preliminary clinical and in silico NPDI data. Once selected, the NPs will be entered into a Decision Tree for assessment of NPDI liability and probable significance of interactions with victim drugs. In parallel, the Pharmacology Core will develop detailed Statements of Work for the human in vitro and in vivo studies - while the Analytical Core will source, acquire, and characterize the selected NPs - for the Interaction Projects. Upon completion of each project, the Analytical Core will analyze the PK samples, and the Pharmacology Core will develop physiologically-based PK models for further assessment of the clinical relevance of the Interaction Project results. Throughout these projects, the Informatics Core will create a data repository embedded within a public portal web-based application named, NaPDI app, for Natural Product-Drug Interaction application. The repository, built using the DIDB framework, will allow researchers to access both raw data and summarized results. The U54 Center will also develop and share Best Practices recommended for the conduct of NPDI studies, based on the experience with and results from the Interaction Projects, with the research communities via the public portal. Effective dissemination of the Interaction Project results will be ensured through user experience and brand content studies with target audiences - researchers/practitioners and lay public - to refine the public portal content. The Informatics Core will ensure that the U54 Center's results are archived, organized, analyzed, and well-publicized, allowing for improved design of future NPDI research and ultimately, improved decisions on the optimal management of clinically relevant NPDIs.         PUBLIC HEALTH RELEVANCE: The goal of this proposed Center of Excellence is to provide leadership on how best to study potential adverse interactions between natural products and conventional medications. The uniquely experienced and multidisciplinary team of investigators will work with NCCAM officials to identify a priority list of natural products that could alter dru disposition and, in turn, significantly alter the efficacy and safety of conventional medications; challenges inherent to studying these interactions will be addressed using a combination of novel and established approaches. Upon assessment of the selected natural products and potential drug interactions through the Interaction Projects, a repository and web-based public portal will be developed that allows other researchers to access the generated data for further analysis and communicate the health implications of the results to health care practitioners and the public.            ",Natural Product-Drug Interaction Research: The Roadmap to Best Practices,8977997,U54AT008909,"['Address', 'Archives', 'Biological Factors', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Complement', 'Computer Simulation', 'Data', 'Databases', 'Decision Trees', 'Drug Interactions', 'Drug Kinetics', 'Ensure', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'In Vitro', 'Individual', 'Informatics', 'Infrastructure Activities', 'Investigation', 'Knowledge', 'Leadership', 'Literature', 'Methods', 'Names', 'National Center for Complementary and Alternative Medicine', 'Natural Product Drug', 'Natural Products Chemistry', 'Nature', 'Online Systems', 'Pharmaceutical Preparations', 'Pharmacology', 'Procedures', 'Process', 'Qualifying', 'Quality Control', 'Research', 'Research Personnel', 'Risk', 'Safety', 'Sales', 'Sampling', 'Series', 'Source', 'Vulnerable Populations', 'Work', 'base', 'clinically relevant', 'data mining', 'design', 'experience', 'improved', 'in vivo', 'innovation', 'liquid chromatography mass spectrometry', 'models and simulation', 'multidisciplinary', 'novel', 'operation', 'perpetrators', 'pharmacokinetic model', 'public health relevance', 'quality assurance', 'repository']",NCCIH,UNIVERSITY OF WASHINGTON,U54,2015,2015154,-0.008257527839958572
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers. PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,8929257,R01GM103859,"['Adverse drug event', 'Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Health', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Population Study', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'improved', 'large-scale database', 'novel', 'open source', 'personalized medicine', 'rapid growth', 'rare variant', 'response', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY,R01,2015,598996,-0.03116650450820731
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome.         PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.                ","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",8965736,U01HG008657,"['Abbreviations', 'Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronics', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hereditary Disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Laboratories', 'Leadership', 'Libraries', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Solutions', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'college', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'improved', 'innovation', 'interest', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'outreach', 'polyposis', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2015,859799,-0.012064988448611277
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in several subprojects which use natural language processing techniques:  1) We have developed a machine learning algorithm for abbreviation definition identification in text which makes use of what we term naturally labeled data. Positive training examples are naturally occurring potential abbreviation-definition pairs in text. Negative training examples are generated by randomly mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Medstract corpora. Our system demonstrated results that compare favourably to the existing Ab3P and BIOADI systems. We achieve an F-measure of 91.36% on Ab3P corpus, and an F-measure of 87.13% on BIOADI corpus which are superior to the results reported by Ab3P and BIOADI systems. Moreover, we outperform these systems in terms of recall, which is one of our goals. 2) We are studying paraphrases in MEDLINE abstracts. These come about because an author is describing some entity of interest and uses a phrase like ""drug abuse"" and then needing to describe the same entity again a sentence or two latter does not wish to use exactly the same wording again and may use a variant of the phrase such as ""drug use"" which in the context of ""drug abuse"" has substantially the same meaning.  3) An author disambiguation algorithm has been developed which relies on machine learning based on the assumption that if an author name is infrequent in the data it probably represents the same person in all documents where it is found. This gives us positive instances. Negative instances are sampled from pairs of documents that have no author in common. Such positive and negative data allows us to do machine learning on all aspects of the document other than the name in question. This allows us to learn how to weight this data for best performance in distinguishing the positive and negative instances from each other. This learning is then applied in individual name cases or spaces to determine which author document pairs represent the same author. 4) We are using results of dependency parsers and syntactic parsers to create features for improved machine learning and also to automatically find good titles for document clusters. n/a",Natural Language Processing Techniques To Enhance Information Access.,8943224,ZIALM000090,"['Abbreviations', 'Algorithms', 'Automated Abstracting', 'Data', 'Dependency', 'Drug abuse', 'Drug usage', 'Goals', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'MEDLINE', 'Machine Learning', 'Measures', 'Names', 'Natural Language Processing', 'Performance', 'Persons', 'Reporting', 'Sampling', 'System', 'Techniques', 'Text', 'Training', 'Variant', 'Weight', 'abstracting', 'base', 'improved', 'indexing', 'interest', 'phrases', 'spelling', 'syntax', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2014,561533,-0.026208653062448548
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8669161,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2014,212994,-0.024457809495793837
"Text Analytics, Machine Learning & Biomedical Data Science The Text Analytics, Machine Learning, and Biomedical Data Science, which operates within the Collaborative Research Office in Computer and Information Science (CROCIS), Division of Computational Bioscience of CIT, is collaborating with NIH investigators to build a critical mass in text and numerical analytics that is envisioned to encompass a number of pertinent and related disciplines in biomedical research including semantic interoperability, knowledge engineering, computational linguistics, text and data mining, natural language processing, machine learning, and visualization.  The program is intended to foster advances in critical domains at NIH including biomedical and clinical informatics, translational research, genomics, proteomics, systems biology, ""big data"" analysis, and portfolio analysis.  In 2013, collaborative efforts in support of these goals included the following.  - In collaboration with NIAID, CROCIS is developing a new algorithm capable of analyzing V(D)J recombination in thousands of immunoglobulin gene sequences produced by high throughput sequencing.  - CROCIS is working with Melissa Friesen of NCI to develop methodologies to improve exposure classification in occupational epidemiologic studies. Initial effort of this collaboration involves a tool that helps experts to classify free-text job descriptions into standard occupational codes. Machine-learning based classification methods will also be utilized to help with evaluating exposure-disease associations.   - In collaboration with NINDS, CROCIS has implemented and compared several methods to locate and characterize lysosomes in 3-D fluorescence images.  The goal is to be able to calculate the pH of each lysosome in the image, for which the ability to resolve their locations is an important step.  - In collaboration with NIA, we are applying machine learning and visualization techniques on large biological datasets to discover novel patterns of functional gene or protein interactions as related to aging. Omnimorph, a graphic data analysis tool, is being developed for multidimensional data visualization. In this collaboration, we are also developing a model to predict the progression of Alzheimer's disease using plasma proteomic biomarker data from the Alzheimer's Disease Neuroimaging Initiative (ADNI).  - Machine-learning methods have been devised and implemented to identify and refine transcription start sites in the fruit fly genome found using cap analysis gene expression (CAGE).  This effort is in collaboration with Brian Oliver of NIDDK.  - CROCIS is collaborating with NIAID in developing an image analysis pipeline to quantify individual transcript molecules in macrophage cells to help understand the molecular mechanism of macrophage adaptation to various stimuli at the single-cell level.  - A freely available plasmid database that is interoperable with popular freeware is currently being developed for the NIDA Optogenetics and Transgenic Technology Core. The plasmid database offers a versatile yet simple platform for scientists to store and analyze their plasmid data. Motivated by the need for a more comprehensive approach to archiving plasmid data, the database platform is enriched with numerous components beyond the repository, serving as an informatics platform designed to enhance the efficiency and analytic capabilities of scientists.   - In collaboration with CSR, CROCIS is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process.   The effort so far has concentrated on exploratory analysis against the NIH portfolio to evaluate clustering methods and assess intrinsic measures of cluster quality.  Content-based application referral tools are being developed to help evaluate the merit of PIs study section requests, and to recommend the most suitable study section for an application if no requests are made.  In addition, CROCIS is analyzing text from quick feedback surveys on peer review.  This effort includes evaluating a pilot study to evaluate the feasibility of analyzing free text from peer reviewers on their perception of the study section quality.  If successful, the pilot results will be used to as initial input for a full-scale implementation.  - CROCIS has been collaborating with the Molecular Libraries Program (MLP), part of the NIH Common Fund, to develop the Common Assay Reporting System (CARS). CARS is an integrated system for managing bioassay information and facilitating communication between all the high-throughput screening centers within the Molecular Libraries Probe Production Centers Network (MLPCN). Goals for this collaboration include: 1) Track project status and related issues at each of the screening centers within the MLPCN, and provide the means for information collection, sharing and retrieval among the centers and the program office at NIH. 2) Establish a standardized protocol to describe raw data from the experiments and report screening data to the scientific community.  - The human salivary protein catalog has been made available online on a community-based Web portal developed by CROCIS, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. This is a major step towards the discovery and use of saliva biomarkers to diagnose oral and systemic diseases.   - CROCIS investigators worked with the Office of Extramural Research (OER) on applying machine-learning methods to identify important terms that peer reviewers use to describe innovative applications.  The goal of the effort was to develop a lexicon of terms that can help estimate the innovation level of a grant application based on peer review critiques from the applications NIH Summary Statement.  - Although the scientific impact of NCI consortia on the advancement of cancer epidemiology research is understood to be significant, accurate quantitative metrics of this impact are needed by program leadership. We are developing methods to track citations to clinical guidelines in the context of evidence-based medicine that could provide funding agencies and program directors insight into individual consortia's contributions in advancing medical knowledge. This work is being conducted in collaboration with Epidemiology and Genomics Research Program (EGRP), NCI.  - Based on its experience in building novel models for classifying research grants and projects, CROCIS is collaborating with DPCPSI/OD and other ICs to develop the Portfolio Learning Tool, a comprehensive classification workflow system that will allow users to select from multiple classification algorithms, feature spaces, and training regimes, to build and run their own classifiers.  A particular prototype of this system is being tailored to assist NCI Intramural investigators in reporting their research to the Annual Report system.  CROCIS has been developing an augmented support vector machine (SVM) that augments a training set by sampling  from a corpus of unknowns and runs a large ensemble on various samples of this augmented space. The results obtained from this classifier suggest that, when coupled with an effective annotation strategy, such a classifier can be quite effective at categorizing a research portfolio.  - The Office of Behavioral and Social Sciences Research (OBSSR) is conducting a pilot investigation in collaboration with CROCIS to evaluate the efficacy of machine learning models for the classification of five BSSR-relevant research categories. n/a","Text Analytics, Machine Learning & Biomedical Data Science",8941588,ZIHCT000200,"['3-Dimensional', 'Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Annual Reports', 'Applications Grants', 'Archives', 'Big Data', 'Biological', 'Biological Assay', 'Biological Markers', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Categories', 'Cells', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computers', 'Computing Methodologies', 'Coupled', 'Critiques', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Discipline', 'Disease Association', 'Drosophila genome', 'Engineering', 'Epidemiologic Studies', 'Epidemiology', 'Evaluation', 'Evidence Based Medicine', 'Extramural Activities', 'Feedback', 'Fostering', 'Funding', 'Funding Agency', 'Gene Expression Profiling', 'Genes', 'Genomics', 'Goals', 'Grant Review Process', 'Guidelines', 'High-Throughput Nucleotide Sequencing', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Immunoglobulin Genes', 'Individual', 'Informatics', 'Information Resources', 'Information Sciences', 'Investigation', 'Job Description', 'Knowledge', 'Leadership', 'Learning', 'Location', 'Lysosomes', 'Machine Learning', 'Management Information Systems', 'Measures', 'Medical', 'Melissa', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Bank', 'Mouth Diseases', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'National Institute of Drug Abuse', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Occupational', 'Online Systems', 'Pattern', 'Peer Review', 'Perception', 'Pilot Projects', 'Plasma', 'Plasmids', 'Production', 'Proteins', 'Proteomics', 'Protocols documentation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Sharing', 'Retrieval', 'Running', 'Saliva', 'Salivary Proteins', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Social Network', 'Software Engineering', 'Stimulus', 'Study Section', 'Surveys', 'System', 'Systemic disease', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Training', 'Transcript', 'Transcription Initiation Site', 'Transgenic Organisms', 'Translational Research', 'United States National Institutes of Health', 'V(D)J Recombination', 'Work', 'base', 'behavioral/social science', 'biological systems', 'biomedical informatics', 'cancer epidemiology', 'cluster computing', 'computer science', 'computing resources', 'data management', 'data mining', 'data sharing', 'design', 'evidence base', 'experience', 'fluorescence imaging', 'high throughput screening', 'improved', 'information organization', 'innovation', 'insight', 'interoperability', 'knowledge base', 'macrophage', 'neuroimaging', 'novel', 'optogenetics', 'peer', 'programs', 'prototype', 'repository', 'research study', 'screening', 'social science research', 'text searching', 'tool']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2014,2410320,-0.030959054412374634
General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method we are currently studying is to perform machine learning with an SVM or other classifier and score the documents based on this learning. Then PAV can be applied to the resulting scores and this score function can be descretized without the loss of significant information. This allows us to make use of the results as features which can be individually weighted in another classifier. 4) We have developed a new algorithm called the periodic random orbiter algorithm (PROBE) which is applicable to minimize any convex loss function. We have applied it to the MeSH classification problem and it seems to work very well and better than the alternatives on such a large problem. 5) We are currently studying ways to apply SGD to large training sets to achieve better efficiency than can be obtained by more conventional methods. 6) We are also investigating methods to create features for machine learning using dependency parses and syntactic parse trees. n/a,General and Semi-supervised Machine Learning Applied to Bioinformatics,8943223,ZIALM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Classification', 'Data Set', 'Dependency', 'Exclusion', 'Learning', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Procedures', 'Retrieval', 'Testing', 'Training', 'Trees', 'Weight', 'Work', 'base', 'design', 'improved', 'loss of function', 'syntax', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2014,561533,0.006121769335267956
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.   1)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 2)	The PubMed database presents a unique challenge because of its very large size of over 19 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  3) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with theresults of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. 4) We are currently working on a problem which arises when several different kinds of documents appear in a dataset and one wants to compute neighboring documents for each document. A simple application of the same approach used to find related citations in PubMed does not produce good results. Analysis of the problem shows that there are many records with words in them that are not keyed to the actual focus of the record and that these words mislead the neighboring process. In some cases this is due to a common author of records who users certain word forms frequently in their writing even on very different subjects. In other cases the problem seems to appear when two different drugs have sections on side effects that are quite generic and have a large overlap, etc. In order to deal with this problem we tried several approaches to filter out useless words. First, we compared the frequency of words in the English Gigaword Corpus with their frequency in PubMed documents in an effort to remove non-biomedical terminology. In the same way we also compared the words in a large set of PubMed user queries to see what terminology may be important to users. By filtering out words that are neither very medical nor very important to users we saw some benefit in computing neighbors. A second approach involved Bayesian calculations to determine which words seemed to be specific to a particular source of PubMed Health documents. This also allowed us to filter terms coming from boiler plate specific to how documents were created in different sources. But the above approaches required a certain amount of hand supervision to avoid mistakes in filtering words. We have since found improved results with a completely automatic approach which examines how related each word in the body of a record is to words in the records title. This is achieved by removing all words related below a certain low threshold. 5) Our latest work uses concepts that appear in multiple article titles to produce document clusters. These are then analyzed using nave Bayesian classification methods to ascertain their significance. Those that are significant are extended using the same Bayesian technique. The result is a set of concepts each represented by a document cluster. This proves to be an effective way to produce significant clusters of relatively small data sets that are difficult to cluster by more standard methods. n/a",Automatic Bayesian Methods In Text Retrieval,8943214,ZIALM000021,"['Accounting', 'Active Learning', 'Adverse effects', 'Bayesian Method', 'Bayesian Modeling', 'Classification', 'Data', 'Data Set', 'Databases', 'Exhibits', 'Frequencies', 'Generic Drugs', 'Genomics', 'Goals', 'Hand', 'Health', 'Label', 'Machine Learning', 'MeSH Thesaurus', 'Medical', 'Methods', 'Modeling', 'Performance', 'Peripheral', 'Pharmaceutical Preparations', 'Poisson Distribution', 'Probability', 'Process', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'Source', 'Supervision', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'Writing', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2014,83190,0.00035427001520800327
"Automatic Analysis and Annotation of Document Keywords in Biomedical Literature 1) Electronic Textbook and PubMed Central Indexing Current processing of the electronic textbook material involves a number of steps designed to produce the most meaningful phrases in the text to be used as reference points. The first task is to identify grammatically reasonable phrases. We use a version of the Brill transformation based tagger, rewritten in C++, for part-of- speech tagging. This forms the basis for determining grammatically reasonable phrases. There is a significant post processing step that removes phrases that involve inappropriate references to context (e.g., different cells, final mutation). After finding grammatically reasonable phrases we attempt to eliminate those that are too common or generic to be useful (e.g., significant result, short time).  The next step is to compare a phrase with previously rated phrases that have been collected over the life of the project. The final stage is to estimate the importance of a phrase in the passage where it is found in a textbook. Such an estimate is based on the frequency of the phrase and the size of the passage compared with the frequency of the phrase throughout the book and the overall size of the book. In order to improve such an estimate we attempt to take account of the phrase or any phrase that represents the same concept. For this purpose we use the UMLS Metathesaurus and also stemming and combine these two approaches into a consistent picture of the concept as it occurs in the text.  The result of this processing is a scored list of phrase-book section pairs for each textbook. These are used to guide the response of general searching in the books. When a user types in a phrase that is on our curated list the first results given are the highly rated book sections for that phrase. We are now applying a similar indexing scheme to the text of articles in PMCentral. This allows us to give a list of highly rated phrases for each article as an enhanced reference point for searchers.    2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text.   3) Currently we are studying how good phrases can be recognized by their characteristics, such as frequency, tendency to be repeated in documents where they occur, and other numerical properties. These features allow one to predict which phrases are of high quality. We have found such predictions to be useful in studying different kinds of terms that may appear in text and how an ontoloogy might be extracted from text.  4) We have found stochastic gradient descent (SGD) with regularization by early stopping to be a very efficient method for training a Support Vector Machine (SVM) for MeSH term assignment. We have discovered that the early stopping can be implemented as stopping after a constant number of iterations and the results are as good as stopping based on held out data and also as good as more conventional methods of training an SVM on large data sets. The SGD approach is much faster and allows one to readily train classifiers for all 27,000 MeSH terms. Results are superior to previously published methods. n/a",Automatic Analysis and Annotation of Document Keywords in Biomedical Literature,8943238,ZIALM091711,"['Accounting', 'Address', 'Books', 'Cells', 'Characteristics', 'Collection', 'Data', 'Data Set', 'Electronics', 'Evaluation', 'Frequencies', 'Generic Drugs', 'Goals', 'Life', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Mutation', 'Outcome', 'Process', 'Property', 'PubMed', 'Publishing', 'Records', 'Retrieval', 'Scheme', 'Speech', 'Staging', 'Techniques', 'Text', 'Textbooks', 'Time', 'Training', 'Training Support', 'UMLS Metathesaurus', 'base', 'design', 'improved', 'indexing', 'insight', 'phrases', 'response', 'stem']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2014,103988,-0.027407349816893835
"Development Of Advanced Computer Hardware And Software The LoBoS cluster has been expanded with 72 new nodes with six core Intel Xeon processors using the Ivy Bridge microarchitecture.  These new nodes have provided a substantial performance boost to LoBoS.  In addition, 48 additional new nodes have been ordered, each of which will contain a Nvidia Tesla K20x GPU computing unit.  Testing performed by the lab using CHARMM and other software have shown that GPUs are now viable for the labs molecular simulations workload, therefore the decision was made to invest in this new technology.  Development of the CHARMMing web front end has continued. In the last year, focus was on enhancing the integrated lessons functionality of the interface.  In addition, we have made enhancements to the on-line CHARMM tutorial (www.charmmtutorial.org).  These led to the publication of a triple paper in PLoS Computational Biology.  Continuing work with the H. Lee Woodcock group at the University of South Florida focuses on necessary infrastructure improvements and enhanced compatibility with non-CHARMM molecular simulation engines.  Molecular simulation and modeling software packages are the vehicle for computational research and experiment.  Implementation of new methods and options is the key to facilitate cutting edge researches.  In recent years, this lab has developed a series new compuatational methods, such as the self-guided Langevin dynamics for efficient conformational searching and sampling, the isotropic periodic sum method for accurate and efficient calculation of long-range interactions, and the map-based modeling tool, EMAP, for electron microscropy studies.    Implementation of these new methods enables researchers to tackle difficult problems. These methods have been implemented into CHARMM to expand its capability in molecular simulation, conformational search, and structure prediction.  These methods are all available in CHARMM version 40. These methods are also been implemented into another widely used simulation package, AMBER, to extend the user scope to access these methods.  The SGLD, IPS, and EMAP methods are available in AMBER version 14.  Ongoing work has been focused on the development of replica exchange methods.  Many of these efforts have centered around pH replica exchange, both with and without the use of reservoirs.  The combination of Enveloping Distribution Sampling with Hamiltonian replica exchange (EDS-HREX) has provided a facile method for sampling different titration states of molecular system.  One of the key benefits of this method is that it can be used effectively with explicit solvent, which removes a key source of error in many constant pH simulations.  Also, a physically realistic conformation is sampled at each time step with EDS-HREX, in contrast with lambda-dynamics based methods. Current efforts involve expanding EDS-HREX to use reservoirs and two use two-dimensional replica exchange.  Using two-dimensional EDS-HREX allows for conformational transitions between multiple pH values or temperatures, which dramatically increase the number of state changes.  Initial testing has shown that this yields substantially faster state transitions than are achievable with EDS-HREX alone.  Additional replica exchange based approaches focus in two directions. Firstly, A new method, Perturbed Reservoir Replica Exchange (P-RREX), is under development. The primary advantage of this method is that it does not make any assumption about the composition of the reservoir. Although sampling efficiency will be limited by reservoir quality, P-RREX will produce a Boltzmann ensemble using any reservoir. Continued testing of the early implementation has been performed.  Secondly, work has been initiated on  applying Suwa-Todo based global balance methods to replica exchange simulations in CHARMM.  Pre-alpha level code has been developed and testing is starting.  Structure Activity Relationship (SAR) and Quantitative SAR (QSAR) have been traditionally used in lead optimization approaches in drug discovery research to improve the probability of certain features such as activity, druglikeness, or absorption, distribution, metabolism, and excretion (ADME) properties of lead compounds. (Q)SAR procedures reduce costs of the early drug discovery pipeline,, and have a long history both in industrial design and regulatory assessment of pharmaceuticals.,  We have developed a Free Web tool for SAR and QSAR modeling based on open source technology to add to the services provided by CHARMMing (www.charmming.org). This new module implements 15 different advanced machine learning algorithms for example Random Forest, Support Vector Machine (SVM), Stochastic Gradient Descent, Gradient Tree Boosting etc. A detailed instructions for creating new models are available from www.charmmtutorial.org.  A user can import training data to create new models (either categorical or numerical) in two ways: a) upload his or her own SD files which contain structures and activity information or b) use the Pubchem Bioassay search interface to query Pubchem database for relevant biological assay data via PUG REST and SOAP interfaces. , ,,,,   For the first time we have provided users with a streamlined procedure for automatically downloading training data sets, as well as tracking the model generation process and running models on new data to predict activity.   CHARMMing (Q)SAR tool automatically verifies new models by using well-known machine learning techniques such as cross-validation and y-randomization so users can immediately see whether the created model is able to calculate valid predictions. A user is presented with Area Under Curve (AUC) measurements for the training set, for the y&#8209;randomized set, and an average AUC for 5-fold cross-validation for categorical modeling. Pearson correlation coefficient R2 is used for the same purpose for regression models.  While there are other recently available tools such as OCHEM and Chembench, which offer web-based SAR model building, CHARMMing (Q)SAR service supports more extensive set of machine learning methods. It offers a user friendly interface developed to present the workflow in a straightforward way for novice and advanced users both and have the potential to dramatically change research strategies of academic research groups and non-profit organizations.  CHARMMing (Q)SAR service can be used as a stand-alone utility or as a supporting filter for the docking procedure, or for challenges such as Tox21 data challenge and Teach-Discover-Treat initiatives. n/a",Development Of Advanced Computer Hardware And Software,8940204,ZIHHL001052,"['Algorithms', 'Architecture', 'Area Under Curve', 'Biological Assay', 'Boxing', 'Code', 'Computational Biology', 'Computer Hardware', 'Computer software', 'Computers', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Docking', 'Educational process of instructing', 'Electrons', 'Equilibrium', 'Error Sources', 'Evaluation', 'Excretory function', 'Florida', 'Generations', 'High Performance Computing', 'Instruction', 'Internet', 'Lead', 'Machine Learning', 'Maps', 'Measurement', 'Metabolism', 'Methods', 'Modeling', 'Molecular', 'Molecular Conformation', 'Nonprofit Organizations', 'Online Systems', 'Paper', 'Performance', 'Personal Computers', 'Pharmacologic Substance', 'Probability', 'Procedures', 'Process', 'Property', 'PubChem', 'Publications', 'Quantitative Structure-Activity Relationship', 'Randomized', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Series', 'Services', 'Solvents', 'Structure', 'Structure-Activity Relationship', 'Sum', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Titrations', 'Training', 'Trees', 'United States National Aeronautics and Space Administration', 'Universities', 'Update', 'Validation', 'Work', 'Workload', 'absorption', 'base', 'cluster computing', 'computerized tools', 'conformational conversion', 'cost', 'cost effective', 'design', 'drug discovery', 'forest', 'human EML2 protein', 'improved', 'induced pluripotent stem cell', 'models and simulation', 'new technology', 'open source', 'parallel computer', 'research study', 'scientific computing', 'simulation', 'software development', 'tool', 'two-dimensional', 'user-friendly']",NHLBI,"NATIONAL HEART, LUNG, AND BLOOD INSTITUTE",ZIH,2014,490596,-0.00860756504635418
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) Code has been multithreaded and memory mapping capabilities added to speed up processing. 6) Most recently the code has been updated to work in a 64 bit environment.   The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system is currently proving useful in testing different retrieval parameters and methods on the PubMedHealth records.  We have recently developed a software system called DStor that allows us to store all of PubMed in a manner which is easily updateable and allows fast access. This system is now being used to maintain and update five different versions of the PubMed data twice a week. This system has greatly improved our access to PubMed data in various useful forms and we anctipate that its use will continue to grow. In addition we have developed software to maintain and update a list of strings where each string is associated with some fixed vector of integers. We currently maintain a list of all multi-word phrases without stop words or punctuation and with each is associated a vector of six integers representing counts of different types associated with each phrase where counts are computed over all PubMed records having abstracts. We also maintain a list of all one and two word phrases and MeSH terms in various forms (with & without stars and subheadings) and two counts with each consisting of the document frequency and the total frequency counting all occurrences in each document over all of PubMed. n/a",A Document Processing System,8943215,ZIALM000022,"['Code', 'Data', 'Databases', 'Environment', 'Frequencies', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'MeSH Thesaurus', 'Memory', 'Methods', 'Process', 'Programming Languages', 'PubMed', 'Records', 'Research', 'Retrieval', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Update', 'Work', 'abstracting', 'base', 'computerized data processing', 'improved', 'phrases', 'repository', 'software development', 'software systems', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2014,187178,-0.0019994026437594697
"GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY     DESCRIPTION:  Many complex diseases such as cancer, cardiovascular disorders, and schizophrenia may be understood as failures in the functioning of nested hierarchies of biomolecular and cellular networks. These nested hierarchies control a range of processes including the differentiation and migration of cells, remodeling of extracellular matrices and tissues, and information encoding in neuronal subsystems. Washington University has established expertise in cutting edge imaging, molecular biology and genomic technologies synergistic with computational approaches such as machine learning and unraveling the principles of hierarchical organization and dynamics of complex systems. This collective expertise is being leveraged to develop new drugs, improve our ability to interpret sophisticated imaging data, understand how populations of neurons act collectively to accomplish complex tasks, and model the onset and progression of complex diseases as dynamical rewiring of hierarchical, multi-scale networks. Biological network analyses provide a rich set of tools for organizing and interpreting the vast quantities of data produced by state-of-the-art experimental protocols. The rapid advancement of computationally intensive research in these areas is outstripping the capabilities of CPU-based high performance computing (HPC) systems. This application would support the acquisition and integration of a large-scale IBM high performance cluster of Graphics Processor Units (GPUs) to be added as an upgrade to the existing IBM-designed Heterogeneous High Performance Computing environment to form a state-of-the-art hybrid computing capability. Such a resource is essential to match the growing need for high performance computing at Washington University and to support state of the art research software applications that are optimized for GPU computing. The acquisition and integration of a high performance GPU cluster will solve critical computing challenges that exist within Washington University's growing NIH research portfolio. The proposed state-of-the-art hybrid GPU/CPU computing capabilities will be deployed within the framework of a stable, productive and rapidly growing resource center. The addition of high-capacity GPU computing capabilities will allow critical calculation to be performed in hours instead of days and enable substantial increases in productivity for existing projects covering a broad range of application areas as well as enabling new research directions.             n/a",GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY,8640341,S10OD018091,"['Area', 'Biological', 'Biology', 'Cardiovascular Diseases', 'Complex', 'Computer Systems', 'Computer software', 'Data', 'Disease', 'Environment', 'Extracellular Matrix', 'Failure', 'Genomics', 'High Performance Computing', 'Hour', 'Hybrids', 'Image', 'Machine Learning', 'Malignant Neoplasms', 'Modeling', 'Molecular Biology', 'Neurons', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Productivity', 'Protocols documentation', 'Research', 'Resources', 'Schizophrenia', 'System', 'Technology', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'cell motility', 'computing resources', 'design', 'improved', 'innovation', 'tool']",OD,WASHINGTON UNIVERSITY,S10,2014,597700,-0.012078949830900151
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8657051,R01GM053163,"['Address', 'Algorithms', 'Biochemical', 'Budgets', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Development', 'Drug Design', 'Evaluation', 'Funding', 'Geometry', 'Goals', 'Image', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Output', 'Pattern', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Relative (related person)', 'Research', 'Shapes', 'Signal Transduction', 'Solutions', 'Specimen', 'Spottings', 'Structure', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2014,320096,-0.033294097746835755
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8661774,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2014,1248956,-0.007816873091463922
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8729006,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Cloud Computing', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,554661,-0.015557364748619115
"Scalable Biomedical Pattern Recognition Via Deep Learning     DESCRIPTION (provided by applicant):  Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.      The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.      Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.      In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes.             Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,8689173,R21LM011664,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Architecture', 'Area', 'Biomedical Research', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computerized Medical Record', 'Couples', 'Data', 'Data Set', 'Data Sources', 'Dependence', 'Development', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Engineering', 'Evaluation', 'Exhibits', 'Feedback', 'Goals', 'Gold', 'Healthcare Systems', 'Human', 'Image', 'Individual', 'Judgment', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Literature', 'Manuals', 'Measures', 'Medical', 'Metformin', 'Methods', 'Modeling', 'Myocardial Infarction', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'PTGS2 gene', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Population', 'Probability', 'Process', 'ROC Curve', 'Records', 'Research', 'Risk', 'Sampling', 'Sea', 'Serum', 'Single Nucleotide Polymorphism', 'Source', 'Specific qualifier value', 'Structure', 'Testing', 'Time', 'To specify', 'Uncertainty', 'Uric Acid', 'Work', 'cell growth', 'clinical care', 'clinical phenotype', 'density', 'diabetic', 'genetic association', 'genetic variant', 'inhibitor/antagonist', 'interest', 'meetings', 'neoplastic cell', 'non-diabetic', 'outcome forecast', 'prevent', 'speech recognition', 'success', 'type I and type II diabetes']",NLM,VANDERBILT UNIVERSITY,R21,2014,202714,-0.014973286811577842
"Scalable Biomedical Pattern Recognition Via Deep Learning DESCRIPTION (provided by applicant): Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.  The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is  being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.  Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.  In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes. Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,9302040,R21LM011664,[' '],NLM,VANDERBILT UNIVERSITY MEDICAL CENTER,R21,2014,7696,-0.014973286811577842
"Protect Privacy of Healthcare Data in the Cloud     DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier.             Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8810023,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2014,183155,-0.004615484000509634
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8714052,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,UNIVERSITY OF UTAH,R01,2014,579144,-0.058476599998070845
"Division of Computational Bioscience Scientific Computing Facility The Division of Computational Bioscience (DCB) Scientific Computing Facility (SCF) collaborates with DCB staff and their NIH investigators to build and provide a diverse but robust scientific computing system. This system provides the infrastructure needed by the collaborations at the same time as complying with all security, confidentiality and integrity requirements, especially those concerned with storing PII and other sensitive data. With common smooth software and hardware refreshes, collaborators are allowed to work uninterrupted. Expert staff are able to provide centralised support and advice to enable the best use of resources in support of the Division's Intramural Research Program.  The Facility comprises of low availability equipment rooms containing a wide range of hardware for scientific computing with approximately 200 continuously running servers, which provide compute nodes and data storage. There are separate racks for machines used for storing and processing sensitive information, such as personal health information and grant information, with appropriate extra layers of protection. DCB also rents space from the Division of Computer System Services (DCSS) of the Center for Information Technology (CIT) for high availability production systems, such as those supporting the Salivary Proteome Wiki.  A common and federated environment across all computational systems, enables DCB staff to quickly build the data analysis tools they need throughout the lifetime of a collaboration. Common file systems, unified authentication, system management, and other system coordinating features are fully integrated into an infrastructure containing the variety of supporting software. Commodity hardware, free software and commercial software are integrated to provide efficient and sustainable solutions.  Staff perform the many common security, maintenance, backup and reporting tasks that are required for the day to day operation of scientific computational systems on behalf of DCB. In particular, managing the Federally mandated Certification and Accreditation process on DCB's Computational Facility that would otherwise be a significant burden to DCB developers and investigators as well as their collaborators. When an collaborative project is concluded, staff work with the ICs to relocate systems to space within the ICs or the CIT Data Center as appropriate.  Facility staff are also refining customised versions of various Linux distributions to comply with NIH computer configuration policies, including the policies on user account life-cycle, user account passwords, and Incident Response Team (IRT) scanning. This ""Lab Linux"" configuration leverages the existing IT infrastructure at NIH, thereby making it easy for NIH laboratory staff to use and maintain the system.  Many prominent investigators panels and science leaders have reported a lack of skilled professional computational experts to help with running similar resources across NIH. Laboratories tend to rely on the incidental expertise found among their, often temporary, staff. With the large amounts of data processing that is required for projects at NIH, the Facility provides the necessary expertise to DCB. Moreover, DCB is receiving an increasing number of requests from across the ICs to jump-start and support laboratory-based scientific computing infrastructures directly because of the success of DCB's own Facility.  In 2014, supported collaborations include:          . The Human Salivary Protein Catalog, with NIDCR, supporting the Salivary Proteome community.          . Molecular Libraries Program (MLP), part of the NIH Common Fund, to develop the Common Assay Reporting System (CARS).          . Molecular Structure Determination (NIDDK, NIDCR, NCI, NHLBI).          . The Center for Molecular Modeling's MMIGNET programme supporting NIH as a whole.          . Microarray Database System (mAdb) with NCI and NIAID.          . The Genetic Association Database archive of human genetic association studies of complex diseases and disorders, in collaboration with NIA.          . Undiagnosed Diseases Program Portal (ORDR, NHGRI, CC) enabling experts from across the world help NIH to diagnose rare diseases.          . Portfolio analysis and portfolio visualization resources supporting many groups throughout NIH, including OD, NCI and NIGMS.          . A collaborative effort with NHLBI to design and execute a program which computes billions of SNP-gene expression association tests, in an effort to find expression - single nucleotide polymorphisms, eSNPs. Efforts so far are producing a 20-fold speed up over previous efforts.          . A joint project with NIDDK to apply random forest learning machines to identify and refine transcription start sites in the fruit fly genome based on data obtained from cap analysis gene expression (CAGE) data.          . A joint project, with NCI and a NCI-funded consortium, implementing an analysis pipeline and providing the necessary bioinformatics tools for transcriptome analysis and biological interpretation of RNA-seq/Exon capture data from next generation sequencing.          . A joint project with NHLBI and other ICs to provide a nexus of computational system administration support and set policies and standards for NIH. n/a",Division of Computational Bioscience Scientific Computing Facility,8941590,ZIHCT000274,"['Accounting', 'Accreditation', 'Architecture', 'Archives', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cataloging', 'Catalogs', 'Certification', 'Collaborations', 'Communities', 'Complex', 'Computer Systems', 'Computer software', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Diagnosis', 'Disease', 'Drosophila genome', 'Environment', 'Equipment', 'Exons', 'Funding', 'Gap Junctions', 'Gene Expression', 'Gene Expression Profile', 'Gene Expression Profiling', 'Goals', 'Grant', 'Health', 'High Performance Computing', 'Human', 'Human Genetics', 'Imagery', 'Information Technology', 'Intramural Research Program', 'Joints', 'Laboratories', 'Life Cycle Stages', 'Linux', 'Machine Learning', 'Maintenance', 'Molecular Bank', 'Molecular Models', 'Molecular Structure', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'National Institute of General Medical Sciences', 'Policies', 'Process', 'Production', 'Proteome', 'Rare Diseases', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Salivary', 'Salivary Proteins', 'Scanning', 'Science', 'Security', 'Services', 'Single Nucleotide Polymorphism', 'Software Tools', 'Solutions', 'Speed', 'System', 'Testing', 'Time', 'Transcription Initiation Site', 'United States National Institutes of Health', 'Work', 'base', 'cluster computing', 'computerized data processing', 'data sharing', 'design', 'forest', 'genetic association', 'member', 'molecular modeling', 'next generation sequencing', 'operation', 'programs', 'response', 'scientific computing', 'success', 'tool', 'transcriptome sequencing', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2014,602580,-0.0030133520236009233
"Building an open-source cloud-based computational platform to improve data access   We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members.  One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. This proposal aims to fill this gap by developing a robust platform that integrates state-of-the-art open-source technologies for data storage, data access, data mining and analysis, annotation, visualization and reporting.  We previously developed a cloud-based BioDatomics platform for Next Generation Sequencing (NGS), BioDX, which has been successful and has been used commercially by several clients. This proposal aims to develop a new platform leveraging our experience with the BioDX platform that integrates: data storage and real-time data querying using Cloudera Impala; powerful and customizable analytics tools using R and its derivative Bioconductor suite of programs for bioinformatics; annotation integration and reporting which is an existing feature of BioDX; and a visual programming interface that will simplify and enhance the development and maintenance of reproducible analytics workflows. We believe this powerful integrated data platform, if successful, will enable real-time collaboration, dramatically reduce data repository costs, and increase the efficiency and efficacy of data analyses for translating experimental data into actionable research products.  We are committed to analyzing stakeholder needs and optimizing hardware, software and information technology systems to meet their demands. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools.  The initial phase of work will involve development of the platform, optimizing performance on the cloud and testing the integration of new technology. BioDatomics is committed to funding the next phase of work which will include usability testing and finalizing a commercial product, following which full commercialization will proceed. Preliminary commercialization plans have demonstrated that the project has the capacity to generate a million dollars in revenue during the first full year after commercial release.  The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This open source platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders. PUBLIC HEALTH RELEVANCE: One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools. The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders.            ",Building an open-source cloud-based computational platform to improve data access,8647860,R43DA036970,"['Academia', 'Apache Indians', 'Bioconductor', 'Bioinformatics', 'Biological', 'Businesses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Clinical', 'Clinical Research', 'Collaborations', 'Commit', 'Complex', 'Computer software', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Distributed Databases', 'Drug Industry', 'Environment', 'Expenditure', 'Funding', 'Genomics', 'Government Agencies', 'Growth', 'Imagery', 'Industry', 'Information Dissemination', 'Information Systems', 'Java', 'Language', 'Licensing', 'Link', 'Machine Learning', 'Maintenance', 'Marketing', 'Messenger RNA', 'Modeling', 'Molecular', 'Molecular Target', 'Mutation', 'Online Systems', 'Patients', 'Performance', 'Pharmacologic Substance', 'Phase', 'Programming Languages', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk Behaviors', 'Savings', 'Services', 'Software Engineering', 'Software Tools', 'Solutions', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Substance Addiction', 'Substance Use Disorder', 'Substance abuse problem', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Visual', 'Work', 'base', 'beneficiary', 'cloud based', 'commercialization', 'computer infrastructure', 'cost', 'cost effective', 'data mining', 'drug discovery', 'experience', 'improved', 'mathematical model', 'meetings', 'member', 'models and simulation', 'new technology', 'next generation sequencing', 'novel', 'open source', 'programs', 'public health relevance', 'substance abuse treatment', 'success', 'tool', 'usability', 'user-friendly']",NIDA,"BIODATOMICS, LLC",R43,2014,195584,-0.01886265749689104
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG. RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8928685,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2014,159202,-0.011198060072204042
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8588958,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2014,2177431,-0.011198060072204042
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453          PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8722026,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2014,342375,-0.03838203176353183
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,-0.004100438388811052
"Informatics Tools for Pharmacogenomic Discovery using Practice-based Data     DESCRIPTION (provided by applicant): Rapid growth in the clinical implementation of large electronic medical records (EMRs) has led to an unprecedented expansion in the availability of dense longitudinal datasets for observational research. More recently, huge efforts have linked EMR databases with archived biological material, to accelerate research in personalized medicine. EMR- linked DNA biobanks have identified common and rare genetic variants that contribute to risk of disease. An appealing vision, which has not been extensively explored, is to use EMRs-linked biobanks for pharmacogenomic studies, which identify associations between genetic variation and drug efficacy and toxicity. The longitudinal nature of the data contained within EMRs make them ideal for quantifying drug outcome (both efficacy and toxicity). Efforts are already underway to link these EMRs across institutions, and standardize the definition of phenotypes for large-scale studies of treatment outcome, specifically within the context of routine clinical care. Despite its success, EMR-based pharmacogenomic studies are often hampered by its data-intensive nature -- it is time- consuming and costly to extract and integrate data from multiple heterogeneous EMR databases, for large-scale pharmacogenomic studies. The Informatics for Integrating Biology and the Bedside (i2b2) is a National Center for Biomedical Computing based at Partners Healthcare System. I2b2 has developed a scalable informatics framework to enable clinical researchers to repurpose existing EMR data for clinical and genomic discovery. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by proposing the following specific aims: 1) Develop new methods to extract and model drug exposure and outcome information from EMR and integrate them with the i2b2 NLP components; 2) Build ontology tools to normalize and integrate pharmacogenomic data across different sites; 3) Conduct known and novel pharmacogenomic studies to evaluate and refine tools developed in Aim 1 and 2; and 4) Disseminate the developed informatics tools among pharmacogenomic researchers.         PUBLIC HEALTH RELEVANCE: Longitudinal electronic medical records (EMRs) linked with DNA biobanks have become valuable resources for genomic and pharmacogenomics research, allowing identification of associations between genetic variations and drug efficacy and toxicity. The Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing based at Partners Healthcare System, has developed a scalable informatics framework to enable clinical researchers to use existing EMR data for genomic knowledge discovery of diseases. In this study, we will collaborate with i2b2 to extend its informatics framework to the pharmacogenomics domain, by developing new natural language processing, ontology components, and user-friendly interfaces, and then apply these tools to real-world pharmacogenomic studies.            ",Informatics Tools for Pharmacogenomic Discovery using Practice-based Data,8629996,R01GM103859,"['Adverse event', 'Algorithms', 'Anthracyclines', 'Archives', 'Award', 'Biocompatible Materials', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Boston', 'Cardiotoxicity', 'Cells', 'Clinic', 'Clinical', 'Clinical Data', 'Clostridium difficile', 'Colitis', 'Communities', 'Computer software', 'Computerized Medical Record', 'Coupled', 'DNA', 'Data', 'Data Set', 'Databases', 'Disease', 'Drug Exposure', 'Drug toxicity', 'Electronics', 'Event', 'Foundations', 'Funding', 'Genetic Variation', 'Genomics', 'Genotype', 'Grant', 'Healthcare Systems', 'Heparin', 'Informatics', 'Information Management', 'Institution', 'Knowledge Discovery', 'Link', 'Medicine', 'Methods', 'Modeling', 'Morphologic artifacts', 'Natural Language Processing', 'Nature', 'Observational Study', 'Ontology', 'Outcome', 'Patients', 'Pediatric Hospitals', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Population Heterogeneity', 'Population Study', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Site', 'Standardization', 'Structure', 'System', 'Terminology', 'Text', 'Thrombocytopenia', 'Time', 'TimeLine', 'Toxic effect', 'Treatment outcome', 'United States National Institutes of Health', 'Vancomycin', 'Vision', 'Warfarin', 'base', 'biobank', 'case control', 'clinical care', 'clopidogrel', 'data integration', 'disorder risk', 'drug efficacy', 'exome sequencing', 'genetic variant', 'improved', 'large-scale database', 'novel', 'open source', 'public health relevance', 'rapid growth', 'rare variant', 'response', 'success', 'surveillance study', 'tool', 'user-friendly', 'virtual']",NIGMS,VANDERBILT UNIVERSITY,R01,2014,648591,-0.03116650450820731
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in three subprojects which use natural language processing techniques:  1) We have developed a machine learning algorithm for abbreviation definition identification in text which makes use of what we term naturally labeled data. Positive training examples are naturally occurring potential abbreviation-definition pairs in text. Negative training examples are generated by randomly mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Medstract corpora. Our system demonstrated results that compare favourably to the existing Ab3P and BIOADI systems. We achieve an F-measure of 91.36% on Ab3P corpus, and an F-measure of 87.13% on BIOADI corpus which are superior to the results reported by Ab3P and BIOADI systems. Moreover, we outperform these systems in terms of recall, which is one of our goals. 2) We are studying paraphrases in MEDLINE abstracts. These come about because an author is describing some entity of interest and uses a phrase like ""drug abuse"" and then needing to describe the same entity again a sentence or two latter does not wish to use exactly the same wording again and may use a variant of the phrase such as ""drug use"" which in the context of ""drug abuse"" has substantially the same meaning.  3) An author disambiguation algorithm has been developed which relies on machine learning based on the assumption that if an author name is infrequent in the data it probably represents the same person in all documents where it is found. This gives us positive instances. Negative instances are sampled from pairs of documents that have no author in common. Such positive and negative data allows us to do machine learning on all aspects of the document other than the name in question. This allows us to learn how to weight this data for best performance in distinguishing the positive and negative instances from each other. This learning is then applied in individual name cases or spaces to determine which author document pairs represent the same author. n/a",Natural Language Processing Techniques To Enhance Information Access.,8746735,ZIALM000090,"['Abbreviations', 'Algorithms', 'Automated Abstracting', 'Data', 'Dependency', 'Drug abuse', 'Drug usage', 'Goals', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'MEDLINE', 'Machine Learning', 'Measures', 'Names', 'Natural Language Processing', 'Performance', 'Persons', 'Reporting', 'Sampling', 'System', 'Techniques', 'Text', 'Training', 'Variant', 'Weight', 'abstracting', 'base', 'indexing', 'interest', 'phrases', 'spelling', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2013,534204,-0.028359116112442576
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8484438,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2013,202118,-0.024457809495793837
"Text Analytics, Machine Learning & High Performance Computing The Text Analytics, Machine Learning, and High Performance Computing Program, which operates within the High Performance Computing and Informatics Office (HPCIO), Division of Computational Bioscience of CIT, is collaborating with NIH investigators to build a critical mass in text and numerical analytics that is envisioned to encompass a number of pertinent and related disciplines in biomedical research including semantic interoperability, knowledge engineering, computational linguistics, text and data mining, natural language processing, machine learning, and visualization.  The program is intended to foster advances in critical domains at NIH including biomedical and clinical informatics, translational research, genomics, proteomics, systems biology, ""big data"" analysis, and portfolio analysis.  In 2013, collaborative efforts in support of these goals included the following.  - In collaboration with NIAID, HPCIO developing a new algorithm capable of analyzing V(D)J recombination in thousands of immunoglobulin gene sequences produced by high throughput sequencing.  - HPCIO is working with Melissa Friesen of NCI to develop methodologies to improve exposure classification in occupational epidemiologic studies. Initial effort of this collaboration involves a tool that helps experts to classify free-text job descriptions into standard occupational codes. Machine-learning based classification methods will also be utilized to help with evaluating exposure-disease associations.   - In collaboration with NINDS, HPCIO has implemented and compared several methods to locate and characterize lysosomes in 3-D fluorescence images.  The goal is to be able to calculate the pH of each lysosome in the image, for which the ability to resolve their locations is an important step.  - In collaboration with NIA, we are applying machine learning and visualization techniques on large biological datasets to discover novel patterns of functional gene or protein interactions as related to aging. Omnimorph, a graphic data analysis tool, is being developed for multidimensional data visualization. In this collaboration, we are also developing a model to predict the progression of Alzheimer's disease using plasma proteomic biomarker data from the Alzheimer's Disease Neuroimaging Initiative (ADNI).  - Machine-learning methods have been devised and implemented to identify and refine transcription start sites in the fruit fly genome found using cap analysis gene expression (CAGE).  This effort is in collaboration with Brian Oliver of NIDDK.  - HPCIO has developed a standardized installation of various distributions of Linux, with customized configurations, that complies with many of the NIH computer configuration policies. This ""Lab Linux"" configuration leverages the existing IT infrastructure at NIH to comply with the user account life-cycle policy, user account password policy, and Incident Response Team (IRT) scanning policy, thereby making it easy for laboratory staff to use and maintain the system. We are currently seeking partners in the Institutes to test this deployment.  - In collaboration with CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process.   The effort so far has concentrated on exploratory analysis against the NIH portfolio to evaluate clustering methods and assess intrinsic measures of cluster quality.  - HPCIO has been collaborating with the Molecular Libraries Program (MLP), part of the NIH Common Fund, to develop the Common Assay Reporting System (CARS). CARS is an integrated system for managing bioassay information and facilitating communication between all the high-throughput screening centers within the Molecular Libraries Probe Production Centers Network (MLPCN). Goals for this collaboration include: 1) Track project status and related issues at each of the screening centers within the MLPCN, and provide the means for information collection, sharing and retrieval among the centers and the program office at NIH. 2) Establish a standardized protocol to describe raw data from the experiments and report screening data to the scientific community.  - The human salivary protein catalog has been made available online on a community-based Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. This is a major step towards the discovery and use of saliva biomarkers to diagnose oral and systemic diseases.   - We are working with the Office of Extramural Research (OER) on applying machine-learning methods to identify important terms that peer reviewers use to describe innovative applications.  The goal of the effort is to develop a lexicon of terms that can help estimate the innovation level of a grant application based on peer review critiques from the applications NIH Summary Statement.  - Although the scientific impact of NCI consortia on the advancement of cancer epidemiology research is understood to be significant, accurate quantitative metrics of this impact are needed by program leadership. We are developing methods to track citations to clinical guidelines in the context of evidence-based medicine that could provide funding agencies and program directors insight into individual consortia's contributions in advancing medical knowledge. This work is being conducted in collaboration with Epidemiology and Genomics Research Program (EGRP), NCI.  - HPCIO is working with NINDS and the Office of Extramural Research (OER) to determine peer-review sentiment of grant applications based on the NIH Summary Statement.  The sentiment analysis results can provide decision support information to NIH program directors considering applications for selective pay.  - Based on its experience in building novel models for classifying research grants and projects, HPCIO is collaborating with DPCPSI/OD and NCI to develop a comprehensive classification workflow system that will allow users to select from multiple classification algorithms, feature spaces, and training regimes, to build and run their own classifiers.  A particular prototype of this system is being tailored to assist NCI Intramural investigators in reporting their research to the Annual Report system.  - The Office of Behavioral and Social Sciences Research (OBSSR) is conducting a pilot investigation in collaboration with HPCIO to evaluate the efficacy of machine learning models for the classification of five BSSR-relevant research categories.   - In response to input from various collaborative groups, HPCIO is developing a portfolio visualization resource, known as PViz, that integrates visualization of categorical data with results of clustering algorithms, to allow analysts to gain new insight into their data.  Users may either construct a portfolio from IMPAC II data or import their own custom portfolio of categorical data.    - In collaboration with various groups including the Division of Planning, Coordination, and Strategic Initiatives (DPCPSI/OD), HPCIO has been developing an augmented support vector machine (SVM) that augments a training set by sampling  from a corpus of unknowns and runs a large ensemble on various samples of this augmented space. The results obtained from this classifier suggest that, when coupled with an effective annotation strategy, such a classifier can be quite effective at categorizing a research portfolio. n/a","Text Analytics, Machine Learning & High Performance Computing",8746900,ZIHCT000200,"['3-Dimensional', 'Accounting', 'Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Annual Reports', 'Applications Grants', 'Architecture', 'Biological', 'Biological Assay', 'Biological Markers', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Categories', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Coupled', 'Critiques', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Discipline', 'Disease Association', 'Drosophila genome', 'Engineering', 'Epidemiologic Studies', 'Epidemiology', 'Evaluation', 'Evidence Based Medicine', 'Extramural Activities', 'Fostering', 'Funding', 'Funding Agency', 'Gene Expression Profiling', 'Genes', 'Genomics', 'Goals', 'Grant Review Process', 'Guidelines', 'High Performance Computing', 'Human', 'Image', 'Imagery', 'Immunoglobulin Genes', 'Individual', 'Informatics', 'Information Resources', 'Institutes', 'Investigation', 'Job Description', 'Knowledge', 'Laboratories', 'Leadership', 'Life Cycle Stages', 'Linguistics', 'Linux', 'Location', 'Lysosomes', 'Machine Learning', 'Management Information Systems', 'Measures', 'Medical', 'Melissa', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Molecular Bank', 'Mouth Diseases', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Occupational', 'Online Systems', 'Pattern', 'Peer Review', 'Plasma', 'Policies', 'Production', 'Proteins', 'Proteomics', 'Protocols documentation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Sharing', 'Resources', 'Retrieval', 'Running', 'Saliva', 'Salivary Proteins', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Semantics', 'Social Network', 'Software Engineering', 'Software Tools', 'System', 'Systemic disease', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Transcription Initiation Site', 'Translational Research', 'United States National Institutes of Health', 'V(D)J Recombination', 'Work', 'base', 'behavioral/social science', 'biological systems', 'biomedical informatics', 'cancer epidemiology', 'cluster computing', 'data management', 'data mining', 'data sharing', 'evidence base', 'experience', 'fluorescence imaging', 'high throughput screening', 'improved', 'information organization', 'innovation', 'insight', 'interoperability', 'knowledge base', 'neuroimaging', 'novel', 'peer', 'programs', 'prototype', 'research study', 'response', 'screening', 'social science research', 'text searching', 'tool']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2013,2419860,-0.017786139417411237
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method we are currently studying is to perform machine learning with an SVM or other classifier and score the documents based on this learning. Then PAV can be applied to the resulting scores and this score function can be descretized without the loss of significant information. This allows us to make use of the results as features which can be individually weighted in another classifier. 4) We have developed a new algorithm called the periodic random orbiter algorithm (PROBE) which is applicable to minimize any convex loss function. We have applied it to the MeSH classification problem and it seems to work very well and better than the alternatives on such a large problem. 5) We are currently studying ways to apply SGD to large training sets to achieve better efficiency than can be obtained by more conventional methods. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,8746734,ZIALM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Classification', 'Data Set', 'Exclusion', 'Learning', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Procedures', 'Retrieval', 'Testing', 'Training', 'Weight', 'Work', 'base', 'design', 'improved', 'loss of function', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2013,575296,0.006050278079395793
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.   1)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 2)	The PubMed database presents a unique challenge because of its very large size of over 19 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  3) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with theresults of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. 4) We are currently working on a problem which arises when several different kinds of documents appear in a dataset and one wants to compute neighboring documents for each document. A simple application of the same approach used to find related citations in PubMed does not produce good results. Analysis of the problem shows that there are many records with words in them that are not keyed to the actual focus of the record and that these words mislead the neighboring process. In some cases this is due to a common author of records who users certain word forms frequently in their writing even on very different subjects. In other cases the problem seems to appear when two different drugs have sections on side effects that are quite generic and have a large overlap, etc. In order to deal with this problem we tried several approaches to filter out useless words. First, we compared the frequency of words in the English Gigaword Corpus with their frequency in PubMed documents in an effort to remove non-biomedical terminology. In the same way we also compared the words in a large set of PubMed user queries to see what terminology may be important to users. By filtering out words that are neither very medical nor very important to users we saw some benefit in computing neighbors. A second approach involved Bayesian calculations to determine which words seemed to be specific to a particular source of PubMed Health documents. This also allowed us to filter terms coming from boiler plate specific to how documents were created in different sources. But the above approaches required a certain amount of hand supervision to avoid mistakes in filtering words. We have since found improved results with a completely automatic approach which examines how related each word in the body of a record is to words in the records title. This is achieved by removing all words related below a certain low threshold. n/a",Automatic Bayesian Methods In Text Retrieval,8746724,ZIALM000021,"['Accounting', 'Active Learning', 'Adverse effects', 'Bayesian Method', 'Classification', 'Data', 'Data Set', 'Databases', 'Exhibits', 'Frequencies', 'Generic Drugs', 'Genomics', 'Goals', 'Hand', 'Health', 'Label', 'Machine Learning', 'MeSH Thesaurus', 'Medical', 'Methods', 'Modeling', 'Performance', 'Peripheral', 'Pharmaceutical Preparations', 'Poisson Distribution', 'Probability', 'Process', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'Source', 'Supervision', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'Writing', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2013,82185,-0.002884218124551328
"Automatic Analysis and Annotation of Document Keywords in Biomedical Literature 1) Electronic Textbook and PubMed Central Indexing Current processing of the electronic textbook material involves a number of steps designed to produce the most meaningful phrases in the text to be used as reference points. The first task is to identify grammatically reasonable phrases. We use a version of the Brill transformation based tagger, rewritten in C++, for part-of- speech tagging. This forms the basis for determining grammatically reasonable phrases. There is a significant post processing step that removes phrases that involve inappropriate references to context (e.g., different cells, final mutation). After finding grammatically reasonable phrases we attempt to eliminate those that are too common or generic to be useful (e.g., significant result, short time).  The next step is to compare a phrase with previously rated phrases that have been collected over the life of the project. The final stage is to estimate the importance of a phrase in the passage where it is found in a textbook. Such an estimate is based on the frequency of the phrase and the size of the passage compared with the frequency of the phrase throughout the book and the overall size of the book. In order to improve such an estimate we attempt to take account of the phrase or any phrase that represents the same concept. For this purpose we use the UMLS Metathesaurus and also stemming and combine these two approaches into a consistent picture of the concept as it occurs in the text.  The result of this processing is a scored list of phrase-book section pairs for each textbook. These are used to guide the response of general searching in the books. When a user types in a phrase that is on our curated list the first results given are the highly rated book sections for that phrase. We are now applying a similar indexing scheme to the text of articles in PMCentral. This allows us to give a list of highly rated phrases for each article as an enhanced reference point for searchers.    2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text.   3) Currently we are studying how good phrases can be recognized by their characteristics, such as frequency, tendency to be repeated in documents where they occur, and other numerical properties. These features allow one to predict which phrases are of high quality. We have found such predictions to be useful in studying different kinds of terms that may appear in text and how an ontoloogy might be extracted from text. n/a",Automatic Analysis and Annotation of Document Keywords in Biomedical Literature,8746749,ZIALM091711,"['Accounting', 'Address', 'Books', 'Cells', 'Characteristics', 'Collection', 'Data', 'Electronics', 'Evaluation', 'Frequencies', 'Generic Drugs', 'Goals', 'Life', 'Literature', 'Machine Learning', 'Methods', 'Mutation', 'Outcome', 'Process', 'Property', 'PubMed', 'Records', 'Retrieval', 'Scheme', 'Speech', 'Staging', 'Techniques', 'Text', 'Textbooks', 'Time', 'Training', 'UMLS Metathesaurus', 'base', 'design', 'improved', 'indexing', 'insight', 'phrases', 'response', 'stem']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2013,184917,-0.02914694108198118
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) Code has been multithreaded and memory mapping capabilities added to speed up processing. 6) Most recently the code has been updated to work in a 64 bit environment.   The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system is currently proving useful in testing different retrieval parameters and methods on the PubMedHealth records.  We have recently developed a software system called DStor that allows us to store all of PubMed in a manner which is easily updateable and allows fast access. This system is now being used to maintain and update five different versions of the PubMed data twice a week. This system has greatly improved our access to PubMed data in various useful forms and we anctipate that its use will continue to grow. In addition we have developed software to maintain and update a list of strings where each string is associated with some fixed vector of integers. We currently maintain a list of all multi-word phrases without stop words or punctuation and with each is associated a vector of six integers representing counts of different types associated with each phrase where counts are computed over all PubMed records having abstracts. We also maintain a list of all one and two word phrases and MeSH terms in various forms (with & without stars and subheadings) and two counts with each consisting of the document frequency and the total frequency counting all occurrences in each document over all of PubMed. n/a",A Document Processing System,8746725,ZIALM000022,"['Code', 'Data', 'Databases', 'Environment', 'Frequencies', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'MeSH Thesaurus', 'Memory', 'Methods', 'Online Systems', 'Process', 'Programming Languages', 'PubMed', 'Records', 'Research', 'Retrieval', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Update', 'Work', 'abstracting', 'base', 'computerized data processing', 'improved', 'phrases', 'repository', 'software development', 'software systems', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2013,102732,-0.0019994026437594697
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8470172,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2013,309127,-0.033294097746835755
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8473164,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2013,1216983,-0.007816873091463922
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8531347,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2013,512888,-0.015557364748619115
"Scalable Biomedical Pattern Recognition Via Deep Learning     DESCRIPTION (provided by applicant):  Patterns extracted from Electronic Medical Records (EMRs) and other biomedical datasets can provide valuable feedback to a learning healthcare system, but our ability to find them is limited by certain manual steps. The dominant approach to finding the patterns uses supervised learning, where a computational algorithm searches for patterns among input variables (or features) that model an outcome variable (or label). This usually requires an expert to specify the learning task, construct input features, and prepare the outcome labels. This workflow has served us well for decades, but the dependence on human effort prevents it from scaling and it misses the most informative patterns, which are almost by definition the ones that nobody anticipates. It is poorly suited to the emerging era of population-scale data, in which we can conceive of massive new undertakings such as surveiling for all emerging diseases, detecting all unanticipated medication effects, or inferring the complete clinical phenotype of all genetic variants.      The approach of unsupervised feature learning overcomes these limitations by identifying meaningful patterns in massive, unlabeled datasets with little or no human involvement. While there is a large literature on feature creation, a new surge of interest in unsupervised methods is being driven by the recent development of deep learning, in which a compact hierarchy of expressive features is learned from large unlabeled datasets. In the domains of image and speech recognition, deep learning has produced features that meet or exceed (by as much as 70%) the previous state of the art on difficult standardized tasks.      Unfortunately, the noisy, sparse, and irregular data typically found in an EMR is a poor substrate for deep learning. Our approach uses Gaussian process regression to convert such an irregular sequence of observations into a longitudinal probability density that is suitable for use with a deep architecture. With this approach, we can learn continuous unsupervised features that capture the longitudinal structure of sparse and irregular observations. In our preliminary results unsupervised features were as powerful (0.96 AUC) in an unanticipated classification task as gold-standard features engineered by an expert with full knowledge of the domain, the classification task, and the class labels.      In this project we will learn unsupervised features for records of all individuals in our deidentifed EMR image, for each of 100 laboratory tests and 200 medications of relevance to type 1 or type 2 diabetes. We will evaluate the features using three pattern recognition tasks that were unknown to the feature-learning algorithm: 1) an easy supervised classification task of distinguishing diabetics vs. nondiabetics, 2) a much more difficult task of distinguishing type 1 vs. type 2 diabetics, and 3) a genetic association task that considers the features as micro-phenotypes and measures their association with 29 different single nucleotide polymorphisms with known associations to type 1 or type 2 diabetes.             Every piece of data generated in the course of medical care can provide crucial feedback to a learning healthcare system, but only if relevant patterns can be found among them. The current practice of using human experts to guide the pattern search has served us well for decades, but it is poorly suited to the emerging era of population- scale data, in which we may conceive of massive new undertakings such as detecting all emerging diseases or all unanticipated medication effects. This project will develop methods to mathematically identify such patterns at a large scale, with no need for human judgment to specify what patterns to look for or where to look for them.",Scalable Biomedical Pattern Recognition Via Deep Learning,8566062,R21LM011664,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Architecture', 'Area', 'Biomedical Research', 'Caring', 'Classification', 'Clinical', 'Clinical Data', 'Computational algorithm', 'Computerized Medical Record', 'Couples', 'Data', 'Data Set', 'Data Sources', 'Dependence', 'Development', 'Diabetes Mellitus', 'Diagnosis', 'Disease', 'Engineering', 'Evaluation', 'Exhibits', 'Feedback', 'Goals', 'Gold', 'Healthcare Systems', 'Human', 'Image', 'Individual', 'Judgment', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Literature', 'Manuals', 'Measures', 'Medical', 'Metformin', 'Methods', 'Modeling', 'Myocardial Infarction', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Outcome', 'PTGS2 gene', 'Pattern', 'Pattern Recognition', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Population', 'Probability', 'Process', 'ROC Curve', 'Records', 'Research', 'Risk', 'Sampling', 'Sea', 'Serum', 'Single Nucleotide Polymorphism', 'Source', 'Specific qualifier value', 'Structure', 'Testing', 'Time', 'To specify', 'Uncertainty', 'Uric Acid', 'Work', 'cell growth', 'clinical care', 'clinical phenotype', 'density', 'diabetic', 'genetic association', 'genetic variant', 'inhibitor/antagonist', 'interest', 'meetings', 'neoplastic cell', 'non-diabetic', 'outcome forecast', 'prevent', 'speech recognition', 'success', 'type I and type II diabetes']",NLM,VANDERBILT UNIVERSITY,R21,2013,175500,-0.014973286811577842
"Development Of Advanced Computer Hardware And Software The LoBoS high performance computing system continues to be developed as a resource for scientists within the Laboratory of Computational Biology and their collaborators. As in previous years, improvements to the LoBoS system are driven primarily by continuous improvements in the price-performance ratio of common off the shelf (COTS) computer hardware. In FY-2013, 12 new nodes were procured; these will be added to the cluster in FY-2014. Furthermore, substantial improvements in power and cooling capacity of the Laboratory of Computational Biologys two machine rooms are being made to accommodate new equipment being procured by the groups of Dr. Jose Faraldo-Gomez and Dr. Lucy Forrest. These groups have procured 256 nodes to be dedicated to their computational needs. In addition, two new nodes with powerful general purpose graphics processing unit (GP-GPU) capabilities based on Nvidia's ""Kepler"" architecture were procured. As more molecular dynamics and simulation codes, including CHARMM, develop advanced features to take advantage of GP-GPU co-processors, the deployment of these nodes and continued presence of existing GPU hardware will allow lab staff to keep up to date with the latest computing developments. The new Kepler nodes will contain two GP-GPU coprocessors each, allowing for extensive testing and production runs using a multi-GPU setup.  Developments in the CHARMM molecular simulation package are also ongoing. In the last year, substantial developments have been made to the replica exchange code, most notably the development of a FAST key-word for replica exchange and new code combining Hamiltonian replica exchange with temperature and SGLD replica exchange. The fast replica exchange code substantially reduces the parallel communication needed for simple temperature based calculations (which are the ones most commonly used), leading to a noticeable speed-up, especially when repeated or frequent exchange attempts are made. Finally, a development summit was held in the lab in August, 2013, that led to a substantial amount of code clean-up.    Development is also active on the CHARMMing web interface to CHARMM. In FY-2013, a new version has been released with substantial front and back end improvements. The visualization system has been entirely revamped, with users now having the option of using JSMol or GLMol for structure visualization. Additionally, users may now graphically build custom ligands or import them from various databases. Automatic parameter generation tools using CGenFF and ParamChem developed by the MacKerell group have been fully integrated. Additionally, support for setting up multiscale modeling using MSCALE has been added to CHARMMing. This new functionality has been integrated with the new graphical atom selection developed for QM/MM. This allows for a seamless graphical setup of both additive and subtractive QM/MM calculation using CHARMMing.  We have developed a web-based tool for SAR and QSAR modeling to add to the services provided by charmming.org. It is an implementation of one of the most recent advances in modern machine learning algorithms  Random Forests. The tool allows a user to create his own models based on submitted training sd files which combine structures with activity information (either categorical or numerical), track the model generation process and run created models on the new data to predict activity. The whole process is presented in a straightforward, user-friendly manner with each step prompting the user for the next action so that even a first time visitor to the web service can feel confident on what stage of the process he or she is currently situated. The SAR/QSAR tool also automatically verifies new models by using well-known machine learning techniques such as cross-validation and y-randomization so users can immediately see whether the created model is able to calculate valid predictions. This is an important and often missed step in QSAR modeling. A user is presented with AUC measurements for the training set, for y-randomized set and an average AUC for 5-fold cross-validation for categorical modeling. A prediction score as well as active/inactive labels and the recommended threshold are the output of the prediction. The threshold is automatically recommended based on the balance between recall and precision. Recall and precision for the training set are also displayed for the user. For numerical modeling the process is similar except that R2 is used instead of AUC and there is no recall and precision of course. It is also easy to validate the model on an external validation set  if the tool finds activity score already present in the prediction file it will automatically compute precision and recall measures or R2. The new SAR/QSAR tool can be used as a stand-alone utility or as a supporting filter for the docking procedure.  Multipoles-Multipole interactions Current course grained modeling techniques of both proteins and lipids have difficulty accurately modeling electrostatic interactions of biomolecules. One source of error in CG models is that the partial charges present in all atom models are lost during the coarse graining process, leaving many electronically neutral CG beads behind. The resulting potentials are isotropic in nature, and have difficulty reproducing atomistic properties. The most dramatic example of this phenomenon comes from the widely used MARTINI CG water model, which often freezes at biological temperatures. To incorporate some of the electrostatic detail from all atom models, we proposed to augment the popular MARTINI model with dipole and quadrupole information. Towards this end we have performed an efficient, arbitrary order multipole implementation in CHARMM (work with Andrew Simmonett, VT) using a novel combination of the quasi-internal reference frame combined with a spherical harmonic expansion. We have also extended the particle mesh Ewald capabilities of CHARMM to allow for the calculation of multipole potentials and gradients in reciprocal space.  This will allow our multipole implementation to have actual real world use capabilitity. The resulting work will not only allow us to pursue our CG model, but it will allow other multipole based models and forcefields to be used by the greater CHARMM community. We have recently begun work on implementing the SSDQO water model (Toshiko Ichiye, Georgetown) into the CHARMM package.  The current SSDQO implementation is done through MSCALE, which is highly inefficient and does not parallelize well.  Our implementation will supersede this work.  Our implementation might also supersede the less general CHARMM package MTP (Markus Meuwly, University Basel).  Discussions have begun on how to incorporate their functionality into our more efficient and fully featured framework. In the coming year we will extend our fixed multipole code to account to also compute polarizable dipole contributions to the potential and gradients.  This added capability will allow the use of the highly accurate AMOEBA forcefield (Jay Ponder, Washington University) in CHARMM.  Furthermore, our implementation will be much faster than that in the TINKER software package, and slightly faster than the AMBER implementation.  Having the AMOEBA forcefield in CHARMM will give users a variety of options when choosing polarizable forcefields. n/a",Development Of Advanced Computer Hardware And Software,8746903,ZIHHL001052,"['Accounting', 'Algorithms', 'Architecture', 'Back', 'Biological', 'Boxing', 'Cereals', 'Charge', 'Code', 'Communication', 'Communities', 'Computational Biology', 'Computer Hardware', 'Computer Systems', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Databases', 'Development', 'Docking', 'Electrostatics', 'Equilibrium', 'Equipment', 'Error Sources', 'Evaluation', 'Freezing', 'Generations', 'High Performance Computing', 'Imagery', 'Label', 'Laboratories', 'Lead', 'Left', 'Ligands', 'Lipids', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Nature', 'Online Systems', 'Output', 'Performance', 'Personal Computers', 'Price', 'Procedures', 'Process', 'Production', 'Property', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Randomized', 'Resources', 'Running', 'Scientist', 'Services', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Training', 'United States National Aeronautics and Space Administration', 'Universities', 'Update', 'Validation', 'Washington', 'Water', 'Work', 'base', 'cluster computing', 'computerized tools', 'cost', 'cost effective', 'forest', 'improved', 'molecular dynamics', 'multi-scale modeling', 'novel', 'parallel computer', 'parallel computing', 'particle', 'scientific computing', 'simulation', 'software development', 'tool', 'user-friendly', 'web interface', 'web services']",NHLBI,"NATIONAL HEART, LUNG, AND BLOOD INSTITUTE",ZIH,2013,773178,-0.01282204293130211
"Division of Computational Bioscience Scientific Computing Facility The Division of Computational Bioscience Scientific (DCB) Computing Facility collaborates with DCB staff and their NIH investigators to build and provide a diverse but robust scientific computing system. This system provides the infrastructure needed by the collaborations at the same time as complying with all security, confidentiality and integrity requirements, especially those concerned with storing PII and other sensitive data. With common smooth software and hardware refreshes, collaborators are allowed to work uninterrupted. Expert staff are able to provide centralised support and advice to enable the best use of resources in support of the Division's Intramural Research Program.  The Facility comprises of low availability equipment rooms containing a wide range of hardware for scientific computing with approximately 200 continuously running servers, which provide compute nodes and data storage. There are separate racks for machines used for storing and processing sensitive information, such as personal health information and grant information, with appropriate extra layers of protection. DCB also rents space from the Division of Computer System Services (DCSS) of the Center for Information Technology (CIT) for high availability production systems, such as those supporting the Salivary Proteome Wiki.  A common and federated environment across all computational systems, enables DCB staff to quickly build the data analysis tools they need throughout the lifetime of a collaboration. Common file systems, unified authentication, system management, and other system coordinating features are fully integrated into an infrastructure containing the variety of supporting software. Commodity hardware, free software and commercial software are integrated to provide efficient and sustainable solutions.  Staff perform the many common security, maintenance, backup and reporting tasks that are required for the day to day operation of scientific computational systems on behalf of DCB. In particular, managing the Federally mandated Certification and Accreditation process on DCB's Computational Facility that would otherwise be a significant burden to DCB developers and investigators as well as their collaborators. When an collaborative project is concluded, staff work with the ICs to relocate systems to space within the ICs or the CIT Data Center as appropriate.  Many prominent investigators panels and science leaders have reported a lack of skilled professional computational experts to help with running similar resources across NIH. Laboratories tend to rely on the incidental expertise found among their, often temporary, staff. With the large amounts of data processing that is required for projects at NIH, the Facility provides the necessary expertise to DCB.  In 2013, supported collaborations include:          . The Human Salivary Protein Catalog, with NIDCR, supporting the Salivary Proteome community.          . Molecular Libraries Program (MLP), part of the NIH Common Fund, to develop the Common Assay Reporting System (CARS).          . Molecular Structure Determination (NIDDK, NIDCR, NCI, NHLBI).          . The Center for Molecular Modeling's MMIGNET programme supporting NIH as a whole.          . Microarray Database System (mAdb) with NCI and NIAID.          . The Genetic Association Database archive of human genetic association studies of complex diseases and disorders, in collaboration with NIA.          . Undiagnosed Diseases Program Portal (ORDR, NHGRI, CC) enabling experts from across the world help NIH to diagnose rare diseases.          . Portfolio analysis and portfolio visualization resources supporting many groups throughout NIH, including OD, NCI and NIGMS.          . A collaborative effort with NHLBI to design and execute a program which computes billions of SNP-gene expression association tests, in an effort to find expression - single nucleotide polymorphisms, eSNPs. Efforts so far are producing a 20-fold speed up over previous efforts.          . A joint project with NIDDK to apply random forest learning machines to identify and refine transcription start sites in the fruit fly genome based on data obtained from cap analysis gene expression (CAGE) data.          . A joint project, with NCI and a NCI-funded consortium, implementing an analysis pipeline and providing the necessary bioinformatics tools for transcriptome analysis and biological interpretation of RNA-seq/Exon capture data from next generation sequencing. n/a",Division of Computational Bioscience Scientific Computing Facility,8746902,ZIHCT000274,"['Accreditation', 'Architecture', 'Archives', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cataloging', 'Catalogs', 'Certification', 'Collaborations', 'Communities', 'Complex', 'Computer Systems', 'Computer software', 'Confidentiality', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Diagnosis', 'Disease', 'Drosophila genome', 'Environment', 'Equipment', 'Exons', 'Funding', 'Gene Expression', 'Gene Expression Profile', 'Gene Expression Profiling', 'Goals', 'Grant', 'Health', 'High Performance Computing', 'Human', 'Human Genetics', 'Imagery', 'Information Technology', 'Intramural Research Program', 'Joints', 'Laboratories', 'Machine Learning', 'Maintenance', 'Molecular Bank', 'Molecular Models', 'Molecular Structure', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'National Institute of General Medical Sciences', 'Process', 'Production', 'Proteome', 'Rare Diseases', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Salivary', 'Salivary Proteins', 'Science', 'Security', 'Services', 'Single Nucleotide Polymorphism', 'Software Tools', 'Solutions', 'Speed', 'System', 'Testing', 'Time', 'Transcription Initiation Site', 'United States National Institutes of Health', 'Work', 'base', 'cluster computing', 'computerized data processing', 'data sharing', 'design', 'forest', 'genetic association', 'member', 'molecular modeling', 'next generation sequencing', 'operation', 'programs', 'scientific computing', 'tool', 'transcriptome sequencing', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2013,468360,-0.0055439713482055945
"Advanced Methods In Statistical Genetics Statistical models for genetics data are often surprisingly challenging, and often require advanced and new statistical methods. Using probability machines on whole genome data is a recent invention, with the original research on probability machines appearing in Methods of Information in Medicine (September 2011). Our methods point to refined and personalized probability predictions using a wide range of biomarkers, medical information and whole genome data. The detection of childhood-onset schizophrenia using 800,000 snps using probability machines has error rates of 15% or less, and the list of predictive snps can be filtered down to a list of less than a few hundred. Other studies of psychiatric conditions (ADHD, bipolar) are also now underway using probability machines and personalized medicine, subject-specific predictions. n/a",Advanced Methods In Statistical Genetics,8746529,ZIACT000268,"['Accounting', 'Attention deficit hyperactivity disorder', 'Biological Markers', 'Biological Neural Networks', 'Books', 'Childhood', 'Classification', 'Clinical', 'Data', 'Data Set', 'Detection', 'Disease model', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Human', 'Human Genome', 'Logic', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Other Genetics', 'Outcome', 'Phenotype', 'Probability', 'Process', 'Research', 'Scheme', 'Schizophrenia', 'Signal Transduction', 'Statistical Methods', 'Statistical Models', 'Transcription Initiation Site', 'Universities', 'base', 'fly', 'forest', 'novel strategies', 'programs']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIA,2013,85800,-0.034888394858223155
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8426190,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'stress disorder', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,315672,-0.020183277735176704
"Integrated Public Use Microdata Series: North Atlantic Population Project    DESCRIPTION (provided by applicant): Research teams in the United States, Britain, Canada, Iceland, Norway, and Sweden have worked in close coordination to create the North Atlantic Population Project (NAPP), a massive integrated cross-national microdatabase that provides a baseline for studies of demographic change and opens fresh paths for spatiotemporal data analysis. We now propose improvements that will multiply the power of the NAPP infrastructure. We have three major aims: (1) Triple the size of the database to approximately 365 million records, adding 40 new datasets for the period 1787 to 1930 from Albania, Britain, Canada, Denmark, Egypt, Iceland, Ireland, Germany, Norway, Mexico, Sweden, and the United States. (2) Leverage our innovative record-linkage technology to create linked national panels that will allow expanded longitudinal analyses. (3) Connect the past to the present by merging NAPP with the Integrated Public Use Microdata Series (IPUMS), simplifying analysis of long-run change and ensuring long-run preservation and maintenance of the database. The landscape of scientific research on the human population is shifting. It is no longer sufficient just to study the relationships among variables at a particular moment in time. Researchers around the world now recognize that to understand the large-scale processes that are transforming society, we must investigate long-term change. The goal of this project is to provide the infrastructure to make such analysis possible. NAPP will make a strategic contribution to demographic infrastructure by providing a baseline for the study of changes in the demography and health of European and North American populations. In each country, NAPP provides the earliest census microdata available. Models and descriptions based on historical experience underlie both theories of past change and projections into the future. The NAPP data provide a unique laboratory for the study of economic and demographic processes; this kind of empirical foundation is essential for testing social and economic theory. The proposed work will be carried out by a team of highly-skilled researchers with unparalleled expertise and experience in data integration and record linkage. Collaborators include leading researchers from the University of Minnesota and the Max Planck Institute for Demographic Research, and local experts from each of the participating countries. Centralized support for international collaboration will leverage the investments of each country and allow us to create an extraordinary resource for comparative social and economic research.        The North Atlantic Population Project (NAPP) provides fundamental infrastructure for scientific research, education, and policy-making and will allow social scientists to make comparisons across Northern Europe, the Mediterranean, and North America during three centuries of transformative change. The proposed work is directly relevant to the central mission of the NIH as the steward of medical and behavioral research for the nation: the new data will advance fundamental knowledge about the nature and behavior of human population dynamics. NAPP will unlock access to some of the largest and longest-running cross-sectional and longitudinal data sources in the world, and stimulate health-related research on population growth and movement, fertility, mortality, nuptiality, and family change, as well as the economic and social correlates of demographic behavior.         ",Integrated Public Use Microdata Series: North Atlantic Population Project,8517775,R01HD052110,"['Albania', 'American', 'Behavior', 'Behavioral Research', 'Biological Preservation', 'Canada', 'Censuses', 'Code', 'Collaborations', 'Collection', 'Communication', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Demographic Transitions', 'Demography', 'Denmark', 'Documentation', 'Economic Development', 'Economics', 'Education', 'Egypt', 'England', 'Ensure', 'European', 'Family', 'Fertility', 'Foundations', 'Funding', 'Future', 'Germany', 'Goals', 'Health', 'Human', 'Iceland', 'Immigration', 'Individual', 'Institutes', 'Institution', 'International', 'Investigation', 'Investments', 'Ireland', 'Knowledge', 'Laboratory Study', 'Link', 'Machine Learning', 'Maintenance', 'Mediation', 'Medical Research', 'Metadata', 'Mexico', 'Minnesota', 'Mission', 'Modeling', 'Nature', 'North America', 'Northern Europe', 'Norway', 'Nuptiality', 'Online Systems', 'Policy Making', 'Population', 'Population Characteristics', 'Population Growth', 'Private Sector', 'Process', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Scientist', 'Scotland', 'Series', 'Social Mobility', 'Societies', 'Subgroup', 'Sum', 'Sweden', 'Technology', 'Testing', 'Time', 'Time Study', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'comparative', 'critical period', 'data integration', 'data mining', 'experience', 'human population dynamics', 'improved', 'innovation', 'mortality', 'population movement', 'social', 'spatiotemporal', 'success', 'theories', 'tool']",NICHD,UNIVERSITY OF MINNESOTA,R01,2013,574473,-0.03358946573101898
"The Cardiovascular Research Grid DESCRIPTION (provided by applicant):    The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinations of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotating ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and motion that can predict the early presence of developing heart disease in time for therapeutic intervention. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informatics system that allows clinical information to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8424997,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'High Performance Computing', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Therapeutic Intervention', 'Time', 'Ultrasonography', 'Work', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2013,2177404,-0.011198060072204042
"National Biomedical Information Services Delivering Biomedical Information Services  In FY13, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public.  Among the NLM's intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLM's premier bibliographic database with over 23 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY13, more than 700,000 new indexed citations were added.  Also in FY13, NLM expanded the use of automated indexing technology to increase efficiency. PubMed Central: The PubMed Central archive of over 2.8 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports.  In FY13, NLM introduced PubReader, an innovative interface designed to enhance readability and navigation of articles in PMC, particularly on tablet and other small screen devices. MedlinePlus and MedlinePlus en espanol: These consumer health information resources cover more than 900 topics, in more than 40 languages. MedlinePlus Connect, linking electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use, was expanded in FY13.  Clinical Trials: ClinicalTrials.gov covers more than 150,000 clinical research studies in nearly 180 countries, with hundreds added weekly. It also contains summary information about trial results, including adverse effects, in accordance with the FDA Amendments Act of 2007 (PL 110-85). In FY13, more than 18,500 new trials were registered. Summary results, including adverse event information, of more than 2000 trials were also added, bringing the total summary results to more than 10,000.  Toxicology and Environmental Health: Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, Toxic Release Inventory, Hazardous Substances Data Bank (HSDB), and LiverTox, produced with NIDDK.  In FY13, NLM continued  efforts to expand information about the effects of nanomaterials. Drug Information Resources: Drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for more than 55,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to NLM's RxNorm standard drug names. Disaster Preparedness and Response: NLM's Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A continuing collaboration with the Bethesda Hospital Emergency Preparedness Partnership (BHEPP) provides backup communication systems and tools for patient tracking, information access, and responder training.  DIMRC produces WISER, CHEMM, and REMM, emergency responder tools for hazardous materials (Hazmat) and chemical, biological, radiological, and nuclear (CBRN) incidents.   Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, dbGAP, Genomes, Genetic Testing Registry, the NCBI software toolkit, and MedGen released  in FY13. NCBI also provides access to PubMed, PubMed Central, and the Books database.  Continuing areas of emphasis in FY13 included addressing the impact of enormous quantities of data emanating from high throughput sequencing and microarrays; improving methodology for representing mammalian genome assemblies; organizing data from genome-wide association studies; and enhancing the interfaces to journal literature retrieval to facilitate search and discovery. NCBI also began pilot projects with CDC and FDA to apply whole genome sequencing for real-time monitoring and investigation of outbreaks of food borne illness. Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. NLM outreach programs enhance awareness of its information services, with emphasis on underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY13, dozens of community-based projects were funded.  Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 9000 health services research projects from more than 110 funding organizations; HSRR, a database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2020 objectives. Advanced Information Systems and Research Tools  In FY13, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; released a new set of high resolution pill-images for use in advanced imaging research; applied natural language processing methods to extract information from biomedical literature and to triage and answer certain types of inquiries from the general public; designed improved methods for integrated search and discovery across multiple databases; and provided expertise for the trans-NIH Big Data to Knowledge (BD2K) initiative to increase data sharing make research data more broadly available, useful, discoverable, and linked to the scientific literature. Health Data Standards: As the central coordinating body for clinical terminology standards within HHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure and provides essential tools for meaningful use of electronic health records (EHRs). NLM develops, supports, or licenses for free US-wide use the key clinical terminologies designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 11 million concept names from more than 160 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY13, In FY13, NLM released a new Value Set Authority Center that to provide ready access to vocabulary value sets for CMS-required clinical quality measures, expanded drug class information released with RxNorm, released a new US edition of SNOMED CT that combines the international n/a",National Biomedical Information Services,8746904,ZIHLM200888,"['Address', 'Administrator', 'Adoption', 'Adverse effects', 'Adverse event', 'African American', 'Amendment', 'Archives', 'Area', 'Automated Indexing', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biological', 'Biotechnology', 'Books', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Chemicals', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Devices', 'Disasters', 'Disease Outbreaks', 'Effectiveness', 'Electronic Health Record', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Family', 'Fostering', 'Funding', 'Genbank', 'General Population', 'Genetic screening method', 'Genome', 'Genomics', 'Goals', 'Guidelines', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'Image', 'Imagery', 'Improve Access', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Services', 'Information Specialists', 'Information Systems', 'International', 'Intervention', 'Intramural Research Program', 'Investigation', 'Journals', 'Knowledge', 'Label', 'Laboratories', 'Language', 'Libraries', 'Licensing', 'Link', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'MeSH Thesaurus', 'Measures', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methodology', 'Methods', 'Minority', 'Molecular Biology', 'Monitor', 'Names', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Native Americans', 'Natural Language Processing', 'Nomenclature', 'Nuclear', 'Package Insert', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Pilot Projects', 'Play', 'Poison Control Centers', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'PubMed', 'Public Health', 'Publishing', 'Readability', 'Readiness', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Software Tools', 'Solid', 'Speed', 'Structure', 'System', 'Tablets', 'Technology', 'Technology Assessment', 'Terminology', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Triage', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vocabulary', 'authority', 'base', 'biomedical informatics', 'biomedical information system', 'comparative effectiveness', 'data sharing', 'design', 'dosage', 'drug market', 'drug standard', 'foodborne illness', 'genome sequencing', 'genome wide association study', 'health disparity', 'health information technology', 'health literacy', 'improved', 'indexing', 'inner city', 'innovation', 'instrument', 'journal article', 'knowledge base', 'language processing', 'mammalian genome', 'metathesaurus', 'nanomaterials', 'outreach', 'outreach program', 'pill', 'research and development', 'research study', 'response', 'scientific computing', 'systems research', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2013,262406110,-0.015821983525543384
"National Biomedical Information Services Delivering Biomedical Information Services  In FY13, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public.  Among the NLM's intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLM's premier bibliographic database with over 23 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY13, more than 700,000 new indexed citations were added.  Also in FY13, NLM expanded the use of automated indexing technology to increase efficiency. PubMed Central: The PubMed Central archive of over 2.8 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports.  In FY13, NLM introduced PubReader, an innovative interface designed to enhance readability and navigation of articles in PMC, particularly on tablet and other small screen devices. MedlinePlus and MedlinePlus en espanol: These consumer health information resources cover more than 900 topics, in more than 40 languages. MedlinePlus Connect, linking electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use, was expanded in FY13.  Clinical Trials: ClinicalTrials.gov covers more than 150,000 clinical research studies in nearly 180 countries, with hundreds added weekly. It also contains summary information about trial results, including adverse effects, in accordance with the FDA Amendments Act of 2007 (PL 110-85). In FY13, more than 18,500 new trials were registered. Summary results, including adverse event information, of more than 2000 trials were also added, bringing the total summary results to more than 10,000.  Toxicology and Environmental Health: Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, Toxic Release Inventory, Hazardous Substances Data Bank (HSDB), and LiverTox, produced with NIDDK.  In FY13, NLM continued  efforts to expand information about the effects of nanomaterials. Drug Information Resources: Drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for more than 55,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to NLM's RxNorm standard drug names. Disaster Preparedness and Response: NLM's Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A continuing collaboration with the Bethesda Hospital Emergency Preparedness Partnership (BHEPP) provides backup communication systems and tools for patient tracking, information access, and responder training.  DIMRC produces WISER, CHEMM, and REMM, emergency responder tools for hazardous materials (Hazmat) and chemical, biological, radiological, and nuclear (CBRN) incidents.   Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, dbGAP, Genomes, Genetic Testing Registry, the NCBI software toolkit, and MedGen released  in FY13. NCBI also provides access to PubMed, PubMed Central, and the Books database.  Continuing areas of emphasis in FY13 included addressing the impact of enormous quantities of data emanating from high throughput sequencing and microarrays; improving methodology for representing mammalian genome assemblies; organizing data from genome-wide association studies; and enhancing the interfaces to journal literature retrieval to facilitate search and discovery. NCBI also began pilot projects with CDC and FDA to apply whole genome sequencing for real-time monitoring and investigation of outbreaks of food borne illness. Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. NLM outreach programs enhance awareness of its information services, with emphasis on underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY13, dozens of community-based projects were funded.  Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 9000 health services research projects from more than 110 funding organizations; HSRR, a database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2020 objectives. Advanced Information Systems and Research Tools  In FY13, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; released a new set of high resolution pill-images for use in advanced imaging research; applied natural language processing methods to extract information from biomedical literature and to triage and answer certain types of inquiries from the general public; designed improved methods for integrated search and discovery across multiple databases; and provided expertise for the trans-NIH Big Data to Knowledge (BD2K) initiative to increase data sharing make research data more broadly available, useful, discoverable, and linked to the scientific literature. Health Data Standards: As the central coordinating body for clinical terminology standards within HHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure and provides essential tools for meaningful use of electronic health records (EHRs). NLM develops, supports, or licenses for free US-wide use the key clinical terminologies designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 11 million concept names from more than 160 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY13, In FY13, NLM released a new Value Set Authority Center that to provide ready access to vocabulary value sets for CMS-required clinical quality measures, expanded drug class information released with RxNorm, released a new US edition of SNOMED CT that combines the international n/a",National Biomedical Information Services,8746904,ZIHLM200888,"['Address', 'Administrator', 'Adoption', 'Adverse effects', 'Adverse event', 'African American', 'Amendment', 'Archives', 'Area', 'Automated Indexing', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biological', 'Biotechnology', 'Books', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Chemicals', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Devices', 'Disasters', 'Disease Outbreaks', 'Effectiveness', 'Electronic Health Record', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Family', 'Fostering', 'Funding', 'Genbank', 'General Population', 'Genetic screening method', 'Genome', 'Genomics', 'Goals', 'Guidelines', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'Image', 'Imagery', 'Improve Access', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Services', 'Information Specialists', 'Information Systems', 'International', 'Intervention', 'Intramural Research Program', 'Investigation', 'Journals', 'Knowledge', 'Label', 'Laboratories', 'Language', 'Libraries', 'Licensing', 'Link', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'MeSH Thesaurus', 'Measures', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methodology', 'Methods', 'Minority', 'Molecular Biology', 'Monitor', 'Names', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Native Americans', 'Natural Language Processing', 'Nomenclature', 'Nuclear', 'Package Insert', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Pilot Projects', 'Play', 'Poison Control Centers', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'PubMed', 'Public Health', 'Publishing', 'Readability', 'Readiness', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Software Tools', 'Solid', 'Speed', 'Structure', 'System', 'Tablets', 'Technology', 'Technology Assessment', 'Terminology', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Triage', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vocabulary', 'authority', 'base', 'biomedical informatics', 'biomedical information system', 'comparative effectiveness', 'data sharing', 'design', 'dosage', 'drug market', 'drug standard', 'foodborne illness', 'genome sequencing', 'genome wide association study', 'health disparity', 'health information technology', 'health literacy', 'improved', 'indexing', 'inner city', 'innovation', 'instrument', 'journal article', 'knowledge base', 'language processing', 'mammalian genome', 'metathesaurus', 'nanomaterials', 'outreach', 'outreach program', 'pill', 'research and development', 'research study', 'response', 'scientific computing', 'systems research', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2013,1411835,-0.015821983525543384
"Advanced training platform and methodologies for emergency responders and skilled     DESCRIPTION (provided by applicant): There is a need to be able to deliver just-in-time training and reference materials for first responders and skilled support personnel. Mobile computing platforms are becoming ubiquitous and provide an ideal means to reach users at any time in any location. The process of translating existing reference materials into mobile-friendly formats is currently manual and very labor intensive. Nicolalde R&D LLC is well under way to commercialize its mTraining mobile technology and service prototyped during a phase I SBIR from NIEHS. The mTraining technology is an objective and checklist-based method for delivering just-in-time training and reference materials, making it an effective Electronic Performance Support System (EPSS) for providing workers easy access to information after training, and on site prior to or during an assignment. It provides short, incident specific awareness and safety training that can be delivered prior to responding to an emergency situation. The proposed development under this phase II SBIR includes: a) a back-end document processing engine that is able to automatically parse, analyze, mark-up, and organize documents so that their content is easily cross-referenced, linked and re-organized for effective delivery on a mobile training platform or other electronic medium. This will be connected to a server and database architecture to facilitate its operation and support storing and accessing content; b) the front-end interface for the mobile training platform (mTraining) was prototyped in Phase I of this project for delivering training content to emergency responders, skilled support personnel, and volunteers before or during an incident. The improved back-end architecture will support intelligent search capabilities for a large repository of training documents with different structures. This capability relies on the document processing engine's ability to semi-automatically extract relevant data and automatically translate this data into a structured format. This data can then be used for display in the mobile application, stored into databases, and automatically populated into ontologies. Throughout this project the participatory-based design paradigm has been used for facilitating the integration of user requirements and the fast prototyping and testing of design alternatives. This approach will continue to be utilized in Phase 2 of the project.         PUBLIC HEALTH RELEVANCE: The proposed research and development will advance the field of environmental health and safety training by bringing to it new and innovative advanced training technologies that are based on the mobile and just-in-time paradigm. Furthermore, the research and development proposed herein will advance the mobile information technology field by developing robust and scalable tools for processing and linking information residing in different source documents that are semantically related.            ",Advanced training platform and methodologies for emergency responders and skilled,8518023,R44ES020135,"['Access to Information', 'Architecture', 'Awareness', 'Back', 'Case Study', 'Data', 'Databases', 'Development', 'Educational Curriculum', 'Educational Materials', 'Electronics', 'Emergency Situation', 'Environmental Health', 'Focus Groups', 'Human Resources', 'Information Technology', 'Link', 'Location', 'Manuals', 'Medical', 'Medical Students', 'Methodology', 'Methods', 'National Institute of Environmental Health Sciences', 'Natural Language Processing', 'Ontology', 'Performance', 'Phase', 'Process', 'Provider', 'Research Infrastructure', 'Retrieval', 'Safety', 'Semantics', 'Services', 'Simulate', 'Site', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Structure', 'Support System', 'Technology', 'Testing', 'Time', 'Time Management', 'Training', 'Training and Education', 'Translating', 'Update', 'base', 'computer based Semantic Analysis', 'design', 'emergency service responder', 'improved', 'innovation', 'interest', 'operation', 'public health relevance', 'repository', 'research and development', 'response', 'tool', 'usability', 'volunteer']",NIEHS,"NICOLALDE R AND D, LLC",R44,2013,142612,-0.011827247675103306
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,-0.004100438388811052
"Continued Development and Evaluation of caTIES Abstract We propose to further develop, test, evaluate and support caTIES - an existing software system for developing networked repositories of sharable de-identified surgical pathology reports. The caTIES system creates a repository of de-identified, structured, and concept-coded clinical reports derived from large corpora of clinical free-text. Documents are automatically coded against a controlled terminology such as the Unified Medical Language System (UMLS), SNOMED-CT, or NCI Metathesaurus. Users construct queries to identify specific kinds of documents and tissue specimens based on the associated clinical report. For example, a researcher studying genetic variation in metastatic breast cancers can identify cases of invasive ductal carcinoma of the breast, followed by metastatic ductal cancer in bone at an interval of three years or greater from the original diagnosis. The caTIES system also supports acquisition and ordering of tissues, using an honest broker model. Through this mechanism, de-identified data and access to tissue can be shared among institutions, enabling multi-center collaborative research. The caTIES system has already been implemented at seven US Cancer Centers, and is being considered for adoption by numerous other institutions including cancer centers, university hospitals and private hospitals. Initial development of caTIES was funded by the Cancer Biomedical Informatics Grid (caBIG). However, interest in the application has far exceeded our expectations and the limitations of caBIG. This grant will allow us to further extend the capabilities of the system by (a) improving the portability of the system and extending the types of documents that can be processed, (b) evaluating the system's NLP performance and usability, (c) building a user community to support this open-source application, and (d) piloting interoperability of caTIES with other enterprise and research systems. This work will preserve and extend a highly novel platform for development of massive repositories of de-identified clinical data that can be used for research within and across institutions. Narrative This grant will fund the further development and evaluation of a system that takes identified clinical documents and converts them into de-identified, concept-coded, structured data. The system enables researchers to access remainder tissues and clinical report data for research purposes within and across institution. This project is important because it will greatly increase the access of researchers to important data and materials while maintaining patient privacy. n/a",Continued Development and Evaluation of caTIES,8403841,R01CA132672,"['Access to Information', 'Adoption', 'Cancer Center', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Communication', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Data Reporting', 'Database Management Systems', 'Development', 'Diagnosis', 'Documentation', 'Ductal', 'Electronic Mail', 'Eligibility Determination', 'Environment', 'Evaluation', 'Funding', 'Future', 'Genetic Variation', 'Genomics', 'Grant', 'Information Retrieval', 'Institution', 'Malignant Neoplasms', 'Methods', 'Metric', 'Modeling', 'Modification', 'Natural Language Processing', 'Operating System', 'Pathology Report', 'Performance', 'Private Hospitals', 'Process', 'Report (document)', 'Reporting', 'Research', 'Research Personnel', 'Services', 'Specimen', 'Structure', 'Surgical Pathology', 'System', 'Systematized Nomenclature of Medicine', 'Terminology', 'Testing', 'Text', 'Tissues', 'Training', 'Translational Research', 'Unified Medical Language System', 'University Hospitals', 'Vocabulary', 'Work', 'abstracting', 'base', 'bone', 'cancer Biomedical Informatics Grid', 'clinical phenotype', 'computer human interaction', 'ductal breast carcinoma', 'expectation', 'flexibility', 'improved', 'interest', 'interoperability', 'malignant breast neoplasm', 'meetings', 'metathesaurus', 'novel', 'open source', 'patient privacy', 'portability', 'repository', 'software development', 'software systems', 'systems research', 'tool', 'usability']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,286637,-0.04836041619038679
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8545224,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,4024095,0.007897458217528647
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known  as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network  (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5  years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases  Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and  procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on  32 current studies, contingent upon the successful re-competition of their associated clinical research  consortia, addition of new studies reflecting the growth of the network, accommodation of federated  databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and  registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per  year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical  trials. We will continue development of new technologies to support scalability and generalizability and tools  for cross-disease data mining. Our international clinical information network is secure providing coordinated  data management services for collection, storage and analysis of diverse data types from multiple diseases  and geographically disparate locations and a portal for the general public and larger community of clinical  investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8734648,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,304154,0.007897458217528647
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known  as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network  (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5  years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases  Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and  procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on  32 current studies, contingent upon the successful re-competition of their associated clinical research  consortia, addition of new studies reflecting the growth of the network, accommodation of federated  databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and  registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per  year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical  trials. We will continue development of new technologies to support scalability and generalizability and tools  for cross-disease data mining. Our international clinical information network is secure providing coordinated  data management services for collection, storage and analysis of diverse data types from multiple diseases  and geographically disparate locations and a portal for the general public and larger community of clinical  investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8734648,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,100000,0.007897458217528647
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in three subprojects which use natural language processing techniques:  1) We have developed a machine learning algorithm for abbreviation definition identification in text which makes use of what we term naturally labeled data. Positive training examples are naturally occurring potential abbreviation-definition pairs in text. Negative training examples are generated by randomly mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Medstract corpora. Our system demonstrated results that compare favourably to the existing Ab3P and BIOADI systems. We achieve an F-measure of 91.36% on Ab3P corpus, and an F-measure of 87.13% on BIOADI corpus which are superior to the results reported by Ab3P and BIOADI systems. Moreover, we outperform these systems in terms of recall, which is one of our goals. 2) We are studying paraphrases in MEDLINE abstracts. These come about because an author is describing some entity of interest and uses a phrase like ""drug abuse"" and then needing to describe the same entity again a sentence or two latter does not wish to use exactly the same wording again and may use a variant of the phrase such as ""drug use"" which in the context of ""drug abuse"" has substantially the same meaning.  3) An author disambiguation algorithm has been developed which relies on machine learning based on the assumption that if an author name is infrequent in the data it probably represents the same person in for all documents where it is found. This gives us positive instances. Negative instances are sampled from pairs of documents that have no author in common. Such positive and negative data allows us to do machine learning on all aspects of the document other than the name in question. This allows us to learn how to weight this data for best performance in distinguishing the positive and negative instances from each other. This learning is then applied in individual name cases or spaces to determine which author document pairs represent the same author. n/a",Natural Language Processing Techniques To Enhance Information Access.,8558106,ZIALM000090,"['Abbreviations', 'Algorithms', 'Automated Abstracting', 'Data', 'Dependency', 'Drug abuse', 'Drug usage', 'Goals', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'MEDLINE', 'Machine Learning', 'Measures', 'Names', 'Natural Language Processing', 'Performance', 'Persons', 'Reporting', 'Sampling', 'System', 'Techniques', 'Text', 'Training', 'Variant', 'Weight', 'abstracting', 'base', 'indexing', 'interest', 'phrases', 'spelling', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2012,477226,-0.028359116112442576
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8471822,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2012,224100,-0.024457809495793837
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method we are currently studying is to perform machine learning with an SVM or other classifier and score the documents based on this learning. Then PAV can be applied to the resulting scores and this score function can be descretized without the loss of significant information. This allows us to make use of the results as features which can be individually weighted in another classifier. 4) We have developed a new algorithm called the periodic random orbiter algorithm (PROBE) which is applicable to minimize any convex loss function. We have applied it to the MeSH classification problem and it seems to work very well and better than the alternatives on such a large problem. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,8558105,ZIALM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Classification', 'Data Set', 'Exclusion', 'Learning', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Procedures', 'Retrieval', 'Testing', 'Weight', 'Work', 'base', 'design', 'improved', 'loss of function', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2012,563995,0.005912716259684188
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.   1)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 2)	The PubMed database presents a unique challenge because of its very large size of over 19 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  3) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with theresults of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. 4) We are currently working on a problem which arises when several different kinds of documents appear in a dataset and one wants to compute neighboring documents for each document. A simple application of the same approach used to find related citations in PubMed does not produce good results. Analysis of the problem shows that there are many records with words in them that are not keyed to the actual focus of the record and that these words mislead the neighboring process. In some cases this is due to a common author of records who users certain word forms frequently in their writing even on very different subjects. In other cases the problem seems to appear when two different drugs have sections on side effects that are quite generic and have a large overlap, etc. In order to deal with this problem we tried several approaches to filter out useless words. First, we compared the frequency of words in the English Gigaword Corpus with their frequency PubMed documents in an effort to remove non-biomedical terminology. In the same way we also compared the words in a large set of PubMed user queries to see what terminology may be important to users. By filtering out words that are neither very medical nor very important to users we saw some benefit in computing neighbors. A second approach involved Bayesian calculations to determine which words seemed to be specific to a particular source of PubMed Health documents. This also allowed us to filter terms coming from boiler plate specific to how documents were created in different sources. But the above approaches required a certain amount of hand supervision to avoid mistakes in filtering words. We have since found improved results with a completely automatic approach which examines how related each word in the body of record is to words in the records title. This is achieved by removing all words related below a certain low threshold. n/a",Automatic Bayesian Methods In Text Retrieval,8558095,ZIALM000021,"['Accounting', 'Active Learning', 'Adverse effects', 'Bayesian Method', 'Classification', 'Data', 'Data Set', 'Databases', 'Exhibits', 'Frequencies', 'Generic Drugs', 'Genomics', 'Goals', 'Hand', 'Health', 'Label', 'Machine Learning', 'MeSH Thesaurus', 'Medical', 'Methods', 'Modeling', 'Performance', 'Peripheral', 'Pharmaceutical Preparations', 'Poisson Distribution', 'Probability', 'Process', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'Source', 'Supervision', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'Writing', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2012,86768,-0.002884218124551328
"A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology    DESCRIPTION (provided by applicant): The non-medical use of pharmaceutical opioids has been identified as one of the fastest growing forms of drug abuse in the U.S. There is a critical need to enhance current epidemiological monitoring, early warning, and post-marketing surveillance systems by providing additional and more timely data. The World Wide Web has been identified as one of the ""leading edge"" data sources for detecting patterns and changes in drug use practices. Many websites provide a venue for individuals to freely share their own experiences, post questions, and offer comments about different drugs. Such User Generated Content (UGC) can be used as a very rich data source to study knowledge, attitudes, and behaviors related to illicit drugs. To harness the full potential of the Web for drug abuse research, the field needs to develop a highly automated way of accessing, extracting, and analyzing Web-based data related to illicit drug use. This exploratory R21 is a multi-principal investigator, collaborative effort between researchers at the Center for Interventions, Treatment and Addictions Research (CITAR) and the Center for Knowledge-Enabled Information Services and Science (Kno.e.sis) at Wright State University. The purpose of this Web-based study is to apply cutting-edge information processing techniques, such as the Semantic Web, Natural Language Processing, and Machine Learning, to qualitative and quantitative content analysis of user generated content to achieve the following aims: 1) Describe drug users' knowledge, attitudes, and behaviors related to the illicit use of Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine); 2) Identify and describe temporal patterns of the illicit use of these drugs as reflected on web-based forums. To collect data, the study will use websites that allow for the free discussion of illicit drugs, contain information on illicit prescription drug use, and are accessible for public viewing. The study will generate new information about the practices of buprenorphine abuse and will contribute to the advancement of public health and substance abuse research by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data. Automated information extraction methods applied in this study will enhance current early warning and epidemiological surveillance systems and could advance qualitative and Web-based research methods in other areas of public health.        Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.            ",A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology,8269958,R21DA030571,"['Accident and Emergency department', 'Archives', 'Area', 'Behavior', 'Buprenorphine', 'Code', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Data', 'Data Sources', 'Drug Rehabilitation Centers', 'Drug abuse', 'Drug usage', 'Drug user', 'Early identification', 'Epidemiologic Monitoring', 'Epidemiologic Studies', 'Epidemiology', 'Face', 'Health', 'Health Knowledge, Attitudes, Practice', 'Health Professional', 'Heroin Dependence', 'Hospitals', 'Illicit Drugs', 'Individual', 'Information Sciences', 'Information Services', 'Information Systems', 'Internet', 'Intervention', 'Intervention Studies', 'Interview', 'Knowledge', 'Label', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Monitor', 'NIH Program Announcements', 'Naloxone', 'Natural Language Processing', 'Online Systems', 'Opioid', 'Overdose', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Poison Control Centers', 'Policies', 'Population', 'Prevention', 'Principal Investigator', 'Process', 'Public Health', 'Published Comment', 'Qualitative Research', 'Reading', 'Reliance', 'Research', 'Research Methodology', 'Research Personnel', 'Sampling', 'Selection Bias', 'Self Disclosure', 'Semantics', 'Source', 'Substance abuse problem', 'Subutex', 'Surveillance Methods', 'Surveys', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'addiction', 'buprenorphine abuse', 'computer based Semantic Analysis', 'design', 'emotional disclosure', 'empowered', 'experience', 'informant', 'information processing', 'misuse of prescription only drugs', 'population survey', 'post-market', 'prescription drug abuse', 'programs', 'response', 'social', 'tool', 'trend', 'web site', 'working group']",NIDA,WRIGHT STATE UNIVERSITY,R21,2012,182500,-0.03320990338915348
"Text Analytics, Knowledge Engineering, & High Performance Computing The Text Analytics, Knowledge Engineering, and High Performance Computing Program, which operates within the High Performance Computing and Informatics Office (HPCIO), Division of Computational Bioscience of CIT, is collaborating with NIH investigators to build a critical mass in text and numerical analytics that is envisioned to encompass a number of pertinent and related disciplines in biomedical research including semantic interoperability, knowledge engineering, computational linguistics, text and data mining, natural language processing, machine learning, and visualization.  The program is intended to foster advances in critical domains at NIH including biomedical and clinical informatics, translational research, genomics, proteomics, systems biology, ""big data"" analysis, and portfolio analysis.  In 2012, collaborative efforts in support of these goals included the following.  - The human salivary protein catalog has been made available online on a community-based Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. This is a major step towards the discovery and use of saliva biomarkers to diagnose oral and systemic diseases.   - In collaboration with NCI, HPCIO is investigating document classifiers trained using machine-learning methods.  One aspect of this collaboration involves the development of a system to match ClinicalTrials.gov protocols with their funding source in IMPAC II.  The need for such a system is motivated by the fact that the NIH project number is specified in only 20% to 25% of all NIH-sponsored protocols.   An intended outcome of this matching system is improved classifier performance by augmenting grant document text with matching protocol text.    - In response to input from various collaborative groups, HPCIO is developing a portfolio visualization resource, dubbed PViz, that integrates visualization of categorical data with results of clustering algorithms, to allow analysts to gain new insight into their data.  Users may either construct a portfolio from IMPAC II data or import their own custom portfolio of categorical data.    - In collaboration with the Division of Planning, Coordination, and Strategic Initiatives (DPCPSI/OD), we have trained a ""one-sided"" classifier on a set of Comparative Effectiveness Research (CER) exemplars.  The results of this investigation suggest that, when coupled with an effective annotation strategy, such a classifier can be quite effective at retrospectively identifying CER grants.  - HPCIO has demonstrated the utility of its integrated portfolio clustering and visualization resource on NIAID's Anti-Microbial Resistance portfolio.  The current focus of the collaboration with NIAID is to investigate various machine-learning methods (including unsupervised, semi-supervised, and fully supervised algorithms) to map projects to NIAID HIV/AIDS priorities, objectives, and initiatives.  - HPCIO has been collaborating with the Molecular Libraries Program (MLP), part of the NIH Common Fund, to develop the Common Assay Reporting System (CARS). CARS is an integrated system for managing bioassay information and facilitating communication bettween all the high-throughput screening centers within the Molecular Libraries Probe Production Centers Network (MLPCN). Goals for this collaboration include: 1) Track project status and related issues at eaach of the screening centers within the MLPCN, and provide the means for information collection, sharing and retrieval among the centers and the program office at NIH. 2) Establish a standardized protocol to describe raw data from the experiments and report screening data to the scientific community.  - A novel statistical test has been developed to identify differential expressed RNA from RNAseq count data.  This work will provide a better idea of the biological differences between cell types.  - HPCIO is working with Melissa Friesen of NCI to develop methodologies to improve exposure classification in occupational epidemiologic studies. Initial effort of this collaboration involves a tool that helps experts to classify free-text job descriptions into standard occupational codes. Machine-learning based classification methods will also be utilized to help with evaluating exposure-disease associations.   - In collaboration with NINDS, HPCIO has implemented and compared several methods to locate and characterize lysosomes in 3-D fluorescence images.  The goal is to be able to calculate the pH of each lysosome in the image, for which the ability to resolve their locations is an important step.  - Machine-learning methods have been devised and implemented to identify and refine transcription start sites in the fruit fly genome found using cap analysis gene expression (CAGE).  This effort is in collaboration with Brian Oliver of NIDDK.  - We are applying machine-learning methods to identify important terms that peer reviewers use to describe innovative applications.  The goal of the effort is to develop a lexicon of terms that can help estimate the innovation level of a grant application based on peer review critiques from the applications NIH Summary Statement.  - HPCIO is working with NINDS and the Office of Extramural Research (OER) to determine peer-review sentiment of grant applications based on the NIH Summary Statement.  The sentiment analysis results can provide decision support information to NIH program directors considering applications for selective pay.  - In collaboration with NIA, we are applying machine learning and visualization techniques on mass biological datasets to discover novel patterns of functional gene or protein interactions as related to aging. Omnimorph, a graphic data analysis tool, is being developed for multidimensional data visualization.   - Although the scientific impact of NCI consortia on the advancement of cancer epidemiology research is understood to be significant, accurate quantitative metrics of this impact are needed by program leadership. We are developing methods to track citations to clinical guidelines in the context of evidence-based medicine that could provide funding agencies and program directors insight into individual consortias contributions in advancing medical knowledge. This work is being conducted in collaboration with Epidemiology and Genomics Research Program (EGRP), NCI.  - In collaboration with George Chacko of CSR, HPCIO is applying text analytics to provide CSR leadership with evidence-based decision support in evaluation of the grant review process.   The effort so far has concentrated on exploratory analysis against the NIH portfolio to evaluate clustering methods and assess intrinsic measures of cluster quality.  - Based on its experience in building novel models for classifying research grants and projects, HPCIO is collaborating with DPCPSI/OD and NCI to develop a comprehensive classification workflow system that will allow users to select from multiple classification algorithms, feature spaces, and training regimes, to build and run their own classifiers.  - The Office of Behavioral and Social Sciences Research (OBSSR) is conducting a pilot investigation in collaboration with HPCIO to evaluate the efficacy of machine learning models for the classification of five BSSR-relevant research categories.    - NIA and the Alzheimer's Association have developed a Common Alzheimer's Disease Research Ontology (CADRO) to categorize Alaheimer's Disease Research.  HPCIO is in collaboration with NIA to develop classifiers for the six categories, 45 topics, and 145 themes. n/a","Text Analytics, Knowledge Engineering, & High Performance Computing",8565613,ZIHCT000200,"['3-Dimensional', 'AIDS/HIV problem', 'Address', 'Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Antimicrobial Resistance', 'Applications Grants', 'Architecture', 'Biological', 'Biological Assay', 'Biological Markers', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Categories', 'Classification', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Complex', 'Computer Analysis', 'Computing Methodologies', 'Coupled', 'Critiques', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Discipline', 'Disease', 'Disease Association', 'Drosophila genome', 'Engineering', 'Epidemiologic Studies', 'Epidemiology', 'Evaluation', 'Evidence Based Medicine', 'Extramural Activities', 'Fostering', 'Funding', 'Funding Agency', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Grant', 'Grant Review Process', 'Guidelines', 'High Performance Computing', 'Human', 'Image', 'Imagery', 'Individual', 'Informatics', 'Information Resources', 'Investigation', 'Job Description', 'Knowledge', 'Leadership', 'Linguistics', 'Location', 'Lysosomes', 'Machine Learning', 'Management Information Systems', 'Maps', 'Measures', 'Medical', 'Melissa', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Molecular Bank', 'Mouth Diseases', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Occupational', 'Online Systems', 'Ontology', 'Outcome', 'Pattern', 'Peer Review', 'Performance', 'Production', 'Proteins', 'Proteomics', 'Protocols documentation', 'RNA', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Sharing', 'Resources', 'Retrieval', 'Running', 'Saliva', 'Salivary Proteins', 'Science', 'Scientist', 'Screening procedure', 'Semantics', 'Side', 'Software Engineering', 'Software Tools', 'Specific qualifier value', 'System', 'Systemic disease', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Transcription Initiation Site', 'Translational Research', 'United States National Institutes of Health', 'Work', 'base', 'behavioral/social science', 'biological systems', 'biomedical informatics', 'cancer epidemiology', 'cell type', 'cluster computing', 'comparative effectiveness', 'data management', 'data mining', 'data sharing', 'effectiveness research', 'evidence base', 'experience', 'fluorescence imaging', 'high throughput screening', 'improved', 'information organization', 'innovation', 'insight', 'interoperability', 'knowledge base', 'novel', 'peer', 'programs', 'research study', 'response', 'social', 'social science research', 'text searching', 'tool']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2012,2726852,-0.024283759102490927
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) Code has been multithreaded and memory mapping capabilities added to speed up processing. 6) Most recently the code has been updated to work in a 64 bit environment.   The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system is currently proving useful in testing different retrieval parameters and methods on the PubMedHealth records.  We have recently developed a software system called DStor that allows us to store all of PubMed in a manner which is easily updateable and allows fast access. This system is now being used to maintain and update five different versions of the PubMed data twice a week. This system has greatly improved our access to PubMed data in various useful forms and we anctipate that its use will continue to grow. n/a",A Document Processing System,8558096,ZIALM000022,"['Code', 'Data', 'Databases', 'Environment', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'Memory', 'Methods', 'Online Systems', 'Process', 'Programming Languages', 'PubMed', 'Records', 'Research', 'Retrieval', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Update', 'Work', 'base', 'computerized data processing', 'improved', 'repository', 'software systems']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2012,86768,-0.002983038722904856
"Automatic Analysis and Annotation of Document Keywords in Biomedical Literature 1) Electronic Textbook and PubMed Central Indexing Current processing of the electronic textbook material involves a number of steps designed to produce the most meaningful phrases in the text to be used as reference points. The first task is to identify grammatically reasonable phrases. We use a version of the Brill transformation based tagger, rewritten in C++, for part-of- speech tagging. This forms the basis for determining grammatically reasonable phrases. There is a significant post processing step that removes phrases that involve inappropriate references to context (e.g., different cells, final mutation). After finding grammatically reasonable phrases we attempt to eliminate those that are too common or generic to be useful (e.g., significant result, short time).  The next step is to compare a phrase with previously rated phrases that have been collected over the life of the project. The final stage is to estimate the importance of a phrase in the passage where it is found in a textbook. Such an estimate is based on the frequency of the phrase and the size of the passage compared with the frequency of the phrase throughout the book and the overall size of the book. In order to improve such an estimate we attempt to take account of the phrase or any phrase that represents the same concept. For this purpose we use the UMLS Metathesaurus and also stemming and combine these two approaches into a consistent picture of the concept as it occurs in the text.  The result of this processing is a scored list of phrase-book section pairs for each textbook. These are used to guide the response of general searching in the books. When a user types in a phrase that is on our curated list the first results given are the highly rated book sections for that phrase. We are now applying a similar indexing scheme to the text of articles in PMCentral. This allows us to give a list of highly rated phrases for each article as an enhanced reference point for searchers.    2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text.   3) Currently we are studying how good phrases can be recognized by their characteristics, such as frequency, tendency to be repeated in documents where they occur, and other numerical properties. These features allow one to predict which phrases are of high quality. We have found such predictions to be useful in studying different kinds of terms that may appear in text and how an ontoloogy might be extracted from text. n/a",Automatic Analysis and Annotation of Document Keywords in Biomedical Literature,8558117,ZIALM091711,"['Accounting', 'Address', 'Books', 'Cells', 'Characteristics', 'Collection', 'Data', 'Electronics', 'Evaluation', 'Frequencies', 'Generic Drugs', 'Goals', 'Life', 'Literature', 'Machine Learning', 'Methods', 'Mutation', 'Outcome', 'Process', 'Property', 'PubMed', 'Records', 'Retrieval', 'Scheme', 'Speech', 'Staging', 'Techniques', 'Text', 'Textbooks', 'Time', 'Training', 'UMLS Metathesaurus', 'base', 'design', 'improved', 'indexing', 'insight', 'phrases', 'response', 'stem']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2012,260305,-0.02914694108198118
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8269876,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2012,323305,-0.033294097746835755
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.        PUBLIC HEALTH RELEVANCE: RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                  RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8268588,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2012,1300000,-0.0021171874700054035
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8333324,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2012,545129,-0.015557364748619115
"Division of Computational Bioscience Scientific Computing Facility The Division of Computational Bioscience Scientific (DCB) Computing Facility collaborates with DCB staff and their NIH investigators to build and provide a diverse but robust scientific computing system. This system provides the infrastructure needed by the collaborations at the same time as complying with all security, confidentiality and integrity requirements, especially those concerned with storing PII and other sensitive data. With common smooth software and hardware refreshes, collaborators are allowed to work uninterrupted. Expert staff are able to provide centralised support and advice to enable the best use of resources in support of the Division's Intramural Research Program.  The Facility comprises of low availability equipment rooms containing a wide range of hardware for scientific computing with approximately 200 continuously running servers, which provide compute nodes and data storage. There are separate racks for machines used for storing and processing sensitive information, such as personal health information and grant information, with appropriate extra layers of protection. DCB also rents space from the Division of Computer System Services (DCSS) of the Center for Information Technology (CIT) for high availability production systems, such as those supporting the Salivary Proteome Wiki.  A common and federated environment across all computational systems, enables DCB staff to quickly build the data analysis tools they need throughout the lifetime of a collaboration. Common file systems, unified authentication, system management, and other system coordinating features are fully integrated into an infrastructure containing the variety of supporting software. Commodity hardware, free software and commercial software are integrated to provide efficient and sustainable solutions.  Staff perform the many common security, maintenance, backup and reporting tasks that are required for the day to day operation of scientific computational systems on behalf of DCB. In particular, managing the Federally mandated Certification and Accreditation process on DCB's Computational Facility that would otherwise be a significant burden to DCB developers and investigators as well as their collaborators. When an collaborative project is concluded, staff work with the ICs to relocate systems to space within the ICs or the CIT Data Center as appropriate.  Many prominent investigators panels and science leaders have reported a lack of skilled professional computational experts to help with running similar resources across NIH. Laboratories tend to rely on the incidental expertise found among their, often temporary, staff. With the large amounts of data processing that is required for projects at NIH, the Facility provides the necessary expertise to DCB.  In 2012, supported collaborations include:          . The Human Salivary Protein Catalog, with NIDCR, supporting the Salivary Proteome community.          . Molecular Libraries Program (MLP), part of the NIH Common Fund, to develop the Common Assay Reporting System (CARS).          . Molecular Structure Determination (NIDDK, NIDCR, NCI, NHLBI).          . The Center for Molecular Modeling's MMIGNET programme supporting NIH as a whole.          . Microarray Database System (mAdb) with NCI and NIAID.          . The Genetic Association Database archive of human genetic association studies of complex diseases and disorders, in collaboration with NIA.          . Undiagnosed Diseases Program Portal (ORDR, NHGRI, CC) enabling experts from across the world help NIH to diagnose rare diseases.          . Portfolio analysis and portfolio visualization resources supporting many groups throughout NIH, including OD, NCI and NIGMS.          . A collaborative effort with NHLBI to design and execute a program which computes billions of SNP-gene expression association tests, in an effort to find expression - single nucleotide polymorphisms, eSNPs. Efforts so far are producing a 20-fold speed up over previous efforts.          . A joint project with NIDDK to apply random forest learning machines to identify and refine transcription start sites in the fruit fly genome based on data obtained from cap analysis gene expression (CAGE) data.          . A joint project, with NCI and a NCI-funded consortium, implementing an analysis pipeline and providing the necessary bioinformatics tools for transcriptome analysis and biological interpretation of RNA-seq/Exon capture data from next generation sequencing. n/a",Division of Computational Bioscience Scientific Computing Facility,8565615,ZIHCT000274,"['Accreditation', 'Architecture', 'Archives', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cataloging', 'Catalogs', 'Certification', 'Collaborations', 'Communities', 'Complex', 'Computer Systems', 'Computer software', 'Confidentiality', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Databases', 'Diagnosis', 'Disease', 'Drosophila genome', 'Environment', 'Equipment', 'Exons', 'Funding', 'Gene Expression', 'Gene Expression Profile', 'Goals', 'Grant', 'Health', 'High Performance Computing', 'Human', 'Human Genetics', 'Imagery', 'Information Technology', 'Intramural Research Program', 'Joints', 'Laboratories', 'Machine Learning', 'Maintenance', 'Molecular Bank', 'Molecular Models', 'Molecular Structure', 'National Heart, Lung, and Blood Institute', 'National Human Genome Research Institute', 'National Institute of Allergy and Infectious Disease', 'National Institute of Dental and Craniofacial Research', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'National Institute of General Medical Sciences', 'Process', 'Production', 'Proteome', 'RNA', 'Rare Diseases', 'Reporting', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Salivary', 'Salivary Proteins', 'Science', 'Security', 'Services', 'Single Nucleotide Polymorphism', 'Software Tools', 'Solutions', 'Speed', 'System', 'Testing', 'Time', 'Transcription Initiation Site', 'United States National Institutes of Health', 'Work', 'base', 'cluster computing', 'computerized data processing', 'data sharing', 'design', 'forest', 'genetic association', 'member', 'molecular modeling', 'next generation', 'operation', 'programs', 'scientific computing', 'tool', 'wiki']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2012,495791,-0.0055439713482055945
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8333306,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,592423,-0.058476599998070845
"Advanced Methods In Statistical Genetics Statistical models for genetics data are often surprisingly challenging, and often require advanced and new statistical methods. Using probability machines on whole genome data is a recent invention, with the original research on probability machines appearing in Methods of Information in Medicine (September 2011). Our methods point to refined and personalized probability predictions using a wide range of biomarkers, medical information and whole genome data. The detection of childhood-onset schizophrenia using 800,000 snps using probability machines has error rates of 15% or less, and the list of predictive snps can be filtered down to a list of less than a few hundred. Other studies of psychiatric conditions (ADHD, bipolar) are also now underway using probability machines and personalized medicine, subject-specific predictions. n/a",Advanced Methods In Statistical Genetics,8565487,ZIACT000268,"['Accounting', 'Attention deficit hyperactivity disorder', 'Biological Markers', 'Biological Neural Networks', 'Books', 'Childhood', 'Classification', 'Clinical', 'Data', 'Data Set', 'Detection', 'Disease model', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Human', 'Human Genome', 'Logic', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Other Genetics', 'Outcome', 'Phenotype', 'Probability', 'Process', 'Research', 'Scheme', 'Schizophrenia', 'Signal Transduction', 'Statistical Methods', 'Statistical Models', 'Transcription Initiation Site', 'Universities', 'base', 'fly', 'forest', 'novel strategies', 'programs']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIA,2012,11052,-0.034888394858223155
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8238371,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,326844,-0.020183277735176704
"Integrated Public Use Microdata Series: North Atlantic Population Project    DESCRIPTION (provided by applicant): Research teams in the United States, Britain, Canada, Iceland, Norway, and Sweden have worked in close coordination to create the North Atlantic Population Project (NAPP), a massive integrated cross-national microdatabase that provides a baseline for studies of demographic change and opens fresh paths for spatiotemporal data analysis. We now propose improvements that will multiply the power of the NAPP infrastructure. We have three major aims: (1) Triple the size of the database to approximately 365 million records, adding 40 new datasets for the period 1787 to 1930 from Albania, Britain, Canada, Denmark, Egypt, Iceland, Ireland, Germany, Norway, Mexico, Sweden, and the United States. (2) Leverage our innovative record-linkage technology to create linked national panels that will allow expanded longitudinal analyses. (3) Connect the past to the present by merging NAPP with the Integrated Public Use Microdata Series (IPUMS), simplifying analysis of long-run change and ensuring long-run preservation and maintenance of the database. The landscape of scientific research on the human population is shifting. It is no longer sufficient just to study the relationships among variables at a particular moment in time. Researchers around the world now recognize that to understand the large-scale processes that are transforming society, we must investigate long-term change. The goal of this project is to provide the infrastructure to make such analysis possible. NAPP will make a strategic contribution to demographic infrastructure by providing a baseline for the study of changes in the demography and health of European and North American populations. In each country, NAPP provides the earliest census microdata available. Models and descriptions based on historical experience underlie both theories of past change and projections into the future. The NAPP data provide a unique laboratory for the study of economic and demographic processes; this kind of empirical foundation is essential for testing social and economic theory. The proposed work will be carried out by a team of highly-skilled researchers with unparalleled expertise and experience in data integration and record linkage. Collaborators include leading researchers from the University of Minnesota and the Max Planck Institute for Demographic Research, and local experts from each of the participating countries. Centralized support for international collaboration will leverage the investments of each country and allow us to create an extraordinary resource for comparative social and economic research.        The North Atlantic Population Project (NAPP) provides fundamental infrastructure for scientific research, education, and policy-making and will allow social scientists to make comparisons across Northern Europe, the Mediterranean, and North America during three centuries of transformative change. The proposed work is directly relevant to the central mission of the NIH as the steward of medical and behavioral research for the nation: the new data will advance fundamental knowledge about the nature and behavior of human population dynamics. NAPP will unlock access to some of the largest and longest-running cross-sectional and longitudinal data sources in the world, and stimulate health-related research on population growth and movement, fertility, mortality, nuptiality, and family change, as well as the economic and social correlates of demographic behavior.         ",Integrated Public Use Microdata Series: North Atlantic Population Project,8310208,R01HD052110,"['Albania', 'American', 'Behavior', 'Behavioral Research', 'Biological Preservation', 'Canada', 'Censuses', 'Code', 'Collaborations', 'Collection', 'Communication', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Demographic Transitions', 'Demography', 'Denmark', 'Documentation', 'Economic Development', 'Economics', 'Education', 'Egypt', 'England', 'Ensure', 'European', 'Family', 'Fertility', 'Foundations', 'Funding', 'Future', 'Germany', 'Goals', 'Health', 'Human', 'Iceland', 'Immigration', 'Individual', 'Institutes', 'Institution', 'International', 'Investigation', 'Investments', 'Ireland', 'Knowledge', 'Laboratory Study', 'Link', 'Machine Learning', 'Maintenance', 'Mediation', 'Medical Research', 'Metadata', 'Mexico', 'Minnesota', 'Mission', 'Modeling', 'Nature', 'North America', 'Northern Europe', 'Norway', 'Nuptiality', 'Online Systems', 'Policy Making', 'Population', 'Population Characteristics', 'Population Growth', 'Private Sector', 'Process', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Scientist', 'Scotland', 'Series', 'Social Mobility', 'Societies', 'Subgroup', 'Sum', 'Sweden', 'Technology', 'Testing', 'Time', 'Time Study', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'comparative', 'critical period', 'data integration', 'data mining', 'experience', 'human population dynamics', 'improved', 'innovation', 'mortality', 'population movement', 'social', 'spatiotemporal', 'success', 'theories', 'tool']",NICHD,UNIVERSITY OF MINNESOTA,R01,2012,588521,-0.03358946573101898
"The Cardiovascular Research Grid    DESCRIPTION (provided by applicant):       The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinafions of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotafing ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and mofion that can predict the early presence of developing heart disease in fime for therapeufic intervenfion. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informafics system that allows clinical informafion to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease. (End of Abstract)          The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store,  manage, and analyze data on the structure and function of the cardiovascular system in health and disease.  The CVRG Project has developed and deployed unique technology that is now being used in a broad range  of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to  explore and analvze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8240702,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Time', 'Ultrasonography', 'Work', 'abstracting', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2012,2194299,-0.0084352008356353
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8319670,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2012,352514,-0.03838203176353183
"National Biomedical Information Services Delivering Biomedical Information Services  In FY12, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public, including significant expansion in mobile applications and open data initiatives via API access. The NLM Mobile app, released in FY12, is a guide to NLM's collection of mobile apps. Among the NLM's intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLM's premier bibliographic database with over 22 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY12, more than 700,000 new indexed citations were added.  PubMed Central: The PubMed Central archive of over 2.5 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports. In FY12, PMC's Journal Archiving Tag Suite (JATS), an XML format for exchange of journal content, was officially adopted as a standard by American National Standards Institute.  MedlinePlus and MedlinePlus en espanol: These consumer health information resources cover more than 900 topics, in more than 40 languages. MedlinePlus Connect, linking electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use, was expanded in FY12.  Clinical Trials: ClinicalTrials.gov covers more than 130,000 clinical research studies in nearly 180 countries, with hundreds added weekly. It also contains reports of summary results and adverse effects, in accordance with the FDA Amendments Act of 2007 (PL 110-85). In FY12, more than 18,000 new trials were registered.  Summary results, including adverse events, of more than 2400 trials were also added, bringing the total summary results to more than 6900.  Toxicology and Environmental Health:  Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, Toxic Release Inventory, Hazardous Substances Data Bank (HSDB), and LiverTox, released in FY12 with NIDDK. In FY12, the GeneEd genetics education resource for high school students was released with NHGRI.  Drug Information Resources: Drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for more than 40,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to NLM's RxNorm standard drug names. Disaster Preparedness and Response:  NLM's Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A continuing collaboration with the Bethesda Hospital Emergency Preparedness Partnership (BHEPP) provides backup communication systems and tools for patient tracking, information access, and responder training.  In FY12, the The Patient Tracking and Locating System, a portable electronic tool to help hospitals handle the surge of patients during a disaster, won an HHSinnovates award. Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, dbGAP, Genomes, Genetic Testing Registry and ClinVar released in FY12, and the NCBI software toolkit. NCBI also provides access to PubMed, PubMed Central, and the Books database. Continuing areas of emphasis in FY12 included addressing the impact of enormous quantities of data emanating from high throughput sequencing and microarrays; improving methodology for representing mammalian genome assemblies; organizing data from genome-wide association studies; and enhancing the interfaces to journal literature retrieval to facilitate search and discovery.  Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. NLM outreach programs enhance awareness of its information services, with emphasis on underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY12, dozens of community-based projects were funded. Also in FY12, NLM opened the Native Voices: Native Peoples' Concepts of Health and Illness exhibition.  The exhibition explores the connection between wellness, illness, and cultural life through interviews with American Indians, Alaska Natives, and Native Hawaiians; artwork; objects; and interactive media. Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 9000 health services research projects from more than 110 funding organizations; HSRR, a database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2020 objectives. Advanced Information Systems and Research Tools  In FY12, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; directed the re-design of the ITK suite of software tools for image analysis; applied natural language processing methods to extract information from biomedical literature; improved standardized reporting of genetic variations and clinical interpretation of genetic test results; and designed improved methods for integrated search and discovery across multiple databases.  Health Data Standards: As the central coordinating body for clinical terminology standards within HHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure and provides essential tools for meaningful use of electronic health records (EHRs). NLM develops, supports, or licenses for free US-wide use the key clinical terminologies designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 10 million concept names from more than 160 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY12, NLM enhanced applications programming interfaces (APIs) to its UMLS and drug vocabulary resources, released new mappings from SNOMED CT to ICD-10-CM and from ICD-9-CM to SNOMED CT, and expanded documentation and training resources for EHR developers and users. n/a",National Biomedical Information Services,8558151,ZIHLM200888,"['Address', 'Administrator', 'Adopted', 'Adoption', 'Adverse effects', 'Adverse event', 'African American', 'Amendment', 'American', 'American Indian and Alaska Native', 'Archives', 'Area', 'Award', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biotechnology', 'Books', 'Characteristics', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disasters', 'Documentation', 'Education', 'Effectiveness', 'Electronic Health Record', 'Electronics', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Extensible Markup Language', 'Family', 'Fostering', 'Funding', 'Genbank', 'General Population', 'Genetic', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Goals', 'Guidelines', 'Hawaiian population', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'ICD-10-CM', 'ICD-9-CM', 'Image', 'Image Analysis', 'Imagery', 'Improve Access', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Services', 'Information Specialists', 'Information Systems', 'Institutes', 'International', 'Intervention', 'Interview', 'Intramural Research Program', 'Journals', 'Label', 'Laboratories', 'Language', 'Libraries', 'Licensing', 'Life', 'Link', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'Maps', 'MeSH Thesaurus', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methodology', 'Methods', 'Minority', 'Molecular Biology', 'Names', 'National Human Genome Research Institute', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Native Americans', 'Native-Born', 'Natural Language Processing', 'Nomenclature', 'Package Insert', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Poison Control Centers', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'PubMed', 'Public Health', 'Publishing', 'Readiness', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Software Tools', 'Solid', 'Speed', 'Structure', 'Students', 'Summary Reports', 'System', 'Technology', 'Technology Assessment', 'Terminology', 'Test Result', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vocabulary', 'Voice', 'base', 'biomedical informatics', 'biomedical information system', 'comparative effectiveness', 'design', 'dosage', 'drug market', 'drug standard', 'exhibitions', 'genome wide association study', 'health disparity', 'health information technology', 'health literacy', 'high school', 'improved', 'indexing', 'inner city', 'instrument', 'journal article', 'knowledge base', 'language processing', 'mammalian genome', 'metathesaurus', 'outreach', 'outreach program', 'programs', 'research and development', 'research study', 'response', 'scientific computing', 'systems research', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2012,269400088,-0.010261381313266869
"National Biomedical Information Services Delivering Biomedical Information Services  In FY12, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public, including significant expansion in mobile applications and open data initiatives via API access. The NLM Mobile app, released in FY12, is a guide to NLM's collection of mobile apps. Among the NLM's intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLM's premier bibliographic database with over 22 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY12, more than 700,000 new indexed citations were added.  PubMed Central: The PubMed Central archive of over 2.5 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports. In FY12, PMC's Journal Archiving Tag Suite (JATS), an XML format for exchange of journal content, was officially adopted as a standard by American National Standards Institute.  MedlinePlus and MedlinePlus en espanol: These consumer health information resources cover more than 900 topics, in more than 40 languages. MedlinePlus Connect, linking electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use, was expanded in FY12.  Clinical Trials: ClinicalTrials.gov covers more than 130,000 clinical research studies in nearly 180 countries, with hundreds added weekly. It also contains reports of summary results and adverse effects, in accordance with the FDA Amendments Act of 2007 (PL 110-85). In FY12, more than 18,000 new trials were registered.  Summary results, including adverse events, of more than 2400 trials were also added, bringing the total summary results to more than 6900.  Toxicology and Environmental Health:  Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, Toxic Release Inventory, Hazardous Substances Data Bank (HSDB), and LiverTox, released in FY12 with NIDDK. In FY12, the GeneEd genetics education resource for high school students was released with NHGRI.  Drug Information Resources: Drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for more than 40,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to NLM's RxNorm standard drug names. Disaster Preparedness and Response:  NLM's Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A continuing collaboration with the Bethesda Hospital Emergency Preparedness Partnership (BHEPP) provides backup communication systems and tools for patient tracking, information access, and responder training.  In FY12, the The Patient Tracking and Locating System, a portable electronic tool to help hospitals handle the surge of patients during a disaster, won an HHSinnovates award. Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, dbGAP, Genomes, Genetic Testing Registry and ClinVar released in FY12, and the NCBI software toolkit. NCBI also provides access to PubMed, PubMed Central, and the Books database. Continuing areas of emphasis in FY12 included addressing the impact of enormous quantities of data emanating from high throughput sequencing and microarrays; improving methodology for representing mammalian genome assemblies; organizing data from genome-wide association studies; and enhancing the interfaces to journal literature retrieval to facilitate search and discovery.  Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. NLM outreach programs enhance awareness of its information services, with emphasis on underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY12, dozens of community-based projects were funded. Also in FY12, NLM opened the Native Voices: Native Peoples' Concepts of Health and Illness exhibition.  The exhibition explores the connection between wellness, illness, and cultural life through interviews with American Indians, Alaska Natives, and Native Hawaiians; artwork; objects; and interactive media. Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 9000 health services research projects from more than 110 funding organizations; HSRR, a database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2020 objectives. Advanced Information Systems and Research Tools  In FY12, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; directed the re-design of the ITK suite of software tools for image analysis; applied natural language processing methods to extract information from biomedical literature; improved standardized reporting of genetic variations and clinical interpretation of genetic test results; and designed improved methods for integrated search and discovery across multiple databases.  Health Data Standards: As the central coordinating body for clinical terminology standards within HHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure and provides essential tools for meaningful use of electronic health records (EHRs). NLM develops, supports, or licenses for free US-wide use the key clinical terminologies designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 10 million concept names from more than 160 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY12, NLM enhanced applications programming interfaces (APIs) to its UMLS and drug vocabulary resources, released new mappings from SNOMED CT to ICD-10-CM and from ICD-9-CM to SNOMED CT, and expanded documentation and training resources for EHR developers and users. n/a",National Biomedical Information Services,8558151,ZIHLM200888,"['Address', 'Administrator', 'Adopted', 'Adoption', 'Adverse effects', 'Adverse event', 'African American', 'Amendment', 'American', 'American Indian and Alaska Native', 'Archives', 'Area', 'Award', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biotechnology', 'Books', 'Characteristics', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disasters', 'Documentation', 'Education', 'Effectiveness', 'Electronic Health Record', 'Electronics', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Extensible Markup Language', 'Family', 'Fostering', 'Funding', 'Genbank', 'General Population', 'Genetic', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Goals', 'Guidelines', 'Hawaiian population', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'ICD-10-CM', 'ICD-9-CM', 'Image', 'Image Analysis', 'Imagery', 'Improve Access', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Services', 'Information Specialists', 'Information Systems', 'Institutes', 'International', 'Intervention', 'Interview', 'Intramural Research Program', 'Journals', 'Label', 'Laboratories', 'Language', 'Libraries', 'Licensing', 'Life', 'Link', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'Maps', 'MeSH Thesaurus', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methodology', 'Methods', 'Minority', 'Molecular Biology', 'Names', 'National Human Genome Research Institute', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Native Americans', 'Native-Born', 'Natural Language Processing', 'Nomenclature', 'Package Insert', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Poison Control Centers', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'PubMed', 'Public Health', 'Publishing', 'Readiness', 'Registries', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Software Tools', 'Solid', 'Speed', 'Structure', 'Students', 'Summary Reports', 'System', 'Technology', 'Technology Assessment', 'Terminology', 'Test Result', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vocabulary', 'Voice', 'base', 'biomedical informatics', 'biomedical information system', 'comparative effectiveness', 'design', 'dosage', 'drug market', 'drug standard', 'exhibitions', 'genome wide association study', 'health disparity', 'health information technology', 'health literacy', 'high school', 'improved', 'indexing', 'inner city', 'instrument', 'journal article', 'knowledge base', 'language processing', 'mammalian genome', 'metathesaurus', 'outreach', 'outreach program', 'programs', 'research and development', 'research study', 'response', 'scientific computing', 'systems research', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2012,2346125,-0.010261381313266869
"Continued Development and Evaluation of caTIES Abstract We propose to further develop, test, evaluate and support caTIES - an existing software system for developing networked repositories of sharable de-identified surgical pathology reports. The caTIES system creates a repository of de-identified, structured, and concept-coded clinical reports derived from large corpora of clinical free-text. Documents are automatically coded against a controlled terminology such as the Unified Medical Language System (UMLS), SNOMED-CT, or NCI Metathesaurus. Users construct queries to identify specific kinds of documents and tissue specimens based on the associated clinical report. For example, a researcher studying genetic variation in metastatic breast cancers can identify cases of invasive ductal carcinoma of the breast, followed by metastatic ductal cancer in bone at an interval of three years or greater from the original diagnosis. The caTIES system also supports acquisition and ordering of tissues, using an honest broker model. Through this mechanism, de-identified data and access to tissue can be shared among institutions, enabling multi-center collaborative research. The caTIES system has already been implemented at seven US Cancer Centers, and is being considered for adoption by numerous other institutions including cancer centers, university hospitals and private hospitals. Initial development of caTIES was funded by the Cancer Biomedical Informatics Grid (caBIG). However, interest in the application has far exceeded our expectations and the limitations of caBIG. This grant will allow us to further extend the capabilities of the system by (a) improving the portability of the system and extending the types of documents that can be processed, (b) evaluating the system's NLP performance and usability, (c) building a user community to support this open-source application, and (d) piloting interoperability of caTIES with other enterprise and research systems. This work will preserve and extend a highly novel platform for development of massive repositories of de-identified clinical data that can be used for research within and across institutions. Narrative This grant will fund the further development and evaluation of a system that takes identified clinical documents and converts them into de-identified, concept-coded, structured data. The system enables researchers to access remainder tissues and clinical report data for research purposes within and across institution. This project is important because it will greatly increase the access of researchers to important data and materials while maintaining patient privacy. n/a",Continued Development and Evaluation of caTIES,8197903,R01CA132672,"['Access to Information', 'Adoption', 'Cancer Center', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Communication', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Data Reporting', 'Database Management Systems', 'Development', 'Diagnosis', 'Documentation', 'Ductal', 'Electronic Mail', 'Eligibility Determination', 'Environment', 'Evaluation', 'Funding', 'Future', 'Genetic Variation', 'Genomics', 'Grant', 'Information Retrieval', 'Institution', 'Malignant Neoplasms', 'Methods', 'Metric', 'Modeling', 'Modification', 'Natural Language Processing', 'Operating System', 'Pathology Report', 'Performance', 'Private Hospitals', 'Process', 'Report (document)', 'Reporting', 'Research', 'Research Personnel', 'Services', 'Specimen', 'Structure', 'Surgical Pathology', 'System', 'Systematized Nomenclature of Medicine', 'Terminology', 'Testing', 'Text', 'Tissues', 'Training', 'Translational Research', 'Unified Medical Language System', 'University Hospitals', 'Vocabulary', 'Work', 'abstracting', 'base', 'bone', 'cancer Biomedical Informatics Grid', 'clinical phenotype', 'computer human interaction', 'ductal breast carcinoma', 'expectation', 'flexibility', 'improved', 'interest', 'interoperability', 'malignant breast neoplasm', 'meetings', 'metathesaurus', 'novel', 'open source', 'patient privacy', 'portability', 'repository', 'software development', 'software systems', 'systems research', 'tool', 'usability']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,304932,-0.04836041619038679
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,-0.004100438388811052
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,8322813,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Clinical assessments', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Health', 'Health Personnel', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Imagery', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'data modeling', 'graphical user interface', 'hemodynamics', 'imaging informatics', 'improved', 'innovation', 'insight', 'mortality', 'network models', 'patient population', 'prognostic', 'prospective', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2012,570954,0.0021754796037888485
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8330246,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2012,2827142,0.007897458217528647
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8330246,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2012,248339,0.007897458217528647
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in four subprojects which use natural language processing techniques:  1) We have developed a machine learning algorithm for abbreviation definition identification in text which makes use of what we term naturally labeled data. Positive training examples are naturally occurring potential abbreviation-definition pairs in text. Negative training examples are generated by randomly mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Medstract corpora. Our system demonstrated results that compare favourably to the existing Ab3P and BIOADI systems. We achieve an F-measure of 91.36% on Ab3P corpus, and an F-measure of 87.13% on BIOADI corpus which are superior to the results reported by Ab3P and BIOADI systems. Moreover, we outperform these systems in terms of recall, which is one of our goals. 2) We are studying paraphrases in MEDLINE abstracts. These come about because an author is describing some entity of interest and uses a phrase like ""drug abuse"" and then needing to describe the same entity again a sentence or two latter does not wish to use exactly the same wording again and may use a variant of the phrase such as ""drug use"" which in the context of ""drug abuse"" has substantially the same meaning.  3) An author disambiguation algorithm has been developed which relies on machine learning based on the assumption that if an author name is infrequent in the data it probably represents the same person in for all documents where it is found. This gives us positive instances. Negative instances are sampled from pairs of documents that have no author in common. Such positive and negative data allows us to do machine learning on all aspects of the document other than the name in question. This allows us to learn how to weight this data for best performance in distinguishing the positive and negative instances from each other. This learning is then applied in individual name cases or spaces to determine which author document pairs represent the same author. n/a",Natural Language Processing Techniques To Enhance Information Access.,8344949,ZIALM000090,"['Abbreviations', 'Algorithms', 'Automated Abstracting', 'Data', 'Dependency', 'Drug abuse', 'Drug usage', 'Goals', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'MEDLINE', 'Machine Learning', 'Measures', 'Names', 'Natural Language Processing', 'Performance', 'Persons', 'Reporting', 'Sampling', 'System', 'Techniques', 'Text', 'Training', 'Variant', 'Weight', 'abstracting', 'base', 'indexing', 'interest', 'phrases', 'spelling', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,539651,-0.028359116112442576
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence    DESCRIPTION (provided by applicant): Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.           Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8190163,K99LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2011,89802,-0.028292098198345945
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.   1)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 2)	The PubMed database presents a unique challenge because of its very large size of over 19 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  3) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with theresults of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. 4) We are currently working on a problem which arises when several different kinds of documents appear in a dataset and one wants to compute neighboring documents for each document. In this situation it is possible that scores will be higher within groups than between groups so that scores do not give a true picture of relatedness. This can happen because within a group terminology may be common that is used but rarely outside the group. We have developed a Bayesian method to test for this kind of terminology and remove it. The removal process requires some care which we exercise by testing terms to be removed by tests to see how specific they are to biology and how useful they are to the users of PubMed. These test together have allowed us to successfully remove terminology within document groups and improve scoring so that it can be used for neighboring successfully. n/a",Automatic Bayesian Methods In Text Retrieval,8344938,ZIALM000021,"['Accounting', 'Active Learning', 'Bayesian Method', 'Biology', 'Caring', 'Classification', 'Data', 'Data Set', 'Databases', 'Excision', 'Exercise stress test', 'Exhibits', 'Frequencies', 'Genomics', 'Goals', 'Label', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Modeling', 'Performance', 'Peripheral', 'Poisson Distribution', 'Probability', 'Process', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,79948,0.0017581223360714192
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method we are currently studying is to perform machine learning with an SVM or other classifier and score the documents based on this learning. Then PAV can be applied to the resulting scores and this score function can be descretized without the loss of significant information. This allows us to make use of the results as features which can be individually weighted in another classifier. 4) We have developed a new algorithm called the periodic random orbiter algorithm (PROBE) which is applicable to minimize any convex loss function. We have applied it to the MeSH classification problem and it seems to work very well and better than the alternatives on such a large problem. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,8344948,ZIALM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Classification', 'Data Set', 'Exclusion', 'Learning', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Procedures', 'Retrieval', 'Testing', 'Weight', 'Work', 'base', 'design', 'improved', 'loss of function', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,599613,0.005912716259684188
"Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens DESCRIPTION (provided by applicant): We appreciate the time and effort spent by all the reviewers, and we are grateful for the useful comments and provided suggestions. We have carefully reviewed the critiques and we are happy to see that the panel was receptive to our proposal. The reviewers expressed three major concerns in the summary statement: (1) although the investigating team is well qualified our history of collaboration is short; (2) details regarding the practical constraints of the BARDOT system are lacking; (3) the machine learning techniques employed in the project are considered fairly standard.  Below we briefly discuss the reviewers comments and indicate how we have changed our revised application to address the critique.  (1) Dr. Dundar moved from industry to academia in the fall of 2008, at which point Dr. Rajwa (one of the original inventors of BARDOT) and Dr. Dundar began their collaboration on new approaches to the problem of non-exhaustively defined classes in phenotypic screening. This scientific partnership immediately produced interesting results, and at the time of submission of the original application, Dr. Dundar and Dr. Rajwa had their first manuscript under review. The approach presented in the original proposal was tested and the results were submitted to the ACM 15th Annual SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'09), which is the largest and one of the most respected conferences in this field. The manuscript was accepted after a full peer review as one of the 50 regular papers selected from 551 submissions [20]. Following the proposal submission, research efforts continued and produced yet another approach to the problem described in this grant application. The preliminary findings are reported in a new manuscript which is currently under review [4].  (2) We rewrote the background and research methods sections of our proposal to include information re- quested by the reviewers regarding practical aspects of the BARDOT system, such as accuracy issues (Section D.3.2), frequency of encountering new, unknown classes (Section B.3.1), and validation (Section D.3.1).  (3) The problem of phenotypic screening and classification of bacteria can be defined within exhaustive (stan- dard) or non-exhaustive learning frameworks. Although we agree that the implementation of an exhaustive clas- sification approach for BARDOT does require only fairly standard tools, the problem of the non-exhaustive nature of training libraries cannot be addressed by straightforward use of any textbook-level technique. In fact, the presence of non-exhaustively defined set of classes violates basic assumptions for most supervised learning systems. The issue of non-exhaustively defined classes is the major obstacle for application of machine learning in phenotypic analysis since the number of possible phenotypes may be infinite. In our original proposal we argued that learning with a non-exhaustively defined set of classes remains a very challenging problem, and presented evidence demonstrating that simple extensions of standard techniques cannot provide an acceptable solution. Subsequently, we proposed a new approach based on Bayesian simulation of classes and showed that preliminary results outperformed benchmark techniques [4].  Although these initial results looked promising, we did not consider the described preliminary algorithms final and definitive, and we do not believe that at this point we are able to provide an exact algorithmic solution to this complex problem. If we were able to do that, it would mean that we had already accomplished all the grant goals. The very essence of the proposed research is finding the answer to the defined problem, and the answer will remain unknown until after the work has been done. However, positive reviews and an acceptance of our work by KDD'09 conference judges, tell us that we are heading in the right direction.  In the amended version of this application we propose a modified Bayesian approach based on Wishart priors (Section D.2.3). The algorithm creates new classes on the fly and evaluates maximum likelihood with the updated set of classes, gradually improving detection accuracy for future samples. We believe that this offers a substantial improvement over the previous method. Consequently, the preliminary results in Section C are updated to reflect our progress. Since the modified technique allows for classification with non-exhaustive and exhaustive sets using the same algorithm, we consolidated the previous specific aims 3 and 5 into one in the revised application. PUBLIC HEALTH RELEVANCE: A Machine Learning Approach to Label-free Detection of Bacterial Pathogens using Laser Light Scattering PIs: Dr. M. Murat Dundar and Dr. Bartek Rajwa Successful implementation of this project will allow for a label-free detection and identification of food pathogens and their mutated subclasses not yet seen earlier. This will reduce the number of food related outbreaks and will help secure public food supply.",Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens,8070004,R21AI085531,"['Academia', 'Accounting', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Bacteria', 'Benchmarking', 'Biochemical Process', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Classification', 'Collaborations', 'Complex', 'Computer Vision Systems', 'Critiques', 'Data Set', 'Detection', 'Disease', 'Disease Outbreaks', 'Escherichia coli', 'Food', 'Food Supply', 'Frequencies', 'Future', 'Genus staphylococcus', 'Goals', 'Grant', 'Head', 'Health', 'Industry', 'Infection', 'International', 'Knowledge Discovery', 'Label', 'Lasers', 'Learning', 'Libraries', 'Listeria', 'Machine Learning', 'Manuscripts', 'Medical', 'Methods', 'Modeling', 'Mutate', 'Mutation', 'Nature', 'Optics', 'Paper', 'Pathogenicity', 'Pattern', 'Pattern Recognition', 'Peer Review', 'Phenotype', 'Process', 'Productivity', 'Public Health', 'Published Comment', 'Qualifying', 'Reagent', 'Recording of previous events', 'Reporting', 'Research', 'Research Methodology', 'Safety', 'Salmonella', 'Sampling', 'Screening procedure', 'Secure', 'Serotyping', 'Solutions', 'Suggestion', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Textbooks', 'Time', 'Training', 'Update', 'Validation', 'Vibrio', 'Work', 'base', 'cost', 'data mining', 'disorder prevention', 'falls', 'foodborne', 'foodborne pathogen', 'image processing', 'improved', 'interest', 'light scattering', 'new technology', 'novel strategies', 'optical sensor', 'pathogen', 'pathogenic bacteria', 'rapid detection', 'sensor', 'simulation', 'symposium', 'text searching', 'tool']",NIAID,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R21,2011,147939,-0.0091128529668751
"A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology    DESCRIPTION (provided by applicant): The non-medical use of pharmaceutical opioids has been identified as one of the fastest growing forms of drug abuse in the U.S. There is a critical need to enhance current epidemiological monitoring, early warning, and post-marketing surveillance systems by providing additional and more timely data. The World Wide Web has been identified as one of the ""leading edge"" data sources for detecting patterns and changes in drug use practices. Many websites provide a venue for individuals to freely share their own experiences, post questions, and offer comments about different drugs. Such User Generated Content (UGC) can be used as a very rich data source to study knowledge, attitudes, and behaviors related to illicit drugs. To harness the full potential of the Web for drug abuse research, the field needs to develop a highly automated way of accessing, extracting, and analyzing Web-based data related to illicit drug use. This exploratory R21 is a multi-principal investigator, collaborative effort between researchers at the Center for Interventions, Treatment and Addictions Research (CITAR) and the Center for Knowledge-Enabled Information Services and Science (Kno.e.sis) at Wright State University. The purpose of this Web-based study is to apply cutting-edge information processing techniques, such as the Semantic Web, Natural Language Processing, and Machine Learning, to qualitative and quantitative content analysis of user generated content to achieve the following aims: 1) Describe drug users' knowledge, attitudes, and behaviors related to the illicit use of Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine); 2) Identify and describe temporal patterns of the illicit use of these drugs as reflected on web-based forums. To collect data, the study will use websites that allow for the free discussion of illicit drugs, contain information on illicit prescription drug use, and are accessible for public viewing. The study will generate new information about the practices of buprenorphine abuse and will contribute to the advancement of public health and substance abuse research by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data. Automated information extraction methods applied in this study will enhance current early warning and epidemiological surveillance systems and could advance qualitative and Web-based research methods in other areas of public health.      PUBLIC HEALTH RELEVANCE: Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.              Building on inter-disciplinary collaboration and cutting-edge information processing techniques, this exploratory, Web-based study will generate new information about Suboxone(R) (buprenorphine/naloxone) and Subutex(R) (buprenorphine) abuse practices, thereby informing public health interventions and policy. It will also contribute to the advancement of public health and substance abuse research methods by providing automatic coding and information extraction tools needed to handle rapidly growing Web-based data.            ",A Study of Social Web Data on Buprenorphine Abuse Using Semantic Web Technology,8190799,R21DA030571,"['Accident and Emergency department', 'Archives', 'Area', 'Behavior', 'Buprenorphine', 'Code', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Data', 'Data Sources', 'Drug Rehabilitation Centers', 'Drug abuse', 'Drug usage', 'Drug user', 'Early identification', 'Epidemiologic Monitoring', 'Epidemiologic Studies', 'Epidemiology', 'Face', 'Health', 'Health Knowledge, Attitudes, Practice', 'Health Professional', 'Heroin Dependence', 'Hospitals', 'Illicit Drugs', 'Individual', 'Information Sciences', 'Information Services', 'Information Systems', 'Internet', 'Intervention', 'Intervention Studies', 'Interview', 'Knowledge', 'Label', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Monitor', 'NIH Program Announcements', 'Naloxone', 'Natural Language Processing', 'Online Systems', 'Opioid', 'Overdose', 'Pathway interactions', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Poison Control Centers', 'Policies', 'Population', 'Prevention', 'Principal Investigator', 'Process', 'Public Health', 'Published Comment', 'Qualitative Research', 'Reading', 'Reliance', 'Research', 'Research Methodology', 'Research Personnel', 'Sampling', 'Selection Bias', 'Self Disclosure', 'Semantics', 'Source', 'Substance abuse problem', 'Subutex', 'Surveillance Methods', 'Surveys', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Universities', 'addiction', 'buprenorphine abuse', 'computer based Semantic Analysis', 'design', 'emotional disclosure', 'empowered', 'experience', 'informant', 'information processing', 'misuse of prescription only drugs', 'population survey', 'post-market', 'prescription drug abuse', 'programs', 'response', 'social', 'tool', 'trend', 'web site', 'working group']",NIDA,WRIGHT STATE UNIVERSITY,R21,2011,219000,-0.026623006132064052
"PubMed Query Log Analysis and Use in Access Enhancement 1) Biomedical literature search is the main entry point for an ever-increasing range of information. PubMed/MEDLINE is the most widely used service for this purpose. However, finding citations relevant to a users information need is not always easy in PubMed. Improving our understanding of the growing population of PubMed users, their information needs and the way in which they meet these needs opens opportunities to improve information services and information access provided by PubMed. One resource for understanding and characterizing patrons of search engines is the transaction logs. Our previous investigation of user query logs has led us to develop and deploy a useful application in assisting user query formulation in PubMed, namely Related Queries (RQ). Inspired by its success, we have continued using log analysis to identify research problems which are closely related to PubMed operations.   2) Another application of query log analysis starts with a disease name or some other significant term and asks how often a document containing this term is clicked on in response to a query containing this term. This click through rate is used as a feature in machine learning to decide whether a document has the disease as a central focus. n/a",PubMed Query Log Analysis and Use in Access Enhancement,8344966,ZIALM200812,"['Data', 'Databases', 'Disease', 'Drug Formulations', 'Goals', 'Information Services', 'Investigation', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Names', 'Population', 'PubMed', 'Research', 'Resources', 'Services', 'improved', 'meetings', 'operation', 'response', 'success', 'text searching']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,59961,-0.0017183585146205134
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) The latest work on this system has involved adding the ability to generate themes using an EM algorithm approach. Also recently code has been multithreaded and memory mapping capabilities added to speed up processing.  The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system is currently proving useful in testing different retrieval parameters and methods on the PubMedHealth records. n/a",A Document Processing System,8344939,ZIALM000022,"['Algorithms', 'Code', 'Data', 'Databases', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'Memory', 'Methods', 'Online Systems', 'Process', 'Programming Languages', 'PubMed', 'Records', 'Research', 'Retrieval', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Work', 'base', 'computerized data processing', 'repository', 'software systems']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,79948,-0.007710897214062832
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,8075593,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2011,274386,-0.03083073596140968
"Automatic Analysis and Annotation of Document Keywords in Biomedical Literature 1) Electronic Textbook and PubMed Central Indexing Current processing of the electronic textbook material involves a number of steps designed to produce the most meaningful phrases in the text to be used as reference points. The first task is to identify grammatically reasonable phrases. We use a version of the Brill transformation based tagger, rewritten in C++, for part-of- speech tagging. This forms the basis for determining grammatically reasonable phrases. There is a significant post processing step that removes phrases that involve inappropriate references to context (e.g., different cells, final mutation). After finding grammatically reasonable phrases we attempt to eliminate those that are too common or generic to be useful (e.g., significant result, short time).  The next step is to compare a phrase with previously rated phrases that have been collected over the life of the project. The final stage is to estimate the importance of a phrase in the passage where it is found in a textbook. Such an estimate is based on the frequency of the phrase and the size of the passage compared with the frequency of the phrase throughout the book and the overall size of the book. In order to improve such an estimate we attempt to take account of the phrase or any phrase that represents the same concept. For this purpose we use the UMLS Metathesaurus and also stemming and combine these two approaches into a consistent picture of the concept as it occurs in the text.  The result of this processing is a scored list of phrase-book section pairs for each textbook. These are used to guide the response of general searching in the books. When a user types in a phrase that is on our curated list the first results given are the highly rated book sections for that phrase. We are now applying a similar indexing scheme to the text of articles in PMCentral. This allows us to give a list of highly rated phrases for each article as an enhanced reference point for searchers.    2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text.   3) Currently we are studying how good phrases can be recognized by their characteristics, such as frequency, tendency to be repeated in documents where they occur, and other numerical properties. These features allow one to predict which phrases are of high quality. We have found such predictions to be useful in studying different kinds of terms that may appear in text and how an ontoloogy might be extracted from text. n/a",Automatic Analysis and Annotation of Document Keywords in Biomedical Literature,8344960,ZIALM091711,"['Accounting', 'Address', 'Books', 'Cells', 'Characteristics', 'Collection', 'Electronics', 'Evaluation', 'Frequencies', 'Generic Drugs', 'Goals', 'Life', 'Literature', 'Machine Learning', 'Methods', 'Mutation', 'Outcome', 'Process', 'Property', 'PubMed', 'Records', 'Retrieval', 'Scheme', 'Speech', 'Staging', 'Techniques', 'Text', 'Textbooks', 'Time', 'UMLS Metathesaurus', 'base', 'design', 'improved', 'indexing', 'insight', 'phrases', 'response', 'stem']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,239845,-0.02914694108198118
"Query Log Analysis for Improving User Access to NCBI Web Services Over the last decade, the online search for biological information has progressed rapidly and has become an integral part of any scientific discovery process. Today, it is virtually impossible to conduct R&D in biomedicine without relying on the kind of Web resources developed and maintained by the NCBI. Indeed, each day millions of users search for biological information via NCBIs online Entrez system. However, finding data relevant to a users information need is not always easy in Entrez. Improving our understanding of the growing population of Entrez users, their information needs and the way in which they meet these needs opens opportunities to improve information services and information access provided by NCBI. One resource for understanding and characterizing patrons of search engines is the transaction logs. Our previous investigation of PubMed query logs has led us to develop and deploy several useful applications in assisting user searches and retrieval such as the query formulation in PubMed, namely Related Queries and Query Autocomplete. Inspired by its success, we have continued using log analysis to identify research problems which are closely related to NCBI operations.    Among all Entrez databases, PubMed is the most used one and often serves as an entry point for people to access related data in other Entrez databases.  In a recent survey, we compared and contrasted PubMed with other similar literature search tools developed by other researchers. Based on our investigation, we found that there are areas where PubMed may learn from others for self-improvement with respect to better retrieval and user search experience. For instance, several tools differ from PubMed in that they allow relevance search, an important feature that can be helpful for some PubMed searches. With respect to user interface, other tools have attempted to visualize search results using novel schemes such as clusters, word clouds or networks. Though these methods are not formally validated in large-scale user studies, the concept of better visualization of search results might still be useful for consideration towards improving PubMeds current list-based presentation.   In 2011, we have also studied query logs beyond PubMed. One specific project involves the analysis of user logs of NCBIs Global Search where user queries are searched against all Entrez databases and results are presented without indicating the relevancy of different databases to the user queries. Hence our task is to predict which Entrez database(s) is mostly likely to contain the relevant information to the users based on their input queries. In our current approach we first collect a data corpus from logs where each data point contains a user query followed by a user click to a specific database. Next, we apply machine-learning algorithms to learn the characteristics in user queries that distinguish users intention for seeking different biological data. Based on the learned features, we classify new input queries and direct users to results in relevant database(s) for their search needs.   Another use of query logs lies in our work for PubMed Health: a newly launched NCBI service offering up-to-date information on diseases, conditions, drugs, treatment options, and healthy living for both health consumers and healthcare professionals. Based on our log analysis of actual usage on disease and drug topics in PubMed Health, we discovered that approximately 80% of the usage falls on 20% of the database content. That is, the user access pattern satisfies the Pareto principle (aka the 80-20 rule), which can have many implications for further improving our Web service. For instance, the principle suggests that we prioritize our resources on those heavily accessed content. In addition, query logs were used when we deployed our research on building links for enriching user access between related drug and disease pages as portlets in PubMed Health. Specifically, we developed text-mining methods for automatically identifying drugs and its closely related diseases (e.g. Lipitor and heart disease). In particular, we took advantage of the co-occurrence information of drug and disease mentions in user queries to help determine their strength in relatedness and popularity in user needs. As a result, we computed several thousand pairs of drug and diseases that are not only closely related but also frequently requested by our users. n/a",Query Log Analysis for Improving User Access to NCBI Web Services,8344934,ZIALM000001,"['Algorithms', 'Area', 'Biological', 'Characteristics', 'Data', 'Databases', 'Disease', 'Drug Formulations', 'Goals', 'Health', 'Health Professional', 'Heart Diseases', 'Imagery', 'Information Services', 'Intention', 'Internet', 'Investigation', 'Learning', 'Life', 'Link', 'Literature', 'Machine Learning', 'Methods', 'Molecular Biology', 'Pattern', 'Pharmaceutical Preparations', 'Population', 'Process', 'PubMed', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Scheme', 'Services', 'Surveys', 'System', 'Work', 'atorvastatin', 'base', 'experience', 'falls', 'improved', 'meetings', 'novel', 'operation', 'research and development', 'success', 'text searching', 'tool', 'web services']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,499678,-0.009326903167174105
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.      PUBLIC HEALTH RELEVANCE: The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.             The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8108523,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2011,341852,-0.030304385519358622
"Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me    DESCRIPTION (provided by applicant):       Computer-assisted medicine is at a crossroads: medical care requires accurate data, but making such data widely available can create unacceptable risks to the privacy of individual patients. This tension between utility and privacy is especially acute in predictive personalized medicine (PPM). PPM holds the promise of making treatment decisions tailored to the individual based on her or his particular genetics and clinical history. Making PPM a reality requires running statistical, data mining and machine learning algorithms on combined genetic, clinical and demographic data to construct predictive models. Access to such data directly competes with the need for healthcare providers to protect the privacy of each patient's data, thus creating a tradeoff between model efficacy and privacy. Thus we find ourselves in an unfortunate standoff: significant medical advances that would result from more powerful mining of the data by a wider variety of researchers are hindered by significant privacy concerns on behalf of the patients represented in the data set. In this proposed work, we seek to develop and evaluate technology to resolve this standoff, enabling health practitioners and researchers to compute on privacy-sensitive medical records in order to make treatment decisions or create accurate models, while protecting patient privacy. We will evaluate our approach on a de-identified actual electronic medical record, with an average of 29 years of clinical history on each patient, and with detailed genetic data (650K SNPs) available for a subset of 5000 of the patients. This data set is available to us now through the Wisconsin Genomics Initiative, but only on a computer at the Marshfield Clinic. If successful our approach will make possible the sharing of this cutting-edge data set, and others like it that are now in development, including our ability to analyze this data at UW-Madison where we have thousands of processors available in our Condor pool. Our privacy approach integrates secure data access environments, including those appropriate to the use of laptops and cloud computing, with novel anonymization algorithms providing differential privacy guarantees for data and/or published results of data analysis. To this end, our specific aims are as follows:       AIM 1: Develop and deploy a secure local environment that, in combination with secure network functionality, will ensure end-to-end security and privacy for electronic medical records and biomedical datasets shared between clinical institutions and researchers.       AIM 2: Develop and deploy a secure virtual environment to allow large-scale, privacy-preserving data analysis ""in the cloud.""       AIM 3: Develop and evaluate privacy-preserving data mining algorithms for use with original (not anonymized) data sets consisting of electronic medical records and genetic data.       AIM 4: Develop and evaluate anonymizing data publishing algorithms and privacy guarantees that are appropriate to the complex structure present in electronic medical records with genetic data.            Project Narrative This project will develop an integrated approach to secure sharing of clinical and genetic data that based on algorithms for anonymization of data to achieve differential privacy guarantees, for privacy-preserving publication of data analysis results, and secure environments for data sharing that include addressing the increasing use of laptops and of cloud computing. The end goal of this project is to meet the competing demands of providing patients with both privacy and accurate predictive models based on clinical history and genetics. This project includes the first concrete evaluation of privacy- preserving data mining algorithms on actual combined EMR and genetic data, using with the Wisconsin Genomics Initiative data set.",Secure Sharing of Clinical History & Genetic Data: Empowering Predictive Pers. Me,8085051,R01LM011028,"['Acute', 'Address', 'Algorithms', 'Caring', 'Clinic', 'Clinical', 'Complex', 'Computer Assisted', 'Computer Security', 'Computer software', 'Computerized Medical Record', 'Computers', 'Confidentiality', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Dose', 'Ensure', 'Environment', 'Evaluation', 'Genetic', 'Genetic Databases', 'Genomics', 'Goals', 'Health', 'Health Personnel', 'Individual', 'Institution', 'Lead', 'Machine Learning', 'Medical', 'Medical Genetics', 'Medical Records', 'Medicine', 'Mining', 'Modeling', 'Operating System', 'Output', 'Patients', 'Privacy', 'Publications', 'Publishing', 'Recording of previous events', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Secure', 'Security', 'Structure', 'System', 'Technology', 'Warfarin', 'Wisconsin', 'Work', 'base', 'data management', 'data mining', 'data sharing', 'design', 'empowered', 'experience', 'laptop', 'meetings', 'novel', 'patient privacy', 'predictive modeling', 'prototype', 'virtual']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2011,588817,-0.015557364748619115
"NLP for Augmentative and Alternative Communication in Adults    DESCRIPTION (provided by applicant):  This proposal relates to the technology of Augmentative and Alternative Communication (AAC).  The research, to be developed over the three-year course of this project, relates to increasing communication speed for adult users of typing-based AAC devices. The proposed method has commonalities both with chatter bots and more sophisticated automated question answering systems. In particular, we propose to develop a program that will mine a very large database of stored interactions for sentences that are similar to the sentence currently being uttered by the interlocutor, and propose a set of plausible responses for the AAC user. The outcome of this research will be a system that improves over the current state of the art in whole utterance approaches in AAC, making use of sophisticated natural language processing techniques.    Through this research and its practical application to helping real people with real communications needs, as well as coursework, seminars, participation in the AAC and disabilities community in Portland, OR, and intensive one-on-one meetings with his mentor Dr. Melanie Fried-Oken, the PI will accrue substantial clinical experience in AAC, and will gain a deep understanding of how technology can be used to help people.      PUBLIC HEALTH RELEVANCE: The proposed project will develop a research program in the field of Augmentative and Alternative Communication. The program proposes to improve the throughput of AAC devices for conversation by modeling dialogue context for literate adult users. Specifically, we will use corpus-based techniques from question answering and chatter bots to select appropriate utterances in response to utterances from interlocutors.              The proposed project will develop a research program in the field of Augmentative and Alternative Communication. The program proposes to improve the throughput of AAC devices for conversation by modeling dialogue context for literate adult users. Specifically, we will use corpus-based techniques from question answering and chatter bots to select appropriate utterances in response to utterances from interlocutors.            ",NLP for Augmentative and Alternative Communication in Adults,8189460,K25DC011308,"['Address', 'Adult', 'Area', 'Clinical', 'Communication', 'Communication Aids for Disabled', 'Communication Disability', 'Communities', 'Computers', 'Data', 'Databases', 'Devices', 'Environment', 'Food', 'Generations', 'Hobbies', 'Interview', 'Length', 'Measures', 'Mentors', 'Methods', 'Mining', 'Modeling', 'Modification', 'Names', 'Natural Language Processing', 'Oregon', 'Outcomes Research', 'Participant', 'Play', 'Questionnaires', 'Recruitment Activity', 'Relative (related person)', 'Research', 'Restaurants', 'Role', 'Savings', 'Self Assessment', 'Simulate', 'Source', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Travel', 'Universities', 'alternative communication', 'base', 'efficacy testing', 'experience', 'improved', 'literate', 'meetings', 'movie', 'novel', 'practical application', 'programs', 'response', 'satisfaction', 'speech recognition', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,K25,2011,162459,-0.02911720896763652
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8055880,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2011,328942,-0.02402649274448676
"Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann    DESCRIPTION (provided by applicant):       A critical element of translating science into practice is the ability to find patient populations for clinical research. Many studies rely on administrative data for selecting relevant patients for studies of comparative effectiveness, but the limitations of administrative data is well-known. Much of the information critical for clinical research is locked in free-text dictated reports, such as history and physical exams and radiology reports. Data repositories, such as the Medical Archival Retrieval System (MARS) at the University of Pittsburgh, are useful for identifying supersets of patients for clinical research studies through indexed word searches. However, simple text-based queries are also limited in their effectiveness, and researchers are often left reading through hundreds or thousands of reports to filter out false positive cases. Current processes are time-consuming and extraordinarily expensive. They lead to long delays between the development of a testable hypothesis and the ability to share findings with the medical community at large.       A potential solution to this problem is pre-annotating de-identified clinical reports to facilitate more intelligent and sophisticated retrieval and review. Clinical reports are rich in meaning and structure and can be annotated at many different levels using natural language processing technology. It is not clear, however, what types of annotations would be most helpful to a clinical researcher, nor is it clear how to display the annotations to best assist manual review of reports. There is interdependence between the annotation schema used by an NLP system and the user interface for assisting researchers in retrieving data for retrospective studies. In this proposal, we will interactively revise an NLP annotation schema as well as explore various methods for annotation display based on feedback from users reviewing patient data for specific research studies.       We hypothesize that an interactive search application that relies on NLP-annotated clinical text will increase the accuracy and efficiency of finding patients for clinical research studies and will support visualization techniques for viewing the data in a way that improves a researcher's ability to review patient data.              Narrative We will develop a novel review application for this proposal that will facilitate translational research from secondary use of EHR data by assisting researchers in more efficiently finding retrospective populations of patients for clinical research studies. The application will rely both on multi-layered annotation of the textual data, using natural langauge processing, and on coordinated views of the patient data.",Interactive Search and Review of Clinical Records with Multi-layered Semantic Ann,8022026,R01LM010964,"['Automated Annotation', 'Clinical', 'Clinical Research', 'Communities', 'Data', 'Databases', 'Development', 'Effectiveness', 'Elements', 'Feedback', 'Imagery', 'Lead', 'Left', 'Manuals', 'Medical', 'Methods', 'Natural Language Processing', 'Outcome', 'Patients', 'Process', 'Property', 'Radiology Specialty', 'Reading', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Retrospective Studies', 'Science', 'Semantics', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Translating', 'Translational Research', 'Universities', 'base', 'comparative effectiveness', 'computer human interaction', 'improved', 'indexing', 'novel', 'patient population', 'research study', 'success']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,591195,-0.058476599998070845
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8044673,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,324859,-0.015386046957030474
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,8306467,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2011,13000,-0.013404996850387759
"Integrated Public Use Microdata Series: North Atlantic Population Project    DESCRIPTION (provided by applicant): Research teams in the United States, Britain, Canada, Iceland, Norway, and Sweden have worked in close coordination to create the North Atlantic Population Project (NAPP), a massive integrated cross-national microdatabase that provides a baseline for studies of demographic change and opens fresh paths for spatiotemporal data analysis. We now propose improvements that will multiply the power of the NAPP infrastructure. We have three major aims: (1) Triple the size of the database to approximately 365 million records, adding 40 new datasets for the period 1787 to 1930 from Albania, Britain, Canada, Denmark, Egypt, Iceland, Ireland, Germany, Norway, Mexico, Sweden, and the United States. (2) Leverage our innovative record-linkage technology to create linked national panels that will allow expanded longitudinal analyses. (3) Connect the past to the present by merging NAPP with the Integrated Public Use Microdata Series (IPUMS), simplifying analysis of long-run change and ensuring long-run preservation and maintenance of the database. The landscape of scientific research on the human population is shifting. It is no longer sufficient just to study the relationships among variables at a particular moment in time. Researchers around the world now recognize that to understand the large-scale processes that are transforming society, we must investigate long-term change. The goal of this project is to provide the infrastructure to make such analysis possible. NAPP will make a strategic contribution to demographic infrastructure by providing a baseline for the study of changes in the demography and health of European and North American populations. In each country, NAPP provides the earliest census microdata available. Models and descriptions based on historical experience underlie both theories of past change and projections into the future. The NAPP data provide a unique laboratory for the study of economic and demographic processes; this kind of empirical foundation is essential for testing social and economic theory. The proposed work will be carried out by a team of highly-skilled researchers with unparalleled expertise and experience in data integration and record linkage. Collaborators include leading researchers from the University of Minnesota and the Max Planck Institute for Demographic Research, and local experts from each of the participating countries. Centralized support for international collaboration will leverage the investments of each country and allow us to create an extraordinary resource for comparative social and economic research.      PUBLIC HEALTH RELEVANCE: The North Atlantic Population Project (NAPP) provides fundamental infrastructure for scientific research, education, and policy-making and will allow social scientists to make comparisons across Northern Europe, the Mediterranean, and North America during three centuries of transformative change. The proposed work is directly relevant to the central mission of the NIH as the steward of medical and behavioral research for the nation: the new data will advance fundamental knowledge about the nature and behavior of human population dynamics. NAPP will unlock access to some of the largest and longest-running cross-sectional and longitudinal data sources in the world, and stimulate health-related research on population growth and movement, fertility, mortality, nuptiality, and family change, as well as the economic and social correlates of demographic behavior.           The North Atlantic Population Project (NAPP) provides fundamental infrastructure for scientific research, education, and policy-making and will allow social scientists to make comparisons across Northern Europe, the Mediterranean, and North America during three centuries of transformative change. The proposed work is directly relevant to the central mission of the NIH as the steward of medical and behavioral research for the nation: the new data will advance fundamental knowledge about the nature and behavior of human population dynamics. NAPP will unlock access to some of the largest and longest-running cross-sectional and longitudinal data sources in the world, and stimulate health-related research on population growth and movement, fertility, mortality, nuptiality, and family change, as well as the economic and social correlates of demographic behavior.         ",Integrated Public Use Microdata Series: North Atlantic Population Project,8105637,R01HD052110,"['Albania', 'American', 'Behavior', 'Behavioral Research', 'Biological Preservation', 'Canada', 'Censuses', 'Code', 'Collaborations', 'Collection', 'Communication', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Demographic Transitions', 'Demography', 'Denmark', 'Documentation', 'Economic Development', 'Economics', 'Education', 'Egypt', 'England', 'Ensure', 'European', 'Family', 'Fertility', 'Foundations', 'Funding', 'Future', 'Germany', 'Goals', 'Health', 'Human', 'Iceland', 'Immigration', 'Individual', 'Institutes', 'Institution', 'International', 'Investigation', 'Investments', 'Ireland', 'Knowledge', 'Laboratory Study', 'Link', 'Machine Learning', 'Maintenance', 'Mediation', 'Medical Research', 'Metadata', 'Mexico', 'Minnesota', 'Mission', 'Modeling', 'Nature', 'North America', 'Northern Europe', 'Norway', 'Nuptiality', 'Online Systems', 'Policy Making', 'Population', 'Population Characteristics', 'Population Growth', 'Private Sector', 'Process', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sampling', 'Scientist', 'Scotland', 'Series', 'Social Mobility', 'Societies', 'Subgroup', 'Sum', 'Sweden', 'Technology', 'Testing', 'Time', 'Time Study', 'United States', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'comparative', 'critical period', 'data integration', 'data mining', 'experience', 'human population dynamics', 'improved', 'innovation', 'mortality', 'population movement', 'social', 'spatiotemporal', 'success', 'theories', 'tool']",NICHD,UNIVERSITY OF MINNESOTA,R01,2011,563921,-0.04550892046343859
"The Cardiovascular Research Grid    DESCRIPTION (provided by applicant):       The Cardiovascular Research Grid (CVRG) Project is an R24 resource supporting the informatics needs of the cardiovascular (CV) research community. The CVRG Project has developed and deployed unique core technology for management and analysis of CV data that is being used in a broad range of Driving Biomedical Projects (DBFs). This includes: a) tools for storing and managing different types of biomedical data; b) methods for securing the data; c) tools for querying combinafions of these data so that users may mine their data for new knowledge; d) new statistical learning methods for biomarker discovery; e) novel tools that analyze image data on heart shape and motion to discover biomarkers that are indicative of disease; f) tools for managing, analyzing, and annotafing ECG data. All of these tools are documented and freely available from the CVRG website and Wiki. In this renewal, we propose a set of new projects that will enhance the capability of our users to explore and analyze their data to understand the cause and treatment of heart disease. Each project is motivated directly by the needs of one or more of our DBFs. Project 1 will develop and apply new algorithms for discovering changes in heart shape and mofion that can predict the early presence of developing heart disease in fime for therapeufic intervenfion. Project 2 will create data management systems for storing CV image data collected in large, multi-center clinical research studies, and for performing quality control operations that assure the integrity of that data. Project 3 will develop a complete infrastructure for managing and analyzing ECG data. Project 4 will develop a comprehensive clinical informafics system that allows clinical informafion to be linked with biomedical data collected from subjects. Project 5 will develop tools by which non-expert users can quickly assemble new procedures for analyzing their data. Project 6 will put in place a project management structure that will assure successful operation of the CVRG.  RELEVANCE: The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store, manage, and analyze data on the structure and function of the cardiovascular system in health and disease. The CVRG Project has developed and deployed unique technology that is now being used in a broad range of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to explore and analyze their data to understand the cause and treatment of heart disease. (End of Abstract)          The Cardiovascular Research Grid (CVRG) Project is a national resource providing the capability to store,  manage, and analyze data on the structure and function of the cardiovascular system in health and disease.  The CVRG Project has developed and deployed unique technology that is now being used in a broad range  of studies. In this renewal, we propose to develop new tools that will enhance the ability of researchers to  explore and analvze their data to understand the cause and treatment of heart disease.",The Cardiovascular Research Grid,8017600,R24HL085343,"['Address', 'Algorithms', 'Archives', 'Atlases', 'Automobile Driving', 'Biological Markers', 'Cardiac', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Research', 'Common Data Element', 'Communities', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Discrimination', 'Disease', 'Electrocardiogram', 'Health', 'Health Insurance Portability and Accountability Act', 'Heart', 'Heart Diseases', 'Hybrids', 'Image', 'Image Analysis', 'Informatics', 'Information Management', 'Institutional Review Boards', 'International', 'Knowledge', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Metadata', 'Methods', 'Mining', 'Monoclonal Antibody R24', 'Motion', 'Ontology', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Policies', 'Procedures', 'Process', 'Property', 'Protocols documentation', 'Quality Control', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Shapes', 'Site', 'Speed', 'Structure', 'System', 'Systems Integration', 'Technology', 'Testing', 'Time', 'Ultrasonography', 'Work', 'abstracting', 'base', 'cardiovascular imaging', 'cluster computing', 'computational anatomy', 'computerized data processing', 'data integration', 'data integrity', 'data management', 'data modeling', 'disease phenotype', 'flexibility', 'imaging informatics', 'insight', 'interdisciplinary collaboration', 'neuroimaging', 'new technology', 'novel', 'operation', 'performance site', 'rapid technique', 'research study', 'technology development', 'tool', 'validation studies', 'web site', 'wiki', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2011,2241978,-0.0084352008356353
"Free Text Gene Name Recognition 1) We have become convinced that more information about the different types of entities that can occur in sentences in MEDLINE can be used to improve name recognition. This has led us to design a set of semantic categories and to attempt to fill these categories with actual names that can be harvested from databases and from web sites. We call the result SEMCAT. It currently recognizes seventy-five categories and contains about four million name strings distributed over those categories. We have experimented with probabilistic context free grammars and Markov models of text strings in an attempt to learn how to recognize the entities in different categories.  However, the best approach we have found for distinguishing the categories of gene/protein and not gene/protein is a new algorithm we term a priority model.  Every token associated with any name in SEMCAT has associated with it two probabilities. The first probability is the probability that the token indicates that it is part of a gene/protein name and the second probability is an indicator of how reliable the token is as an indicator. With this model, given a phrase, one can compute an estimate of the probability that the phase is a gene/protein name. We find that with the priority model we can achieve an F score of 96% as compared with 95% for our best PCFG approach. (with Lorrie Tanabe). The top performance for gene mention recognition in BioCreative II was by Rie Ando from IBM who introduced a technique called alternating structural optimization. This approach takes many labeling problems similar to named entity tagging, but simply tries to predict the occurrence of the names or the tokens from the surrounding textual context. When the SVM solution weight vectors for these many auxiliary problems have been learned, one performs a singular value decomposition and subtracts from each vector its first h components in the decomposition. This subtraction is only used to decrease the penalty in the regularization term of the cost function. The weight vectors are then relearned and the process is repeated. This is continued until convergence. The final result is a set of h components of the decomposition of the many weight vectors. One uses these components to enhance the learning on the actual named entity recognition task. This is a bit complicated and difficult to use. We are studying how we may be able to use a similar approach, but with a simpler method of applying the auxiliary learning to improve named entity recognition. One problem is how to combine such auxiliary learning with the SEMCAT data. We are currently working to improve this model by finding a way to apply it to more than two classes at a time. 2)We recently co-chaired the BioCreative III Workshop in which the main competitive tasks were to find gene mentions in a full text article and map them to their GenBank identifiers and score them as to reliability, to classify PubMed records as likely to represent articles containing information on protein-protein interactions, and to find the text in full papers that describes the method used by an experimenter to experimentally verify a protein-protein interaction. We organized the first of these task and participated in the second. In the second task we used the priority model to locate protein mentions and it proved very successful and competitive with other approaches. 3) We are currently working to develop more general methods of finding high value articles for PPI based on their abstracts. This effort involves not only more powerful ranking methods, but also ways to display evidence to the user for a users quick evaluation. n/a",Free Text Gene Name Recognition,8344950,ZIALM000093,"['Algorithms', 'Biology', 'Categories', 'Data', 'Databases', 'Educational workshop', 'Evaluation', 'Genbank', 'Gene Proteins', 'Genes', 'Genetic', 'Harvest', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Paper', 'Performance', 'Phase', 'Probability', 'Process', 'Proteins', 'Proteomics', 'PubMed', 'Records', 'Semantics', 'Solutions', 'Techniques', 'Text', 'Time', 'Variant', 'Weight', 'Work', 'abstracting', 'base', 'cost', 'design', 'improved', 'markov model', 'phrases', 'protein protein interaction', 'research study', 'vector', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2011,179884,-0.022416153793070995
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,-0.012993268554043225
"National Biomedical Information Services Delivering Biomedical Information Services  In FY11, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public, including significant expansion in mobile applications and open data initiatives via API access. Among the NLM's intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLM's premier bibliographic database with over 20 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY11, more than 800,000 new indexed citations were added, plus 48,000 historical citations from 1946.  PubMed Central: The PubMed Central archive of over 2 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports. In FY11, PMC's Journal Article Tag Suite (JATS), a set of standards to publish, author, archive or exchange journal articles, was formally issued as a draft NISO standard, and the process for final acceptance as an ANSI standard is underway.  MedlinePlus and MedlinePlus en espanol: These resources include consumer health information on more than 900 topics, in more than 40 languages. MedlinePlus Connect, a service linking patients or providers in electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use, was launched in FY11.  Clinical Trials: ClinicalTrials.gov covers more than 114,000 clinical research studies in more than 177 countries, with hundreds added each week. It also contains reports of summary results and adverse effects, in accordance with the FDA Amendments Act of 2007 (PL 110-85). In FY11, nearly 17,000 new trials were registered.  Summary results, including adverse events, of more than 1600 trials were also added, bringing the total number of summary results to nearly 5500.  Toxicology and Environmental Health:  Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, the Toxic Release Inventory, and the Hazardous Substances Data Bank (HSDB). In FY11, an Environmental Health Student Portal was developed to introduce middle school students to environmental health science within the context of science curriculum standards.  Drug Information Resources: NLM's drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for nearly 30,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to RxNorm standard drug names. Disaster Preparedness and Response:  NLM's Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A collaboration with the Bethesda Hospital Emergency Preparedness Partnership provides backup communication systems and tools for patient tracking, information access, and responder training. The Chemical Hazards Emergency Medical Management (CHEMM) service was launched in FY11, providing a comprehensive resource to help responders to chemical emergencies make safer decisions and provide them with the right information when it is needed most. Specialized resources were also developed in response to the Japan earthquake, tsunami, and radiation event.  Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI information resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, UniGene, LocusLink, Genomes, and the NCBI software toolkit. NCBI also produces the retrieval systems for PubMed, PubMed Central, and the Books database. Continuing areas of emphasis in FY11 included addressing the impact of enormous quantities of data emanating from high throughput sequencing, microarray, and small molecule screening techniques; organizing data from large-scale clinical studies involving genotyping; and enhancing database interfaces to facilitate search and discovery across multiple resources.  Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. NLM outreach programs enhance awareness of its information services, with emphasis on underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY11, dozens of community-based projects were funded. Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 8000 health services research projects from more than 110 funding organizations; HSRR, a  database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2020 objectives. Advanced Information Systems, Data Standards and Research Tools  In FY11, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; re-designed an extensive suite of software tools for image analysis; applied natural language processing methods to extract information from biomedical literature; improved standardized reporting of genetic variations and clinical interpretation of genetic test results; and designed improved methods for integrated search and discovery across multiple databases.  Health Data Standards: As the central coordinating body for clinical terminology standards within DHHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure. NLM develops or licenses key clinical terminologies and problem lists designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 10 million concept names from more than 160 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY11, a UMLS Terminology Services web site was developed to facilitate use of NLMs growing suite of vocabulary resources. n/a",National Biomedical Information Services,8345008,ZIHLM200888,"['Address', 'Administrator', 'Adoption', 'Adverse effects', 'Adverse event', 'African American', 'Amendment', 'Archives', 'Area', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biotechnology', 'Books', 'Characteristics', 'Chemicals', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disasters', 'Earthquakes', 'Educational Curriculum', 'Effectiveness', 'Electronic Health Record', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Event', 'Family', 'Fostering', 'Funding', 'Genbank', 'General Population', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Guidelines', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'Image', 'Image Analysis', 'Imagery', 'Improve Access', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Services', 'Information Specialists', 'Information Systems', 'International', 'Intervention', 'Intramural Research Program', 'Japan', 'Journals', 'Label', 'Laboratories', 'Language', 'Libraries', 'Licensing', 'Link', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'MeSH Thesaurus', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methods', 'Minority', 'Molecular Biology', 'Names', 'Native Americans', 'Natural Language Processing', 'Nomenclature', 'Package Insert', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Poison Control Centers', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'Provider', 'PubMed', 'Public Health', 'Publishing', 'Radiation', 'Readiness', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Science', 'Screening procedure', 'Services', 'Software Tools', 'Solid', 'Speed', 'Structure', 'Students', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Technology Assessment', 'Terminology', 'Test Result', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Tsunami', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vocabulary', 'base', 'biomedical informatics', 'biomedical information system', 'comparative effectiveness', 'design', 'dosage', 'drug market', 'drug standard', 'hazard', 'health disparity', 'health information technology', 'health literacy', 'improved', 'indexing', 'inner city', 'instrument', 'journal article', 'knowledge base', 'language processing', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'middle school', 'outreach', 'outreach program', 'research and development', 'research study', 'response', 'scientific computing', 'small molecule', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2011,241172313,-0.008002590639953247
"National Biomedical Information Services Delivering Biomedical Information Services  In FY11, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public, including significant expansion in mobile applications and open data initiatives via API access. Among the NLM's intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLM's premier bibliographic database with over 20 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY11, more than 800,000 new indexed citations were added, plus 48,000 historical citations from 1946.  PubMed Central: The PubMed Central archive of over 2 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports. In FY11, PMC's Journal Article Tag Suite (JATS), a set of standards to publish, author, archive or exchange journal articles, was formally issued as a draft NISO standard, and the process for final acceptance as an ANSI standard is underway.  MedlinePlus and MedlinePlus en espanol: These resources include consumer health information on more than 900 topics, in more than 40 languages. MedlinePlus Connect, a service linking patients or providers in electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use, was launched in FY11.  Clinical Trials: ClinicalTrials.gov covers more than 114,000 clinical research studies in more than 177 countries, with hundreds added each week. It also contains reports of summary results and adverse effects, in accordance with the FDA Amendments Act of 2007 (PL 110-85). In FY11, nearly 17,000 new trials were registered.  Summary results, including adverse events, of more than 1600 trials were also added, bringing the total number of summary results to nearly 5500.  Toxicology and Environmental Health:  Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, the Toxic Release Inventory, and the Hazardous Substances Data Bank (HSDB). In FY11, an Environmental Health Student Portal was developed to introduce middle school students to environmental health science within the context of science curriculum standards.  Drug Information Resources: NLM's drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for nearly 30,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to RxNorm standard drug names. Disaster Preparedness and Response:  NLM's Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A collaboration with the Bethesda Hospital Emergency Preparedness Partnership provides backup communication systems and tools for patient tracking, information access, and responder training. The Chemical Hazards Emergency Medical Management (CHEMM) service was launched in FY11, providing a comprehensive resource to help responders to chemical emergencies make safer decisions and provide them with the right information when it is needed most. Specialized resources were also developed in response to the Japan earthquake, tsunami, and radiation event.  Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI information resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, UniGene, LocusLink, Genomes, and the NCBI software toolkit. NCBI also produces the retrieval systems for PubMed, PubMed Central, and the Books database. Continuing areas of emphasis in FY11 included addressing the impact of enormous quantities of data emanating from high throughput sequencing, microarray, and small molecule screening techniques; organizing data from large-scale clinical studies involving genotyping; and enhancing database interfaces to facilitate search and discovery across multiple resources.  Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. NLM outreach programs enhance awareness of its information services, with emphasis on underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY11, dozens of community-based projects were funded. Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 8000 health services research projects from more than 110 funding organizations; HSRR, a  database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2020 objectives. Advanced Information Systems, Data Standards and Research Tools  In FY11, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; re-designed an extensive suite of software tools for image analysis; applied natural language processing methods to extract information from biomedical literature; improved standardized reporting of genetic variations and clinical interpretation of genetic test results; and designed improved methods for integrated search and discovery across multiple databases.  Health Data Standards: As the central coordinating body for clinical terminology standards within DHHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure. NLM develops or licenses key clinical terminologies and problem lists designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 10 million concept names from more than 160 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY11, a UMLS Terminology Services web site was developed to facilitate use of NLMs growing suite of vocabulary resources. n/a",National Biomedical Information Services,8345008,ZIHLM200888,"['Address', 'Administrator', 'Adoption', 'Adverse effects', 'Adverse event', 'African American', 'Amendment', 'Archives', 'Area', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biotechnology', 'Books', 'Characteristics', 'Chemicals', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disasters', 'Earthquakes', 'Educational Curriculum', 'Effectiveness', 'Electronic Health Record', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Event', 'Family', 'Fostering', 'Funding', 'Genbank', 'General Population', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Guidelines', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'Image', 'Image Analysis', 'Imagery', 'Improve Access', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Services', 'Information Specialists', 'Information Systems', 'International', 'Intervention', 'Intramural Research Program', 'Japan', 'Journals', 'Label', 'Laboratories', 'Language', 'Libraries', 'Licensing', 'Link', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'MeSH Thesaurus', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methods', 'Minority', 'Molecular Biology', 'Names', 'Native Americans', 'Natural Language Processing', 'Nomenclature', 'Package Insert', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Poison Control Centers', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'Provider', 'PubMed', 'Public Health', 'Publishing', 'Radiation', 'Readiness', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Science', 'Screening procedure', 'Services', 'Software Tools', 'Solid', 'Speed', 'Structure', 'Students', 'Summary Reports', 'System', 'Techniques', 'Technology', 'Technology Assessment', 'Terminology', 'Test Result', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Tsunami', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vocabulary', 'base', 'biomedical informatics', 'biomedical information system', 'comparative effectiveness', 'design', 'dosage', 'drug market', 'drug standard', 'hazard', 'health disparity', 'health information technology', 'health literacy', 'improved', 'indexing', 'inner city', 'instrument', 'journal article', 'knowledge base', 'language processing', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'middle school', 'outreach', 'outreach program', 'research and development', 'research study', 'response', 'scientific computing', 'small molecule', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2011,2218125,-0.008002590639953247
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8138590,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2011,356340,-0.03838203176353183
"Continued Development and Evaluation of caTIES    DESCRIPTION (provided by applicant): We propose to further develop, test, evaluate and support caTIES - an existing software system for developing networked repositories of sharable de-identified surgical pathology reports. The caTIES system creates a repository of de-identified, structured, and concept-coded clinical reports derived from large corpora of clinical free-text. Documents are automatically coded against a controlled terminology such as the Unified Medical Language System (UMLS), SNOMED-CT, or NCI Metathesaurus. Users construct queries to identify specific kinds of documents and tissue specimens based on the associated clinical report. For example, a researcher studying genetic variation in metastatic breast cancers can identify cases of invasive ductal carcinoma of the breast, followed by metastatic ductal cancer in bone at an interval of three years or greater from the original diagnosis. The caTIES system also supports acquisition and ordering of tissues, using an honest broker model. Through this mechanism, de-identified data and access to tissue can be shared among institutions, enabling multi-center collaborative research. The caTIES system has already been implemented at seven US Cancer Centers, and is being considered for adoption by numerous other institutions including cancer centers, university hospitals and private hospitals. Initial development of caTIES was funded by the Cancer Biomedical Informatics Grid (caBIG). However, interest in the application has far exceeded our expectations and the limitations of caBIG. This grant will allow us to further extend the capabilities of the system by (a) improving the portability of the system and extending the types of documents that can be processed, (b) evaluating the system's NLP performance and usability, (c) building a user community to support this open-source application, and (d) piloting interoperability of caTIES with other enterprise and research systems. This work will preserve and extend a highly novel platform for development of massive repositories of de-identified clinical data that can be used for research within and across institutions. PUBLIC HEALTH RELEVANCE: This grant will fund the further development and evaluation of a system that takes identified clinical documents and converts them into de-identified, concept-coded, structured data. The system enables researchers to access remainder tissues and clinical report data for research purposes within and across institution. This project is important because it will greatly increase the access of researchers to important data and materials while maintaining patient privacy.          n/a",Continued Development and Evaluation of caTIES,7999244,R01CA132672,"['Access to Information', 'Adoption', 'Cancer Center', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Communication', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Data Reporting', 'Database Management Systems', 'Development', 'Diagnosis', 'Documentation', 'Ductal', 'Electronic Mail', 'Eligibility Determination', 'Environment', 'Evaluation', 'Funding', 'Future', 'Genetic Variation', 'Genomics', 'Grant', 'Health', 'Information Retrieval', 'Institution', 'Malignant Neoplasms', 'Methods', 'Metric', 'Modeling', 'Modification', 'Natural Language Processing', 'Operating System', 'Pathology Report', 'Performance', 'Private Hospitals', 'Process', 'Report (document)', 'Reporting', 'Research', 'Research Personnel', 'Services', 'Specimen', 'Structure', 'Surgical Pathology', 'System', 'Systematized Nomenclature of Medicine', 'Terminology', 'Testing', 'Text', 'Tissues', 'Training', 'Translational Research', 'Unified Medical Language System', 'University Hospitals', 'Vocabulary', 'Work', 'base', 'bone', 'cancer Biomedical Informatics Grid', 'clinical phenotype', 'computer human interaction', 'ductal breast carcinoma', 'expectation', 'flexibility', 'improved', 'interest', 'interoperability', 'malignant breast neoplasm', 'meetings', 'metathesaurus', 'novel', 'open source', 'patient privacy', 'portability', 'repository', 'software development', 'software systems', 'systems research', 'tool', 'usability']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,304932,-0.048103796657499774
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,-0.009113286981200092
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,8137721,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Clinical assessments', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Health', 'Health Personnel', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Imagery', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'data modeling', 'graphical user interface', 'hemodynamics', 'imaging informatics', 'improved', 'innovation', 'insight', 'mortality', 'network models', 'patient population', 'prognostic', 'programs', 'prospective', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2011,581841,0.0021754796037888485
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in four subprojects which use natural language processing techniques:  1) The presence of unrecognized abbreviations in text hinders indexing algorithms and adversely affects information retrieval and extraction.  Automatic abbreviation definition identification can help resolve these issues.  However, abbreviations and their definitions identified by an automatic process are of uncertain validity.  Due to the size of databases such as MEDLINE only a small fraction of abbreviation-definition pairs can be examined manually.  An automatic way to estimate the accuracy of abbreviation-definition pairs extracted from text is needed.  We have proposed an abbreviation definition identification algorithm that employs a variety of strategies to identify the most probable abbreviation definition.  In addition our algorithm produces an accuracy estimate, pseudo-precision, for each strategy without using a human-judged gold standard. The pseudo-precisions determine the order in which the algorithm applies the strategies in seeking to identify the definition of an abbreviation. The results are generally a couple of percentage points better than the Schwartz-Hearst algorithm and also allow one to enforce a threshold for those applications where high precision is critical. In recent work we are extending this approach using machine learning to apply it to more abbreviation instances. 2) We are studying paraphrases in MEDLINE abstracts. These come about because an author is describing some entity of interest and uses a phrase like ""drug abuse"" and then needing to describe the same entity again a sentence or two latter does not wish to use exactly the same wording again and may use a variant of the phrase such as ""drug use"" which in the context of ""drug abuse"" has substantially the same meaning.  3) An author disambiguation algorithm has been developed which relies on machine learning based on the assumption that if an author name is infrequent in the data it probably represents the same person in for all documents where it is found. This gives us positive instances. Negative instances are sampled from pairs of documents that have no author in common. Such positive and negative data allows us to do machine learning on all aspects of the document other than the name in question. This allows us to learn how to weight this data for best performance in distinguishing the positive and negative instances from each other. This learning is then applied in individual name cases or spaces to determine which author document pairs represent the same author. n/a",Natural Language Processing Techniques To Enhance Information Access.,8149603,ZIALM000090,"['Natural Language Processing', 'Techniques']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2010,450501,-0.030912489583062575
"Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens    DESCRIPTION (provided by applicant): We appreciate the time and effort spent by all the reviewers, and we are grateful for the useful comments and provided suggestions. We have carefully reviewed the critiques and we are happy to see that the panel was receptive to our proposal. The reviewers expressed three major concerns in the summary statement: (1) although the investigating team is well qualified our history of collaboration is short; (2) details regarding the practical constraints of the BARDOT system are lacking; (3) the machine learning techniques employed in the project are considered fairly standard.  Below we briefly discuss the reviewers comments and indicate how we have changed our revised application to address the critique.  (1) Dr. Dundar moved from industry to academia in the fall of 2008, at which point Dr. Rajwa (one of the original inventors of BARDOT) and Dr. Dundar began their collaboration on new approaches to the problem of non-exhaustively defined classes in phenotypic screening. This scientific partnership immediately produced interesting results, and at the time of submission of the original application, Dr. Dundar and Dr. Rajwa had their first manuscript under review. The approach presented in the original proposal was tested and the results were submitted to the ACM 15th Annual SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'09), which is the largest and one of the most respected conferences in this field. The manuscript was accepted after a full peer review as one of the 50 regular papers selected from 551 submissions [20]. Following the proposal submission, research efforts continued and produced yet another approach to the problem described in this grant application. The preliminary findings are reported in a new manuscript which is currently under review [4].  (2) We rewrote the background and research methods sections of our proposal to include information re- quested by the reviewers regarding practical aspects of the BARDOT system, such as accuracy issues (Section D.3.2), frequency of encountering new, unknown classes (Section B.3.1), and validation (Section D.3.1).  (3) The problem of phenotypic screening and classification of bacteria can be defined within exhaustive (stan- dard) or non-exhaustive learning frameworks. Although we agree that the implementation of an exhaustive clas- sification approach for BARDOT does require only fairly standard tools, the problem of the non-exhaustive nature of training libraries cannot be addressed by straightforward use of any textbook-level technique. In fact, the presence of non-exhaustively defined set of classes violates basic assumptions for most supervised learning systems. The issue of non-exhaustively defined classes is the major obstacle for application of machine learning in phenotypic analysis since the number of possible phenotypes may be infinite. In our original proposal we argued that learning with a non-exhaustively defined set of classes remains a very challenging problem, and presented evidence demonstrating that simple extensions of standard techniques cannot provide an acceptable solution. Subsequently, we proposed a new approach based on Bayesian simulation of classes and showed that preliminary results outperformed benchmark techniques [4].  Although these initial results looked promising, we did not consider the described preliminary algorithms final and definitive, and we do not believe that at this point we are able to provide an exact algorithmic solution to this complex problem. If we were able to do that, it would mean that we had already accomplished all the grant goals. The very essence of the proposed research is finding the answer to the defined problem, and the answer will remain unknown until after the work has been done. However, positive reviews and an acceptance of our work by KDD'09 conference judges, tell us that we are heading in the right direction.  In the amended version of this application we propose a modified Bayesian approach based on Wishart priors (Section D.2.3). The algorithm creates new classes on the fly and evaluates maximum likelihood with the updated set of classes, gradually improving detection accuracy for future samples. We believe that this offers a substantial improvement over the previous method. Consequently, the preliminary results in Section C are updated to reflect our progress. Since the modified technique allows for classification with non-exhaustive and exhaustive sets using the same algorithm, we consolidated the previous specific aims 3 and 5 into one in the revised application.      PUBLIC HEALTH RELEVANCE: A Machine Learning Approach to Label-free Detection of Bacterial Pathogens  using Laser Light Scattering  PIs: Dr. M. Murat Dundar and Dr. Bartek Rajwa Successful implementation of this project will allow for a label-free detection and identification of food pathogens and their mutated subclasses not yet seen earlier. This will reduce the number of food related outbreaks and will help secure public food supply.           Public Health Relevance Title: A Machine Learning Approach to Label-free Detection of Bacterial Pathogens  using Laser Light Scattering  PIs: Dr. M. Murat Dundar and Dr. Bartek Rajwa Successful implementation of this project will allow for a label-free detection and identification of food pathogens and their mutated subclasses not yet seen earlier. This will reduce the number of food related outbreaks and will help secure public food supply.",Machine-Learning Approach to Label-free Detection of new Bacterial Pathogens,7896355,R21AI085531,"['Academia', 'Accounting', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Bacteria', 'Benchmarking', 'Biochemical Process', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Classification', 'Collaborations', 'Complex', 'Computer Vision Systems', 'Critiques', 'Data Set', 'Detection', 'Disease', 'Disease Outbreaks', 'Escherichia coli', 'Food', 'Food Supply', 'Frequencies', 'Future', 'Genus staphylococcus', 'Goals', 'Grant', 'Head', 'Industry', 'Infection', 'International', 'Knowledge', 'Label', 'Lasers', 'Learning', 'Libraries', 'Listeria', 'Machine Learning', 'Manuscripts', 'Medical', 'Methods', 'Modeling', 'Mutate', 'Mutation', 'Nature', 'Optics', 'Paper', 'Pathogenicity', 'Pattern', 'Pattern Recognition', 'Peer Review', 'Phenotype', 'Process', 'Productivity', 'Public Health', 'Published Comment', 'Qualifying', 'Reagent', 'Recording of previous events', 'Reporting', 'Research', 'Research Methodology', 'Safety', 'Salmonella', 'Sampling', 'Screening procedure', 'Secure', 'Serotyping', 'Solutions', 'Suggestion', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Textbooks', 'Time', 'Training', 'Update', 'Validation', 'Vibrio', 'Work', 'base', 'cost', 'data mining', 'falls', 'fly', 'foodborne', 'foodborne pathogen', 'image processing', 'improved', 'interest', 'light scattering', 'new technology', 'novel strategies', 'optical sensor', 'pathogen', 'pathogenic bacteria', 'public health relevance', 'rapid detection', 'sensor', 'simulation', 'symposium', 'tool']",NIAID,INDIANA UNIV-PURDUE UNIV AT INDIANAPOLIS,R21,2010,234831,-0.021214484528001308
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method of interest is the method known as sparse coding, which is  the basis of self-taught learning. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,8149602,ZIALM000089,"['Bioinformatics', 'Machine Learning']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2010,470088,0.005040959865546382
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7936991,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,BOSTON CHILDREN'S HOSPITAL,RC1,2010,491408,-0.01931971857907716
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.  1)	In the literature on the Naive Bayes machine learning method there have long been two models that have been used. One is the multivariate Bernoulli model (MBM) and the other the multinomial model (MM).  The MBM method only counts the presence or absence of a feature, while the MM method counts the number of times a feature appears in a record. A number of comparisons of the two approaches have been made in the area of text categorization and the MM approach has usually won out. It is our belief that the reason for this is that the MBM model has not been properly optimized. In fact we have found that in the area of text categorization the local term frequency contributes virtually nothing to the performance of the MM model. In support of this contention we have developed a simplified form of the MM model which ignores local term frequency (but is still much closer to the MM than to the MBM) and we find that it performs essentially the same as the MM model. In fact we do not find an advantage for the use of local term frequency in text categorization using MM or in several other models including the SVM. The advantage to ignoring local term frequency is that it greatly simplifies the data storage and the calculations when applying the Naive Bayes approach to a very large database such as PubMed. One aspect of this work which calls for further investigation is what happens when the records are long and the local frequencies can then be much larger. We do not have the final answer, but  our initial work with the TREC genomics data (160,000 full text documents) suggests that there is a small advantage in the use of local term frequencies in some models, but the advantage is not in any case over about a 3% improvement in break even point.  2)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 3)	The PubMed database presents a unique challenge because of its very large size of over 19 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  4) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with theresults of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. n/a",Automatic Bayesian Methods In Text Retrieval,8149591,ZIALM000021,"['Bayesian Method', 'Retrieval', 'Text']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2010,137109,-0.010015879032300973
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7851323,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2010,278345,-0.03083073596140968
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7945336,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'abstracting', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs']",NICHD,SEQUOIA FOUNDATION,RC2,2010,1397584,-0.0044115889843305
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7945336,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'abstracting', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs']",NICHD,SEQUOIA FOUNDATION,RC2,2010,1,-0.0044115889843305
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7784533,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,341606,-0.02402649274448676
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,8056227,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical data warehouse', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2010,177422,-0.02402649274448676
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7754089,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2010,2037327,-0.0020349760302730757
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,8115481,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'operation', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,99989,-0.017913058313874537
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,8129905,R43HG005046,"['Address', 'Benefits and Risks', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2010,35000,-0.004923962430068353
"Online Social Networking as an Alternative Information Source for Clinical Resear    DESCRIPTION (provided by applicant): Clinical trials and patient records have been the main information sources for clinical research. While well- designed clinical trials can produce high quality data, they are generally very expensive and time consuming. Prior studies have also shown that patients enrolled in clinical trials are not necessarily representative of the general patient population. Chart reviews, which rely on the patient records, avoid some of the drawbacks of the clinical trials approach. Although chart review studies are more labor intensive, new developments in structured data entry and natural language processing (NLP) are helping to automate the process. However, studies which use chart reviews are limited by the accuracy and completeness of the data in the records.       In the past decade, online social networks have grown exponentially. Some health-focused social network sites have attracted large numbers of users and begun accumulating large quantities of detailed clinical information. The PatientsLikeMe site, for instance, has about 3,200 amyotrophic lateral sclerosis (ALS) patients worldwide, and includes about 5% of the ALS population in the US. Information gathered by online social networks is primarily intended for patients to share with each other. Such information has also begun to attract the attention of medical researchers.[3, 4]       Because using information from online social networks for medical research is a fairly new phenomenon, the value and limitation of this type of information source have not been systematically examined. To do so, we propose to conduct a comparison study of patient-contributed information from PatientsLikeMe and records from a large medical record data repository - the Research Patient Data Registry (RPDR) of the Partners Healthcare Systems. The proposed study will focus on ALS, multiple sclerosis (MS), and Parkinson's disease (PD). The general goal is to explore how the medical record and online networking data differ, and if and how online networking data could complement the medical record data. The specific aims are:    1) Extract symptom and treatment information from the two different data sources.    2) Compare the prevalence of symptoms and treatments from the two information sources and analyze the difference.    3) Extract treatment response of prescription medications from PatientsLikeMe and analyze the confounding effect of the misunderstanding of medication indication.      PUBLIC HEALTH RELEVANCE: The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.           The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.",Online Social Networking as an Alternative Information Source for Clinical Resear,7941839,R21NS067463,"['Amyotrophic Lateral Sclerosis', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Communities', 'Comparative Study', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Enrollment', 'Frequencies', 'Goals', 'Healthcare Systems', 'Medical Records', 'Medical Research', 'Multiple Sclerosis', 'Natural Language Processing', 'Nature', 'Parkinson Disease', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Prevalence', 'Process', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Site', 'Source', 'Structure', 'Symptoms', 'System', 'Text', 'Time', 'Update', 'experience', 'information gathering', 'medical attention', 'patient population', 'public health relevance', 'social networking website', 'statistics', 'treatment response', 'web-based social networking']",NINDS,UNIVERSITY OF UTAH,R21,2010,233374,-0.043132214448643395
"Collective Intelligence, Knowledge Infrastructure, & High Performance Computing The Collective Intelligence, Knowledge Infrastructure, and High Performance Program, which  operates within the High Performance Computing and Informatics Office (HPCIO), Division of Computational Bioscience of CIT, is collaborating with NIH investigators to build a critical mass in collective intelligence that is envisioned to encompass a number of pertinent and related disciplines in biomedical research including semantic interoperability, knowledge engineering, computational linguistics, text and data mining, natural language processing, machine learning, and visualization.  The program is intended to foster advances in critical domains at NIH including biomedical and clinical informatics, translational research, proteomics research, genomics, systems biology, and portfolio analysis.  In 2009, collaborations in support of these goals included the following.  - The human salivary protein catalog has been made available online on a community-based Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. This is a major step towards the discovery and use of saliva biomarkers to diagnose oral and systemic diseases.   - HPCIO has developed context-sensitive text-mining methodology for identifying High-Risk, High-Reward (HRHR) research based from NIH Summary Statements.  The method, which uses natural language processing to parse text and classify documents, has been successful in retrospective analysis of the most recent five-years summaries.  This work is being conducted in collaboration with Division of Program Coordination, Planning, and Strategic Initiatives (DPCPSI), NIH Office of the Director (OD).   - HPCIO is developing a corpus of annotated NIH medical records for use in developing methods of document de-identification.  The goal is to create a gold standard that de-identification algorithms for use at NIH can be measured against.  This work, in collaboration with the Clinical Center, will enhance the availability of medical records stored within the Biomedical Translational Research Information System.    - HPCIO is working with the caBIG Clinical Trial Management System Workspace (in collaboration with NCI) to develop a Protocol Lifecycle Tracking (PLT) tool.  By providing real-time protocol status information on all relevant trials to clinicians and researchers, bottlenecks and latencies in protocol management can be identified and corrected by those responsible for conduct of a trail and the overall success of a clinical trial program.   - As a component of the Molecular Libraries Roadmap imitative, the Common Assay Reporting System (CARS) allows investigators and program directors to track the status of assay projects related information at each screening center within the Molecular Libraries Program Center Network (MLPCN). The system also provides a means for collecting, sharing and retrieving of bioassay information among the centers and program office at NIH.    - HPCIO is collaborating with NCI to develop deep knowledge bases representing NCIs scientific portfolio.  The effort will explore several different representation paradigms (which store not only scientific concepts but the relationships between concepts as well) to evaluate their effectiveness at various tasks including document categorization, clustering, and visualization.  A similar collaboration has recently been initiated with DPCPSI/OD. n/a","Collective Intelligence, Knowledge Infrastructure, & High Performance Computing",8149742,ZIHCT000200,"['Address', 'Data', 'Effectiveness', 'Engineering', 'High Performance Computing', 'Information Resources', 'Intelligence', 'Knowledge', 'Molecular Bank', 'National Institute of Dental and Craniofacial Research', 'Oral Diagnosis', 'Research Infrastructure', 'Systems Biology', 'data mining', 'text searching']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2010,2823000,-0.027576943893112653
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7846105,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,326254,-0.015386046957030474
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7893787,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2010,1512082,-0.013404996850387759
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7893787,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2010,205892,-0.013404996850387759
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7902293,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's Disease patient registry', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronics', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'National Human Genome Research Institute', 'National Institute on Aging', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Statistical Methods', 'System', 'Testing', 'Text', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'biobank', 'case control', 'cohort', 'cost', 'data sharing', 'development policy', 'economic cost', 'gene environment interaction', 'genome wide association study', 'genome-wide', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'population based', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2010,889184,-0.04846333903882686
"Enhancing Clinical Effectiveness Research with Natural Language Processing of EMR    DESCRIPTION (provided by applicant): To successfully use large linked clinical databases for comparative effectiveness research (CER) requires addressing some key informatics challenges associated with distributed, heterogeneous clinical data. Electronic networks of researchers are part of the solution because they can bridge the physical and organizational divides created by distinct health systems' individual electronic medical records (EMRs). In addition, informatics research has demonstrated the feasibility of automatically coding clinical text, enhancing the capacity to integrate both unstructured and non-standardized clinical data from EMRs. With this study, we propose to develop CER infrastructure, make broadly available the proven MediClass technology for automated classification of EMRs containing both coded data and text clinical notes, and demonstrate the potential of this infrastructure for addressing CER questions within the asthma and tobacco-using patient populations of 6 diverse health systems. Asthma and smoking each impose huge and modifiable burdens on the healthcare system, and multiple morbidities related to asthma and smoking have been targeted by the IOM and AHRQ as priority areas in efforts to improve the healthcare system through comparative effectiveness research. We propose to develop, deploy, operate and evaluate the CER HUB, an Internet-based platform for conducting CER, and to demonstrate its utility in studying clinical interventions in asthma and smoking. Researchers who register to use the HUB, beginning with the research team from the 6 participating study sites, will be able to use a secure website to configure and download MediClass applications addressing CER questions within their respective healthcare organizations, to contribute these IRB-approved, processed datasets back to a centralized data coordinating center to be pooled with data similarly processed from other healthcare organizations, and to use the pooled database to answer diverse comparative effectiveness questions of large, real-world populations. A central function of the CER HUB will be facilitating (through online, interactive tools) development of a shared library of MediClass knowledge modules that afford uniform, standardized coding of EMR data. This shared library of knowledge modules could permit researchers to assess effectiveness in multiple areas of healthcare and gain access to data otherwise locked away in text clinical notes. A goal of the CER HUB is to accelerate creation of standardized knowledge used to normalize heterogeneous EMR data as representations of clinical events for CER. During the project period we will conduct 2 studies using this infrastructure to address the effectiveness of interventions for asthmatics and tobacco users across the 6 participating health systems. As an ongoing resource, the HUB will provide a collaborative development platform for enhancing comparative effectiveness research in potentially any health care domain.      CER researchers can build software applications that will process their EMRs, creating standardized datasets permitting CER using a secure website to configure and download MediClass applications addressing CER questions within their respective healthcare organizations, to contribute these IRB-approved, processed datasets back to a centralized data coordinating center to be pooled with data similarly processed from other healthcare organizations, and to use the pooled database to answer diverse comparative effectiveness questions of large, real-world populations      PUBLIC HEALTH RELEVANCE: Comparative effectiveness research (CER) requires that clinical data be in standard forms allowing multiple, large databases to be efficiently combined, and requires that all of the data be coded so that automated summarization of the data is possible. However, much of the clinical data necessary for CER is in the text clinical notes written by clinicians when caring for patients. We will build a centralized website where CER researchers can build software applications that will process their electronic medical records, including both the text and coded data, creating standardized datasets permitting comparative effectiveness research. We will demonstrate the utility of this infrastructure by conducting CER studies investigating the effectiveness of interventions in asthma and smoking, across the 6 participating health systems.           PROJECT NARRATIVE Comparative effectiveness research (CER) requires that clinical data be in standard forms allowing multiple, large databases to be efficiently combined, and requires that all of the data be coded so that automated summarization of the data is possible. However, much of the clinical data necessary for CER is in the text clinical notes written by clinicians when caring for patients. We will build a centralized website where CER researchers can build software applications that will process their electronic medical records, including both the text and coded data, creating standardized datasets permitting comparative effectiveness research. We will demonstrate the utility of this infrastructure by conducting CER studies investigating the effectiveness of interventions in asthma and smoking, across the 6 participating health systems.",Enhancing Clinical Effectiveness Research with Natural Language Processing of EMR,8032928,R01HS019828,[' '],AHRQ,KAISER FOUNDATION RESEARCH INSTITUTE,R01,2010,8696942,-0.03644358352097131
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,-0.012993268554043225
"Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts    DESCRIPTION (provided by applicant):  Accurate and complete medication lists are critical inputs to effective medication reconciliation to prevent medication prescribing and administration errors. Previous research aggregated structured medication data form multiple sources to generate and maintain a reconciled medication list. Medications documented in clinical texts also need to be reconciled. However, most reconciliation methods currently have limited capability to process textual data and temporal information (e.g., dates, duration and status). Our goal is to pilot and test methodologies and applications in the fields of natural language processing (NLP) and temporal reasoning to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. Clinic notes and free-text ""comments"" fields in medication lists in an ambulatory electronic medical record system will be considered in the study. An NLP system and a temporal reasoning system will be adapted to automatically extract medication and associated temporal information from clinical texts and encode the medications using a controlled terminology. Multiple knowledge bases will be used to develop a mechanism to represent the timing of medication use, detect the changes (e.g., active or inactive), and then to organize medications into appropriate groups (e.g., by ingredient or by status). The feasibility and efficiency of the proposed methods and tools in improving the process of medication   reconciliation will be assessed. Domain experts will serve as judges to assess the success of capturing, coding, and organizing the medications and temporal information and also to evaluate whether our methods are complementary to those currently used for medication management.           Accurate and complete medication information at the point of care is crucial for delivery of high-quality care and prevention of adverse events. Most previous studies aggregated structured medication data from EMR and CPOE (Computerized Physician Order Entry) systems to generate and maintain a reconciled medication list. However, medications in non-structured narrative sources (such as clinic notes and free-text comments) must also be reconciled. Structured data presented in a standard, predictable form can be easily processed by a computer. By contrast, narrative data does not have a well-defined structure, so processing such data is very challenging. Our goal is to pilot and test methodologies and applications in the fields of natural language processing (any system that manipulates text) and temporal reasoning (e.g., identifying the timing of medication use) to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. The feasibility and efficiency of the proposed methods and tools in improving the process of medication reconciliation will be assessed.",Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts,7935475,R03HS018288,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R03,2010,50100,-0.053441776594794294
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,7946175,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Arts', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2010,370693,-0.03838203176353183
"Enhancing 3dsvm to improve its interoperability and dissemination    DESCRIPTION (provided by applicant): This research plan outlines crucial software enhancements to a program called 3dsvm, which is a command line program and graphical user interface (gui) plugin for AFNI (Cox, 1996). 3dsvm performs support vector machine (SVM) analysis on fMRI data, which constitutes one important approach to performing multivariate supervised learning of neuroimaging data. 3dsvm originally provided the ability to analyze fMRI data as described in (LaConte et al., 2005). Since its first distribution as a part of AFNI, it has been steadily extended to provide new functionality including regression and non-linear kernels, as well as multiclass classification capabilities. In addition to its integration into AFNI, features that make 3dsvm particularly well suited for fMRI analysis are that it is easy to spatially mask voxels (to include/exclude them in the SVM analysis) as well as to flexibly select subsets of a dataset to use as training or testing samples. It has been used to generate results for our own work and for collaborative efforts and has been cited as a resource by others (Mur et al. 2009; Hanke et al. 2009). Despite many positive aspects of 3dsvm, the priorities of PAR-07-417 address a genuine need that this software project has - the ability to focus on improvements that will increase its dissemination and interoperability. A major motivation for PAR-07-417 is to facilitate the improved interface, characterization, and documentation to enhance the extent of sharing and to provide the groundwork for future extensions. Our aims are well aligned with this program announcement. Further, there is a growing need in the neuroimaging community for tools such as 3dsvm. Since 3dsvm is not a new project, is tightly integrated into the software environment of AFNI, and can be further integrated to enable better functionality to support needs as diverse as NIfTI format capabilities to rtFMRI, this proposed project will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.      PUBLIC HEALTH RELEVANCE: This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.           NARRATIVE This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.",Enhancing 3dsvm to improve its interoperability and dissemination,8278135,R03EB012464,[' '],NIBIB,VIRGINIA POLYTECHNIC INST AND ST UNIV,R03,2010,156500,-0.0023863354118737945
"Automatic Analysis and Annotation of Document Keywords in Biomedical Literature As a document retrieval system, PubMed aims at providing efficient access to millions of scientific documents. For this purpose, it relies on matching keywords and semantic representations of PubMed documents to user queries. One type of semantic representation used in MEDLINE citations is known as Medical Subject Heading (MeSH) indexing terms, which are assigned by professional human indexers at the National Library of Medicine. Alternatively, author keywords, provided by authors when submitting an article, capture the essence of the topic of a document from the authors perspective. Last but not least, readers have their own opinions about what words are of importance to an article, which may or may not agree with either MeSH terms or author keywords of the same article.  PubMed relies on human indexers to assign the appropriate MeSH indexing terms to PubMed articles  a very time and labor-intensive process. As a result, these terms are not immediately available for new articles. In fact, our analysis shows that on average it takes over 90 days for a PubMed citation to be manually annotated with MeSH terms. In response, we have developed a machine learning algorithm for automatically predicting MeSH terms with a set of novel features. When compared to other state-of-the-art methods, our approach achieved significantly better performance. We are currently exploring its potential for assisting the manual MeSH curation process in practice.   As MeSH terms require human curation, author keywords can be obtained freely from journal articles when they are available. We conducted a first study on author keywords in biomedical articles where we described the growth of author keywords in biomedical journal articles and presented a comparative study of author keywords and MeSH indexing terms. A similarity metric from our past study was used to automatically assess the relatedness between pairs of author keywords and MeSH indexing terms. Furthermore, a set of 300 pairs was manually reviewed to evaluate the metric and characterize the relationships between the term types. Results show that author keywords are increasingly available in biomedical articles and that over 60% of author keywords can be linked to a closely related indexing term. Results of this work have implications in both MEDLINE document indexing and MeSH terminology development.   Finally by comparison, we found neither MeSH terms nor author keywords overlap significantly with the important words from the users point of view, which motivated us to learn what characteristics make document words important from a collective user perspective. Specifically, we applied machine learning to identify document keywords which would likely be used frequently in user queries. Each word was represented by a set of features that included different types of information, such as semantic type, part of speech tag, TF-IDF weight and location in the abstract. We examined both traditional features such as TF-IDF, as well as novel ones such as named entity, which have not been explored before in this context. We identified the most important features and evaluated our model using months of real-world PubMed log data. Our results suggest that, in addition to carrying high TF-IDF weight, important words from the users perspective tend to be biomedical entities, to exist in article titles, and to occur repeatedly in article abstracts. This study enabled us to automatically predict words likely to appear in user queries that lead to document clicks. The relative importance of predicted words can also play a role in ranking documents by relevance. n/a",Automatic Analysis and Annotation of Document Keywords in Biomedical Literature,8149607,ZIALM091711,['Literature'],NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2010,391740,-0.023590237427000827
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) The latest work on this system has involved adding the ability to generate themes using an EM algorithm approach. Also recently code has been multithreaded and memory mapping capabilities added to speed up processing.  The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system has been used for mining email communications for the NLM help desk. n/a",A Document Processing System,8149592,ZIALM000022,"['Data', 'Electronic Mail', 'Process', 'PubMed', 'System', 'computerized data processing']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2010,176283,-0.005707392313372737
"Continued Development and Evaluation of caTIES    DESCRIPTION (provided by applicant): We propose to further develop, test, evaluate and support caTIES - an existing software system for developing networked repositories of sharable de-identified surgical pathology reports. The caTIES system creates a repository of de-identified, structured, and concept-coded clinical reports derived from large corpora of clinical free-text. Documents are automatically coded against a controlled terminology such as the Unified Medical Language System (UMLS), SNOMED-CT, or NCI Metathesaurus. Users construct queries to identify specific kinds of documents and tissue specimens based on the associated clinical report. For example, a researcher studying genetic variation in metastatic breast cancers can identify cases of invasive ductal carcinoma of the breast, followed by metastatic ductal cancer in bone at an interval of three years or greater from the original diagnosis. The caTIES system also supports acquisition and ordering of tissues, using an honest broker model. Through this mechanism, de-identified data and access to tissue can be shared among institutions, enabling multi-center collaborative research. The caTIES system has already been implemented at seven US Cancer Centers, and is being considered for adoption by numerous other institutions including cancer centers, university hospitals and private hospitals. Initial development of caTIES was funded by the Cancer Biomedical Informatics Grid (caBIG). However, interest in the application has far exceeded our expectations and the limitations of caBIG. This grant will allow us to further extend the capabilities of the system by (a) improving the portability of the system and extending the types of documents that can be processed, (b) evaluating the system's NLP performance and usability, (c) building a user community to support this open-source application, and (d) piloting interoperability of caTIES with other enterprise and research systems. This work will preserve and extend a highly novel platform for development of massive repositories of de-identified clinical data that can be used for research within and across institutions. PUBLIC HEALTH RELEVANCE: This grant will fund the further development and evaluation of a system that takes identified clinical documents and converts them into de-identified, concept-coded, structured data. The system enables researchers to access remainder tissues and clinical report data for research purposes within and across institution. This project is important because it will greatly increase the access of researchers to important data and materials while maintaining patient privacy.          n/a",Continued Development and Evaluation of caTIES,7749583,R01CA132672,"['Access to Information', 'Adoption', 'Cancer Center', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Communication', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Data Reporting', 'Database Management Systems', 'Development', 'Diagnosis', 'Documentation', 'Ductal', 'Electronic Mail', 'Eligibility Determination', 'Environment', 'Evaluation', 'Funding', 'Future', 'Genetic Variation', 'Genomics', 'Grant', 'Information Retrieval', 'Institution', 'Malignant Neoplasms', 'Methods', 'Metric', 'Modeling', 'Modification', 'Natural Language Processing', 'Operating System', 'Pathology Report', 'Performance', 'Private Hospitals', 'Process', 'Report (document)', 'Reporting', 'Research', 'Research Personnel', 'Services', 'Specimen', 'Structure', 'Surgical Pathology', 'System', 'Terminology', 'Testing', 'Text', 'Tissues', 'Training', 'Translational Research', 'Unified Medical Language System', 'University Hospitals', 'Vocabulary', 'Work', 'base', 'bone', 'cancer Biomedical Informatics Grid', 'clinical phenotype', 'computer human interaction', 'ductal breast carcinoma', 'expectation', 'flexibility', 'improved', 'interest', 'interoperability', 'malignant breast neoplasm', 'meetings', 'metathesaurus', 'novel', 'open source', 'patient privacy', 'portability', 'public health relevance', 'repository', 'software development', 'software systems', 'systems research', 'tool', 'usability']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2010,314363,-0.048103796657499774
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8150047,U54NS064808,"['Address', 'Adherence', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Bite', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials Cooperative Group', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Computer software', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Quality', 'Data Reporting', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Disasters', 'Disease', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family history of', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Image', 'Individual', 'Industry', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Manuals', 'Maps', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patients', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Positron-Emission Tomography', 'Principal Investigator', 'Printing', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Reader', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Scanning', 'Scientist', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Support Groups', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'computerized data processing', 'data management', 'data mining', 'electronic data', 'follow-up', 'improved', 'interest', 'meetings', 'patient advocacy group', 'programs', 'quality assurance', 'radiologist', 'sample collection', 'statistics', 'tool', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2010,959195,0.007897458217528647
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,0.004370542114199357
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,7928213,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Stroke', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'computer based statistical methods', 'data modeling', 'graphical user interface', 'hemodynamics', 'improved', 'innovation', 'insight', 'mortality', 'nervous system disorder', 'network models', 'prognostic', 'programs', 'prospective', 'public health relevance', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2010,600856,0.0021754796037888485
"National Biomedical Information Services Delivering Biomedical Information Services  In FY10, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public, including significant expansion in social media and open data initiatives via API access. Among the NLMs intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLMs premier bibliographic database with over 20 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY10, more than 800,000 new indexed citations were added, plus 60,000 historical citations from 1947.  PubMed Central: The PubMed Central archive of now over 2 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports. In FY10, NLM continued to enhance the NIH Manuscript Submission system by which authors submit articles to comply with the NIH Public Access Policy. MyBibliography was implemented to help NIH-funded scientists manage their publications. MedlinePlus and MedlinePlus en espaol: These resources include consumer health information on more than 800 topics, in more than 40 languages. In FY10, the Web site was redesigned to improve usability and mobile access was added via Mobile MedlinePlus. MedlinePlus Connect, scheduled for general release in Fall 2010, connects electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use.  Clinical Trials: ClinicalTrials.gov covers more than 95,000 clinical research studies in more than 170 countries, with hundreds added each week. It was significantly expanded to respond to new clinical trial registration and results reporting requirements established by the FDA Amendments Act of 2007 (PL 110-85). In FY10, nearly 16,000 trials were registered. Since NLM implemented the results database required by law in September 2008, summary results of more than 3200 clinical trials have been added. Summaries of adverse events that occurred during the trial became mandatory in September 2009. Toxicology and Environmental Health:  Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, the Toxic Release Inventory, and the Hazardous Substances Data Bank (HSDB). In FY10, nanomaterials were added to HSDB. Drug Information Resources: NLMs drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for more than 10,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to RxNorm standard drug names. Disaster Preparedness and Response:  NLMs Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A collaboration with the Bethesda Hospital Emergency Preparedness Partnership provides backup communication systems and tools for patient tracking, information access, and responder training. In FY10, specialized resources for earthquakes and oil spills were developed, including the Haiti Earthquake People Locator, an interactive re-unification tool. Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI information resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, UniGene, LocusLink, Genomes, and the NCBI software toolkit. NCBI also produces the retrieval systems for PubMed, PubMed Central, and the Books database. Continuing areas of emphasis in FY10 included addressing the impact of enormous quantities of data emanating from high throughput sequencing, microarray, and small molecule screening techniques; organizing data from large-scale clinical studies involving genotyping; and enhancing database interfaces to facilitate search and discovery across multiple resources.  Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. In FY10, distribution of the magazine increased to more than 600,000, with additional access via online versions. Special population websites address specific minority health concerns in various racial and ethnic groups. NLM  outreach programs enhance awareness of its information services, with emphasis on. underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY10, dozens of community-based projects were funded. Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 8000 health services research projects from more than 110 funding organizations; HSRR, a  database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2010 objectives. Advanced Information Systems, Data Standards and Research Tools  In FY10, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; extended the use of digital pen technology for community-based research in Africa; applied natural language processing methods to extract information from biomedical literature; improved standardized reporting of genetic variations and clinical interpretation of genetic test results; and designed improved methods for integrated search and discovery across multiple databases.  Health Data Standards: As the central coordinating body for clinical terminology standards within DHHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure. NLM develops or licenses key clinical terminologies and problem lists designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 9.9 million concept names from more than 125 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY10, NLM produced a common lab test subset and added the VAs drug classes to RxNorm. n/a",National Biomedical Information Services,8158475,ZIHLM200888,['Information Services'],NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2010,239909648,-0.009135107909217672
"National Biomedical Information Services Delivering Biomedical Information Services  In FY10, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the public, including significant expansion in social media and open data initiatives via API access. Among the NLMs intramural programs that contribute to its national biomedical information services are the following examples: PubMed/MEDLINE: PubMed, which incorporates MEDLINE, is NLMs premier bibliographic database with over 20 million references to biomedical journal articles. MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually. In FY10, more than 800,000 new indexed citations were added, plus 60,000 historical citations from 1947.  PubMed Central: The PubMed Central archive of now over 2 million full-text journal articles is central to the NIH effort to make accessible the published results of research it supports. In FY10, NLM continued to enhance the NIH Manuscript Submission system by which authors submit articles to comply with the NIH Public Access Policy. MyBibliography was implemented to help NIH-funded scientists manage their publications. MedlinePlus and MedlinePlus en espaol: These resources include consumer health information on more than 800 topics, in more than 40 languages. In FY10, the Web site was redesigned to improve usability and mobile access was added via Mobile MedlinePlus. MedlinePlus Connect, scheduled for general release in Fall 2010, connects electronic health records to MedlinePlus drug information and health topics by leveraging standardized codes and vocabularies required for meaningful use.  Clinical Trials: ClinicalTrials.gov covers more than 95,000 clinical research studies in more than 170 countries, with hundreds added each week. It was significantly expanded to respond to new clinical trial registration and results reporting requirements established by the FDA Amendments Act of 2007 (PL 110-85). In FY10, nearly 16,000 trials were registered. Since NLM implemented the results database required by law in September 2008, summary results of more than 3200 clinical trials have been added. Summaries of adverse events that occurred during the trial became mandatory in September 2009. Toxicology and Environmental Health:  Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as TOXLINE, GENE-TOX, the Toxic Release Inventory, and the Hazardous Substances Data Bank (HSDB). In FY10, nanomaterials were added to HSDB. Drug Information Resources: NLMs drug information resources include DailyMed and Pillbox. DailyMed provides medication content and labeling information from medical package inserts for more than 10,000 marketed drugs. Pillbox enables rapid identification of unknown solid-dosage medications based on physical characteristics and high-resolution images. Both are linked to RxNorm standard drug names. Disaster Preparedness and Response:  NLMs Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists for disaster management, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur. A collaboration with the Bethesda Hospital Emergency Preparedness Partnership provides backup communication systems and tools for patient tracking, information access, and responder training. In FY10, specialized resources for earthquakes and oil spills were developed, including the Haiti Earthquake People Locator, an interactive re-unification tool. Molecular Biology, Bioinformatics, and Human Genome Resources: NCBI information resources include more than 40 integrated molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, UniGene, LocusLink, Genomes, and the NCBI software toolkit. NCBI also produces the retrieval systems for PubMed, PubMed Central, and the Books database. Continuing areas of emphasis in FY10 included addressing the impact of enormous quantities of data emanating from high throughput sequencing, microarray, and small molecule screening techniques; organizing data from large-scale clinical studies involving genotyping; and enhancing database interfaces to facilitate search and discovery across multiple resources.  Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine, in English and Spanish, transmit the latest useful research findings in lay language. In FY10, distribution of the magazine increased to more than 600,000, with additional access via online versions. Special population websites address specific minority health concerns in various racial and ethnic groups. NLM  outreach programs enhance awareness of its information services, with emphasis on. underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY10, dozens of community-based projects were funded. Health Services Research  NICHSR promotes access to public health and health services research through such information systems as: HSRProj, a database of more than 8000 health services research projects from more than 110 funding organizations; HSRR, a  database of research datasets, instruments and software relevant to health services research; and HSTAT, a full-text database of high quality evidence reports, guidelines, technology assessments, consensus statements, and treatment protocols. Structured search queries are developed to aid in searching PubMed, ClinicalTrials.gov, and HSRProj for information on health services research, comparative effectiveness, health disparities, and HealthyPeople 2010 objectives. Advanced Information Systems, Data Standards and Research Tools  In FY10, LHC and NCBI continued to conduct research in biomedical informatics and computational biology, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; extended the use of digital pen technology for community-based research in Africa; applied natural language processing methods to extract information from biomedical literature; improved standardized reporting of genetic variations and clinical interpretation of genetic test results; and designed improved methods for integrated search and discovery across multiple databases.  Health Data Standards: As the central coordinating body for clinical terminology standards within DHHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure. NLM develops or licenses key clinical terminologies and problem lists designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 9.9 million concept names from more than 125 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY10, NLM produced a common lab test subset and added the VAs drug classes to RxNorm. n/a",National Biomedical Information Services,8158475,ZIHLM200888,['Information Services'],NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2010,2436250,-0.009135107909217672
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in four subprojects which use natural language processing techniques:  1) The presence of unrecognized abbreviations in text hinders indexing algorithms and adversely affects information retrieval and extraction.  Automatic abbreviation definition identification can help resolve these issues.  However, abbreviations and their definitions identified by an automatic process are of uncertain validity.  Due to the size of databases such as MEDLINE only a small fraction of abbreviation-definition pairs can be examined manually.  An automatic way to estimate the accuracy of abbreviation-definition pairs extracted from text is needed.  We have proposed an abbreviation definition identification algorithm that employs a variety of strategies to identify the most probable abbreviation definition.  In addition our algorithm produces an accuracy estimate, pseudo-precision, for each strategy without using a human-judged gold standard. The pseudo-precisions determine the order in which the algorithm applies the strategies in seeking to identify the definition of an abbreviation. The results are generally a couple of percentage points better than the Schwartz-Hearst algorithm and also allow one to enforce a threshold for those applications where high precision is critical.  2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text. 3) We have developed a spell checking algorithm that does quite accurate correction ( 87%) and handles one or two edits, and more edits if the string to be corrected is sufficiently long. It handles words that are fragmented or merged. Where queries consist of more than a single token the algorithm attempts to make use of the additional information as context to aid the correction process. The algorithm is based on the noisy channel model of spelling correction and makes use of statistics on miss-spellings gathered from approximately one million miss-spelling incidents in the PubMed log files. These incidents were identified as cases where a user entered a query and then within five minutes corrected that query to another term which is close in edit distance and with at least ten times as many hits in the PubMed database. These statistics are not only used in the actual correction process, but were used to simulate miss-spellings in real words and phrases to discover the regions of validity of the method of correction and estimates of its accuracy. Additional work was done on the vocabulary of the PubMed database to remove frequent miss-spellings and improve performance. The algorithm is implemented in the PubMed search engine and there it frequently makes over 200,000 suggestions in a day and about 45% of these suggestions are accepted by users. The algorithm is efficient in adding only about 25% to the average query response time for users and much of this is seen only for misspelled queries. There is the possibility of improving the algorithm by the use of more context around the sites of errors within words. There is also the possibility of improving the algorithm by learning how to make better use of the context supplied by queries consisting of multiple tokens. But in both cases such an effort must consider how to maintain efficiency in the light of a huge vocabulary of phrases (>14 million) and individual words (>2.5 million) recognized by the search engine. There is also the possibility to use phonetic encodings to improve the handling of some of the errors that currently challenge the system. However, preliminary calculations suggest it would be difficult to make a major improvement by using phonetic encodings. 4) We explored a syntactic approach to sentence compression in the biomedical domain, grounded in the context of result presentation for related article search in the PubMed search engine. By automatically trimming inessential fragments of article titles, a system can effectively display more results in the same amount of space. Our implemented prototype operates by applying a sequence of syntactic trimming rules over the parse trees of article titles. Two separate studies were conducted using a corpus of manually compressed examples from MEDLINE: an automatic evaluation using Bleu and a summative evaluation involving human assessors. Experiments show that a syntactic approach to sentence compression is effective in the biomedical domain and that the presentation of compressed article titles supports accurate interest judgments, decisions by users as to whether an article is worth examining in more detail. n/a",Natural Language Processing Techniques To Enhance Information Access.,7969224,ZIALM000090,"['Abbreviations', 'Address', 'Affect', 'Algorithms', 'Automated Abstracting', 'Body of uterus', 'Classification', 'Collection', 'Data', 'Databases', 'Evaluation', 'Gold', 'Human', 'Individual', 'Information Retrieval', 'Judgment', 'Learning', 'Light', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Performance', 'Phonetics', 'Process', 'PubMed', 'Reaction Time', 'Records', 'Retrieval', 'Simulate', 'Site', 'Suggestion', 'System', 'Techniques', 'Text', 'Time', 'Trees', 'Vocabulary', 'Work', 'base', 'improved', 'indexing', 'insight', 'interest', 'phrases', 'prototype', 'research study', 'spelling', 'statistics', 'syntax']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2009,608137,-0.02015255494309796
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.  1)	In the literature on the Naive Bayes machine learning method there have long been two models that have been used. One is the multivariate Bernoulli model (MBM) and the other the multinomial model (MM).  The MBM method only counts the presence or absence of a feature, while the MM method counts the number of times a feature appears in a record. A number of comparisons of the two approaches have been made in the area of text categorization and the MM approach has usually won out. It is our belief that the reason for this is that the MBM model has not been properly optimized. In fact we have found that in the area of text categorization the local term frequency contributes virtually nothing to the performance of the MM model. In support of this contention we have developed a simplified form of the MM model which ignores local term frequency (but is still much closer to the MM than to the MBM) and we find that it performs essentially the same as the MM model. In fact we do not find an advantage for the use of local term frequency in text categorization using MM or in several other models including the SVM. The advantage to ignoring local term frequency is that it greatly simplifies the data storage and the calculations when applying the Naive Bayes approach to a very large database such as PubMed. One aspect of this work which calls for further investigation is what happens when the records are long and the local frequencies can then be much larger. We do not have the final answer, but  our initial work with the TREC genomics data (160,000 full text documents) suggests that there is a small advantage in the use of local term frequencies in some models, but the advantage is not in any case over about a 3% improvement in break even point.  2)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 3)	The PubMed database presents a unique challenge because of its very large size of over 18 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Naive Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Naive Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Naive Bayes.  4) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with theresults of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. n/a",Automatic Bayesian Methods In Text Retrieval,7969197,ZIALM000021,"['Accounting', 'Active Learning', 'Area', 'Bayesian Method', 'Belief', 'Classification', 'Data', 'Data Storage and Retrieval', 'Databases', 'Exhibits', 'Frequencies', 'Genomics', 'Goals', 'Investigation', 'Label', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Modeling', 'Performance', 'Peripheral', 'Poisson Distribution', 'Probability', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2009,128999,-0.010015879032300973
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7918614,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'clinical practice', 'forgetting', 'innovation', 'natural language', 'preference', 'speech processing', 'symposium']",NLM,UNIVERSITY OF CHICAGO,R01,2009,185745,-0.01256280053605713
"Collective Intelligence, Knowledge Infrastructure, & High Performance Computing The Collective Intelligence, Knowledge Infrastructure, and High Performance Program, which  operates within the High Performance Computing and Informatics Office (HPCIO), Division of Computational Bioscience of CIT, is collaborating with NIH investigators to build a critical mass in collective intelligence that is envisioned to encompass a number of pertinent and related disciplines in biomedical research including semantic interoperability, knowledge engineering, computational linguistics, text and data mining, natural language processing, machine learning, and visualization.  The program is intended to foster advances in critical domains at NIH including biomedical and clinical informatics, translational research, proteomics research, genomics, systems biology, and portfolio analysis.  In 2009, collaborations in support of these goals included the following.  - The human salivary protein catalog has been made available online on a community-based Web portal developed by HPCIO, in collaboration with NIDCR, to enable scientists to add their own research data, share results, and discover new knowledge. This is a major step towards the discovery and use of saliva biomarkers to diagnose oral and systemic diseases.   - HPCIO has developed context-sensitive text-mining methodology for identifying High-Risk, High-Reward (HRHR) research based from NIH Summary Statements.  The method, which uses natural language processing to parse text and classify documents, has been successful in retrospective analysis of the most recent five-years summaries.  This work is being conducted in collaboration with Division of Program Coordination, Planning, and Strategic Initiatives (DPCPSI), NIH Office of the Director (OD).   - HPCIO is developing a corpus of annotated NIH medical records for use in developing methods of document de-identification.  The goal is to create a gold standard that de-identification algorithms for use at NIH can be measured against.  This work, in collaboration with the Clinical Center, will enhance the availability of medical records stored within the Biomedical Translational Research Information System.    - HPCIO is working with the caBIG Clinical Trial Management System Workspace (in collaboration with NCI) to develop a Protocol Lifecycle Tracking (PLT) tool.  By providing real-time protocol status information on all relevant trials to clinicians and researchers, bottlenecks and latencies in protocol management can be identified and corrected by those responsible for conduct of a trail and the overall success of a clinical trial program.   - As a component of the Molecular Libraries Roadmap imitative, the Common Assay Reporting System (CARS) allows investigators and program directors to track the status of assay projects related information at each screening center within the Molecular Libraries Program Center Network (MLPCN). The system also provides a means for collecting, sharing and retrieving of bioassay information among the centers and program office at NIH.    - HPCIO is initiating a pilot endeavor with NCI to develop deep knowledge bases representing NCIs scientific portfolio.  The pilot will explore several different representation paradigms (which store not only scientific concepts but the relationships between concepts as well) to evaluate their effectiveness at various tasks including document categorization, clustering, and visualization.  A similar collaboration has recently been initiated with DPCPSI/OD. n/a","Collective Intelligence, Knowledge Infrastructure, & High Performance Computing",7970400,ZIHCT000200,"['Address', 'Algorithms', 'Architecture', 'Biological Assay', 'Biological Markers', 'Biomedical Research', 'Body of uterus', 'Cataloging', 'Catalogs', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Data', 'Development', 'Discipline', 'Effectiveness', 'Engineering', 'Fostering', 'Genomics', 'Goals', 'Gold', 'High Performance Computing', 'Human', 'Imagery', 'Informatics', 'Information Resources', 'Information Systems', 'Information Technology', 'Intelligence', 'Knowledge', 'Linguistics', 'Machine Learning', 'Management Information Systems', 'Measures', 'Medical Imaging', 'Medical Records', 'Methodology', 'Methods', 'Molecular Bank', 'National Institute of Dental and Craniofacial Research', 'Natural Language Processing', 'Online Systems', 'Oral Diagnosis', 'Performance', 'Proteomics', 'Protocols documentation', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Sharing', 'Resources', 'Saliva', 'Salivary Proteins', 'Science', 'Scientist', 'Screening procedure', 'Semantics', 'Software Engineering', 'Software Tools', 'Strategic Planning', 'System', 'Systemic disease', 'Systems Biology', 'Technology', 'Text', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Work', 'base', 'biological systems', 'cancer Biomedical Informatics Grid', 'cluster computing', 'data management', 'data mining', 'data sharing', 'high reward', 'high risk', 'improved', 'information organization', 'interoperability', 'knowledge base', 'programs', 'repository', 'social', 'success', 'text searching', 'tool']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2009,2941656,-0.027116251890365168
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7842799,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,MAYO CLINIC ROCHESTER,RC1,2009,497477,-0.01931971857907716
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method of interest is the method known as sparse coding, which is  the basis of self-taught learning. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,7969222,ZIALM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Code', 'Educational process of instructing', 'Exclusion', 'Learning', 'Literature', 'Machine Learning', 'Methods', 'Procedures', 'Retrieval', 'Testing', 'base', 'design', 'improved', 'interest', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2009,221141,0.005040959865546382
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7664538,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'systematic review', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,318898,-0.004540319095064122
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) The latest work on this system has involved adding the ability to generate themes using an EM algorithm approach. Also recently code has been multithreaded and memory mapping capabilities added to speed up processing.  The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system has been used for mining email communications for the NLM help desk. n/a",A Document Processing System,7969199,ZIALM000022,"['Algorithms', 'Code', 'Communication', 'Data', 'Databases', 'Electronic Mail', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'Memory', 'Methods', 'Mining', 'Online Systems', 'Process', 'Programming Languages', 'PubMed', 'Records', 'Research', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Work', 'base', 'computerized data processing', 'repository', 'software systems']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2009,202712,-0.005707392313372737
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7656692,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,282304,-0.03083073596140968
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7685518,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Series', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'new therapeutic target', 'novel', 'pathway tools', 'programs', 'reconstruction']",NLM,SRI INTERNATIONAL,R01,2009,175647,-0.016442598182056248
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7690941,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'clinical practice', 'design', 'foot', 'journal article', 'language processing', 'meetings', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2009,351549,-0.00988371894210066
"Development of a Research-Ready Pregnancy and Newborn Biobank in California    DESCRIPTION (provided by applicant): Development of a Research-Ready Pregnancy and Newborn Biobank in California Population-based biobanks are a critical resource for identifying disease mechanisms and developing screening tests for biomarkers associated with certain disorders. The California Department of Public Health has been banking newborn specimens statewide since 1982 (N~14 million) and maternal prenatal specimens for a portion of the state since 2000 (N~1 million), creating one of the largest, if not the largest single biological specimen banks with linked data in the world. With the fast pace of new knowledge in genetics and laboratory methods, the demand for specimens and data from researchers around the world now far surpasses the Department's ability to fill them. The goal of this infrastructure development project is to create an efficient, high throughput, low cost newborn screening and prenatal/maternal screening specimen biobank and linked data base that could be used by large numbers of researchers around the world for a wide range of studies through the following aims: (1) establishment of highly efficient protocols and procurement and integration of automated systems for pulling and processing specimens; (2) development of an integrated specimen tracking system into the Department's existing web-based Screening Information System; (3) development of a computerized system to track application requests for specimens and data; and (4) development of a linked screening program-vital records database that is organized into a life course, client based system. These aims will be accomplished through expansion of the Department's award-winning Screening Integration System to include web-based tracking of specimens and research requests, and use of an innovative machine-learning record matching application for high-performance linkages. After the 2 year grant period is completed, the California Research Ready Biospecimen Bank will be able to provide researchers with valuable biological specimens in a timely, cost-effective manner, thereby enabling a dramatic expansion of epidemiological research nationwide. The continuity of the system will be ensured by codifying human subjects-sensitive policies and procedures into Departmental regulations and by charging researchers modest fees for specimens, data and other research services.      PUBLIC HEALTH RELEVANCE: Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.           Program Narrative Development of a research-ready pregnancy and newborn biobank in California This proposal funds infrastructure development to create a research-ready, efficient, high- throughput, and low-cost prenatal and newborn biobank in California. Specimens spanning 28 years will be linked to existing records of fetal death, live birth, death, prenatal and newborn screening to develop a rich, client-based, cross-generational, life- course database. Specimens and linked data from the California Research-Ready Biospecimen Bank will be made available to researchers in the U.S. and around the world to enable a broad and expanded array of studies.",Development of a Research-Ready Pregnancy and Newborn Biobank in California,7853378,RC2HD065514,"['Area', 'Award', 'Biological', 'Biological Markers', 'Biological Specimen Banks', 'California', 'Case-Control Studies', 'Cessation of life', 'Charge', 'Client', 'Data', 'Data Files', 'Databases', 'Development', 'Disease', 'Ensure', 'Epidemiology', 'Family Study', 'Fees', 'Fetal Death', 'Funding', 'Future', 'Genetic', 'Goals', 'Government', 'Grant', 'Information Systems', 'Infusion procedures', 'Knowledge', 'Laboratories', 'Laws', 'Life Cycle Stages', 'Link', 'Live Birth', 'Machine Learning', 'Methods', 'Multiple Pregnancy', 'Neonatal Screening', 'Newborn Infant', 'Online Systems', 'Performance', 'Policies', 'Population', 'Pregnancy', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Public Health', 'Records', 'Regulation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Screening procedure', 'Services', 'Specimen', 'Specimen Handling', 'Study Subject', 'System', 'Systems Integration', 'Testing', 'Time', 'Woman', 'base', 'biobank', 'cohort', 'computerized', 'cost', 'human subject', 'infrastructure development', 'innovation', 'population based', 'prenatal', 'programs', 'public health relevance']",NICHD,SEQUOIA FOUNDATION,RC2,2009,2003191,-0.0044115889843305
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7908952,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'clinical practice', 'design', 'foot', 'journal article', 'language processing', 'meetings', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2009,170662,-0.00988371894210066
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7582301,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2009,2057843,-0.0020349760302730757
"Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data    DESCRIPTION (provided by applicant):       Our long-term objective is to enlarge the scope and efficiency of clinical research through enhanced use of clinical data to support clinical research decisions. This proposal aims to improve the use of electronic health records (EHR) to automate clinical trials eligibility screening by developing a new semantic alignment framework. Clinical trials research is an important step for translating breakthroughs in basic biomedical sciences into knowledge that will benefit clinical practice and human health. However, a significant obstacle is identifying eligible participants. Eighty-six percent of all clinical trials are delayed in patient recruitment for from one to six months and 13% are delayed by more than six months. Enrollment delay is expensive. In a recent large, multi-center trial, about 86.8 staff hours and more than $1000 was spent to enroll each participant. Ineffective enrollment also produces a big social cost in that up to 60% of patients can miss being identified. The broad deployment of EHR systems has created unprecedented opportunities to solve the problem because EHR systems contain a rich source of information about potential participants. However, it is often a knowledge-intensive, time-consuming, and inefficient manual procedure to match eligibility criteria such as ""renal in- sufficiency"" to clinical data such as ""serum creatinine = 1.0 mg/dl for an 80-year old white female patient."" This enduring challenge is partly caused by the disconnection between abstract and ambiguous eligibility criteria and highly specific clinical data manifestations; we call this a semantic gap. Despite earlier work on computer-based clinical guidelines and protocols, limited effort has been devoted to support automatic matching between concepts and their manifestations in patient phenotypes such as signs and symptoms.       We hypothesize that we can characterize the semantic gap and design a knowledge-based, natural-language processing assisted semantic alignment framework to bridge the semantic gap. Therefore, our specific aims are: (1) to investigate the semantic gap between clinical trials eligibility criteria and clinical data; (2) to design a concept-based, computable knowledge representation for eligibility criteria; (3) to design a semantic alignment framework linking an eligibility criteria knowledge base and a clinical data warehouse to generate semantic queries for eligibility identification; and (4) to evaluate the utility of the semantic alignment framework.       This research is novel and unique in that (1) there are no prior studies about the semantic gap between eligibility criteria and clinical data; and (2) for the first time, we design a semantic alignment framework to automatically match eligibility criteria to clinical data. The research team comprising expertise from the Department of Biomedical Informatics at Columbia University and the Division of General Medicine from UCSF are uniquely positioned to carry out this research, given the experience of the team (medical knowledge representation, natural language processing, controlled clinical terminology, ontology-based semantic reasoning, data mining, statistics, health data organization, semantic harmonization, and clinical trials), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data.            This research has the potential to improve process efficiency and accuracy, as well as to reduce cost and required human skills for clinical trials eligibility screening. The ultimate goal is to accelerate scientific discovery of more effective treatments for illness.",Bridging the Semantic Gap Between Research Eligibility Criteria and Clinical Data,7653874,R01LM009886,"['Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Code', 'Complex', 'Computers', 'Creatinine', 'Data', 'Databases', 'Drug Formulations', 'Electronic Health Record', 'Eligibility Determination', 'Enrollment', 'Female', 'Goals', 'Guidelines', 'Health', 'Hour', 'Human', 'Kidney', 'Kidney Failure', 'Knowledge', 'Link', 'Manuals', 'Medical', 'Medicine', 'Methods', 'Natural Language Processing', 'Ontology', 'Participant', 'Patient Recruitments', 'Patients', 'Phenotype', 'Population Surveillance', 'Positioning Attribute', 'Problem Solving', 'Procedures', 'Process', 'Protocols documentation', 'Reporting', 'Research', 'Science', 'Screening procedure', 'Semantics', 'Serum', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Terminology', 'Text', 'Time', 'Translating', 'Translations', 'Universities', 'Work', 'abstracting', 'base', 'biomedical informatics', 'clinical phenotype', 'clinical practice', 'cost', 'data mining', 'design', 'effective therapy', 'eligible participant', 'experience', 'improved', 'information organization', 'knowledge base', 'natural language', 'novel', 'repository', 'skills', 'social', 'statistics']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2009,357875,-0.02402649274448676
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7652508,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,363929,-0.017913058313874537
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,0.004370542114199357
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,0.004370542114199357
"New Resources for e-Patients    DESCRIPTION (provided by applicant): ""New Resources for e-Patients"" addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in currently available online health information resources. It will maximize the value of public domain health information from U.S. Government sources. Textual consumer health information will be collected from NIH, FDA and other government sources. This information will be subjected to automated topic analysis and classification using methods of natural language processing and statistical text-mining to discover and extract topics on i) diseases and conditions; ii) treatments, benefits and risks; and iii) genomic risks and responses. These topics will be integrated and mapped to the most frequent health topics of interest to consumers. Personally-controlled electronic health records and personal genotypes will be studied for their potential contributions to personalized medicine for e-patients. Phase I of this project will achieve proof-of-principle and develop an advanced prototype as a foundation for construction of a new web-based resource in Phase II.    PUBLIC HEALTH RELEVANCE: This project addresses the unmet medical needs of consumers who search for health and healthcare information online, currently a population of more than 160 million people in the U.S. It will fill gaps and address deficiencies in current online health information resources and also target new opportunities in genomic and personalized medicine. In the process we will create consumer-friendly, automated systems that make online information search and retrieval more efficient more efficient and maximize the value of public domain health information from U.S. Government sources. The work will lead to more reliable, personalized and actionable information for a new generation of web-savvy and socially-networked ""e-patients"" and will lead to more efficient and productive encounters between patients and healthcare systems.           This project addresses the unmet medical needs of consumers who search for  health and healthcare information online, currently a population of more than  160 million people in the U.S. It will fill gaps and address deficiencies in current  online health information resources and also target new opportunities in  genomic and personalized medicine. In the process we will create consumer-  friendly, automated systems that make online information search and retrieval  more efficient more efficient and maximize the value of public domain health  information from U.S. Government sources. The work will lead to more reliable,  personalized and actionable information for a new generation of web-savvy and  socially-networked ""e-patients"" and will lead to more efficient and productive  encounters between patients and healthcare systems.",New Resources for e-Patients,7748337,R43HG005046,"['Address', 'Benefits and Risks', 'Body of uterus', 'Businesses', 'Classification', 'Communication', 'Data', 'Development', 'Development Plans', 'Disease', 'Electronic Health Record', 'Foundations', 'Fund Raising', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Government', 'Health', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Information Resources', 'Institutes', 'Internet', 'Lead', 'Maps', 'Marketing', 'Medical', 'Medicine', 'Methods', 'Modeling', 'National Heart, Lung, and Blood Institute', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Process', 'Proxy', 'Public Domains', 'Research', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Site', 'Source', 'Surveys', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Update', 'Validation', 'Work', 'base', 'commercialization', 'data integration', 'design', 'health record', 'interest', 'prototype', 'public health relevance', 'research study', 'response', 'text searching', 'web site']",NHGRI,"RESOUNDING HEALTH, INC.",R43,2009,119499,-0.004923962430068353
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7908946,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'meetings', 'natural language', 'population based', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2009,71200,-0.016785311480589316
"Online Social Networking as an Alternative Information Source for Clinical Resear    DESCRIPTION (provided by applicant): Clinical trials and patient records have been the main information sources for clinical research. While well- designed clinical trials can produce high quality data, they are generally very expensive and time consuming. Prior studies have also shown that patients enrolled in clinical trials are not necessarily representative of the general patient population. Chart reviews, which rely on the patient records, avoid some of the drawbacks of the clinical trials approach. Although chart review studies are more labor intensive, new developments in structured data entry and natural language processing (NLP) are helping to automate the process. However, studies which use chart reviews are limited by the accuracy and completeness of the data in the records.       In the past decade, online social networks have grown exponentially. Some health-focused social network sites have attracted large numbers of users and begun accumulating large quantities of detailed clinical information. The PatientsLikeMe site, for instance, has about 3,200 amyotrophic lateral sclerosis (ALS) patients worldwide, and includes about 5% of the ALS population in the US. Information gathered by online social networks is primarily intended for patients to share with each other. Such information has also begun to attract the attention of medical researchers.[3, 4]       Because using information from online social networks for medical research is a fairly new phenomenon, the value and limitation of this type of information source have not been systematically examined. To do so, we propose to conduct a comparison study of patient-contributed information from PatientsLikeMe and records from a large medical record data repository - the Research Patient Data Registry (RPDR) of the Partners Healthcare Systems. The proposed study will focus on ALS, multiple sclerosis (MS), and Parkinson's disease (PD). The general goal is to explore how the medical record and online networking data differ, and if and how online networking data could complement the medical record data. The specific aims are:    1) Extract symptom and treatment information from the two different data sources.    2) Compare the prevalence of symptoms and treatments from the two information sources and analyze the difference.    3) Extract treatment response of prescription medications from PatientsLikeMe and analyze the confounding effect of the misunderstanding of medication indication.      PUBLIC HEALTH RELEVANCE: The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.           The proposed project will investigate an emerging data source for clinical research: online social network. This data source may complement and supplement the data from clinical trials and medical records, with a unique emphasis on patients' experience and perspectives.",Online Social Networking as an Alternative Information Source for Clinical Resear,7777633,R21NS067463,"['Amyotrophic Lateral Sclerosis', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Clinical Trials Design', 'Communities', 'Comparative Study', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Databases', 'Development', 'Enrollment', 'Frequencies', 'Goals', 'Healthcare Systems', 'Medical Records', 'Medical Research', 'Multiple Sclerosis', 'Natural Language Processing', 'Nature', 'Parkinson Disease', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Prevalence', 'Process', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Site', 'Source', 'Structure', 'Symptoms', 'System', 'Text', 'Time', 'Update', 'experience', 'information gathering', 'medical attention', 'patient population', 'public health relevance', 'social networking website', 'statistics', 'treatment response', 'web-based social networking']",NINDS,UNIVERSITY OF UTAH,R21,2009,219896,-0.043132214448643395
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7582189,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Body of uterus', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,312453,-0.015386046957030474
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7671509,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,1455866,-0.013404996850387759
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7671509,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,202086,-0.013404996850387759
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7922465,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,415228,-0.013404996850387759
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7911405,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'National Human Genome Research Institute', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'clinical care', 'clinical practice', 'data modeling', 'data sharing', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'patient population', 'phenome', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2009,229436,-0.013404996850387759
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7663288,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Computer Systems Development', 'Computer software', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Source', 'Structure', 'System', 'Systems Integration', 'Technology', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'meetings', 'models and simulation', 'open source', 'outreach', 'protein complex', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2009,437938,-0.00840333475153819
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7684273,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's Disease patient registry', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronics', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'National Human Genome Research Institute', 'National Institute on Aging', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Statistical Methods', 'System', 'Testing', 'Text', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'biobank', 'case control', 'cohort', 'cost', 'data sharing', 'development policy', 'economic cost', 'gene environment interaction', 'genome wide association study', 'genome-wide', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'population based', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2009,1039667,-0.04846333903882686
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7921317,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's Disease patient registry', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Electronics', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'National Human Genome Research Institute', 'National Institute on Aging', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Statistical Methods', 'System', 'Testing', 'Text', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'biobank', 'case control', 'cohort', 'cost', 'data sharing', 'development policy', 'economic cost', 'gene environment interaction', 'genome wide association study', 'genome-wide', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'population based', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2009,118469,-0.04846333903882686
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7790821,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Support', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'flexibility', 'innovation', 'interest', 'meetings', 'prevent', 'product development', 'public health relevance', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2009,4047,-0.01172319047677961
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7788875,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Ontology', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'empowered', 'genome wide association study', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'histone modification', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'public health relevance', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2009,142123,-0.018419452554687795
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7595813,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'biological systems', 'comparative', 'computer based statistical methods', 'data integration', 'design', 'flexibility', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface', 'web site']",NIGMS,PRINCETON UNIVERSITY,R01,2009,243004,-0.020035029184906298
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,-0.012993268554043225
"Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts    DESCRIPTION (provided by applicant):  Accurate and complete medication lists are critical inputs to effective medication reconciliation to prevent medication prescribing and administration errors. Previous research aggregated structured medication data form multiple sources to generate and maintain a reconciled medication list. Medications documented in clinical texts also need to be reconciled. However, most reconciliation methods currently have limited capability to process textual data and temporal information (e.g., dates, duration and status). Our goal is to pilot and test methodologies and applications in the fields of natural language processing (NLP) and temporal reasoning to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. Clinic notes and free-text ""comments"" fields in medication lists in an ambulatory electronic medical record system will be considered in the study. An NLP system and a temporal reasoning system will be adapted to automatically extract medication and associated temporal information from clinical texts and encode the medications using a controlled terminology. Multiple knowledge bases will be used to develop a mechanism to represent the timing of medication use, detect the changes (e.g., active or inactive), and then to organize medications into appropriate groups (e.g., by ingredient or by status). The feasibility and efficiency of the proposed methods and tools in improving the process of medication   reconciliation will be assessed. Domain experts will serve as judges to assess the success of capturing, coding, and organizing the medications and temporal information and also to evaluate whether our methods are complementary to those currently used for medication management.           Accurate and complete medication information at the point of care is crucial for delivery of high-quality care and prevention of adverse events. Most previous studies aggregated structured medication data from EMR and CPOE (Computerized Physician Order Entry) systems to generate and maintain a reconciled medication list. However, medications in non-structured narrative sources (such as clinic notes and free-text comments) must also be reconciled. Structured data presented in a standard, predictable form can be easily processed by a computer. By contrast, narrative data does not have a well-defined structure, so processing such data is very challenging. Our goal is to pilot and test methodologies and applications in the fields of natural language processing (any system that manipulates text) and temporal reasoning (e.g., identifying the timing of medication use) to facilitate the use of electronic clinical texts in order to improve the ""correctness"" and ""completeness"" of medication lists. The feasibility and efficiency of the proposed methods and tools in improving the process of medication reconciliation will be assessed.",Improving Outpatient Medication Lists Using Temporal Reasoning and Clinical Texts,7774682,R03HS018288,[' '],AHRQ,BRIGHAM AND WOMEN'S HOSPITAL,R03,2009,48782,-0.053441776594794294
"Free Text Gene Name Recognition One of the problems that is important for semantic processing of natural language text is named entity recognition. This problem seems to be inherently more difficult in the biological realm than it proved to be in the realm of business applications or news story analysis as in the MUC conferences.  Our interest in the issue stems from its potential importance in indexing and retrieval of information dealing with a particular gene or protein. However really high quality named entity recognition in biology would have many applications as a starting point for semantic analysis. In past work on this problem we developed a tagger for gene/protein name recognition in text called ABGene and subsequently a database of 20,000 sentences annotated for the occurrence of gene/protein names. The first 15,000 of these sentences formed the basis of the gene/protein mention recognition task for the BioCreative I Workshop held in 2004. Subsequent to the BioCreative I Workshop the whole 20,000 sentence corpus was revised by 1) Removing tokenization and instead providing the text of the original sentence; 2) changing the annotations to be character based instead of token based; 3) revising the annotation guidelines to deal with some of the problems which had become apparent in the Workshop; 4) correcting some erroneous annotations that had come to our attention. The resulting data has become known as the GENETAG corpus. It has at least one unique property. Many of the annotated entities have alternative annotations associated with them so that more than one answer is correct for a particular entity. We believe this is important as many entities can be annotated in more than one way and for quite a number there is no clear single correct answer.  In 2005 we were invited to be co-organizers of BioCreative II and to be responsible for the gene mention recognition task. For this purpose we gave out the first 15,000 sentences of GENETAG as practice and training data and the last 5,000 sentences were used for testing. Whereas 14 teams participated in BioCreative I, 21 teams participated in BioCreative II. The top F score obtained on the gene/protein mention task in BioCreative I was 83.2% while the top score in BioCreative II was 87.2%. Because there were some changes in the annotation guidelines and some corrections to the data, one cannot say definitively how much progress this represents, but it does suggest progress. Conditional random fields were much more commonly used in BioCreative II and new approaches to the use of unannotated data also appeared. We performed an analysis of the annotations provided by all the participants and applied a conditional random fields approach to learn how to combine all predictions to make an improved prediction. In this we used 200 fold cross validation. We were able to achieve a balanced F score of 90.7%. This indicates that there is yet room for improvement in how individual systems perform on the problem of gene/protein mention detection. (with Larry Smith and Lorrie Tanabe). We have become convinced that more information about the different types of entities that can occur in sentences in MEDLINE can be used to improve name recognition. This has led us to design a set of semantic categories and to attempt to fill these categories with actual names that can be harvested from databases and from web sites. We call the result SEMCAT. It currently recognizes seventy-five categories and contains about four million name strings distributed over those categories. We have experimented with probabilistic context free grammars and Markov models of text strings in an attempt to learn how to recognize the entities in different categories.  However, the best approach we have found for distinguishing the categories of gene/protein and not gene/protein is a new algorithm we term a priority model.  Every token associated with any name in SEMCAT has associated with it two probabilities. The first probability is the probability that the token indicates that it is part of a gene/protein name and the second probability is an indicator of how reliable the token is as an indicator. With this model, given a phrase, one can compute an estimate of the probability that the phase is a gene/protein name. We find that with the priority model we can achieve an F score of 96% as compared with 95% for our best PCFG approach. (with Lorrie Tanabe). The top performance for gene mention recognition in BioCreative II was by Rie Ando from IBM who introduced a technique called alternating structural optimization. This approach takes many labeling problems similar to named entity tagging, but simply tries to predict the occurrence of the names or the tokens from the surrounding textual context. When the SVM solution weight vectors for these many auxiliary problems have been learned, one performs a singular value decomposition and subtracts from each vector its first h components in the decomposition. This subtraction is only used to decrease the penalty in the regularization term of the cost function. The weight vectors are then relearned and the process is repeated. This is continued until convergence. The final result is a set of h components of the decomposition of the many weight vectors. One uses these components to enhance the learning on the actual named entity recognition task. This is a bit complicated and difficult to use. We are studying how we may be able to use a similar approach, but with a simpler method of applying the auxiliary learning to improve named entity recognition. One problem is how to combine such auxiliary learning with the SEMCAT data. n/a",Free Text Gene Name Recognition,7969225,ZIALM000093,"['Algorithms', 'Attention', 'Biological', 'Biology', 'Bite', 'Body of uterus', 'Businesses', 'Categories', 'Data', 'Databases', 'Detection', 'Educational workshop', 'Equilibrium', 'Gene Proteins', 'Genes', 'Genetic', 'Guidelines', 'Harvest', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Participant', 'Performance', 'Phase', 'Probability', 'Process', 'Property', 'Proteins', 'Proteomics', 'Semantics', 'Solutions', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Validation', 'Variant', 'Weight', 'Work', 'base', 'cost', 'design', 'improved', 'indexing', 'interest', 'markov model', 'natural language', 'news', 'novel strategies', 'phrases', 'research study', 'semantic processing', 'stem', 'symposium', 'vector', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2009,221141,-0.033069994979648985
"National Biomedical Information Services Delivering Biomedical Information Services   In FY 2009, NLM expanded the quantity and range of high quality information available to researchers, health professionals, and the general public. Among the NLMs intramural programs that contribute to its national biomedical information services are the following examples:  PubMed/MEDLINE:  PubMed, which incorporates MEDLINE, is NLMs premier bibliographic database with over 19 million references to biomedical journal articles.  MEDLINE articles are indexed by experts using the Medical Subject Headings (MeSH) controlled vocabulary, updated annually.  In FY2009, more than 700,000 new indexed citations were added.  PubMed Central:  The PubMed Central  archive of over 1.8 million full-text journal articles is central to the  NIH effort to make accessible the published results of research it supports.  In FY09,  NLM enhanced the NIH Manuscript Submission system (NIHMS) by which authors can submit articles to PubMed Central in compliance with the NIH Public Access Policy.  NLM has also made PMC software available to archiving organizations in the UK and Canada.   MedlinePlus and MedlinePlus en espaol:  MedlinePlus and the Spanish language MedlinePlus en espaol provide access to consumer health information on more than 800 diseases and conditions.  Recent enhancements included improved search capabilities, addition of summary information, and expansion to include information in more than 40 languages.   Clinical Trials:  ClinicalTrials.gov contains information on more than 79,000 clinical research studies in more than 169 countries, with hundreds added each week. It was significantly expanded to respond to new clinical trial registration and results reporting requirements established by the FDA Amendments Act of 2007 (PL 110-85).  In FY09, some 32,000 trials were registered, raising the total to 79,000.  Since NLM implemented the results database required by the law in September 2008, summary results of more than 880 clinical trials have been added.  In September 2009, the previously optional adverse events module of the results database became mandatory. Toxicology and Environmental Health:  The Toxicology Data Network (TOXNET) is a primary reference for toxicologists, poison control centers, public health administrators, physicians and other environmental health professionals, and includes databases such as Hazardous Substances Data Bank, TOXLINE, GENE-TOX, and the Toxic Release Inventory.    Influenza Virus Resource: The Influenza Virus Resource contains influenza virus sequences and enables researchers to compare different virus strains, identify genetic factors that determine the virulence of virus strains, and look for new therapeutic, diagnostic and vaccine targets. Updated daily, it includes over 90,000 influenza sequences and more than 2,000 complete genomes. Disaster Preparedness and Response:   NLMs Disaster Information Management Research Center facilitates access to disaster information, promotes effective use of libraries and disaster information specialists in disaster management efforts, and supports initiatives to ensure uninterrupted access to critical health information resources when disasters occur.  A collaboration with the Bethesda Hospital Emergency Preparedness Partnership provides backup communication systems and develop tools for patient tracking, information sharing and access, and responder training, and to serve as a model for hospitals across the nation.  Molecular Biology, Bioinformatics, and Human Genome Resources:  NCBI information resources include molecular biology databases and bioinformatics software tools such as GenBank, Entrez, BLAST, RefSeq, UniGene, LocusLink, annotation and assemply of complete genomes, and the NCBI software toolkit.  NCBI also produces the information retrieval systems for the PubMed, PubMed Central,  and the NCBI Bookshelf. Some areas of emphasis in FY09  augmenting the Short Read Archive of more than 400GB  of raw sequence data derived from massively parallel sequencing technologies; augmenting dbGAP with more than 20 GWAS studies and developing a new access system using the high-efficiency FASP protocol; and continuing the discovery initiative to alleviate difficulties in finding relevant information in diverse resources and develop interface improvements such as Releated Reviews and Patient Drug Information.   Outreach: Promoting Public Awareness and Access  Consumer health websites and the NIH MedlinePlus Magazine transmit the latest useful research findings in lay language. In FY09, NLM increased  distribution of the magazine to 600,000, launched a  Spanish language edition, Salud!, and introduced online versions the magazines.  Special population websites address specific minority health concerns in various racial and ethnic groups. NLM   outreach programs enhance awareness of its information services. Special attention is given to minority groups and other underserved populations, including African American, Hispanic, and Native American communities, as well as health professionals serving minority populations and practicing in rural and inner city communities. In FY 2009, dozens of community-based projects were funded across the country.  Health Services Research  NICHSR improves access to health services research through information systems such as: HSRProj, a database covering over 5000 ongoing or recently completed health services research projects; HSRR (Health Services and Sciences Research Resources), a  database of research datasets, instruments and software relevant to health services research, behavioral and social sciences, and public health; and HSTAT (Health Services/Technology Assessment Text), a full-text database including evidence reports, guidelines technology assessments, consensus statements, and treatment protocols.  Advanced Information Systems, Data Standards and Research Tools  In FY 2009, LHC and NCBI continued to conduct research in biomedical informatics and computational biology information systems, tested the effectiveness of medical informatics interventions, and developed new scientific computing tools. To cite a few examples, intramural researchers developed tools that support standards-based personal health records; applied natural language processing methods to extract information from biomedical literature; improved automatic detection of gene and protein names in scientific text; and provided software tools that enabled rapid expansion of the PubChem database. NLM made substantial contributions toward standardized reporting of genetic variations and clinical interpretation of genetic test results by augmenting RefSeqGene and dbSNP; expanding the LOINC ; and launching the Newborn Screening Coding and Terminology Guide to enable more effective use of newborn screening test results.   Health Data Standards: As the central coordinating body for clinical terminology standards within DHHS, NLM supports nationwide implementation of an interoperable health information technology infrastructure. NLM develops or licenses key clinical terminologies and problem lists designated as standards for U.S. health information exchange. The Unified Medical Language System Metathesaurus, with more than 7.7 million concept names from more than 100 vocabularies, is a distribution mechanism for standard code sets and vocabularies used in health data systems. NLM also produces RxNorm, a standard clinical drug vocabulary; supports the LOINC nomenclature for laboratory tests and patient observations; and promotes international adoption of the SNOMED CT clinical terminology. In FY09, NLM released the first version of the CORE Problem List Subset of SNOMED CT and launched the Newborn Screening Codes and Terminology Guide, which provides a standard framework for reporting the results of newborn screening tests. n/a",National Biomedical Information Services,7970410,ZIHLM200888,"['Address', 'Administrator', 'Adoption', 'Adverse event', 'African American', 'Amendment', 'Archives', 'Area', 'Attention', 'Awareness', 'Bibliographic Databases', 'Bioinformatics', 'Biotechnology', 'Blast Cell', 'Canada', 'Clinical', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Computational Biology', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Country', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnostic', 'Disasters', 'Disease', 'Effectiveness', 'Electronic Health Record', 'Emergency Situation', 'Ensure', 'Environmental Health', 'Ethnic group', 'Family', 'Fellow of American College of Physicians', 'Fostering', 'Funding', 'Genbank', 'Gene Proteins', 'General Population', 'Genes', 'Genetic', 'Genetic Variation', 'Genetic screening method', 'Genome', 'Genomics', 'Goals', 'Guidelines', 'Hazardous Substances', 'Health', 'Health Policy', 'Health Professional', 'Health Sciences', 'Health Services', 'Health Services Accessibility', 'Health Services Research', 'Health Technology', 'Healthcare', 'Hispanics', 'Hospitals', 'Human Genome', 'Imagery', 'Improve Access', 'Influenza', 'Information Centers', 'Information Dissemination', 'Information Management', 'Information Resources', 'Information Retrieval Systems', 'Information Services', 'Information Specialists', 'Information Systems', 'Information Technology', 'International', 'Intervention', 'Intramural Research Program', 'Journals', 'Laboratories', 'Language', 'Laws', 'Libraries', 'Licensing', 'Literature', 'Logical Observation Identifiers Names and Codes', 'MEDLINE', 'Manuscripts', 'MeSH Thesaurus', 'Medical', 'Medical Informatics', 'MedlinePlus', 'Methods', 'Minority', 'Minority Groups', 'Modeling', 'Molecular Biology', 'Names', 'Native Americans', 'Natural Language Processing', 'Neonatal Screening', 'Nomenclature', 'Patient observation', 'Patients', 'Personal Health Records', 'Pharmaceutical Preparations', 'Physicians', 'Play', 'Poison Control Centers', 'Policies', 'Policy Maker', 'Population', 'Practice Guidelines', 'Process', 'Protocols documentation', 'PubMed', 'Public Health', 'Publishing', 'Readiness', 'Reading', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Role', 'Rural', 'SNOMED Clinical Terms', 'Science', 'Services', 'Software Tools', 'Speed', 'System', 'Technology', 'Technology Assessment', 'Terminology', 'Test Result', 'Testing', 'Text', 'Time', 'Toxicology', 'Toxicology Data Network', 'Toxics Release Inventory', 'Training', 'Treatment Protocols', 'Underserved Population', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Vaccines', 'Virulence', 'Virus', 'Vocabulary', 'base', 'behavioral/social science', 'biomedical informatics', 'biomedical information system', 'database of Genotypes and Phenotypes', 'genome wide association study', 'health disparity', 'health information technology', 'health literacy', 'improved', 'indexing', 'influenza virus resource', 'influenzavirus', 'inner city', 'instrument', 'journal article', 'knowledge base', 'language processing', 'metathesaurus', 'minority health', 'named group', 'novel therapeutics', 'outreach', 'outreach program', 'racial and ethnic', 'research and development', 'research study', 'response', 'scientific computing', 'tool', 'web site']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2009,250355446,-0.018213664158360465
"Continued Development and Evaluation of caTIES    DESCRIPTION (provided by applicant): We propose to further develop, test, evaluate and support caTIES - an existing software system for developing networked repositories of sharable de-identified surgical pathology reports. The caTIES system creates a repository of de-identified, structured, and concept-coded clinical reports derived from large corpora of clinical free-text. Documents are automatically coded against a controlled terminology such as the Unified Medical Language System (UMLS), SNOMED-CT, or NCI Metathesaurus. Users construct queries to identify specific kinds of documents and tissue specimens based on the associated clinical report. For example, a researcher studying genetic variation in metastatic breast cancers can identify cases of invasive ductal carcinoma of the breast, followed by metastatic ductal cancer in bone at an interval of three years or greater from the original diagnosis. The caTIES system also supports acquisition and ordering of tissues, using an honest broker model. Through this mechanism, de-identified data and access to tissue can be shared among institutions, enabling multi-center collaborative research. The caTIES system has already been implemented at seven US Cancer Centers, and is being considered for adoption by numerous other institutions including cancer centers, university hospitals and private hospitals. Initial development of caTIES was funded by the Cancer Biomedical Informatics Grid (caBIG). However, interest in the application has far exceeded our expectations and the limitations of caBIG. This grant will allow us to further extend the capabilities of the system by (a) improving the portability of the system and extending the types of documents that can be processed, (b) evaluating the system's NLP performance and usability, (c) building a user community to support this open-source application, and (d) piloting interoperability of caTIES with other enterprise and research systems. This work will preserve and extend a highly novel platform for development of massive repositories of de-identified clinical data that can be used for research within and across institutions. PUBLIC HEALTH RELEVANCE: This grant will fund the further development and evaluation of a system that takes identified clinical documents and converts them into de-identified, concept-coded, structured data. The system enables researchers to access remainder tissues and clinical report data for research purposes within and across institution. This project is important because it will greatly increase the access of researchers to important data and materials while maintaining patient privacy.          n/a",Continued Development and Evaluation of caTIES,7558128,R01CA132672,"['Access to Information', 'Adoption', 'Body of uterus', 'Cancer Center', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Code', 'Communication', 'Communities', 'Community Developments', 'Computer software', 'Data', 'Data Reporting', 'Database Management Systems', 'Development', 'Diagnosis', 'Documentation', 'Ductal', 'Electronic Mail', 'Eligibility Determination', 'Environment', 'Evaluation', 'Funding', 'Future', 'Genetic Variation', 'Genomics', 'Grant', 'Information Retrieval', 'Institution', 'Malignant Neoplasms', 'Methods', 'Metric', 'Modeling', 'Modification', 'Natural Language Processing', 'Operating System', 'Pathology Report', 'Performance', 'Private Hospitals', 'Process', 'Report (document)', 'Reporting', 'Research', 'Research Personnel', 'Services', 'Specimen', 'Structure', 'Surgical Pathology', 'System', 'Terminology', 'Testing', 'Text', 'Tissues', 'Training', 'Translational Research', 'Unified Medical Language System', 'University Hospitals', 'Vocabulary', 'Work', 'base', 'bone', 'cancer Biomedical Informatics Grid', 'clinical phenotype', 'computer human interaction', 'ductal breast carcinoma', 'expectation', 'flexibility', 'improved', 'interest', 'interoperability', 'malignant breast neoplasm', 'meetings', 'metathesaurus', 'novel', 'open source', 'patient privacy', 'portability', 'public health relevance', 'repository', 'software development', 'software systems', 'systems research', 'tool', 'usability']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,314363,-0.048103796657499774
"Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms    DESCRIPTION (provided by applicant): Intracranial aneurysms (ICAs) are an increasingly common finding, both from incidental discovery on imaging studies and on autopsy; it is estimated that anywhere from 1-6% of the American population will develop this problem. Unfortunately, while our ability to detect ICAs has grown, our fundamental understanding of this disease entity remains lacking and significant debate continues in regards to its treatment. Given the high degree of mortality and comorbidity associated with ruptured intracranial aneurysms, it is imperative that new insights and approaches be developed to inform medical decision making involving ICAs. Thus, the objective of this proposal is the creation of an informatics infrastructure to help elucidate the genesis, progression, and treatment of intracranial aneurysms. Building from our efforts from the previous R01, a set of technical developments is outlined to transform the array of information routinely collected from clinical as- sessment of ICA patients into a Bayesian belief network (BBN) that models the disease. First, we evolve the concept of a phenomenon-centric data model (PCDM) as the basis for (temporally) organizing clinically-derived observations, enabling the model to be associated with processing pipelines that can identify and transform targeted variables from the content of clinical data sources. Through these pipelines, specific values in free- text reports (radiology, surgery, pathology, discharge summaries) and imaging studies will be automatically extracted into a scientific-quality database. Second, the PCDM schema for ICAs is mapped to a Bayesian belief network: the linkage between the PCDM and BBN allows automatic updating of the network and its progressive refinement from a growing dataset. The BBN's topology will be determined by clinical experts and conditional probabilities computed from the extracted clinical data. A basic graphical user interface (GUI) will permit users to interact with the BBN, aiding in medical decision making tasks. The GUI will allow a clinician to pose questions from either a set of common clinical queries or to create new queries: loading a patient's medical record into this application will automatically populate BBN variables with extracted information (i.e., from the pipelines). Each technical component of this proposal will be evaluated in a laboratory setting. In addition, the BBN will be tested for its predictive capabilities and compared to other statistical models to assess its potential in guiding ICA treatment. This proposal leverages a clinical collaboration with the UCLA Division of Interventional Neuroradiology, a leader in ICA research and treatment. A combined dataset of 2,000 retrospective and prospective subjects will be used to create the ICA database and BBN. Data collection will encompass a comprehensive set of variables including clinical presentation, imaging assessment (morphology, hemodynamics), histopathology, gene expression, treatment, and outcomes. We will additionally leverage the NIH/NINDS Human Genetic DNA and Cell Line Repository for additional ICA-related data. PUBLIC HEALTH RELEVANCE: Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data routinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.           PROGRAM NARRATIVE Intracranial aneurysms (ICAs) are an increasingly common finding on routine computed tomography (CT) and magnetic resonance (MR) neuro-imaging studies. The associated mortality rate and comorbidity resultant from ruptured ICAs are extreme: subarachnoid hemorrhage causes 50% of individuals to die within one month of rupture, and more than one third of survivors develop major neurological deficits. Hence, the focus of this re- search is the creation of a comprehensive research database for ICA patients, using the spectrum of data rou- tinely acquired in the diagnosis and treatment of the problem; from this database, a new probabilistic model of ICAs will be created, providing new insights into the disease and its optimal treatment for a given individual.",Toward Individually-tailored Medicine: Probabilistic Models of Cerebral Aneurysms,7727890,R01EB000362,"['Affect', 'American', 'Architecture', 'Autopsy', 'Belief', 'Cell Line', 'Cerebral Aneurysm', 'Clinical', 'Clinical Data', 'Collaborations', 'Collection', 'Comorbidity', 'Control Groups', 'DNA', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Disease', 'Disease Management', 'Disease model', 'Etiology', 'Future', 'Gene Expression', 'General Population', 'Genetic', 'Genomics', 'Healthcare', 'Histopathology', 'Human Genetics', 'Image', 'Incidental Discoveries', 'Individual', 'Informatics', 'Institution', 'Intracranial Aneurysm', 'Knowledge', 'Laboratories', 'Logistic Regressions', 'Magnetic Resonance', 'Manuals', 'Maps', 'Medical', 'Medical Records', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'National Institute of Neurological Disorders and Stroke', 'Natural Language Processing', 'Neurologic', 'Operative Surgical Procedures', 'Pathology', 'Patient Care', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Process', 'Radiology Specialty', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Rupture', 'Statistical Models', 'Stroke', 'Subarachnoid Hemorrhage', 'Survivors', 'Tail', 'Techniques', 'Testing', 'Text', 'Translations', 'Treatment outcome', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'X-Ray Computed Tomography', 'base', 'biomedical informatics', 'computer based statistical methods', 'data modeling', 'graphical user interface', 'hemodynamics', 'improved', 'innovation', 'insight', 'mortality', 'nervous system disorder', 'network models', 'prognostic', 'programs', 'prospective', 'public health relevance', 'repository', 'statistics', 'tool', 'trend']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2009,568500,0.0021754796037888485
"Natural Language Processing Techniques To Enhance Information Access. Recently we have been involved in four subprojects which use natural language processing techniques:  1) The presence of unrecognized abbreviations in text hinders indexing algorithms and adversely affects information retrieval and extraction.  Automatic abbreviation definition identification can help resolve these issues.  However, abbreviations and their definitions identified by an automatic process are of uncertain validity.  Due to the size of databases such as MEDLINE only a small fraction of abbreviation-definition pairs can be examined manually.  An automatic way to estimate the accuracy of abbreviation-definition pairs extracted from text is needed.  We have proposed an abbreviation definition identification algorithm that employs a variety of strategies to identify the most probable abbreviation definition.  In addition our algorithm produces an accuracy estimate, pseudo-precision, for each strategy without using a human-judged gold standard. The pseudo-precisions determine the order in which the algorithm applies the strategies in seeking to identify the definition of an abbreviation. The results are generally a couple of percentage points better than the Schwartz-Hearst algorithm and also allow one to enforce a threshold for those applications where high precision is critical.  2) A significant fraction of queries in PubMed are multiterm queries and PubMed generally handles them as a Boolean conjunction of the terms. However, analysis of queries in PubMed indicates that many such queries are meaningful phrases, rather than simply collections of terms. We have examined whether or not it makes a difference, in terms of retrieval quality, if such queries are interpreted as a phrase or as a conjunction of query terms. And, if it does, what is the optimal way of searching with such queries. To address the question, we developed an automated retrieval evaluation method, based on machine learning techniques, that enables us to evaluate and compare various retrieval outcomes. We show that classes of records that contain all the search terms, but not the phrase, qualitatively differ from the class of records containing the phrase. We also show that the difference is systematic, depending on the proximity of query terms to each other within the record. Based on these results, one can establish the best retrieval order for the records. Our findings are consistent with studies in proximity searching. The important insight here for indexing is that in some cases where the words of a phrase occur in text, but not as the phrase, the phrase may still be an appropriate concept to use in indexing the text. 3) We have developed a spell checking algorithm that does quite accurate correction ( 87%) and handles one or two edits, and more edits if the string to be corrected is sufficiently long. It handles words that are fragmented or merged. Where queries consist of more than a single token the algorithm attempts to make use of the additional information as context to aid the correction process. The algorithm is based on the noisy channel model of spelling correction and makes use of statistics on miss-spellings gathered from approximately one million miss-spelling incidents in the PubMed log files. These incidents were identified as cases where a user entered a query and then within five minutes corrected that query to another term which is close in edit distance and with at least ten times as many hits in the PubMed database. These statistics are not only used in the actual correction process, but were used to simulate miss-spellings in real words and phrases to discover the regions of validity of the method of correction and estimates of its accuracy. Additional work was done on the vocabulary of the PubMed database to remove frequent miss-spellings and improve performance. The algorithm is implemented in the PubMed search engine and there it frequently makes over 200,000 suggestions in a day and about 45% of these suggestions are accepted by users. The algorithm is efficient in adding only about 25% to the average query response time for users and much of this is seen only for misspelled queries. There is the possibility of improving the algorithm by the use of more context around the sites of errors within words. There is also the possibility of improving the algorithm by learning how to make better use of the context supplied by queries consisting of multiple tokens. But in both cases such an effort must consider how to maintain efficiency in the light of a huge vocabulary of phrases (>14 million) and individual words (>2.5 million) recognized by the search engine. There is also the possibility to use phonetic encodings to improve the handling of some of the errors that currently challenge the system. However, preliminary calculations suggest it would be difficult to make a major improvement by using phonetic encodings. 4) We explored a syntactic approach to sentence compression in the biomedical domain, grounded in the context of result presentation for related article search in the PubMed search engine. By automatically trimming inessential fragments of article titles, a system can effectively display more results in the same amount of space. Our implemented prototype operates by applying a sequence of syntactic trimming rules over the parse trees of article titles. Two separate studies were conducted using a corpus of manually compressed examples from MEDLINE: an automatic evaluation using Bleu and a summative evaluation involving human assessors. Experiments show that a syntactic approach to sentence compression is effective in the biomedical domain and that the presentation of compressed article titles supports accurate interest judgments, decisions by users as to whether an article is worth examining in more detail. n/a",Natural Language Processing Techniques To Enhance Information Access.,7735077,Z01LM000090,"['Abbreviations', 'Address', 'Affect', 'Algorithms', 'Automated Abstracting', 'Body of uterus', 'Class', 'Classification', 'Collection', 'Data', 'Databases', 'Evaluation', 'Gold', 'Human', 'Individual', 'Information Retrieval', 'Judgment', 'Learning', 'Light', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Natural Language Processing', 'Outcome', 'Performance', 'Phonetics', 'Process', 'PubMed', 'Reaction Time', 'Records', 'Retrieval', 'Simulate', 'Site', 'Standards of Weights and Measures', 'Suggestion', 'System', 'Techniques', 'Text', 'Time', 'Title', 'Trees', 'Vocabulary', 'Work', 'base', 'concept', 'day', 'improved', 'indexing', 'insight', 'interest', 'prototype', 'research study', 'size', 'spelling', 'statistics', 'syntax']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2008,224159,-0.02015255494309796
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7675157,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Language', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'day', 'forgetting', 'innovation', 'preference', 'speech processing', 'symposium']",NLM,UNIVERSITY OF CHICAGO,R01,2008,354823,-0.01256280053605713
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7498449,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2008,153203,-0.028449872848548337
"General and Semi-supervised Machine Learning Applied to Bioinformatics 1) Many different methods have been investigated for the purpose of clustering sets of documents with the hope of improving retrieval. Unfortunately these have generally failed to provide improved retrieval capability. Part of the problem is clearly the fact that a given document often involves more than one subject so that it is not possible to make a clean categorization of the documents into definite categories to the exclusion of others. In order to overcome this problem we have developed methods that are designed to identify a theme among a set of documents. The theme need not encompass the whole of any document. It only needs to exist in some subset of the documents in order to be identifiable. Some of these same documents may participate in the definition of several themes. One method of finding themes is based on the EM algorithm and requires an iterative procedure which converges to themes. The method has been implemented and tested and found to be successful.  2) A second approach can be based on the singular value decomposition and essentially is a vector approach. 3) We are also investigating other methods to extract higher level features. One method of interest is the method known as sparse coding, which is  the basis of self-taught learning. n/a",General and Semi-supervised Machine Learning Applied to Bioinformatics,7735076,Z01LM000089,"['Algorithms', 'Bioinformatics', 'Categories', 'Code', 'Educational process of instructing', 'Exclusion', 'Learning', 'Literature', 'Machine Learning', 'Methods', 'Procedures', 'Purpose', 'Retrieval', 'Techniques', 'Testing', 'base', 'design', 'improved', 'interest', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2008,86215,0.005040959865546382
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7468470,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2008,286582,-0.004540319095064122
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) The latest work on this system has involved adding the ability to generate themes using an EM algorithm approach. Also recently code has been multithreaded and memory mapping capabilities added to speed up processing.  The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system has been used for mining email communications for the NLM help desk. n/a",A Document Processing System,7735064,Z01LM000022,"['Algorithms', 'Class', 'Code', 'Communication', 'Data', 'Databases', 'Electronic Mail', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'Memory', 'Methods', 'Mining', 'Numbers', 'Online Systems', 'Process', 'Programming Languages', 'PubMed', 'Purpose', 'Records', 'Research', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Work', 'base', 'computerized data processing', 'repository', 'software systems']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2008,206916,-0.005707392313372737
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.  1)	In the literature on the Nave Bayes machine learning method there have long been two models that have been used. One is the multivariate Bernoulli model (MBM) and the other the multinomial model (MM).  The MBM method only counts the presence or absence of a feature, while the MM method counts the number of times a feature appears in a record. A number of comparisons of the two approaches have been made in the area of text categorization and the MM approach has usually won out. It is our belief that the reason for this is that the MBM model has not been properly optimized. In fact we have found that in the area of text categorization the local term frequency contributes virtually nothing to the performance of the MM model. In support of this contention we have developed a simplified form of the MM model which ignores local term frequency (but is still much closer to the MM than to the MBM) and we find that it performs essentially the same as the MM model. In fact we do not find an advantage for the use of local term frequency in text categorization using MM or in several other models including the SVM. The advantage to ignoring local term frequency is that it greatly simplifies the data storage and the calculations when applying the Nave Bayes approach to a very large database such as PubMed. One aspect of this work which calls for further investigation is what happens when the records are long and the local frequencies can then be much larger. We do not have the final answer, but  our initial work with the TREC genomics data (160,000 full text documents) suggests that there is a small advantage in the use of local term frequencies in some models, but the advantage is not in any case over about a 3% improvement in break even point.  2)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 3)	The PubMed database presents a unique challenge because of its very large size of over 18 million records. Because of this size few machine learning methods can be applied with a reasonable turn-around time. One method that can be applied efficiently is Nave Bayes, but it performs poorly when the different classes to be distinguished exhibit a marked size discrepancy. But such an imbalance is common for the problems one wishes to study in PubMed.  In such a situation we have discovered that a training set much smaller than the whole set can be selected by an active learning inspired method. The result yields an almost 200% improvement in the performance of Nave Bayes in classifying documents for MeSH term assignment. The results are significantly better than a KNN method and there is the added advantage that the optimal training sets defined in this way can be used as the training sets for more sophisticated machine learning methods with even better results than those obtained from Nave Bayes.  4) We compute the documents related to a document using a probability calculation based on two Poisson distributions, one for the terms in a document that are more central to the documents content and one for the terms that are more peripheral. These are combined into a probability estimate of the importance of a term in a document based on its relative frequency in the document. This probability estimate is combined with the global IDF weight of a term to account for that terms importance in computing the similarity between two documents. We have known from the time this approach was developed that it worked well. In the last several years data has become available in the TREC genomics track that has allowed us to test this approach by comparing it with the results of the bm25 formula developed by Robertson and colleagues. We find a small but statistically significant advantage for our probabilistic approach. n/a",Automatic Bayesian Methods In Text Retrieval,7735063,Z01LM000021,"['Accounting', 'Active Learning', 'Area', 'Bayesian Method', 'Belief', 'Class', 'Classification', 'Count', 'Data', 'Data Storage and Retrieval', 'Databases', 'Exhibits', 'Frequencies', 'Genomics', 'Goals', 'Investigation', 'Label', 'Literature', 'Machine Learning', 'MeSH Thesaurus', 'Methods', 'Modeling', 'Numbers', 'Performance', 'Peripheral', 'Poisson Distribution', 'Probability', 'PubMed', 'Records', 'Relative (related person)', 'Resources', 'Retrieval', 'Sampling', 'Statistically Significant', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Testing', 'Text', 'Time', 'Training', 'Uncertainty', 'Weight', 'Work', 'base', 'improved', 'interest', 'novel strategies', 'size']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2008,327617,-0.00989483206664155
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7432910,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Eukaryotic Cell', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'desire', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,273906,-0.03083073596140968
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7504002,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2008,176002,-0.016442598182056248
"Using Natural Language Processing to Monitor Product Claims Compliance for FDA    DESCRIPTION (provided by applicant): Linguastat, Inc. proposes to develop a means to automate the process of monitoring and identifying companies engaged in false advertising and deceptive practices in the marketing of drugs, dietary supplements, and/or food products. By leveraging state of the art approaches in computational linguistics such as Information Extraction and Natural Language Processing, it should be feasible, with some adaptation, to use this technology to: 1) automatically and continuously monitor the websites, TV transcripts, press releases and other electronic marketing text communications of tens of thousands of companies for various claims and product information 2) automatically ""red-flag"" instances in which claims have a high likelihood of potential harm to consumers, according to FDA priorities 3) automatically identify and extract the companies, products and claims embedded in electronic product information and electronic promotional materials to create a database easily searchable by the FDA and 4) automatically capture web-based or other electronic content for human review and store it as ""evidence."" Such automated technology would enable the FDA to significantly stretch its limited human resource to more effectively and comprehensively identify noncompliant product information, detect deceptive ads and other illegal practices, successfully prosecute offenders, and prevent harm to American consumers. For this Phase I SBIR project we propose to assess the feasibility of automated claims monitoring in three steps: In the first step, we will train information extraction and natural language processing algorithms to extract product marketing claims from text. In the second, step we will apply data mining and rules-based algorithms to assess which claims are likely to be non-compliant and merit further attention by FDA staff. In the third step, we will design and build a database of product claims that allows analysts to search, organize, and prioritize product claims based on the type of claim (e.g. what ailments does the product claim to treat), the type of product, and the likelihood of non- compliance. This technology will enable regulators and consumers to better monitor and detect cases of false, misleading, or deceptive advertising and product information. By enabling more effective enforcement of FDA regulations and giving consumers tools to make better buying decisions, the public health can be better protected by minimizing the impact of products that cause harm, give false hope, or entice consumers to forgo conventional remedies.          n/a",Using Natural Language Processing to Monitor Product Claims Compliance for FDA,7677599,R43FD003406,[' '],FDA,"LINGUASTAT, INC.",R43,2008,20000,-0.03299564458022935
"The Miner Suite: State of the Art Bioinformatic Tools and Data Resources The Genomics and Bioinformatics Group (GBG) represents the Center for Cancer       Research's most substantial enterprise in bioinformatics. That bioinformatic       activity is well-integrated with the experimental activities of the group. In fact, all of the       algorithmic and software developments have been motivated by needs of the group's       experimental science. Over the last several years we've developed the       ""Miner Suite"" as a state-of-the-art, web-based, portable, principally-Java       set of tools and databases focused on needs that we found were not being served by other       bioinformatic developments. Our web site, http://discover.nci.nih.gov, has been featured in       Science and, locally, in the NIH Catalyst. The following is a list of the Miner Suite modules,       ending with four that went online in 2007: CellMiner, SpliceMiner, AffyProbeMiner, and       LeFEminer: * MedMiner accelerates searching and organization of PubMed literature       5-10 fold (L. Tanabe, et al.). * MatchMiner translates among gene identifiers for       lists of genes from microarrays and other high-throughput omic platforms (K. Bussey, et al.).       * GoMiner leverages the Gene Ontology for lists of genes (e.g., from microarrays)       (B. Zeeberg, et al.). * CIMminer produces Clustered Image Maps (CIMs) (i.e.,       ""clustered heat maps""), the ever-present icon of       'Postgenomic' biology. We introduced CIMs in the early 1990's.       * LeadScope/LeadMiner links molecular targets (e.g., in the NCI-60 cancer cell       lines) to 27,000 chemical substructures in a fluently browsable format (with P. Blower, et       al.). * MIMminer provides searchable electronic forms (eMIM's) of the       elegant, scholarly Molecular Interaction Maps developed by K. Kohn, M. Aladjem, and Y.       Pommier. * High-throughput GoMiner extends GoMiner's statistics and       visualizations to encompass large sets of microarrays (e.g., from time course studies or       clinical trials) (B. Zeeberg, et al.). * AbMiner provides a browsable relational       database of available monoclonal antibodies that we have characterized. It includes our       quality control results and multiple link-outs (S. Major, et al.). * SmudgeMiner       diagnoses spatial artifacts on Affymetrix and spotted arrays (M. Reimers, et al.). *       MethMiner provides pattern visualization and statistics for DNA methylation. Program not yet       public. (S. Kim, et al.). Miner Suite programs that went online publicly in 2007 were as       follows: * CellMiner provides the NCI-60 and other molecular profile databases in a       SQL-searchable relational format, with metadata on the experimental platforms and cell types       (U. Shankavaram, S. Varma, et al. -- see 2007 publication in Molecular Cancer Therapeutics).       * SpliceMiner provides a robust infrastructure for dealing with transcript splice       variation (A. Kahn, M. Ryan, et al. -- see 2007 publication in BMC Bioinformatics).       * AffyProbeMiner provides what we believe to be the best tool for re-mapping       Affymetrix probes to achieve sharper results from the commercially available Affymetrix       arrays. It also integrates the re-mapped array data with GoMiner and High-Throughput GoMiner       (H.-F. Liu, B. Zeeberg, et al. -- see 2007 publication in Bioinformatics). *       LeFEminer is a web-based implementation of the LeFE (Learner of Functional Enrichment)       algorithm, which uses the random forest machine-learning paradigm in a novel way to predict       functional relationships from microarray and other high-throughput data types. (G. Eichler, et       al. -- see 2007 publication in Genome Biology). Those programs, largely designed and       implemented under a competed contract with SRA International, (contract #263-01-D-0050,       CIO-SP2 Delivery Order 2313028) are freely available and used by thousands of investigators       worldwide. The success of the programs is attributable in part to our adoption of the Agile       software development paradigm, which promotes close, iterative interaction between software       engineers, biologists, and bioinformaticists. That success is also partially attributable to       adoption of Unit and System Testing methods; whenever code is re-deposited in our version       control system, it's automatically subjected to more than 1700 tests to minimize the       chance that changes made have broken something elsewhere in the overall code base. The       development team won an SRA Project Excellence Award for the Miner Suite (1 of 4 awarded out       of more than 700 competing projects). We're integrating the Miner Suite applications       with a variety of public bioinformatic software projects, including caBIG, The Cancer Genome       Atlas, and the CGEMS genome-wide association project. * caBIG: We were awarded two       caBIG pilot grants (one to caBIG-enable GoMiner, the other to caBIG-enable our NCI-60       databases). The group, particularly lead software engineer David Kane, has made strong       contributions to caBIG's Integrative Cancer Research and Architecture Working       Groups. * The Cancer Genome Atlas: Working with TCGA development staff, we used       SpliceMiner as the basis for development of bioinformatic infrastructure to support the       integration of genotypic/phenotypic data of multiple types. That infrastructure has now been       adopted by TCGA's Data Integration Committee for the purpose. * CGEMS:       Working with bioinformaticists in the NCI Division of Cancer Epidemiology and Genetics (DCEG),       we have developed a program package provisionally named ChromMiner for visual integration of       SNP data from the genome-wide association studies with phenotypic data on the cancers       (including gene expression and comparative genomic hybridization data). That program package       is central to a proposal, which I was instrumental in drafting, to combine the       epidemiological/genotypic expertise of DCEG with the phenotypic expertise of CCR. The proposal       is titled ""DCEG/CCR Plan for Follow-up of Genomic Regions of Association Identified       by CGEMS."" n/a",The Miner Suite: State of the Art Bioinformatic Tools and Data Resources,7733275,Z01BC010842,"['Adopted', 'Adoption', 'Algorithms', 'Architecture', 'Arts', 'Atlases', 'Award', 'Binding', 'Bioinformatics', 'Biological', 'Biology', 'CCR', 'Cancer cell line', 'Categories', 'Chemicals', 'Clinical Trials', 'Code', 'Communities', 'Complex', 'Computer software', 'Computers', 'Contracts', 'DNA Methylation', 'Data', 'Databases', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Division of Cancer Epidemiology and Genetics', 'Electronics', 'Gene Expression', 'Genes', 'Genetic Markers', 'Genome', 'Genomics', 'Goals', 'Grant', 'Heating', 'Image', 'Imagery', 'International', 'Internet', 'Java', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metadata', 'Methods', 'Microarray Analysis', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Monoclonal Antibodies', 'Morphologic artifacts', 'NCI Center for Cancer Research', 'Names', 'Online Systems', 'Ontology', 'Pattern', 'Personal Satisfaction', 'Predisposition', 'PubMed', 'Publications', 'Publishing', 'Purpose', 'Quality Control', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Site', 'Software Engineering', 'Spottings', 'Staff Development', 'System', 'Testing', 'Therapeutic', 'Time', 'Transcript', 'Translating', 'United States National Institutes of Health', 'Variant', 'Visual', 'Work', 'anticancer research', 'base', 'cancer Biomedical Informatics Grid', 'cancer genetics', 'cancer genome', 'catalyst', 'cell type', 'comparative genomic hybridization', 'data integration', 'design', 'follow-up', 'forest', 'genome wide association study', 'mecarzole', 'novel', 'programs', 'software development', 'statistics', 'success', 'tool']",NCI,DIVISION OF BASIC SCIENCES - NCI,Z01,2008,336151,-0.028327456439942097
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,7477279,R01LM005652,"['Active Sites', 'Algorithms', 'Appendix', 'Area', 'Basic Science', 'Binding', 'Binding Sites', 'Biochemical', 'Biological', 'Biology', 'Catalysis', 'Communities', 'Computer software', 'Crystallography', 'Data', 'Data Collection', 'Databases', 'Development', 'Elements', 'Engineering', 'Evaluation', 'Failure', 'Growth', 'Informatics', 'Internet', 'Libraries', 'Location', 'Machine Learning', 'Manuals', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Neighborhoods', 'Online Systems', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Property', 'Proteins', 'Proteomics', 'Resolution', 'Retrieval', 'Seeds', 'Site', 'Speed', 'Statistical Methods', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'Work', 'base', 'design', 'follow-up', 'functional genomics', 'genome sequencing', 'high throughput technology', 'improved', 'indexing', 'macromolecule', 'novel', 'open source', 'programs', 'size', 'structural biology', 'structural genomics', 'success', 'technology development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vector']",NLM,STANFORD UNIVERSITY,R01,2008,338976,-0.044431013628765205
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7502749,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Numbers', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'design', 'foot', 'journal article', 'language processing', 'mecarzole', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2008,352226,-0.00988371894210066
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7367958,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2008,2037396,-0.0020349760302730757
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7495734,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,353327,-0.017913058313874537
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7414601,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2008,142400,-0.016785311480589316
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7502672,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'concept', 'data modeling', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2008,1432331,-0.013404996850387759
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7502672,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'concept', 'data modeling', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY,U01,2008,196200,-0.013404996850387759
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7459910,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2008,118789,-0.0409067132101871
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7457647,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2008,437938,-0.00840333475153819
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7535414,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Analysis', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Purpose', 'Records', 'Research', 'Research Methodology', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'Support of Research', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'desire', 'innovation', 'interest', 'prevent', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2008,99843,-0.01172319047677961
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7688756,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Economics', 'Electronics', 'Elevation', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genome', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Institutes', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Numbers', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Policy Developments', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Testing', 'Text', 'Thinking', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'case control', 'cohort', 'cost', 'development policy', 'gene environment interaction', 'genome wide association study', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'patient registry', 'prescription document', 'prescription procedure', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2008,219307,-0.04846333903882686
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7502172,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Economics', 'Electronics', 'Elevation', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genome', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Institutes', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Numbers', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Policy Developments', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Testing', 'Text', 'Thinking', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'case control', 'cohort', 'cost', 'development policy', 'gene environment interaction', 'genome wide association study', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'patient registry', 'prescription document', 'prescription procedure', 'prospective', 'success', 'trait', 'virtual']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2008,987473,-0.04846333903882686
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7404447,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2008,243004,-0.020035029184906298
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7538527,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'GDF15 gene', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Histones', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Modification', 'Numbers', 'Ontology', 'PLAB Protein', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'RNA', 'Range', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2008,157474,-0.018419452554687795
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): The proposed Clinical Cytometry Analysis Software Project described in this grant application is designed to create a new, more efficient and effective way of analyzing cells for the presence of cancer, HIV/AIDS and other disease, using a fully automated software system. Using modern data mining techniques (pattern recognition, feature recognition, image analysis) we will design software which will analyze data (the cell samples from patients) at a much faster rate and with fewer false positives and negatives than the manual method now in use. Objectives: Assemble and validate algorithms in software that can automatically classify regions of interest in flow cytometry data. We will demonstrate that the particular populations required by our use cases can be validly, rigorously and repeatably identified automatically. Develop and validate graphical and statistical results that satisfy FDA requirements for medical device software, simplify regulatory compliance by the clinical user, and automatically deliver analysis results to diagnostic expert systems and/or LIMS systems. Satisfy the  translational medicine  goals outlined in the NIH Roadmap. This software will bring the clinician streamlined testing currently only available in research labs. Methods: Four use cases have been selected, one employing synthetic data and three clinical data; Leukemia/Lymphoma test, Analysis of longitudinal Graft vs. Host Disease (GvHD) in bone marrow transplant specimens for predictive markers and HIV/AIDS - Gag-specific T cell cytokine response profile assay. For each we have access to a substantial body of existing data, analyzed by experts. Beginning with the autogating routines in our own FlowJo software, we will test and expand the application of Magnetic gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural and Support Vector Machines (SVM). Using a sampling of human operators to establish a control range, we will test each of these five techniques against the four use cases in cooperation with our collaborators. Events in the manually classified samples are given a weighted score based on the frequency with which they are included by all the operators. A single operator's score or the gating algorithm's score is compared with the cumulative score of the expert group and a match rating is computed. Additional validation techniques include combinatory validation on internal measures with respect to Pareto optimality, and Predictive Power/Stability self consistency checks using resampled or perturbed data measured with external indices such as the adjusted Rand index and the Variation of Information index.  PUBLIC HEALTH RELEVANCE: By eliminating the operator's time, we estimate that the cost of clinical flow cytometry analysis can be reduced to half the current figure while delivering the results much faster. By eliminating the subjectivity and human error of manually created regions and reducing the range of variability of the so created, there would result fewer false positives and false negatives, improving the clinical outcome for those patients needing therapy but undetected by current methods. An order of magnitude increase in speed means faster therapeutic intervention.  A less expensive test improves outcome by making the test accessible to more patients.          n/a",Clinical Cytometry Analysis Software with Automated Gating,7482923,R43RR024094,"['AIDS/HIV problem', 'Algorithms', 'Applications Grants', 'B-Lymphocytes', 'Biological Assay', 'Biological Neural Networks', 'Bone Marrow Transplantation', 'Cells', 'Characteristics', 'Class', 'Classification', 'Clinical', 'Clinical Data', 'Cluster Analysis', 'Complex', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Cytometry', 'Data', 'Data Analyses', 'Data Files', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Educational process of instructing', 'Environment', 'Evaluation', 'Event', 'Expert Systems', 'Facility Construction Funding Category', 'Flow Cytometry', 'Frequencies', 'Funding', 'Future', 'Gagging', 'Generations', 'Goals', 'Grant', 'Graph', 'Human', 'Image Analysis', 'Individual', 'Instruction', 'Knowledge', 'Legal patent', 'Life Cycle Stages', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Noise', 'Numbers', 'Outcome', 'Output', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Phase', 'Population', 'Probability', 'Process', 'Public Health', 'Publishing', 'Range', 'Rate', 'Regulation', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Scientist', 'Score', 'Software Design', 'Software Engineering', 'Software Tools', 'Solutions', 'Specific qualifier value', 'Specimen', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'T-Lymphocyte', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Tube', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Voting', 'Weight', 'base', 'clinical Diagnosis', 'commercialization', 'cost', 'cytokine', 'data mining', 'design', 'improved', 'indexing', 'innovation', 'interest', 'leukemia/lymphoma', 'novel', 'predictive modeling', 'relating to nervous system', 'research study', 'response', 'software systems', 'statistics', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R43,2008,100854,-0.01815624090525362
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.       n/a",Bioconductor: an open computing resource for genomics,7495201,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2008,805222,-0.0053787975025878084
"Free Text Gene Name Recognition One of the problems that is important for semantic processing of natural language text is named entity recognition. This problem seems to be inherently more difficult in the biological realm than it proved to be in the realm of business applications or news story analysis as in the MUC conferences.  Our interest in the issue stems from its potential importance in indexing and retrieval of information dealing with a particular gene or protein. However really high quality named entity recognition in biology would have many applications as a starting point for semantic analysis. In past work on this problem we developed a tagger for gene/protein name recognition in text called ABGene and subsequently a database of 20,000 sentences annotated for the occurrence of gene/protein names. The first 15,000 of these sentences formed the basis of the gene/protein mention recognition task for the BioCreative I Workshop held in 2004. Subsequent to the BioCreative I Workshop the whole 20,000 sentence corpus was revised by 1) Removing tokenization and instead providing the text of the original sentence; 2) changing the annotations to be character based instead of token based; 3) revising the annotation guidelines to deal with some of the problems which had become apparent in the Workshop; 4) correcting some erroneous annotations that had come to our attention. The resulting data has become known as the GENETAG corpus. It has at least one unique property. Many of the annotated entities have alternative annotations associated with them so that more than one answer is correct for a particular entity. We believe this is important as many entities can be annotated in more than one way and for quite a number there is no clear single correct answer.  In 2005 we were invited to be co-organizers of BioCreative II and to be responsible for the gene mention recognition task. For this purpose we gave out the first 15,000 sentences of GENETAG as practice and training data and the last 5,000 sentences were used for testing. Whereas 14 teams participated in BioCreative I, 21 teams participated in BioCreative II. The top F score obtained on the gene/protein mention task in BioCreative I was 83.2% while the top score in BioCreative II was 87.2%. Because there were some changes in the annotation guidelines and some corrections to the data, one cannot say definitively how much progress this represents, but it does suggest progress. Conditional random fields were much more commonly used in BioCreative II and new approaches to the use of unannotated data also appeared. We performed an analysis of the annotations provided by all the participants and applied a conditional random fields approach to learn how to combine all predictions to make an improved prediction. In this we used 200 fold cross validation. We were able to achieve a balanced F score of 90.7%. This indicates that there is yet room for improvement in how individual systems perform on the problem of gene/protein mention detection. (with Larry Smith and Lorrie Tanabe). We have become convinced that more information about the different types of entities that can occur in sentences in MEDLINE can be used to improve name recognition. This has led us to design a set of semantic categories and to attempt to fill these categories with actual names that can be harvested from databases and from web sites. We call the result SEMCAT. It currently recognizes seventy-five categories and contains about four million name strings distributed over those categories. We have experimented with probabilistic context free grammars and Markov models of text strings in an attempt to learn how to recognize the entities in different categories.  However, the best approach we have found for distinguishing the categories of gene/protein and not gene/protein is a new algorithm we term a priority model.  Every token associated with any name in SEMCAT has associated with it two probabilities. The first probability is the probability that the token indicates that it is part of a gene/protein name and the second probability is an indicator of how reliable the token is as an indicator. With this model, given a phrase, one can compute an estimate of the probability that the phase is a gene/protein name. We find that with the priority model we can achieve an F score of 96% as compared with 95% for our best PCFG approach. (with Lorrie Tanabe). The top performance for gene mention recognition in BioCreative II was by Rie Ando from IBM who introduced a technique called alternating structural optimization. This approach takes many labeling problems similar to named entity tagging, but simply tries to predict the occurrence of the names or the tokens from the surrounding textual context. When the SVM solution weight vectors for these many auxiliary problems have been learned, one performs a singular value decomposition and subtracts from each vector its first h components in the decomposition. This subtraction is only used to decrease the penalty in the regularization term of the cost function. The weight vectors are then relearned and the process is repeated. This is continued until convergence. The final result is a set of h components of the decomposition of the many weight vectors. One uses these components to enhance the learning on the actual named entity recognition task. This is a bit complicated and difficult to use. We are studying how we may be able to use a similar approach, but with a simpler method of applying the auxiliary learning to improve named entity recognition. One problem is how to combine such auxiliary learning with the SEMCAT data. n/a",Free Text Gene Name Recognition,7735078,Z01LM000093,"['Algorithms', 'Attention', 'Biological', 'Biology', 'Bite', 'Body of uterus', 'Businesses', 'Categories', 'Data', 'Databases', 'Detection', 'Educational workshop', 'Equilibrium', 'Gene Proteins', 'Genes', 'Genetic', 'Guidelines', 'Harvest', 'Individual', 'Information Retrieval', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Numbers', 'Participant', 'Performance', 'Phase', 'Probability', 'Process', 'Property', 'Proteins', 'Proteomics', 'Purpose', 'Score', 'Semantics', 'Site', 'Solutions', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Validation', 'Variant', 'Weight', 'Work', 'base', 'cost', 'design', 'improved', 'indexing', 'interest', 'markov model', 'news', 'novel strategies', 'research study', 'semantic processing', 'stem', 'symposium', 'vector']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2008,224159,-0.033069994979648985
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,-0.03020798299470184
"Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi    DESCRIPTION (provided by applicant): The study of complex biological systems increasingly depends on vast amounts of dynamic information from diverse sources. The scientific analysis of the parasite Trypanosoma cruzi (T.cruzi), the principal causative agent of human Chagas disease, is the driving biological application of this proposal. Approximately 18 million people, predominantly in Latin America, are infected with the T.cruzi parasite. As many as 40 percent of these are predicted eventually to suffer from Chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. Research on T. cruzi is therefore an important human disease related effort. It has reached a critical juncture with the quantities of experimental data being generated by labs around the world, due in large part to the publication of the T.cruzi genome in 2005. Although this research has the potential to improve human health significantly, the data being generated exist in independent heterogeneous databases with poor integration and accessibility. The scientific objectives of this research proposal are to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for T.cruzi. This is in collaboration with the National Center for Biomedical Ontologies (NCBO) and will leverage its resources to achieve the objectives of this proposal as well as effectively to disseminate results to the broader life science community, including researchers in human pathogens. The PSE allows the dynamic integration of local and public data to answer biological questions at multiple levels of granularity. The PSE will utilize state-of- the-art semantic technologies for effective querying of multiple databases and, just as important, feature an intuitive and comprehensive set of interfaces for usability and easy adoption by biologists. Included in the multimodal datasets will be the genomic data and the associated bioinformatics predictions, functional information from metabolic pathways, experimental data from mass spectrometry and microarray experiments, and textual information from Pubmed. Researchers will be able to use and contribute to a rigorously curated T.cruzi knowledge base that will make it reusable and extensible. The resources developed as part of this proposal will be also useful to researchers in T.cruzi related kinetoplastids, Trypanosoma brucei and Leishmania major (among other pathogenic organisms), which use similar research protocols and face similar informatics challenges. PUBLIC HEALTH RELEVANCE: The scientific objective of this proposal is to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for Trypanosoma cruzi, a parasite that infects approximately 18 million people, predominantly in Latin America. As many as 40 percent of those infected are predicted to eventually suffer from Chagas disease, the leading cause of heart disease and sudden death in middle-aged adults in the region. Facilitating T.cruzi research through the PSE, with the aim of identifying vaccine, diagnostic, and therapeutic targets, is an important human disease related endeavor.          n/a",Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi,7428761,R01HL087795,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Animal Model', 'Anti-Retroviral Agents', 'Architecture', 'Archives', 'Area', 'Arts', 'Automobile Driving', 'Beds', 'Behavior', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Body of uterus', 'Buffaloes', 'California', 'Caring', 'Chagas Disease', 'Childhood', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Computers', 'Controlled Vocabulary', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Doctor of Public Health', 'Drops', 'Drosophila genus', 'Educational Activities', 'Educational workshop', 'Electronics', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evolution', 'Face', 'Feedback', 'Foundations', 'Future', 'Gene Mutation', 'Generations', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Geographic Locations', 'Goals', 'HIV', 'HIV Infections', 'Health', 'Heart Diseases', 'Homologous Gene', 'Human', 'Human Resources', 'Imagery', 'Immunologic Deficiency Syndromes', 'Immunology', 'Individual', 'Infection', 'Informatics', 'Information Management', 'Information Resources', 'Information Services', 'Information Technology', 'International', 'Internet', 'Interruption', 'Knowledge', 'Laboratories', 'Laboratory Organism', 'Language', 'Latin America', 'Lead', 'Learning', 'Leishmania major', 'Libraries', 'Link', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Medical Informatics', 'Medicine', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Mind', 'Mining', 'Modeling', 'Mutation', 'Natural Language Processing', 'Nature', 'Online Mendelian Inheritance In Man', 'Online Systems', 'Ontology', 'Operative Surgical Procedures', 'Oregon', 'Organism', 'Orthologous Gene', 'Outcome', 'Parasites', 'Pathogenesis', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Philosophy', 'Physiology', 'Prevention strategy', 'Principal Investigator', 'Problem Solving', 'Process', 'Proteomics', 'Protocols documentation', 'PubMed', 'Public Health', 'Publications', 'Publishing', 'Purpose', 'Randomized Clinical Trials', 'Range', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Resources', 'San Francisco', 'Science', 'Scientist', 'Semantics', 'Services', 'Site', 'Software Tools', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Structure', 'Study models', 'Sudden Death', 'Sum', 'System', 'TAF8 gene', 'Talents', 'Techniques', 'Technology', 'Terminology', 'Testing', 'Thinking', 'Training', 'Treatment Protocols', 'Trypanosoma brucei brucei', 'Trypanosoma cruzi', 'USA Georgia', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Vaccines', 'Vertical Disease Transmission', 'Victoria Austrailia', 'Virtual Library', 'Virus', 'Western Asia Georgia', 'Work', 'Zebrafish', 'abstracting', 'base', 'biocomputing', 'biomedical scientist', 'college', 'computer based Semantic Analysis', 'computer science', 'concept', 'data integration', 'design', 'desire', 'fundamental research', 'human disease', 'improved', 'indexing', 'innovative technologies', 'knowledge base', 'member', 'metabolomics', 'middle age', 'novel', 'novel strategies', 'open source', 'outreach', 'pandemic disease', 'pathogen', 'prevent', 'programs', 'protein protein interaction', 'repository', 'research and development', 'research study', 'syntax', 'theories', 'therapeutic target', 'tool', 'usability']",NHLBI,WRIGHT STATE UNIVERSITY,R01,2008,393930,-0.022742208833736843
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7217497,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Language', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'day', 'forgetting', 'innovation', 'preference', 'speech processing', 'symposium']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,361696,-0.01256280053605713
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7299922,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2007,181125,-0.028449872848548337
"Technology Development for a MolBio Knowledge-Base Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges adsing from the proliferation of high- throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent iadvances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB. n/a",Technology Development for a MolBio Knowledge-Base,7473405,R01LM008111,"['Address', 'Area', 'Biology', 'Class', 'Clinical', 'Collection', 'Data', 'Databases', 'Evaluation', 'Facility Construction Funding Category', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Human', 'Indium', 'Informatics', 'Information Resources', 'Information Resources Management', 'Investments', 'Knowledge', 'Knowledge Base (Computer)', 'Laboratories', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Metabolism', 'Methods', 'Metric System', 'Molecular', 'Molecular Biology', 'Mus', 'Names', 'Natural Language Processing', 'Numbers', 'Ontology', 'Persons', 'Pharmaceutical Preparations', 'Plug-in', 'Proteins', 'Purpose', 'Semantics', 'Source', 'SwissProt', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'base', 'concept', 'data integration', 'gene function', 'heuristics', 'instrumentation', 'knowledge base', 'success', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,234058,-0.029296250982461042
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7242352,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH AND SCI UNIVERSITY,R01,2007,292133,-0.004540319095064122
"A Document Processing System A system of C++ language programs has been developed for the purpose of finding the closely related documents in Medline and for the purpose of performing machine learning on sets of documents. The system has a number of unique features: 1) It is based on a number of C++ classes and highly modular so that alterations in the system are relatively simple to perform. 2) The system currently processes PubMed data by extracting from the Sybase repositories using a C++ interface to Sybase. However, a change in the interface portion of the system would allow it to be applied to any large database consisting of discrete textual records. 3) Data processed by the system is stored as compressed file structures, etc. These structures are updatable so that new data may be continually added to the system as it becomes available. 4) Documents are compared with each other using a Bayesian form of analysis. 5) The latest work on this system has involved adding the ability to generate themes using an EM algorithm approach. Also recently code has been multithreaded and memory mapping capabilities added to speed up processing.  The system described here is now not only being used to process all of MEDLINE for our research purposes, but also to produce the related documents for arbitrary pieces of text by other groups here in the NLM and outside of the NLM. The system has been used for mining email communications for the NLM help desk. n/a",A Document Processing System,7594456,Z01LM000022,"['Algorithms', 'Class', 'Code', 'Communication', 'Data', 'Databases', 'Electronic Mail', 'Literature', 'MEDLINE', 'Machine Learning', 'Maps', 'Memory', 'Methods', 'Mining', 'Numbers', 'Online Systems', 'Process', 'Programming Languages', 'PubMed', 'Purpose', 'Records', 'Research', 'Speed', 'Structure', 'System', 'Testing', 'Text', 'Work', 'base', 'computerized data processing', 'repository', 'software systems']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2007,52962,-0.005707392313372737
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7301424,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2007,173307,-0.016442598182056248
"Using Natural Language Processing to Monitor Product Claims Compliance for FDA    DESCRIPTION (provided by applicant): Linguastat, Inc. proposes to develop a means to automate the process of monitoring and identifying companies engaged in false advertising and deceptive practices in the marketing of drugs, dietary supplements, and/or food products. By leveraging state of the art approaches in computational linguistics such as Information Extraction and Natural Language Processing, it should be feasible, with some adaptation, to use this technology to: 1) automatically and continuously monitor the websites, TV transcripts, press releases and other electronic marketing text communications of tens of thousands of companies for various claims and product information 2) automatically ""red-flag"" instances in which claims have a high likelihood of potential harm to consumers, according to FDA priorities 3) automatically identify and extract the companies, products and claims embedded in electronic product information and electronic promotional materials to create a database easily searchable by the FDA and 4) automatically capture web-based or other electronic content for human review and store it as ""evidence."" Such automated technology would enable the FDA to significantly stretch its limited human resource to more effectively and comprehensively identify noncompliant product information, detect deceptive ads and other illegal practices, successfully prosecute offenders, and prevent harm to American consumers. For this Phase I SBIR project we propose to assess the feasibility of automated claims monitoring in three steps: In the first step, we will train information extraction and natural language processing algorithms to extract product marketing claims from text. In the second, step we will apply data mining and rules-based algorithms to assess which claims are likely to be non-compliant and merit further attention by FDA staff. In the third step, we will design and build a database of product claims that allows analysts to search, organize, and prioritize product claims based on the type of claim (e.g. what ailments does the product claim to treat), the type of product, and the likelihood of non- compliance. This technology will enable regulators and consumers to better monitor and detect cases of false, misleading, or deceptive advertising and product information. By enabling more effective enforcement of FDA regulations and giving consumers tools to make better buying decisions, the public health can be better protected by minimizing the impact of products that cause harm, give false hope, or entice consumers to forgo conventional remedies.          n/a",Using Natural Language Processing to Monitor Product Claims Compliance for FDA,7326883,R43FD003406,[' '],FDA,"LINGUASTAT, INC.",R43,2007,99773,-0.03299564458022935
"The Miner Suite: State of the Art Bioinformatic Tools and Data Resources The Genomics and Bioinformatics Group (GBG) represents the Center for Cancer      Researchs most substantial enterprise in bioinformatics. That bioinformatic      activity is well-integrated with the experimental activities of the group. In fact, all of the      algorithmic and software developments have been motivated by needs of the groups      experimental science. Over the last several years weve developed the      Miner Suite as a state-of-the-art, web-based, portable, principally-Java      set of tools and databases focused on needs that we found were not being served by other      bioinformatic developments. Our web site, http://discover.nci.nih.gov, has been featured in      Science and, locally, in the NIH Catalyst. The following is a list of the Miner Suite modules,      ending with four that went online in 2007: CellMiner, SpliceMiner, AffyProbeMiner, and      LeFEminer:  MedMiner accelerates searching and organization of PubMed literature      5-10 fold (L. Tanabe, et al.).  MatchMiner translates among gene identifiers for      lists of genes from microarrays and other high-throughput omic platforms (K. Bussey, et al.).       GoMiner leverages the Gene Ontology for lists of genes (e.g., from microarrays)      (B. Zeeberg, et al.).  CIMminer produces Clustered Image Maps (CIMs) (i.e.,      clustered heat maps), the ever-present icon of      Postgenomic biology. We introduced CIMs in the early 1990s.       LeadScope/LeadMiner links molecular targets (e.g., in the NCI-60 cancer cell      lines) to 27,000 chemical substructures in a fluently browsable format (with P. Blower, et      al.).  MIMminer provides searchable electronic forms (eMIMs) of the      elegant, scholarly Molecular Interaction Maps developed by K. Kohn, M. Aladjem, and Y.      Pommier.  High-throughput GoMiner extends GoMiners statistics and      visualizations to encompass large sets of microarrays (e.g., from time course studies or      clinical trials) (B. Zeeberg, et al.).  AbMiner provides a browsable relational      database of available monoclonal antibodies that we have characterized. It includes our      quality control results and multiple link-outs (S. Major, et al.).  SmudgeMiner      diagnoses spatial artifacts on Affymetrix and spotted arrays (M. Reimers, et al.).       MethMiner provides pattern visualization and statistics for DNA methylation. Program not yet      public. (S. Kim, et al.). Miner Suite programs that went online publicly in 2007 were as      follows:  CellMiner provides the NCI-60 and other molecular profile databases in a      SQL-searchable relational format, with metadata on the experimental platforms and cell types      (U. Shankavaram, S. Varma, et al. -- see 2007 publication in Molecular Cancer Therapeutics).       SpliceMiner provides a robust infrastructure for dealing with transcript splice      variation (A. Kahn, M. Ryan, et al. -- see 2007 publication in BMC Bioinformatics).       AffyProbeMiner provides what we believe to be the best tool for re-mapping      Affymetrix probes to achieve sharper results from the commercially available Affymetrix      arrays. It also integrates the re-mapped array data with GoMiner and High-Throughput GoMiner      (H.-F. Liu, B. Zeeberg, et al. -- see 2007 publication in Bioinformatics).       LeFEminer is a web-based implementation of the LeFE (Learner of Functional Enrichment)      algorithm, which uses the random forest machine-learning paradigm in a novel way to predict      functional relationships from microarray and other high-throughput data types. (G. Eichler, et      al. -- see 2007 publication in Genome Biology). Those programs, largely designed and      implemented under a competed contract with SRA International, (contract #263-01-D-0050,      CIO-SP2 Delivery Order 2313028) are freely available and used by thousands of investigators      worldwide. The success of the programs is attributable in part to our adoption of the Agile      software development paradigm, which promotes close, iterative interaction between software      engineers, biologists, and bioinformaticists. That success is also partially attributable to      adoption of Unit and System Testing methods; whenever code is re-deposited in our version      control system, its automatically subjected to >1700 tests to minimize the      chance that changes made have broken something elsewhere in the overall code base. The      development team won an SRA Project Excellence Award for the Miner Suite (1 of 4 awarded out      of >700 competing projects). Were integrating the Miner Suite applications      with a variety of public bioinformatic software projects, including caBIG, The Cancer Genome      Atlas, and the CGEMS genome-wide association project.  caBIG: We were awarded two      caBIG pilot grants (one to caBIG-enable GoMiner, the other to caBIG-enable our NCI-60      databases). The group, particularly lead software engineer David Kane, has made strong      contributions to caBIGs Integrative Cancer Research and Architecture Working      Groups.  The Cancer Genome Atlas: Working with TCGA development staff, we used      SpliceMiner as the basis for development of bioinformatic infrastructure to support the      integration of genotypic/phenotypic data of multiple types. That infrastructure has now been      adopted by TCGAs Data Integration Committee for the purpose.  CGEMS:      Working with bioinformaticists in the NCI Division of Cancer Epidemiology and Genetics (DCEG),      we have developed a program package provisionally named ChromMiner for visual integration of      SNP data from the genome-wide association studies with phenotypic data on the cancers      (including gene expression and comparative genomic hybridization data). That program package      is central to a proposal, which I was instrumental in drafting, to combine the      epidemiological/genotypic expertise of DCEG with the phenotypic expertise of CCR. The proposal      is titled DCEG/CCR Plan for Follow-up of Genomic Regions of Association Identified      by CGEMS n/a",The Miner Suite: State of the Art Bioinformatic Tools and Data Resources,7592989,Z01BC010842,"['Adopted', 'Adoption', 'Algorithms', 'Architecture', 'Arts', 'Atlases', 'Award', 'Binding', 'Bioinformatics', 'Biological', 'Biology', 'CCR', 'Cancer Center', 'Cancer cell line', 'Categories', 'Chemicals', 'Clinical Trials', 'Code', 'Communities', 'Complex', 'Computer software', 'Computers', 'Contracts', 'DNA Methylation', 'Data', 'Databases', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Division of Cancer Epidemiology and Genetics', 'Electronics', 'Gene Expression', 'Genes', 'Genetic Markers', 'Genome', 'Genomics', 'Goals', 'Grant', 'Heating', 'Image', 'Imagery', 'International', 'Internet', 'Java', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metadata', 'Methods', 'Microarray Analysis', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Monoclonal Antibodies', 'Morphologic artifacts', 'Names', 'Online Systems', 'Ontology', 'Pattern', 'Personal Satisfaction', 'Predisposition', 'PubMed', 'Publications', 'Publishing', 'Purpose', 'Quality Control', 'RNA Splicing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Site', 'Software Engineering', 'Spottings', 'Staff Development', 'System', 'Testing', 'Therapeutic', 'Time', 'Transcript', 'Translating', 'United States National Institutes of Health', 'Variant', 'Visual', 'Work', 'anticancer research', 'base', 'cancer Biomedical Informatics Grid', 'cancer genetics', 'cancer genome', 'catalyst', 'cell type', 'comparative genomic hybridization', 'data integration', 'design', 'follow-up', 'forest', 'genome wide association study', 'mecarzole', 'novel', 'programs', 'software development', 'statistics', 'success', 'tool']",NCI,DIVISION OF BASIC SCIENCES - NCI,Z01,2007,1051653,-0.028327456439942097
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,7282065,R01LM005652,"['Active Sites', 'Algorithms', 'Appendix', 'Area', 'Basic Science', 'Binding', 'Binding Sites', 'Biochemical', 'Biological', 'Biology', 'Catalysis', 'Communities', 'Computer software', 'Crystallography', 'Data', 'Data Collection', 'Databases', 'Development', 'Elements', 'Engineering', 'Evaluation', 'Failure', 'Growth', 'Informatics', 'Internet', 'Libraries', 'Location', 'Machine Learning', 'Manuals', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Neighborhoods', 'Online Systems', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Property', 'Proteins', 'Proteomics', 'Resolution', 'Retrieval', 'Seeds', 'Site', 'Speed', 'Statistical Methods', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Validation', 'Work', 'base', 'design', 'follow-up', 'functional genomics', 'genome sequencing', 'high throughput technology', 'improved', 'indexing', 'macromolecule', 'novel', 'open source', 'programs', 'size', 'structural biology', 'structural genomics', 'success', 'technology development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vector']",NLM,STANFORD UNIVERSITY,R01,2007,355458,-0.044431013628765205
"HERMES - Help physicians to Extract and aRticulate Multimedia information from li    DESCRIPTION (provided by applicant): Physicians have many questions when seeing patients. Primary care physicians are reported to generate between 0.7 and 18.5 questions for every 10 patient visits. The published medical literature is an important resource helping physicians to access up-to-date clinical information and thereby to enhance the quality of patient care. For example, the case study in the above example (i.e., diagnostic procedures and treatment for cellulites) was published in a ""Clinical Practice"" article in the New England Journal of Medicine (NEJM). Although PubMed is frequently used by physicians in large hospitals, it does not return answers to specific questions. Frequently, PubMed returns a large number of articles in response to a specific user query. Physicians have limited time for browsing the articles retrieved; it has been found that physicians spend on average two minutes or less seeking an answer to a question, and that if a search takes longer it is likely to be abandoned. An evaluation study has shown that it takes an average of more than 30 minutes for a healthcare provider to search for answer from PubMed, which makes ""information seeking ... practical only `after hours' and not in the clinical setting."" It has been concluded that a lack of time is the most common obstacle resulting in many unanswered medical questions.       The importance of answering physicians' questions at the point of patient care has been widely recognized by the medical community. Many medical databases (e.g., UpToDate and Thomson MICROMEDEX) provide summaries to answer important medical questions related to patient care. However, most of the summaries are written by medical experts who manually review the literature information. The databases are limited in their scope and timeliness.       We hypothesize that we can develop medical language processing (MLP) approaches to build a fully automated system HERMES - Help physicians to Extract and aRticulate Multimedia information from literature to answer their ad-hoc medical quEstionS. HERMES will automatically retrieve, extract, analyze, and integrate text, image, and video from the literature and formulate them as answers to ad-hoc medical questions posed by physicians. Our preliminary results show that even a limited HERMES working system outperformed other information retrieval systems and can generate answers within a timeframe necessary to meet the demands of physicians. HERMES promise to assist physicians for practicing evidence-based medicine (EBM), the medical practice that involves the explicit use of current best evidence, i.e., high-quality patient-centered clinical research reported in the primary medical literature.       Our specific aims are:       1) Identify information needs from ad-hoc medical questions. We will incorporate rich semantic, statistical, and machine learning approaches to map ad-hoc medical questions to their component question types automatically. A component question type is a generic, simple question type that requires an answer strategy that is different from other component question types.       2) Develop new information retrieval models that integrate domain-specific knowledge for retrieving relevant documents in response to an ad-hoc medical question.       3) Extract relevant text, images, and videos from the retrieved documents in response to an ad-hoc medical question.       4) Integrate text, images, and videos, fusing information to generate a short and coherent multimedia summary.       5) Design a usability study to measure efficacy, accuracy and perceived ease of use of HERMES and to compare HERMES with other information systems.          n/a",HERMES - Help physicians to Extract and aRticulate Multimedia information from li,7380099,R01LM009836,"['Area', 'Back', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Research', 'Communities', 'Databases', 'Diagnostic Procedure', 'Edema', 'Erythema', 'Evaluation Studies', 'Evidence Based Medicine', 'Generic Drugs', 'Health Personnel', 'Hospitals', 'Hour', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Information Systems', 'Journals', 'Knowledge', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Medical', 'Medical Imaging', 'Medicine', 'Modeling', 'Multimedia', 'New England', 'Numbers', 'Pain', 'Patient Care', 'Patients', 'Physicians', 'Primary Care Physician', 'PubMed', 'Publishing', 'Redness', 'Reporting', 'Resources', 'Review Literature', 'Semantics', 'System', 'Text', 'Time', 'Toes', 'Ultrasonography', 'Visit', 'Work', 'Writing', 'design', 'foot', 'journal article', 'language processing', 'mecarzole', 'older men', 'patient oriented', 'response', 'usability']",NLM,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2007,383550,-0.00988371894210066
"The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments    DESCRIPTION (provided by applicant): The goal of this research is to create software that uses mathematical algorithms to detect medical billing coding errors prior to payment. The well-publicized failure of current healthcare cost containment technologies to prevent improper payments in both the commercial healthcare market and the federal Medicare program highlights the urgent need for a new approach to the growing problem of out of control medical costs. A recent federal study by the GAO estimated that improper payments by Medicare alone were in excess of 21 billion dollars, a truly staggering 48.1 percent of all improper payments by federal programs. Like SPAM, whose dynamic nature makes static or post hoc remedies ineffective, effective cost containment in one area often merely leads to the creation of new areas of abuse. Clearly, the ideal solution is a system that can evaluate the fairness of payments before they are made, and that can respond to dynamic patterns of abuse. The first step in creating such a system is the creation of robust method for sorting bills for appropriate rule-based analysis on the basis of the type of bill. Currently neither Medicare nor major insurers are capable of making this classification reliably except through the use of inefficient, static rules and the use of manual sorting--a costly and inefficient approach to assuring timely payment to hospitals and medical providers. We propose a novel method for using mathematical algorithms that utilize machine-learning (ML) methods to address the problem of medical bill categorization, the first step in coding error detection. Specifically, we propose the evaluation of a variety of genetic algorithms that are well adapted to the problems of large, dynamic datasets and can be ""trained"" using real world correctly coded datasets in healthcare claims. This work is particularly timely due to recent Medicare contracting reform. Using more than 50 contractors and carriers, bill classification is largely determined by the carrier's contract. Centralizing this process to only four payment centers will require the classification system we propose. [This research is directed toward the development of software applications that will detect billing errors and perform proper edits to payment of medical bills. Current anticipated changes and reforms in the Medicare system will require these systems, which do not currently exist in the public or private sector.]             n/a",The Use of Mathematic Algorithms in the Prevention of Improper Medical Payments,7316071,R43LM009190,"['Address', 'Age', 'Algorithms', 'Area', 'Arts', 'Classification', 'Code', 'Collaborations', 'Computer Simulation', 'Computer software', 'Contractor', 'Contracts', 'Cost Control', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Development', 'Elements', 'Environment', 'Evaluation', 'Failure', 'Genetic Programming', 'Goals', 'Health Care Costs', 'Health Care Fraud', 'Health Personnel', 'Healthcare', 'Healthcare Market', 'Healthcare Systems', 'Hospitals', 'Industry', 'Inpatients', 'Insurance Carriers', 'Learning', 'Machine Learning', 'Manuals', 'Mathematics', 'Medical', 'Medicare', 'Methods', 'Mining', 'Modeling', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outpatients', 'Pattern', 'Phase', 'Policies', 'Population', 'Prevention', 'Private Sector', 'Process', 'Provider', 'Rate', 'Reporting', 'Research', 'Running', 'Small Business Technology Transfer Research', 'Solutions', 'Sorting - Cell Movement', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Training', 'Work', 'base', 'college', 'computerized', 'cost', 'design', 'experience', 'improved', 'mathematical algorithm', 'novel', 'novel strategies', 'payment', 'prevent', 'programs', 'size', 'software development', 'stem', 'success']",NLM,"QMEDTRIX SYSTEMS, INC.",R43,2007,92482,-0.012798085842168454
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7246847,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2007,2183988,-0.0020349760302730757
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7318595,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA IRVINE,R01,2007,372000,-0.017913058313874537
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7195053,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2007,145510,-0.016785311480589316
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,0.004370542114199357
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,7249382,R01EB000362,"['Address', 'Architecture', 'Caring', 'Chronic', 'Clinical', 'Clinical Medicine', 'Complex', 'Computer Architectures', 'Condition', 'Data', 'Data Collection', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Documentation', 'Early Diagnosis', 'Effectiveness', 'Engineering', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Extensible Markup Language', 'Geographic Locations', 'Healthcare', 'Image', 'Imagery', 'Individual', 'Information Management', 'Laboratories', 'Language', 'Link', 'Management Information Systems', 'Measures', 'Medical', 'Medical Records', 'Medicine', 'Methodology', 'Metric', 'Multimedia', 'Natural Language Processing', 'Numbers', 'Online Systems', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Problem Solving', 'Procedures', 'Process', 'Provider', 'Radiology Specialty', 'Records', 'Reporting', 'Research', 'Research Design', 'Series', 'Services', 'Solutions', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'TimeLine', 'United States', 'Urology', 'Visit', 'base', 'concept', 'cost', 'data acquisition', 'data management', 'data modeling', 'improved', 'information gathering', 'innovation', 'innovative technologies', 'insight', 'peer', 'point of care', 'portability', 'research clinical testing', 'urologic', 'usability']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2007,517331,-0.04910053696222924
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7427367,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'concept', 'data modeling', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY MED CTR,U01,2007,1463449,-0.013404996850387759
"Vanderbilt Genome-Electronic Records Project    DESCRIPTION (provided by applicant):  VGER: The Vanderbilt Genome-Electronic Record project An important potential enabling resource for Personalized Medicine is the combination of a DNA repository with Electronic Medical Record (EMR) systems sufficiently robust to provide excellence in clinical care and to serve as resources for analysis of disease susceptibility and therapeutic outcomes across patient populations. The Vanderbilt EMR is a state of the art clinical and research tool (that includes >1.4 million records), and is associated with a DNA repository which has been in development for over 3 years; these are the key components of VGER, the Vanderbilt Genome-Electronic Records project proposed here. The VGER model acquires DNA from discarded blood samples collected from routine patient care, and can link these to de-identified data extracted and readily updated from the EMR. The phenotype we will analyze here is the QRS duration on the electrocardiogram, since slow conduction (indicated by longer QRS duration) is a marker of arrhythmia susceptibility. This will not only exploit the power of Genome-Wide Association (GWA) approaches to generate new biologic knowledge that impacts an area of public health concern, but also provides a platform for the development of tools, such as Natural Language Processing approaches, to optimally mine EMRs. This project brings together a team of investigators with nationally recognized records of accomplishment in genome science, medical ethics, bioinformatics, de-identification science, and translational and cardiovascular medicine to address four Specific Aims: (1) perform a GWA comparing samples from subjects with QRS durations at the extremes of the normal range, and validate by genotyping high likelihood associations in prospectively ascertained clinical trial sets for QRS duration and for arrhythmia susceptibility; (2) evaluate the validity and utility of structured and unstructured components of EMR data for genome-phenome correlations; (3) assess the ethical, scientific, and societal advantages and disadvantages of the VGER model, and determine best practices for oversight, community involvement, and communication as the resource grows; and (4) develop and evaluate formal privacy protection models for data derived from databanks and EMRs, establishing data sharing and integration practices. We also include here a proposal to develop the Administrative Coordinating Center whose mission will be to facilitate communication and collaboration among nodes in this network, the NHGRI, and external advisors. We subscribe to a vision of Personalized Medicine in which genomic and other patient-specific information drives personalized, predictive, preemptive, and participatory health care, and VGER represents an important step in that direction.           n/a",Vanderbilt Genome-Electronic Records Project,7427367,U01HG004603,"['Address', 'Area', 'Arrhythmia', 'Arts', 'Bioinformatics', 'Blood specimen', 'Cardiac', 'Cardiovascular system', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Commit', 'Communication', 'Communities', 'Computerized Medical Record', 'DNA', 'Data', 'Databases', 'Development', 'Disadvantaged', 'Disease', 'Disease susceptibility', 'EKG QRS Complex', 'Electrocardiogram', 'Electronics', 'Ethics', 'Genome', 'Genomics', 'Genotype', 'Healthcare', 'Heart Diseases', 'Institution', 'Knowledge', 'Lead', 'Legal', 'Link', 'Measures', 'Medical Ethics', 'Medicine', 'Methods', 'Mining', 'Mission', 'Modeling', 'Natural Language Processing', 'Normal Range', 'Outcome', 'Patient Care', 'Patients', 'Phenotype', 'Population', 'Predisposition', 'Privacy', 'Public Health', 'Records', 'Research', 'Research Ethics Committees', 'Research Personnel', 'Resources', 'Sampling', 'Science', 'Structure', 'System', 'Testing', 'Therapeutic', 'Translational Research', 'Update', 'Validation', 'Variant', 'Vision', 'concept', 'data modeling', 'egg', 'endophenotype', 'genome wide association study', 'heart rhythm', 'indexing', 'repository', 'tool', 'tool development']",NHGRI,VANDERBILT UNIVERSITY MED CTR,U01,2007,125000,-0.013404996850387759
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7269383,R01RR014477,[' '],NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2007,307022,-0.027819689432835328
"Comparative Visualization and Analysis for GCxGC    DESCRIPTION (provided by applicant): Project Summary. This project will investigate and develop effective information technologies for comparative analysis and visualization of complex data generated by comprehensive two-dimensional gas chromatography (GCxGC). GCxGC is an emerging technology that provides an order-of-magnitude greater separation capacity, significantly better signal-to-noise ratio, and higher dimensional retention-structure relations than traditional GC. The principal challenge for utilization of GCxGC, in a wide range of public-health and other applications, is the difficulty of analyzing and interpreting the large, complex data it generates. The quantity and complexity of GCxGC data necessitates the investigation and development of new information technologies. This project will develop and demonstrate innovative methods and tools for comparative analysis of GCxGC datasets. The expected results of this research and development include a PCA-based method for chemical fingerprinting, decision trees with chemical constraints for sample classification, genetic programming for template and constraint-based matching and classification, and visualization methods for comparative GCxGC analyses. These methods will be implemented in commercial software that will support researchers and laboratory analysts in a wide range of commercial applications, including health care, environmental monitoring, and chemical processing. The power of GCxGC, supported by effective information technologies, will enable better understanding of chemical compositions and processes, a foundation for future scientific advances and discoveries. Relevance to Public Health. Today, a few advanced laboratories are pioneering GCxGC for a variety of applications such as environmental monitoring of exposure profiles in air, soil, food, and water; identification and quantification of toxic products in blood, urine, milk, and breath samples; and qualitative and quantitative metabolomics to provide a holistic view of the biochemical status or biochemical phenotype of an organism. Many analyses in these applications require detailed chemical comparisons of samples, e.g..monitoring changes, comparison to reference standards, chemical matching or ""fingerprinting"", and classification. GCxGC is a powerful new technology for such comparative analyses. This proposal will provide innovative information technologies to support users in these applications.           n/a",Comparative Visualization and Analysis for GCxGC,7270029,R44RR020256,"['Air', 'Archives', 'Biochemical', 'Blood', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Emerging Technologies', 'Environmental Monitoring', 'Fingerprint', 'Food', 'Foundations', 'Future', 'Gas Chromatography', 'Genetic Programming', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Information Technology', 'Investigation', 'Laboratories', 'Language', 'Machine Learning', 'Marketing', 'Methods', 'Milk', 'Monitor', 'Noise', 'Organism', 'Pattern', 'Phase', 'Phenotype', 'Principal Component Analysis', 'Process', 'Public Health', 'Range', 'Reference Standards', 'Reporting', 'Research Personnel', 'Sales', 'Sampling', 'Schedule', 'Scientific Advances and Accomplishments', 'Signal Transduction', 'Software Tools', 'Soil', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Today', 'Trademark', 'Urine', 'Water', 'base', 'chemical fingerprinting', 'commercial application', 'comparative', 'innovation', 'innovative technologies', 'instrument', 'metabolomics', 'new technology', 'research and development', 'tool', 'two-dimensional']",NCRR,"GC IMAGE, LLC",R44,2007,239373,-0.019958201372900837
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7252088,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2007,276104,-0.0409067132101871
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7287965,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2007,446875,-0.00840333475153819
"Automatic Bayesian Methods In Text Retrieval Current work on the project is focusing on developing an improved Bayesian classification model and developing new approaches to active learning with a Bayesian model.  1)	We have found through extensive testing that our version of naive Bayes, a form of MBM (multivariate Bernoulli model), is at least as effective as the MM (multinomial model). The MM model attempts to extract information from local feature counts in text documents. We have developed what we call a Stacked MBM model, which shows that there is not sufficient independent information in the local counts to make a significant improvement in performance.  2)	We have developed term based active learning methods which provide a different approach to active learning and have shown that they are in many cases more effective then simple uncertainty sampling or error reduction sampling. 3)	We have developed an example selection method that is very powerful in improving Bayes on all of MEDLINE. This is important because there are few methods that can really be applied to all of MEDLINE. n/a",Automatic Bayesian Methods In Text Retrieval,7594455,Z01LM000021,"['Active Learning', 'Bayesian Method', 'Classification', 'Count', 'Data', 'Databases', 'Goals', 'Label', 'MEDLINE', 'Methods', 'Modeling', 'Performance', 'Resources', 'Retrieval', 'Sampling', 'Testing', 'Text', 'Training', 'Uncertainty', 'Work', 'base', 'improved', 'interest', 'novel strategies']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2007,353078,-0.019057291523811016
"Development and Use of Network Infrastructure for High-Throughput GWA Studies    DESCRIPTION (provided by applicant):  Linking biorepositories of patients in healthcare delivery systems with electronic medical records (EMRs) is an efficient strategy for high-throughput genome wide association (GWA) studies, as phenotype, covariable and exposure data of public health importance can be economically abstracted and pooled across delivery systems to facilitate the large numbers of subjects needed for GWA studies of each phenotype. Key obstacles to the success of this strategy remain. In this project, which will use population-based genomic and phenotype data from a well characterized population served by a delivery system which captures virtually all health care encounters in its data bases. Researchers from Group Health Cooperative's Center for Health Studies, the University of Washington, and the Fred Hutchinson Cancer Research Center will address these obstacles by pursuing the following specific aims:       1. Informed by results from targeted focus groups, implement a consensus process with key stakeholders to develop recommendations concerning consent, data sharing, and return of research results to subjects.    2. Work together with other network sites to develop a virtual data warehouse (VDW) analogous to that used in the Cancer Research Network, and extend natural language processing (NLP) to pathology, radiology, and clinical chart notes.   3. Develop and test strategies to determine whether each candidate EMR-based phenotype is sufficiently valid to pursue analyses of GWA data, and develop statistical methods that explicitly account for heterogeneous phenotype validity within and between sites.    4. Perform a series of GWA analyses in the GHC biorepository and linked biorepositories. 4a: Alzheimer's disease (AD). 4b: Carotid artery atherosclerotic disease (CAAD). 4c: Complications of statin use, including elevations of CPK and muscle pain.       Through cooperation with other investigators and the NHGRI, this work will facilitate development of policies and procedures to realize the incredible potential of EMR-linked biorepositories for GWA studies to improve understanding, prevention and treatment of chronic diseases and illnesses. Specific GWA research will allow us to explore both etiologic research (AD and CAAD progression) and pharmacogenetics (statin therapy). The implications of this portfolio of research extend far beyond the specific phenotypes we have chosen to emphasize; we expect this work represents the beginning of a large and productive enterprise.              n/a",Development and Use of Network Infrastructure for High-Throughput GWA Studies,7427364,U01HG004610,"['Abbreviations', 'Accounting', 'Address', 'Adult', 'Adverse event', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'Blood Pressure', 'Cancer Research Network', 'Carotid Arteries', 'Carotid Artery Diseases', 'Cholesterol', 'Chronic Disease', 'Clinic', 'Clinical', 'Clinical Data', 'Cognition', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consent', 'Creatinine', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Economics', 'Electronics', 'Elevation', 'Enrollment', 'Environmental Exposure', 'Exposure to', 'Focus Groups', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genome', 'Genomics', 'Genotype', 'Gold', 'Health', 'Healthcare', 'Healthcare Systems', 'High Density Lipoproteins', 'Individual', 'Inpatients', 'Institutes', 'Knowledge', 'Laboratories', 'Leadership', 'Life', 'Link', 'Malignant Neoplasms', 'Maps', 'Medical', 'Meta-Analysis', 'Methods', 'Myalgia', 'National Cancer Institute', 'Natural Language Processing', 'Neurofibrillary Tangles', 'Numbers', 'Outcome', 'Outpatients', 'Participant', 'Pathology', 'Patients', 'Performance', 'Personal Satisfaction', 'Persons', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Pharmacy facility', 'Phenotype', 'Policy Developments', 'Population', 'Prevention', 'Procedures', 'Process', 'Public Domains', 'Public Health', 'Quality of Care', 'Radiology Specialty', 'Recommendation', 'Recruitment Activity', 'Research', 'Research Ethics Committees', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sampling', 'Senile Plaques', 'Series', 'Single Nucleotide Polymorphism', 'Site', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Testing', 'Text', 'Thinking', 'Time', 'Universities', 'Ursidae Family', 'Washington', 'Work', 'abstracting', 'base', 'case control', 'cohort', 'cost', 'development policy', 'gene environment interaction', 'genome wide association study', 'health care delivery', 'human disease', 'improved', 'interest', 'member', 'patient registry', 'prescription document', 'prescription procedure', 'prospective', 'success', 'trait', 'virtual']",NHGRI,GROUP HEALTH COOPERATIVE,U01,2007,970601,-0.04846333903882686
"Biosurveillance Utilizing SNOMED-CT Based Natural Language Processing Bioterrorism remains a significant threat to our public health. Early identification of an increased rate of occurance of patient presentations consistent with an exposure to an agent of bioterrorism is one important method to contain bioterror attacks and effect more rapid treatment of exposed individuals. Often presentations consistent with an exposure to an agent of bioterrorism occur in significant numbers prior to the recognition that a bioterrorism related exposure has occurred. This presents an opportunity to capture and analyze signals from patient records. We propose to provide an abstraction of automated clinical information from the clinical record (section by section) that will be coded using SNOMED-CT which can serve as the substrate for surveillance data. We believe that this data (sets of codes by section of the clinical record) which holds the important and salient medical facts (codes) regarding the patients' presentation, findings, medications, allergies and co-morbidities could be abstracted from clinical records. In this study, we will analyze SNOMED-CT's ability to provide adequate content coverage for constellations of symptoms associated with exposures to agents of bioterrorism (i.e. Anthrax, Small Pox, Ricin, and Radiation exposure). Our method builds on the considerable body of research already available within our laboratory. We have been researching methods for codifying medical content using controlled medical vocabularies since 1987. For this study, we will employ the Mayo Vocabulary Server (MVS) developed in the Mayo Laboratory of Biomedical Informatics and has been used at Mayo, Johns Hopkins University and the VA medical centers all with great success. Our performanc2 of the MVS toolkit has been validated for diagnoses where we showed a sensitivity of 99.7% and a specificity of 97.9%. This proposal deals with practical issues that lead the way toward interoperable data. The fruits of this research will assist our national initiatives to pave the way toward a safe and effective BioSecure biosurveillance solution. n/a",Biosurveillance Utilizing SNOMED-CT Based Natural Language Processing,7790142,R01PH000022,[' '],PHPPO,MOUNT SINAI SCHOOL OF MEDICINE,R01,2007,677278,-0.060325637013826844
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7214148,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2007,240927,-0.020035029184906298
"Free Text Gene Name Recognition Currently we are pursuing two projects designed to make progress on the problem of gene/protein name recognition:  1) We have produced a set of 20,000 sentences with all occurrences of gene/protein names in them marked up with the character offset for name beginning and name ending in the sentence. The sentences were taken as random samples from restricted classes of MEDLINE abstracts. Half were chosen as likely to have gene/protein names in them and half were selected as unlikely to have such names. Since there is ambiguity in marking names, alternative markings are listed as correct answers where this is thought to be appropriate. Three fourths of these names formed the basis for a task in the BioCreAtIvE1 (Critical Assessment of Information Extraction in Biology) Workshop held in Granada, Spain in 2004. Twelve teams attempted to designed systems that could correctly tag the gene/protein names in the sentences. Several teams obtained precisions and recalls in the low 80% range. A number of different approaches were successful and these results suggest ways in which gene/protein name tagging.  The 20,000 sentences forming the basis of this work have been re-edited and a number of errors corrected. The 15,000 sentences which formed the basis of BioCreAtIvE1 and currently being used for the training phase of BioCreAtIvE2 and the last 5,000 sentences which have never been released will form the testing material for the BioCreAtIvE2 which is planned for early 2007.   2) We have become convinced that more information about the different types of entities that can occur in sentences in MEDLINE can be used to improve name recognition. This has led us to design a set of semantic categories and to attempt to fill these categories with actual names that can be harvested from databases and from web sites. We call the result SEMCAT. It currently recognizes seventy-five categories and contains about five million name strings distributed over those categories. We have experimented with probabilistic context free grammars and Markov models of text strings in an attempt to learn how to recognize the entities in different categories. In order to improve performance we have developed a new model term a Priority Model for name recognition. This model allows us to categorize names as gene/protein names with an F-score of 0.96 and better then what we were able to achieve with either a language model of a probabilistic context free grammar. We are currently using this to create features for using in a conditional random fields approach to gene/protein name recognition and are achieving about an 0.83 F-score. n/a",Free Text Gene Name Recognition,7594470,Z01LM000093,"['Biology', 'Categories', 'Class', 'Databases', 'Educational workshop', 'Gene Proteins', 'Genes', 'Genetic', 'Harvest', 'Internet', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Materials Testing', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Numbers', 'Performance', 'Phase', 'Proteins', 'Proteomics', 'Purpose', 'Range', 'Sampling', 'Score', 'Semantics', 'Site', 'Spain', 'System', 'Techniques', 'Text', 'Thinking', 'Training', 'Variant', 'Work', 'abstracting', 'base', 'design', 'improved', 'markov model', 'research study']",NLM,NATIONAL LIBRARY OF MEDICINE,Z01,2007,194193,-0.01786469325973303
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methodswithin the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data,  earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user  nterface to support access to information resources for enhanced decision-making by practitioners. The ong-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3  Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technologywork; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C). n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7284424,P01HK000027,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2007,1889501,0.00418695137634592
"Accelerating metabolic discovery using characterization data    DESCRIPTION (provided by applicant): The long term goal of this project is to develop methods that will allow researchers to gain insight into the metabolic networks of organisms for which we have little or no high-throughput data. Such metabolic networks can reveal aspects of the organism's metabolism that might make it vulnerable to new or existing therapies. A core data set using genomic and other omic data from data-rich bacteria that are related to the organisms of interest will be assembled. The statistical tools needed to integrate these data and to infer metabolic networks using these core data plus characterization (phenotypic) data will then be built. Using the statistical inference algorithms, the characterization data can be leveraged to reveal the metabolic networks of data-poor bacteria for which we have only characterization data. This approach can eliminate the need for genome sequencing, gene expression experiments and the like for thousands of Gram-negative facultative rod bacteria (GNF). There are five tasks in the project: (1) assemble the data sets from data-rich organisms that will be used to inform the inference algorithm. These data include (a) the genomic sequences and annotation information, (b) extant pathway data and (c) gene expression data. All these data contain some level of information about the connectivity within the metabolic network; (2) process the genomic data to enhance its predictive value; (3) develop a data integration algorithm; (4) investigate modeling frameworks to be used for Bayesian data fusion and network inference; (5) validate the metabolic networks. Deliverables from this project should include: (1) a set of pathway genome databases for 35 GNF, This group includes 20 strains classified as category A or B biothreat agents, (2) a core dataset that integrates all the information we have relevant to the metabolic pathways in the 35 sequenced GNF, (3) a probabilistic graphical modeling framework capable of integrating disparate types of data and inferring networks from the integrated data, (4) a method for using characterization data, along with deliverables 2 and 3, to infer metabolic networks for bacterial strains for which we have only characterization data. The ability to rapidly construct models of metabolic networks means researchers will be able to respond to emerging infectious agents or biothreats more quickly. Relevance The methods developed as part of this proposal will allow us to quickly make metabolic maps for thousands of bacteria. Such maps can guide researchers to promising new targets for therapeutic or preventative measures against pathogenic bacteria. The fight against well-known pathogens and biothreat agents, as well as against new, emerging pathogens will be greatly aided by these tools.              n/a",Accelerating metabolic discovery using characterization data,7267998,R21AI067543,"['Adopted', 'Algorithms', 'American Type Culture Collection', 'Artificial Intelligence', 'Bacteria', 'Bacteriology', 'Biochemical', 'Biochemical Pathway', 'Biological Models', 'Biology', 'Bypass', 'Categories', 'Cholera', 'Code', 'Data', 'Data Collection', 'Data Set', 'Depth', 'Disease', 'Electronics', 'Facility Construction Funding Category', 'Gammaproteobacteria', 'Gene Expression', 'Genomics', 'Goals', 'Gram&apos', 's stain', 'Infectious Agent', 'Information Networks', 'Manuals', 'Maps', 'Measures', 'Meta-Analysis', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Nosocomial Infections', 'Numbers', 'Nutritional', 'Organism', 'Outcome', 'Oxygen', 'Pathway interactions', 'Plague', 'Predictive Value', 'Process', 'Prophylactic treatment', 'Proteomics', 'Research Personnel', 'Salmonella typhi', 'Shapes', 'Shigella', 'Shigella Infections', 'Signal Transduction', 'Source', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Typhoid Fever', 'Variant', 'Vibrio cholerae', 'Work', 'Writing', 'Yersinia pestis', 'biothreat', 'computerized data processing', 'data integration', 'design', 'falls', 'fight against', 'genome database', 'genome sequencing', 'gram negative facultative rods', 'innovation', 'insight', 'interest', 'network models', 'novel', 'pathogen', 'pathogenic bacteria', 'programs', 'research study', 'retinal rods', 'routine Bacterial stain', 'sound', 'success', 'therapeutic target', 'tool', 'transcriptomics']",NIAID,AMERICAN TYPE CULTURE COLLECTION,R21,2007,185677,-0.022227151974625207
"Natural Language Processing for Respiratory Surveillance    DESCRIPTION (provided by applicant): The applicant's long-term career goal is to become an independent investigator in biomedical informatics research. She will dedicate her career to developing and evaluating methodologies for improving the processes and outcomes of healthcare using data locked in textual documents. This career development award will provide her with initial support for achieving her career goals.      The applicant has three goals for her career development over the next three years. First, she will compare the performance of different machine learning techniques at detecting patients with a respiratory syndrome. The proposed research will expand the state-of-the-art syndromic surveillance capabilities by integrating findings, symptoms, and diseases described in textual medical records. The product of the first research goal will be a model for respiratory syndromic case detection for monitoring natural and bioterrorism induced respiratory outbreaks. Second, she will apply existing methods and develop new techniques for extracting clinical conditions required for respiratory case detection from emergency department notes, contributing new knowledge to the medical language processing field using sentence and report level models that account for uncertainty, negation, and temporal occurrence. The product of the second goal will be a better understanding of the information required for accurate detection of respiratory related conditions from text and useful tools for automatically extracting that information. Third, she will teach, promote, and facilitate the use of natural language processing in the biomedical informatics field. The product of the third goal will be a graduate class surveying medical language processing methodology and applications and development of general tools sets for researchers who need encoded data from textual patient records.        The proposed research will focus on:   Aim 1. Development and evaluation of a respiratory case detection model;   Aim 2. Integration of existing natural language processing tools and development of new methodologies for extracting clinical conditions needed for respiratory case detection from textual records; and   Aim 3. Comparison of existing syndromic detection algorithms that use admit data against the same algorithms using conditions extracted from textual reports.         n/a",Natural Language Processing for Respiratory Surveillance,7076099,K22LM008301,"['automated data processing', 'automated medical record system', 'bioinformatics', 'bioterrorism /chemical warfare', 'clinical research', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease outbreaks', 'early diagnosis', 'hospital utilization', 'human data', 'language translation', 'respiratory disorder epidemiology', 'severe acute respiratory syndrome', 'sign /symptom']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K22,2006,162000,-0.0478326133943759
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7111722,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2006,314029,-0.027819689432835328
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,7123058,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,611872,-0.02956377762558385
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,-0.03472082637522466
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,7015648,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2006,500182,-0.012863714767771635
"Collaborative Brain Mapping: Tools for Sharing    DESCRIPTION (provided by applicant): Sharing data between research centers is increasingly important for contemporary brain imaging studies because they involve large numbers of subjects and complex analysis protocols that require highly specialized expertise. Our long-term objective is to facilitate brain-imaging research by enabling remote researchers to pool data between institutions and to analyze data using the appropriate algorithms executing on distributed resources. There are a number of difficult data management and technology challenges that have limited the success of data sharing environments. Rather than attempt to develop a comprehensive and general solution, we propose to develop a set of open, interoperable, and portable software tools that address critical issues currently limiting efforts to share and analyze brain-imaging data. Building upon years of providing brain-mapping expertise to collaborators, we propose to solve problems that we repeatedly encounter and that currently limit progress in brain imaging research. We propose to develop validated tools that enable collaborators to remotely access a variety of data analysis methods and databases. We will create web-based tools to perform multi-institutional studies, and provide access to complex data processing protocols executing on distributed computing resources. There are three specific aims. 1) Enable the web based acquisition and management of data utilizing an access control system that includes consideration of subject consent limits and investigator imposed conditions to facilitate data pooling for multi-institutional studies. This system will convert data files between different formats and schemas so that data can be used consistently between analysis programs and databases. It will also anonymize images and metadata according to institution-specific protocols. 2) Develop a system that is aware of data type and provenance so that it may act intelligently to arbitrate between different analysis programs. This system will capture the expertise of experienced lab personnel in the usage of various tools and assist new users in designing appropriate analytic strategies. 3) Create meta-algorithms that improve the robustness of techniques for neuroimaging analysis by intelligently combining the results from multiple algorithms. The proposed approach will provide a set of tools that address significant problems in data sharing and utilization. The resulting information technology will be scalable and applicable to other scientific data sharing problems.         n/a",Collaborative Brain Mapping: Tools for Sharing,7109207,R01MH071940,"['Internet', 'artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain mapping', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'confidentiality', 'data management', 'information dissemination', 'information systems', 'mathematics']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2006,649537,0.003996034025620501
"Community Deposit and Review of Biochemical Databases DESCRIPTION (provided by applicant):    Understanding how the phenotype of an organism is produced by its genotype and environment is fundamental to modern biomedicine. Cellular biochemistry forms the best-characterized complex biochemical system available, and provides a unique opportunity to better understand the structure-function relationships that produce phenotypes. Understood mathematically, cells are a biochemical network whose topology, function, and possible behaviors are only still only dimly understood.      Our previous work has resulted in the development of several databases (END, BND, Klotho) and software systems (The Agora, Glossa) to provide, accumulate, and use data on biochemical networks by users worldwide. In this application for competitive renewal, we propose to embark on a systematic study of structure-function relationships in biochemical networks, focusing on the discovery of patterns of biochemical function --- functional motives. We will identify these motives and their distribution over all enzymatic reactions by comparing changes in reactants' structures and enzymes' specificities, rather than just examine keywords. This systematic examination will allow us to determine, at much greater resolution than ever before, what functions each molecule and reaction have. To do this we must significantly extend the functionalities of our current systems in three major ways. First, we will add significant new data on the structure of small molecules of biochemical interest, enzymatic reactions, the mechanisms of gene expression, and dementia. Second, we will further develop and test methods that produce semantic interoperability among independent, disparate databases. Third, we will develop algorithms to detect and catalogue patterns of biochemical function among thousands of reactions and molecules; to more speedily enumerate paths among molecules and subnets in the biochemical network; to determine the extent of convergent evolution among enzymes; to trace atoms through a sequence of reactions such as a metabolic pathway; and to suggest novel biochemical reactions. We will use these capabilities to define and search for functional motives among enzymatic reactions, testing their correlation with network topology and dynamics, and to estimate the extent to which functional similarities arise from convergent evolution of enzymes. n/a",Community Deposit and Review of Biochemical Databases,7107923,R01GM056529,"['artificial intelligence', 'biochemistry', 'computer program /software', 'computer system design /evaluation', 'dementia', 'enzyme mechanism', 'functional /structural genomics', 'information system analysis', 'mathematical model', 'molecular biology information system', 'molecular dynamics', 'physiology', 'protein structure function']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2006,509027,-0.015290931875031005
"Computer System for Functional Analysis of Genomic Data    DESCRIPTION (provided by applicant): In two previous stages of this project, both funded by the National Institute of General Medical Sciences and carried out successfully, we developed GeneWays, a completely automated system that efficiently distills information about molecular interactions from an astronomical number of full-text biomedical articles. The next logical stage of the project is to carry this system from the computational laboratory into a practical, useful, and even indispensable tool that researchers can use to solve complex problems currently posed in experimental medicine and biology. The central hypothesis of our work on GeneWays has been that our computational tools will generate biological predictions of a quality sufficiently high that the biomedical community will invest in serious experimental validation. Specifically, we propose the following. 1. We will improve significantly the precision and recall of the GeneWays system. 2. We will develop and implement a probabilistic belief-network formalism?a belief-graph relative of the Bayesian network formalism that allows us to place and update beliefs on both the vertices and the edges of the graph for probabilistic reasoning over the large collection of facts in the GeneWays database. We will develop and implement a coordinated collection of methods for computing and updating beliefs on individual nodes and edges of the belief graph. 3. We will develop and implement a mathematical framework for incorporating pathway information into a genetic- linkage analysis formalism in such a way that each piece of pathway knowledge includes a specified degree of confidence. 4. We will process an enormous collection of texts, such as open-access biomedical journals, PubMed abstracts, and the GeneWays corpus, and thus will build a comprehensive GeneHighWays database. We will make the GeneHighWays database easily and freely accessible to academic researchers through a web interface. We will evaluate the new version of the GeneWays system and the GeneHighWays database for the quality of data, performance of the mathematical methods, and quality of the interface.           n/a",Computer System for Functional Analysis of Genomic Data,7148274,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,303688,-0.03699958793303503
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7007706,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2006,153843,-0.02586176362678344
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,7109270,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2006,301738,-0.004013043966968757
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,7109270,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2006,1958689,-0.004013043966968757
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,7109270,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2006,169911,-0.004013043966968757
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,7109270,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2006,402318,-0.004013043966968757
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,7243612,R33RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R33,2006,350636,-0.020465559855141147
"Intelligent Tutor for WMD EMS Incident Management    DESCRIPTION (provided by applicant): We propose to develop EMS/IM ITS, a suite of simulation-based intelligent tutoring systems and scenarios that will enable practice-based learning of WMD emergency medical services incident management principles and skills, including situation assessment, decision-making, and real-time execution of EMS tasks within an incident command structure. To support practical and economical development of many EMS/IM ITS training scenarios, we will also develop software tools and development methods that enable efficient authoring of new scenarios and adaptation/enhancement of existing scenarios by instructors or subject matter experts, without programming. We will leverage our tutoring system development tools and our experience developing tutoring systems for medical training, command and control, and tactical decision-making. The National Incident Management System (NIMS) was mandated by HSPD-5 to provide a comprehensive, national approach to domestic incident management, so that all levels of government across the nation could work efficiently and effectively together to prepare for, respond to, and recover from domestic incidents. We believe that EMS/IM ITS can contribute to NIMS by providing scenario-based learning of incident management principles for medical first responders, consistent with NIMS, and tailorable via scenario authoring to the specific circumstances and incident management plans of each government organization. This proposed Phase I effort will lay the groundwork for the Phase II effort, by producing 1) requirements and design of the system to be developed during Phase II, 2) a software prototype that illustrates our concept, and 3) a formative evaluation of the prototype and design that provides a basis for estimating the feasibility and effectiveness of the operational system that would be developed during Phase II.             n/a",Intelligent Tutor for WMD EMS Incident Management,7115108,R43ES014801,"['artificial intelligence', 'computer assisted instruction', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'educational resource design /development', 'emergency service /first responder', 'health care personnel education', 'health care professional practice', 'health services research tag', 'medical education', 'method development', 'patient care management', 'training']",NIEHS,"STOTTLER HENKE ASSOCIATES, INC.",R43,2006,99999,-0.0015904131595173142
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7101997,R01LM008799,"['clinical research', 'human', 'language', 'memory disorders', 'model', 'physicians', 'training', 'voice']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,372499,-0.01256280053605713
"10th International Fragile X Conference    DESCRIPTION (Provided by the Applicant):  10th International Fragile X Conference: The National Fragile X Foundation's 10th International Fragile X Conference in Atlanta, Georgia, at the OMNI Hotel - CNN Center, July 19-23, 2006, will bring together the world's leading researchers in molecular biology and genetics as well as leading clinicians and treatment specialists, selected by its Scientific and Clinical Advisory Committee, with hundreds of parents, extended family members and students engaged in research training. Both scientific and family-friendly sessions covering the three conditions resulting from the fragile X gene mutation will be addressed: fragile X syndrome; fragile X associated tremor ataxia syndrome; fragile X related premature ovarian failure. Keynote presentations, breakout session lectures, research abstract sessions, panels and posters will present the latest knowledge regarding the underlying mechanisms for the fragile X related conditions, plus evidence-based medical, therapeutic and educational interventions. The conference will benefit multiple disciplines including those engaged in clinical practice, epidemiology and delivery system organization. The National Fragile X Foundation will publish and disseminate the results in conference proceedings as well as other formats and utilize the recommendations as the basis for advancing the research and treatment fields.    n/a",10th International Fragile X Conference,7166760,R13HS016448,"['fragile X syndromes', 'meeting /conference /symposium', 'travel']",AHRQ,NATIONAL FRAGILE X FOUNDATION,R13,2006,25000,-0.007727846550710166
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7019753,G08LM008983,"['clinical research', 'public health']",NLM,SYRACUSE UNIVERSITY,G08,2006,149472,-0.016785311480589316
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,7083613,R01EB000362,"['automated medical record system', 'clinical research', 'computer assisted medical decision making', 'data collection methodology /evaluation', 'human data', 'informatics', 'information display', 'information system analysis', 'mathematical model', 'medical records', 'outcomes research', 'patient care management', 'performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2006,535569,-0.04910053696222924
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,7284550,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2006,498959,-0.025116160918481754
"24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS    DESCRIPTION (provided by applicant):    This conference grant (R13) application requests funds to partially cover the cost of planning, organizing, publicizing and hosting the 24th Annual Symposium on Nonhuman Primate Models for AIDS. The symposium will be held October 4-7, 2006, at the Omni Hotel at CNN Center in downtown Atlanta, Georgia, and will be hosted by the Yerkes National Primate Research Center, Emory University. This meeting is the premier forum for the presentation and exchange of the most recent scientific advances in AIDS research utilizing the nonhuman primate model. The latest findings in primate pathogenesis, immunology, genomics, virology, vaccines and therapeutics will be presented. It is anticipated more than 300 scientists from the United States and other countries will attend. The symposium will encompass five half-day scientific sessions and an evening poster session. The scientific sessions will be: Virology, Pathogenesis, Immunology, Vaccines and Therapeutics/Genomics. Each session will have an invited Chair, a scientific leader in the field, who will give a 30-minute state-of-the-field presentation to open the session, and a Co-Chair from the Scientific Committee, who will moderate the session and entertain questions. In addition, there will be an invited keynote speaker and a banquet speaker, who will address scientific approaches and concerns regarding the global AIDS crisis and related issues of public health. A Scientific Program Committee consisting of eight-ten members drawn from the Yerkes/Emory community and other institutions will review abstracts and assign oral or poster presentations for each of the scientific sessions. Committee members will include leaders in the field from a variety of scientific disciplines. Criteria for selection of oral presentations will include relevance of the topic as well as originality and quality of the information contained in the abstract. Those giving talks will be invited to submit their presentations in manuscript form for publication in the Journal of Medical Primatology. A poster session will include meritorious presentations that cannot be accommodated in one of the platform sessions. A local Organizing Committee will handle arrangements and logistics for the symposium. Feedback from the participants will be obtained through written questionnaires or oral comments to members of the organizing committee. This format has been successfully followed using NCRR support for the previous Annual symposium.           n/a",24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS,7114527,R13RR022961,"['AIDS', 'Primates', 'disease /disorder model', 'meeting /conference /symposium', 'travel']",NCRR,EMORY UNIVERSITY,R13,2006,63089,0.00029805907363412027
"24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS    DESCRIPTION (provided by applicant):    This conference grant (R13) application requests funds to partially cover the cost of planning, organizing, publicizing and hosting the 24th Annual Symposium on Nonhuman Primate Models for AIDS. The symposium will be held October 4-7, 2006, at the Omni Hotel at CNN Center in downtown Atlanta, Georgia, and will be hosted by the Yerkes National Primate Research Center, Emory University. This meeting is the premier forum for the presentation and exchange of the most recent scientific advances in AIDS research utilizing the nonhuman primate model. The latest findings in primate pathogenesis, immunology, genomics, virology, vaccines and therapeutics will be presented. It is anticipated more than 300 scientists from the United States and other countries will attend. The symposium will encompass five half-day scientific sessions and an evening poster session. The scientific sessions will be: Virology, Pathogenesis, Immunology, Vaccines and Therapeutics/Genomics. Each session will have an invited Chair, a scientific leader in the field, who will give a 30-minute state-of-the-field presentation to open the session, and a Co-Chair from the Scientific Committee, who will moderate the session and entertain questions. In addition, there will be an invited keynote speaker and a banquet speaker, who will address scientific approaches and concerns regarding the global AIDS crisis and related issues of public health. A Scientific Program Committee consisting of eight-ten members drawn from the Yerkes/Emory community and other institutions will review abstracts and assign oral or poster presentations for each of the scientific sessions. Committee members will include leaders in the field from a variety of scientific disciplines. Criteria for selection of oral presentations will include relevance of the topic as well as originality and quality of the information contained in the abstract. Those giving talks will be invited to submit their presentations in manuscript form for publication in the Journal of Medical Primatology. A poster session will include meritorious presentations that cannot be accommodated in one of the platform sessions. A local Organizing Committee will handle arrangements and logistics for the symposium. Feedback from the participants will be obtained through written questionnaires or oral comments to members of the organizing committee. This format has been successfully followed using NCRR support for the previous Annual symposium.           n/a",24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS,7114527,R13RR022961,"['AIDS', 'Primates', 'disease /disorder model', 'meeting /conference /symposium', 'travel']",NCRR,EMORY UNIVERSITY,R13,2006,10000,0.00029805907363412027
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,7117852,R01LM005652,"['active sites', 'binding sites', 'computer simulation', 'macromolecule', 'model design /development', 'physical model', 'structural biology']",NLM,STANFORD UNIVERSITY,R01,2006,326259,-0.044431013628765205
"Accessible Artificial Intelligence Tutoring Software DESCRIPTION (provided by applicant): Quantum has successfully developed, tested and brought to the classroom the first artificial intelligence (Al) tutoring systems in chemistry education. This work successfully addressed several longstanding, clearly articulated needs for improved interactive educational software. A leading distributor for the U.S. and Canada, Science Kit & Boreal Laboratories, as well as prominent textbook publisher, Holt, Rinehart and Winston, have entered into long-term contracts with Quantum, resulting in rapid dissemination to an established end user base. The aim of this Phase I SBIR proposal is to bring the full power and benefits of this cutting-edge new educational technology to students who are blind and visually impaired. There is a considerable need for improved educational software for science education in general, but the problem of quality educational software materials for the blind is known to be particularly acute. Certain unique attributes of the Quantum Al Tutors make them potentially very well suited for full accessibility to the blind using Internet-capable screen reader technology. The potential technological innovation here is the development of advanced Al tutoring technology that has accessibility built into its framework design. If successful, an immediate outcome will be the first Al tutoring systems that are accessible to blind students, delivered through the Internet. A formulation of an Al tutoring methodology with accessibility inherent to the design will have broad implications for the prospect of developing sophisticated accessible educational software in all content areas, beyond chemistry. This project can only be accomplished by working intimately with experts in education for the blind, and Quantum has arranged a number of important partnerships in this respect, for research as well as commercialization of the resulting technology, including: the National Federation of the Blind, the American Printing House for the Blind, Pearson Learning Group, Bartimaeus Group and Henter Mathematics. n/a",Accessible Artificial Intelligence Tutoring Software,6880607,R43EY016251,"['Internet', 'artificial intelligence', 'blind aid', 'chemistry', 'computer assisted instruction', 'computer human interaction', 'computer program /software', 'educational resource design /development', 'science education', 'technology /technique development']",NEI,"QUANTUM SIMULATIONS, INC.",R43,2005,100721,-0.015224697575703265
"Natural Language Processing for Respiratory Surveillance    DESCRIPTION (provided by applicant): The applicant's long-term career goal is to become an independent investigator in biomedical informatics research. She will dedicate her career to developing and evaluating methodologies for improving the processes and outcomes of healthcare using data locked in textual documents. This career development award will provide her with initial support for achieving her career goals.      The applicant has three goals for her career development over the next three years. First, she will compare the performance of different machine learning techniques at detecting patients with a respiratory syndrome. The proposed research will expand the state-of-the-art syndromic surveillance capabilities by integrating findings, symptoms, and diseases described in textual medical records. The product of the first research goal will be a model for respiratory syndromic case detection for monitoring natural and bioterrorism induced respiratory outbreaks. Second, she will apply existing methods and develop new techniques for extracting clinical conditions required for respiratory case detection from emergency department notes, contributing new knowledge to the medical language processing field using sentence and report level models that account for uncertainty, negation, and temporal occurrence. The product of the second goal will be a better understanding of the information required for accurate detection of respiratory related conditions from text and useful tools for automatically extracting that information. Third, she will teach, promote, and facilitate the use of natural language processing in the biomedical informatics field. The product of the third goal will be a graduate class surveying medical language processing methodology and applications and development of general tools sets for researchers who need encoded data from textual patient records.        The proposed research will focus on:   Aim 1. Development and evaluation of a respiratory case detection model;   Aim 2. Integration of existing natural language processing tools and development of new methodologies for extracting clinical conditions needed for respiratory case detection from textual records; and   Aim 3. Comparison of existing syndromic detection algorithms that use admit data against the same algorithms using conditions extracted from textual reports.         n/a",Natural Language Processing for Respiratory Surveillance,6898458,K22LM008301,"['automated data processing', 'automated medical record system', 'bioinformatics', 'bioterrorism /chemical warfare', 'clinical research', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease outbreaks', 'early diagnosis', 'hospital utilization', 'human data', 'language translation', 'respiratory disorder epidemiology', 'severe acute respiratory syndrome', 'sign /symptom']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K22,2005,162000,-0.0478326133943759
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6916483,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,321788,-0.027819689432835328
"Biosurveillance/SNOMED-CT Natural Language Processing Bioterrorism remains a significant threat to our public health. Early identification of an increased rate of occurance of patient presentations consistent with an exposure to an agent of bioterrorism is one important method to contain bioterror attacks and effect more rapid treatment of exposed individuals. Often presentations consistent with an exposure to an agent of bioterrorism occur in significant numbers prior to the recognition that a bioterrorism related exposure has occurred. This presents an opportunity to capture and analyze signals from patient records. We propose to provide an abstraction of automated clinical information from the clinical record (section by section) that will be coded using SNOMED-CT which can serve as the substrate for surveillance data. We believe that this data (sets of codes by section of the clinical record) which holds the important and salient medical facts (codes) regarding the patients' presentation, findings, medications, allergies and co-morbidities could be abstracted from clinical records. In this study, we will analyze SNOMED-CT's ability to provide adequate content coverage for constellations of symptoms associated with exposures to agents of bioterrorism (i.e. Anthrax, Small Pox, Ricin, and Radiation exposure). Our method builds on the considerable body of research already available within our laboratory. We have been researching methods for codifying medical content using controlled medical vocabularies since 1987. For this study, we will employ the Mayo Vocabulary Server (MVS) developed in the Mayo Laboratory of Biomedical Informatics and has been used at Mayo, Johns Hopkins University and the VA medical centers all with great success. Our performanc2 of the MVS toolkit has been validated for diagnoses where we showed a sensitivity of 99.7% and a specificity of 97.9%. This proposal deals with practical issues that lead the way toward interoperable data. The fruits of this research will assist our national initiatives to pave the way toward a safe and effective BioSecure biosurveillance solution.  n/a",Biosurveillance/SNOMED-CT Natural Language Processing,7098649,R01PH000022,"['abstracting', 'artificial intelligence', 'biohazard detection', 'bioterrorism /chemical warfare', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'disease outbreaks', 'early diagnosis', 'environmental health', 'human data', 'mathematical model', 'medical records', 'model design /development', 'public health', 'rapid diagnosis', 'vocabulary development for information system']",PHPPO,MAYO CLINIC,R01,2005,514616,-0.061082851200302025
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6953701,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2005,613495,-0.02956377762558385
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6892934,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,384538,-0.03534311046823037
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6946761,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2005,100000,-0.003079402504554836
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6946761,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2005,236551,-0.003079402504554836
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The overall goal of this proposal is to develop an information integration architecture and associated tools to support rapid integration of data and knowledge from distributed heterogeneous data sources. The architecture aims to play a significant role in extracting coherent knowledge bases for biomedical research and improving the accuracy, completeness and quality of the extracted knowledge. Towards achieving these goals, the proposed scalable architecture includes new innovative generalized integration algorithms and tools for the generation of mediators to capture the functional behavior of data sources, semantic representation of data sources to support automated generation of integration agents, and optimization of integrated data queries. The information integration architecture keeps pace with the evolving Internet-based XML electronic data interchange, semantic web services, and web services discovery standards. Thus, leveraging the Internet technologies and standards for the purpose of providing lasting state-of-the-art solutions for information integration. In addition, the proposed architecture is inherently scalable in terms of the number of data sources that can be integrated, the number of users of the integrated system, and the range of biomedical problems that can be tackled. During phase I of the project, prototypes of the proposed integration algorithms and tools will be developed as proofs of concept and to form the foundation for evaluation and pilot testing of the proposed integration mechanisms, using private and public data sources, in terms of scalability and integration capabilities.         n/a",Information Integration of Heterogeneous Data Sources,6881960,R43RR018667,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'information system analysis', 'information systems', 'mathematics']",NCRR,"INFOTECH SOFT, INC.",R43,2005,260661,-0.019282144433932637
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,-0.03472082637522466
"Perception and Inter-Observer Variability in Mammography DESCRIPTION (provided by applicant):    Inter-observer variability in mammogram reading has been well documented in the literature. Various factors have been used to explain this variability; among them, the most significant are related to the management of perceived findings.  However, the nature of this inter-observer variability has not been explored. Namely, were the lesions that were consistently reported by the radiologists any different from the ones that yield disagreement? Furthermore, could these differences be quantitatively assessed? Moreover, were these differences in any way related with the experience level of the observer? In addition, the interpretation of perceived findings is closely related with the visual search strategy used to scan the breast tissue, because observers compare perceived findings with the background, in order to determine their uniqueness. Hence, what is the effect of visual search strategy on inter-observer variability? Can this effect be modeled using Artificial Neural Networks (ANNs)? Can inferences be made regarding the observers' decision patterns by analyzing the results of simulations run on the ANNs?  The work described here aims at answering these questions. We will use spatial frequency analysis to characterize the areas on mammogram cases where mammographers, chest radiologists with experience reading mammograms and radiology residents at the end of their mammography rotation, indicate the presence of a finding, or fail to do so. We will assess inter-observer agreement, as well as intra- and inter-group agreement for the various groups of observers. In addition, we will train artificial neural networks to represent each observer, in such a way that by changing the nature of the features input to the ANNs we will be able to simulate how such changes would have affected the actual observer. We will assess the effects on inter-observer variability of changing the search strategy used by the observer to sample the breast tissue. In our setting, the inter-observer variability will be assessed by comparing the outputs of the ANNs that represent each observer. In addition, the changes in sampling strategy will correspond to actual possible strategies for the human observers themselves. n/a",Perception and Inter-Observer Variability in Mammography,6924688,R21CA100107,"['artificial intelligence', 'bioimaging /biomedical imaging', 'breast neoplasm /cancer diagnosis', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'human data', 'human therapy evaluation', 'mammography', 'visual perception']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2005,167063,-0.03825191587720622
"Linking Information, Families and Technology (LIFT) DESCRIPTION (provided by applicant): KIT Solutions, a private firm specializing in developing intelligent, Knowledge-based Information Technology (KIT) solutions for the field of health and human services will partner with the University of Pittsburgh, Office of Child Development (OCD) to develop a Web-based, interactive software application of a Family Support Management Information System (FS MIS) for nationwide dissemination. This innovation is called Linking Information, Families and Technology (LIFT).  Family support centers, like other human service programs and agencies across the country, are being required to implement best practices and document the impact of their services to funders, policy makers, and the community. However, most family centers do not have a state-of-the-art web-based information system available to them that integrates best practice, expert knowledge, and daily management functions. The proposed LIFT system will address these critical needs and has great potential for nationwide commercial distribution. The combination of KIT'S proven record of developing knowledge based information technology and OCD's over 20 years of research and practice in family support services will greatly enhances the chance of success for this business venture. In Phase I of the project, we will produce prototype software demonstrating the benefit, usability, and feasibility of a web-based, interactive, intelligent system for use by family support centers across the nation. The extent of which LIFT enables family center staff to build skill, capacity, access information and expert knowledge, to enhance their work will be the focus of this phase. In Phase II, we will fully develop the prototype LIFT to a commercial grade web application for nationwide dissemination and further validate the commercial potential and impact of LIFT, using a quasiexperimental design, which will involve a large number of users across multiple sites. In Phase III, we will seek private funding for marketing the system to the national market. We intend to use the Microsoft.Net Platform and follow XML web service concepts to develop the proposed innovation. Collection of a subscription fee will be used to support the maintenance and future development of the system. n/a","Linking Information, Families and Technology (LIFT)",6990440,R43HD049229,"['Internet', 'artificial intelligence', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'family', 'focus groups', 'human subject', 'information dissemination', 'social service']",NICHD,"KIT SOLUTIONS, INC.",R43,2005,104545,0.0069002032982079815
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,6863029,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2005,498368,-0.012863714767771635
"Collaborative Brain Mapping: Tools for Sharing    DESCRIPTION (provided by applicant): Sharing data between research centers is increasingly important for contemporary brain imaging studies because they involve large numbers of subjects and complex analysis protocols that require highly specialized expertise. Our long-term objective is to facilitate brain-imaging research by enabling remote researchers to pool data between institutions and to analyze data using the appropriate algorithms executing on distributed resources. There are a number of difficult data management and technology challenges that have limited the success of data sharing environments. Rather than attempt to develop a comprehensive and general solution, we propose to develop a set of open, interoperable, and portable software tools that address critical issues currently limiting efforts to share and analyze brain-imaging data. Building upon years of providing brain-mapping expertise to collaborators, we propose to solve problems that we repeatedly encounter and that currently limit progress in brain imaging research. We propose to develop validated tools that enable collaborators to remotely access a variety of data analysis methods and databases. We will create web-based tools to perform multi-institutional studies, and provide access to complex data processing protocols executing on distributed computing resources. There are three specific aims. 1) Enable the web based acquisition and management of data utilizing an access control system that includes consideration of subject consent limits and investigator imposed conditions to facilitate data pooling for multi-institutional studies. This system will convert data files between different formats and schemas so that data can be used consistently between analysis programs and databases. It will also anonymize images and metadata according to institution-specific protocols. 2) Develop a system that is aware of data type and provenance so that it may act intelligently to arbitrate between different analysis programs. This system will capture the expertise of experienced lab personnel in the usage of various tools and assist new users in designing appropriate analytic strategies. 3) Create meta-algorithms that improve the robustness of techniques for neuroimaging analysis by intelligently combining the results from multiple algorithms. The proposed approach will provide a set of tools that address significant problems in data sharing and utilization. The resulting information technology will be scalable and applicable to other scientific data sharing problems.         n/a",Collaborative Brain Mapping: Tools for Sharing,6953037,R01MH071940,"['Internet', 'artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain mapping', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'confidentiality', 'data management', 'information dissemination', 'information systems', 'mathematics']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2005,647432,0.003996034025620501
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,-0.011710152268307192
"Community Deposit and Review of Biochemical Databases DESCRIPTION (provided by applicant):    Understanding how the phenotype of an organism is produced by its genotype and environment is fundamental to modern biomedicine. Cellular biochemistry forms the best-characterized complex biochemical system available, and provides a unique opportunity to better understand the structure-function relationships that produce phenotypes. Understood mathematically, cells are a biochemical network whose topology, function, and possible behaviors are only still only dimly understood.      Our previous work has resulted in the development of several databases (END, BND, Klotho) and software systems (The Agora, Glossa) to provide, accumulate, and use data on biochemical networks by users worldwide. In this application for competitive renewal, we propose to embark on a systematic study of structure-function relationships in biochemical networks, focusing on the discovery of patterns of biochemical function --- functional motives. We will identify these motives and their distribution over all enzymatic reactions by comparing changes in reactants' structures and enzymes' specificities, rather than just examine keywords. This systematic examination will allow us to determine, at much greater resolution than ever before, what functions each molecule and reaction have. To do this we must significantly extend the functionalities of our current systems in three major ways. First, we will add significant new data on the structure of small molecules of biochemical interest, enzymatic reactions, the mechanisms of gene expression, and dementia. Second, we will further develop and test methods that produce semantic interoperability among independent, disparate databases. Third, we will develop algorithms to detect and catalogue patterns of biochemical function among thousands of reactions and molecules; to more speedily enumerate paths among molecules and subnets in the biochemical network; to determine the extent of convergent evolution among enzymes; to trace atoms through a sequence of reactions such as a metabolic pathway; and to suggest novel biochemical reactions. We will use these capabilities to define and search for functional motives among enzymatic reactions, testing their correlation with network topology and dynamics, and to estimate the extent to which functional similarities arise from convergent evolution of enzymes. n/a",Community Deposit and Review of Biochemical Databases,6944266,R01GM056529,"['artificial intelligence', 'biochemistry', 'computer program /software', 'computer system design /evaluation', 'dementia', 'enzyme mechanism', 'functional /structural genomics', 'information system analysis', 'mathematical model', 'molecular biology information system', 'molecular dynamics', 'physiology', 'protein structure function']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2005,523667,-0.015290931875031005
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6908174,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2005,375000,-0.0236053418355095
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7125331,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2005,152083,-0.02586176362678344
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,6942558,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2005,300000,-0.004013043966968757
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,6942558,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2005,1982826,-0.004013043966968757
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,6942558,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2005,169000,-0.004013043966968757
"The PharmGKB: Catalyzing Research in Pharmacogenetics    DESCRIPTION (provided by applicant): The Pharmacogenetics and pharmacogenomics Knowledge Base (PharmGKB) is designed to catalyze research on how genetic variation contributes to variation in drug response. It provides information and analytical tools by means of a public website that is devoted to linking genotype and phenotype information in the post-genome era. The PharmGKB development team has five foci: user interface & functionality, data curation, outreach & dissemination, administration, and infrastructure.  This proposal is based on four years of experience working with the scientific community to define and prioritize opportunities to support pharmacogenetics and pharmacogenomics. The PharmGKB has accumulated genotype data, phenotype data, curated literature annotations, and drug-related pathways-and currently attracts more than 23,000 unique visitors (IP addresses) each month. Our   plan stresses 1) extending the PharmGKB to the entire scientific community by increasing data submissions, functionality, and ease of use, 2) focusing on acquiring high-quality drug-related pathways and using them as interfaces to, pharmacogenetic data and knowledge, 3) catalyzing the use of standards for information exchange within the field, and 4) participating in the public discussion of methods to protect the privacy and confidentiality of study subject data.         n/a",The PharmGKB: Catalyzing Research in Pharmacogenetics,6942558,U01GM061374,"['artificial intelligence', 'biomedical resource', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'human data', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2005,400000,-0.004013043966968757
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6914863,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2005,167918,-0.020465559855141147
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS    DESCRIPTION (provided by applicant): The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methods within the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data, earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user interface to support access to information resources for enhanced decision-making by practitioners. The long-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3 Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technology work; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C).             n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7084856,P01CD000261,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2005,1270432,0.003922571103101891
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,6917854,R01EB000362,"['automated medical record system', 'clinical research', 'computer assisted medical decision making', 'data collection methodology /evaluation', 'human data', 'informatics', 'information display', 'information system analysis', 'mathematical model', 'medical records', 'outcomes research', 'patient care management', 'performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2005,593264,-0.04910053696222924
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6848697,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2005,1446246,-0.025116160918481754
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6943136,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2005,467823,-0.023969591249837104
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,6942980,R01LM005652,"['active sites', 'binding sites', 'computer simulation', 'macromolecule', 'model design /development', 'physical model', 'structural biology']",NLM,STANFORD UNIVERSITY,R01,2005,324673,-0.044431013628765205
"Natural Language Processing for Respiratory Surveillance    DESCRIPTION (provided by applicant): The applicant's long-term career goal is to become an independent investigator in biomedical informatics research. She will dedicate her career to developing and evaluating methodologies for improving the processes and outcomes of healthcare using data locked in textual documents. This career development award will provide her with initial support for achieving her career goals.      The applicant has three goals for her career development over the next three years. First, she will compare the performance of different machine learning techniques at detecting patients with a respiratory syndrome. The proposed research will expand the state-of-the-art syndromic surveillance capabilities by integrating findings, symptoms, and diseases described in textual medical records. The product of the first research goal will be a model for respiratory syndromic case detection for monitoring natural and bioterrorism induced respiratory outbreaks. Second, she will apply existing methods and develop new techniques for extracting clinical conditions required for respiratory case detection from emergency department notes, contributing new knowledge to the medical language processing field using sentence and report level models that account for uncertainty, negation, and temporal occurrence. The product of the second goal will be a better understanding of the information required for accurate detection of respiratory related conditions from text and useful tools for automatically extracting that information. Third, she will teach, promote, and facilitate the use of natural language processing in the biomedical informatics field. The product of the third goal will be a graduate class surveying medical language processing methodology and applications and development of general tools sets for researchers who need encoded data from textual patient records.        The proposed research will focus on:   Aim 1. Development and evaluation of a respiratory case detection model;   Aim 2. Integration of existing natural language processing tools and development of new methodologies for extracting clinical conditions needed for respiratory case detection from textual records; and   Aim 3. Comparison of existing syndromic detection algorithms that use admit data against the same algorithms using conditions extracted from textual reports.         n/a",Natural Language Processing for Respiratory Surveillance,6768325,K22LM008301,"['automated data processing', 'automated medical record system', 'bioinformatics', 'bioterrorism /chemical warfare', 'clinical research', 'communicable disease diagnosis', 'computer assisted diagnosis', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease outbreaks', 'early diagnosis', 'hospital utilization', 'human data', 'language translation', 'respiratory disorder epidemiology', 'severe acute respiratory syndrome', 'sign /symptom']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K22,2004,135000,-0.0478326133943759
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6799187,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,321983,-0.027819689432835328
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6822280,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2004,577307,-0.02956377762558385
"LiFESim: Software for health science education (NCRR)    DESCRIPTION (provided by applicant): Stottler Henke Associates in collaboration with Teachers College, Columbia University, proposes to build a software system for teaching scientific inquiry in the context of nutrition science, The goal of the proposed research is to develop a computer based instructional system - called LiFESim - that teaches accurate and detailed information about food and the food system -- from production of food on the farm through food processing and transportation, to impacts of food on personal health and on the natural environment in terms of waste and pollution. The software will complement an existing health science curriculum, developed at Teachers College for 4th-6th graders, called ""Linking Food and the Environment"" or LIFE, developed from an NIH Science Education Partnership Award (SEPA) RR 12374 (1997-2004). Our system will be based on the paradigm of role-playing simulation used in such popular computer games as SimCity and The Sims: students using the software assume roles in a simulated environment and learn from the consequences of the decisions that they make in those roles. Using the simulation paradigm students will be able to explore the dynamics of large-scale systems, such as those the food transportation system in ways that are not possible with the existing curriculum. For example, the simulation would allow students to explore the impact of changes in transportation patterns on food delivery. Our system will provide explicit coaching in applying scientific methods for investigation. We will also explore learning strategies that will encourage students to critically examine - and hopefully improve - their dietary choices. We will complement simulation-based learning with two other artificial intelligence based methodologies - the use of lifelike pedagogical agents, and the use of case-based reasoning. During Phase I, we will develop a set of detailed instructional goals, use these to develop an initial system design, develop a limited prototype of the system, and then develop and perform an informal pilot study to evaluate the viability of our design. The pilot study will be conducted over a one-month period at schools in Hayward, California and New York City. Our Phase II effort will focus on developing an extensive design and performing detailed use testing of the system developed during Phase I.         n/a",LiFESim: Software for health science education (NCRR),6790401,R43RR019780,"['artificial intelligence', 'bioengineering /biomedical engineering', 'clinical research', 'computer assisted instruction', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'education evaluation /planning', 'educational resource design /development', 'environmental contamination', 'food', 'food processing /preparation', 'health education', 'human subject', 'interactive multimedia', 'nutrition', 'nutrition related tag', 'science education']",NCRR,"STOTTLER HENKE ASSOCIATES, INC.",R43,2004,100000,-0.023726717886533116
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6754395,R01LM006910,"['artificial intelligence', 'classification', 'clinical research', 'computer assisted medical decision making', 'computer assisted patient care', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'health care facility information system', 'human data', 'information system analysis', 'method development', 'vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,380979,-0.03534311046823037
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6805962,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2004,100000,-0.003079402504554836
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6805962,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2004,222326,-0.003079402504554836
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6703756,R44CA093112,"['artificial intelligence', 'clinical research', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'mathematics', 'statistics /biometry']",NCI,"CYTEL, INC",R44,2004,411387,-0.011472104262433322
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,-0.03472082637522466
"AI Software for Science Education Related to Drug Abuse DESCRIPTION (provided by applicant): This Phase I SBIR proposal is aimed at advancing the state of the art in chemistry education software in a critically important respect demanded by students, teachers, administrators and Quantum Simulations, Inc. customers. The focus of this innovation is the development of meaningful interactive tutoring and assessment capabilities for chemistry problem solving. Empowerment of students to make the proper decisions about drugs through experiencing the scientific process requires a solid education in basic chemistry. Chemical formulas comprise much of the fundamental ""language"" of chemistry in which students must be fluent in order to succeed. The topic of writing and understanding chemical formulas, a cornerstone of all general chemistry classes, is a reasonable starting point for the development of an AI assessment system for student learning in chemistry. A solid understanding of chemical formulas is a prerequisite to success in chemistry required not only for literacy to make informed decisions about drugs from a scientific standpoint, but also to enable and prepare students to pursue careers in research related to drug abuse. Quantum has already successfully developed and commercialized an ITS for writing chemical formulas which will be used as the starting point for the present work. The proposed technology will benefit all students; however, it is specifically targeted to help those who have the greatest need, such as students of average or marginal performance and students from historically underserved groups, by lowering barriers to accessing high-quality science instructional software.  Quantum customers include textbook publishers, software providers, hardware vendors and distance learning companies. A prominent textbook publisher, Holt, Rinehart and Winston, has entered into two long-term contracts with Quantum, resulting in rapid dissemination to an established end user base. Quantum intends to employ an identical business model to commercialize the results of this project. n/a",AI Software for Science Education Related to Drug Abuse,6831228,R43DA018455,"['adolescence (12-20)', 'artificial intelligence', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'drug abuse education', 'educational resource design /development', 'human subject', 'science education']",NIDA,"QUANTUM SIMULATIONS, INC.",R43,2004,61750,-0.05213556590531239
"Perception and Inter-Observer Variability in Mammography DESCRIPTION (provided by applicant):    Inter-observer variability in mammogram reading has been well documented in the literature. Various factors have been used to explain this variability; among them, the most significant are related to the management of perceived findings.  However, the nature of this inter-observer variability has not been explored. Namely, were the lesions that were consistently reported by the radiologists any different from the ones that yield disagreement? Furthermore, could these differences be quantitatively assessed? Moreover, were these differences in any way related with the experience level of the observer? In addition, the interpretation of perceived findings is closely related with the visual search strategy used to scan the breast tissue, because observers compare perceived findings with the background, in order to determine their uniqueness. Hence, what is the effect of visual search strategy on inter-observer variability? Can this effect be modeled using Artificial Neural Networks (ANNs)? Can inferences be made regarding the observers' decision patterns by analyzing the results of simulations run on the ANNs?  The work described here aims at answering these questions. We will use spatial frequency analysis to characterize the areas on mammogram cases where mammographers, chest radiologists with experience reading mammograms and radiology residents at the end of their mammography rotation, indicate the presence of a finding, or fail to do so. We will assess inter-observer agreement, as well as intra- and inter-group agreement for the various groups of observers. In addition, we will train artificial neural networks to represent each observer, in such a way that by changing the nature of the features input to the ANNs we will be able to simulate how such changes would have affected the actual observer. We will assess the effects on inter-observer variability of changing the search strategy used by the observer to sample the breast tissue. In our setting, the inter-observer variability will be assessed by comparing the outputs of the ANNs that represent each observer. In addition, the changes in sampling strategy will correspond to actual possible strategies for the human observers themselves. n/a",Perception and Inter-Observer Variability in Mammography,6821032,R21CA100107,"['artificial intelligence', 'bioimaging /biomedical imaging', 'breast neoplasm /cancer diagnosis', 'clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'human data', 'human therapy evaluation', 'mammography', 'visual perception']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2004,153968,-0.03825191587720622
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,-0.011710152268307192
"Collaborative Brain Mapping: Tools for Sharing    DESCRIPTION (provided by applicant): Sharing data between research centers is increasingly important for contemporary brain imaging studies because they involve large numbers of subjects and complex analysis protocols that require highly specialized expertise. Our long-term objective is to facilitate brain-imaging research by enabling remote researchers to pool data between institutions and to analyze data using the appropriate algorithms executing on distributed resources. There are a number of difficult data management and technology challenges that have limited the success of data sharing environments. Rather than attempt to develop a comprehensive and general solution, we propose to develop a set of open, interoperable, and portable software tools that address critical issues currently limiting efforts to share and analyze brain-imaging data. Building upon years of providing brain-mapping expertise to collaborators, we propose to solve problems that we repeatedly encounter and that currently limit progress in brain imaging research. We propose to develop validated tools that enable collaborators to remotely access a variety of data analysis methods and databases. We will create web-based tools to perform multi-institutional studies, and provide access to complex data processing protocols executing on distributed computing resources. There are three specific aims. 1) Enable the web based acquisition and management of data utilizing an access control system that includes consideration of subject consent limits and investigator imposed conditions to facilitate data pooling for multi-institutional studies. This system will convert data files between different formats and schemas so that data can be used consistently between analysis programs and databases. It will also anonymize images and metadata according to institution-specific protocols. 2) Develop a system that is aware of data type and provenance so that it may act intelligently to arbitrate between different analysis programs. This system will capture the expertise of experienced lab personnel in the usage of various tools and assist new users in designing appropriate analytic strategies. 3) Create meta-algorithms that improve the robustness of techniques for neuroimaging analysis by intelligently combining the results from multiple algorithms. The proposed approach will provide a set of tools that address significant problems in data sharing and utilization. The resulting information technology will be scalable and applicable to other scientific data sharing problems.         n/a",Collaborative Brain Mapping: Tools for Sharing,6802141,R01MH071940,"['Internet', 'artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain mapping', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'confidentiality', 'data management', 'information dissemination', 'information systems', 'mathematics']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,668796,0.003996034025620501
"Community Deposit and Review of Biochemical Databases DESCRIPTION (provided by applicant):    Understanding how the phenotype of an organism is produced by its genotype and environment is fundamental to modern biomedicine. Cellular biochemistry forms the best-characterized complex biochemical system available, and provides a unique opportunity to better understand the structure-function relationships that produce phenotypes. Understood mathematically, cells are a biochemical network whose topology, function, and possible behaviors are only still only dimly understood.      Our previous work has resulted in the development of several databases (END, BND, Klotho) and software systems (The Agora, Glossa) to provide, accumulate, and use data on biochemical networks by users worldwide. In this application for competitive renewal, we propose to embark on a systematic study of structure-function relationships in biochemical networks, focusing on the discovery of patterns of biochemical function --- functional motives. We will identify these motives and their distribution over all enzymatic reactions by comparing changes in reactants' structures and enzymes' specificities, rather than just examine keywords. This systematic examination will allow us to determine, at much greater resolution than ever before, what functions each molecule and reaction have. To do this we must significantly extend the functionalities of our current systems in three major ways. First, we will add significant new data on the structure of small molecules of biochemical interest, enzymatic reactions, the mechanisms of gene expression, and dementia. Second, we will further develop and test methods that produce semantic interoperability among independent, disparate databases. Third, we will develop algorithms to detect and catalogue patterns of biochemical function among thousands of reactions and molecules; to more speedily enumerate paths among molecules and subnets in the biochemical network; to determine the extent of convergent evolution among enzymes; to trace atoms through a sequence of reactions such as a metabolic pathway; and to suggest novel biochemical reactions. We will use these capabilities to define and search for functional motives among enzymatic reactions, testing their correlation with network topology and dynamics, and to estimate the extent to which functional similarities arise from convergent evolution of enzymes. n/a",Community Deposit and Review of Biochemical Databases,6784554,R01GM056529,"['artificial intelligence', 'biochemistry', 'computer program /software', 'computer system design /evaluation', 'dementia', 'enzyme mechanism', 'functional /structural genomics', 'information system analysis', 'mathematical model', 'molecular biology information system', 'molecular dynamics', 'physiology', 'protein structure function']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2004,522252,-0.015290931875031005
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,-0.011710152268307192
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6788945,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2004,375000,-0.0236053418355095
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,337653,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,750042,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,164000,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6736326,U01GM061374,"['artificial intelligence', 'biomedical resource', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'computer system hardware', 'cooperative study', 'drug interactions', 'drug metabolism', 'gene expression', 'genetic polymorphism', 'informatics', 'information dissemination', 'interactive multimedia', 'molecular biology information system', 'online computer', 'pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2004,900407,0.0012861963955300068
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6810083,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2004,178840,-0.020465559855141147
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6733529,R01LM007273,"['Internet', 'behavioral /social science research tag', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'confidentiality', 'data management', 'decision making', 'health care facility information system', 'health care policy', 'human data', 'human rights', 'information dissemination', 'information retrieval', 'mathematical model', 'medical records', 'model design /development', 'patient oriented research', 'statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2004,406979,-0.0029438762910813194
"Using Narrative Data to Enrich the Online Medical Record  DESCRIPTION (provided by applicant):    Narrative information is vital to health care, because it enables physicians to synthesize the raw facts and provide a context and interpretation for them.  Electronic medical record systems contain a wealth of clinical data, but typically lack the clinical narrative found in paper records, e.g., the patient history and progress notes.  Numerous barriers prevent the timely acquisition of narrative data, and most computer systems are unable to use such information productively.  Current approaches offer a tradeoff, capture of rich clinical data that lacks structure (using transcription services or speech technology), versus entry of structured data that lacks flexibility and expressiveness (using template systems).  Natural language processing can integrate these approaches by allowing physicians full freedom of expression while producing structured documents that preserve the richness and enable further computer processing.   This proposal seeks to capture and structure narrative in the online medical record in order to improve entry time, completeness, information content and consistency of clinical documentation.  The specific aims of this proposal are:  1) Maintain the continuity of the medical record; a lengthy medical record requires significant time to review and digest.  Many facts from past narratives remain true in the present or persist with minor changes.  By automatically bringing these facts forward into the current narrative, the system can reduce the time to enter the document, and improve the completeness of documentation by maintaining continuity of what is known about a patient; 2) Integrate the medical record:  Electronic medical records contain a vast amount of data.  However, most of these data are raw facts.  By helping the physician to connect, interpret and summarize these facts, the system can improve the usefulness of the information in the record, and reduce the time to enter documents by performing some syntheses automatically; and 3) Harmonize the medical record; the multidisciplinary nature of health care creates the potential for the differing perspectives and interpretations in the medical record, and even contradictions.  By bringing possible discrepancies to the attention of the physician, the system can help resolve the inconsistencies.     n/a",Using Narrative Data to Enrich the Online Medical Record,6802698,R01LM007268,"['automated medical record system', 'clinical research', 'computer data analysis', 'data collection methodology /evaluation', 'human data', 'information systems', 'medical records', 'online computer', 'primary care physician', 'vocabulary']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,437194,-0.03718850613939348
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,6749459,R01EB000362,"['automated medical record system', 'clinical research', 'computer assisted medical decision making', 'data collection methodology /evaluation', 'human data', 'informatics', 'information display', 'information system analysis', 'mathematical model', 'medical records', 'outcomes research', 'patient care management', 'performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,594553,-0.04910053696222924
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6795323,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2004,754129,-0.023969591249837104
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6699330,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2004,1405759,-0.025116160918481754
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6777517,R43EB000608,"['archives', 'clinical research', 'computer data analysis', 'computer system design /evaluation', 'confidentiality', 'data management', 'health care policy', 'health related legal', 'human data', 'imaging /visualization /scanning', 'information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2004,149200,-0.0036210505856802935
"Annotating functional sites in 3D biological structures DESCRIPTION:    High-throughput data collection methods have revolutionized many areas of biology and medicine. The National Library of Medicine has targeted the representation, management, and manipulation of biological structure as a key element of its mission. Following upon the success of genome sequencing and functional genomics projects, the structural biology community is creating technologies to streamline the process of determining three-dimensional biological structures--with efforts in structural genomics. Like other high-throughput efforts, a major challenge for these efforts is the appropriate annotation and indexing of structures for retrieval and analysis by biologists who are trying to understand molecular function at an atomic detail: Where are the important functional sites, and how confident are we in their location?      In this proposal, we plan to develop and apply methods for annotating biological structures, so that active sites, binding sites and interaction sites in biological structures can be automatically identified and annotated. Our novel computational representation of functional sites has been successful in characterizing these sites, and recognizing them based on their biochemical and biophysical signature--a 3D motif. We propose to improve the performance of our method with basic research in the representations and algorithms used for our site models. Because our site models are manually created, our library of available models has grown slowly. We therefore further propose to accelerate the growth of our model library using a combination of supervised and unsupervised machine learning methods. First, we will use known 1D sequence motifs as ""seeds"" to create corresponding 3D motifs. Second, we will develop techniques for discovering entirely new motifs using cluster techniques. We will evaluate our models and resulting predictions through analysis of known structural sites, follow-up and dissemination of predictions with the structural genomics community, and large-scale evaluation on decoy and predicted structures. We will make the resulting models available on the Web for real-time structural annotation, and will distribute the software for open source development. n/a",Annotating functional sites in 3D biological structures,6825272,R01LM005652,"['active sites', 'binding sites', 'computer simulation', 'macromolecule', 'model design /development', 'physical model', 'structural biology']",NLM,STANFORD UNIVERSITY,R01,2004,396367,-0.044431013628765205
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6717704,P01EB000216,"['automated medical record system', 'health care facility information system', 'radiology', 'telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2004,1723576,-0.037445039011905996
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6682996,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' automated data processing', ' chemical structure', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' crystallization', ' data collection methodology /evaluation', ' image processing', ' mathematics', ' method development', ' protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,298672,-0.027819689432835328
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6637557,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2003,148818,0.005636612795160531
"Discovering and Applying Knowledge in Clinical Databases DESCRIPTION (provided by applicant):     With the advent of improved clinical information system products (e.g., ambulatory systems, order entry  systems), improved data entry technologies (e.g., speech recognition, text processing techniques), and  further adoption of data interchange standards, more institutions are generating electronic medical records, and these records will expand in breadth, depth, and degree of coding in the future. The records are used mainly for individual patient care, but exploiting the records for clinical research and quality functions has lagged behind. Major challenges include the wide range of complex data and missing and inaccurate data.      We propose to continue our work to develop and test methods to mine a clinical data repository. A  special emphasis will be to exploit the vast amount of information in the repository (latent associations and knowledge) and to use computer intensive techniques and advances in data representation and manipulation to better interpret what is in the database and to overcome the challenges of complex, missing, and inaccurate data. We hypothesize that data mining techniques can be applied to a repository to generate accurate clinical interpretations. We further hypothesize that associations latent in a clinically rich repository can be used to improve the classification of cases in that repository.      We aim to develop methods to prepare data for mining; to characterize the information in the clinical data repository; to develop similarity measures based on manipulation of natural language processor output and on information retrieval techniques; to apply nearest neighbor technique and case-based reasoning to improve classification; to develop a statistically based method to improve classification of cases with incomplete or inaccurate data; and to apply our methods to real clinical research questions and carry out additional data mining research.      The researchers in the Department of Medical Informatics at Columbia University are uniquely positioned to carry out this research, given the experience of the team (data mining, statistics, health data organization, health knowledge representation, natural language processing), the availability of a repository of 13 years of data on 2 million patients, and the availability of a natural language processor called MedLEE to convert millions of narrative reports into richly coded clinical data. n/a",Discovering and Applying Knowledge in Clinical Databases,6630735,R01LM006910,"['artificial intelligence', ' classification', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information system analysis', ' method development', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,377617,-0.03534311046823037
"Ab-Initio Geometry Optimization of Large Molecules    DESCRIPTION (provided by applicant):  While density-functional calculations of the energy are now feasible for biomolecules, the use of density-functional geometry optimizers is still confined to relatively small molecules containing no more than thirty atoms. The key limitation of conventional density-functional geometry optimizers is that the cost of the geometry optimization scales at least quadratically with the number of atoms in the molecule. In contrast the energy at a fixed geometry can be evaluated for a cost which scales linearly with molecule size, enabling very large molecules to be treated. This proposal is based on a radical change in the algorithm for density-functional geometry optimization, potentially reducing the total cost from quadratic to linear in molecule size and enabling a quantum leap in the size of molecules that can be optimized. The proposed algorithm resembles a conventional self-consistent calculation of the energy at a fixed geometry but at convergence the proposed algorithm yields not only the density but also the optimized geometry. This is achieved by simultaneous optimization of the wavefunction and the geometry via a modified self-consistent-field procedure. The proposed algorithm will be implemented in the QChem software package and, if successful, widely distributed through QChem Inc. and Spartan Inc.           n/a",Ab-Initio Geometry Optimization of Large Molecules,6583907,R43GM067335,"['artificial intelligence', ' chemical models', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' molecular dynamics', ' molecular size', ' quantum chemistry']",NIGMS,"Q-CHEM, INC.",R43,2003,99639,-0.020377756368102976
"BioMediator: Biologic Data Integration & Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration & Analysis System,6681249,R01HG002288,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' information retrieval', ' molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2003,100000,-0.003079402504554836
"BioMediator: Biologic Data Integration & Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration & Analysis System,6681249,R01HG002288,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' information retrieval', ' molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2003,120133,-0.003079402504554836
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6587476,R44CA093112,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' human data', ' mathematical model', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R44,2003,400084,-0.011472104262433322
"Software to Handle Missing Values in Large Data DESCRIPTION (provided by applicant):    This SBIR aims to produce commercial software for handling missing data in large data sets, where the goal is data mining and knowledge discovery. There may be a large number of subjects, variables, or both. Examples include microarray data, surveys, genomic data, and high throughput screening data.      Handling missing data is one important step of careful data preparation, which is key to the success of an entire project. Missing values often arise in medical data. This is an obstacle because many data mining tools either require complete data or are not robust to missing data.      Principled methods of handling missing data are computationally intensive. Therefore computational feasibility is a challenge to handling missing values in large data sets.      Phase I work will explore strategies such as sampling, constraining parameters, and monotone data algorithms for model based techniques. Factor analysis and multivariate linear mixed effects models will be used to reduce the number of parameters. A variable-by-variable approach using a popular data mining technique, recursive partitioning, will also be used to impute missing values.      For each of the methods, we will write prototype software and test performance on missing data patterns simulated on real data. Several ad hoc techniques will serve as a baseline for comparison.   Experience writing prototypes and using them in simulations will lead to preliminary software design that will serve as the foundation of Phase II work.       This proposed software will enable medical researchers to gain more from their data mining efforts: maximally extracting information and achieving unbiased predictions, despite missing data. n/a",Software to Handle Missing Values in Large Data,6690119,R43RR017862,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' human data', ' mathematical model', ' statistics /biometry']",NCRR,INSIGHTFUL CORPORATION,R43,2003,99847,-0.006097910594736953
"Community Deposit and Review of Biochemical Databases DESCRIPTION (provided by applicant):    Understanding how the phenotype of an organism is produced by its genotype and environment is fundamental to modern biomedicine. Cellular biochemistry forms the best-characterized complex biochemical system available, and provides a unique opportunity to better understand the structure-function relationships that produce phenotypes. Understood mathematically, cells are a biochemical network whose topology, function, and possible behaviors are only still only dimly understood.      Our previous work has resulted in the development of several databases (END, BND, Klotho) and software systems (The Agora, Glossa) to provide, accumulate, and use data on biochemical networks by users worldwide. In this application for competitive renewal, we propose to embark on a systematic study of structure-function relationships in biochemical networks, focusing on the discovery of patterns of biochemical function --- functional motives. We will identify these motives and their distribution over all enzymatic reactions by comparing changes in reactants' structures and enzymes' specificities, rather than just examine keywords. This systematic examination will allow us to determine, at much greater resolution than ever before, what functions each molecule and reaction have. To do this we must significantly extend the functionalities of our current systems in three major ways. First, we will add significant new data on the structure of small molecules of biochemical interest, enzymatic reactions, the mechanisms of gene expression, and dementia. Second, we will further develop and test methods that produce semantic interoperability among independent, disparate databases. Third, we will develop algorithms to detect and catalogue patterns of biochemical function among thousands of reactions and molecules; to more speedily enumerate paths among molecules and subnets in the biochemical network; to determine the extent of convergent evolution among enzymes; to trace atoms through a sequence of reactions such as a metabolic pathway; and to suggest novel biochemical reactions. We will use these capabilities to define and search for functional motives among enzymatic reactions, testing their correlation with network topology and dynamics, and to estimate the extent to which functional similarities arise from convergent evolution of enzymes. n/a",Community Deposit and Review of Biochemical Databases,6694513,R01GM056529,"['artificial intelligence', ' biochemistry', ' computer program /software', ' computer system design /evaluation', ' dementia', ' enzyme mechanism', ' functional /structural genomics', ' information system analysis', ' mathematical model', ' molecular biology information system', ' molecular dynamics', ' physiology', ' protein structure function']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2003,556670,-0.015290931875031005
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,-0.011710152268307192
"Smart Power Assistance Module for Manual Wheelchairs    DESCRIPTION (provided by applicant): We propose to use power assistance as the basis for a Smart Power Assistance Module (SPAM) that provides independent power assistance to the right and left rear wheels of a manual wheelchair. The SPAM will detect obstacles near the wheelchair, and modify the forces applied to each wheel to avoid obstacles.  For individuals with visual impairments that are unable to walk with a long cane or walker, the SPAM will provide safe travel by assisting the user to avoid obstacles. This research will build on the investigative team's previous experience with power assistance for manual wheelchairs and obstacle avoidance for power wheelchairs and rollators. Extensive outside evaluation of the SPAM will be provided throughout the course of the project by clinicians active in wheelchair seating and mobility.         n/a",Smart Power Assistance Module for Manual Wheelchairs,6667132,R43EY014490,"['artificial intelligence', ' assistive device /technology', ' biomedical device power system', ' biomedical equipment development', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' field study', ' human subject', ' medical rehabilitation related tag', ' vision aid', ' vision disorders']",NEI,AT SCIENCES,R43,2003,209800,-0.01951787284112941
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6636465,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,327818,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6636465,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,737629,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6636465,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,874182,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6738628,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2003,159000,0.0012861963955300068
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6620783,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2003,380761,-0.0029438762910813194
"Using Narrative Data to Enrich the Online Medical Record  DESCRIPTION (provided by applicant):    Narrative information is vital to health care, because it enables physicians to synthesize the raw facts and provide a context and interpretation for them.  Electronic medical record systems contain a wealth of clinical data, but typically lack the clinical narrative found in paper records, e.g., the patient history and progress notes.  Numerous barriers prevent the timely acquisition of narrative data, and most computer systems are unable to use such information productively.  Current approaches offer a tradeoff, capture of rich clinical data that lacks structure (using transcription services or speech technology), versus entry of structured data that lacks flexibility and expressiveness (using template systems).  Natural language processing can integrate these approaches by allowing physicians full freedom of expression while producing structured documents that preserve the richness and enable further computer processing.   This proposal seeks to capture and structure narrative in the online medical record in order to improve entry time, completeness, information content and consistency of clinical documentation.  The specific aims of this proposal are:  1) Maintain the continuity of the medical record; a lengthy medical record requires significant time to review and digest.  Many facts from past narratives remain true in the present or persist with minor changes.  By automatically bringing these facts forward into the current narrative, the system can reduce the time to enter the document, and improve the completeness of documentation by maintaining continuity of what is known about a patient; 2) Integrate the medical record:  Electronic medical records contain a vast amount of data.  However, most of these data are raw facts.  By helping the physician to connect, interpret and summarize these facts, the system can improve the usefulness of the information in the record, and reduce the time to enter documents by performing some syntheses automatically; and 3) Harmonize the medical record; the multidisciplinary nature of health care creates the potential for the differing perspectives and interpretations in the medical record, and even contradictions.  By bringing possible discrepancies to the attention of the physician, the system can help resolve the inconsistencies.     n/a",Using Narrative Data to Enrich the Online Medical Record,6665504,R01LM007268,"['automated medical record system', ' clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' information systems', ' medical records', ' online computer', ' primary care physician', ' vocabulary']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,424735,-0.03718850613939348
"Engineering Approach to Individually Tailored Medicine DESCRIPTION (provided by applicant):    Technological advances in medicine, particularly imaging, have resulted in early detection, objective documentation, and overall better insight into medical conditions. These advances, however, have also led to an increasingly complex medical record. Physicians now spend a significant portion of their time retrieving, structuring, organizing, and analyzing patient data, inaccurately and inefficiently: current information management systems in clinical medicine do not adequately support these functions, critical to the real-world practice of evidence-based medicine. Objective evidence, tailored to an individual patient, must be readily available to physicians as part of routine practice if true evidence-based medical practice is to become a reality. This proposal details the development and evaluation of several innovative technologies, providing solutions for the information management problems faced by physicians: 1) a distributed XML-based peer-to-peer medical record architecture, to enable portability and accessibility of patient information, regardless of geographical location; 2) a natural language processing (NLP) system for free-text medical reports, to automatically structure and characterize the contents of medical documents; 3) a phenomenon-centric data model, which supports the problem-solving tasks of the physician through explicit linking of objective findings (e.g., images, lab values) to medical problems; and 4) a time-based, problem-centric, context-sensitive visualization of the medical record, supporting a ""gestalt"" view of the patient, with access to detailed patient data when needed. Together, these technologies will form a comprehensive system facilitating evidence-based medicine in a real-world environment. System evaluation will proceed in two parts. Technical evaluation focuses on each of the proposed technologies individually, gauging classical performance metrics: scalability of the distributed medical record; NLP precision/recall; expressibility/comprehensibility of the data model; and the usability of the new medical record user interface. Clinical evaluation will follow a time series study design (""off-on-off""), with implementation of the entire system in a real-world clinical environment, the UCLA Clark Urological Center. Clinical evaluation will measure the effectiveness of the system as a whole on intermediate outcomes (process of care) including the number of visits, number of procedures performed, and time to final diagnosis (disposition), as well as the impact on physician efficiency (time required to gather information and review charts). n/a",Engineering Approach to Individually Tailored Medicine,6678913,R01EB000362,"['automated medical record system', ' clinical research', ' computer assisted medical decision making', ' data collection methodology /evaluation', ' human data', ' informatics', ' information display', ' information system analysis', ' mathematical model', ' medical records', ' outcomes research', ' patient care management', ' performance']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,572597,-0.04910053696222924
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6626535,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2003,1357166,-0.025116160918481754
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6647589,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2003,729100,-0.023969591249837104
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6694270,R43EB000608,"['archives', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data management', ' health care policy', ' health related legal', ' human data', ' imaging /visualization /scanning', ' information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2003,250800,-0.0036210505856802935
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6682850,P01EB000216,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2003,1645392,-0.037445039011905996
"DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES A real-time clinical repository contains a wealth of detailed information useful for clinical care, research, and administration.  In their raw form, however, the data are difficult to use there is too much volume, too much detail, missing values, and inaccuracies. Clinicians, researchers, and administrators require higher level interpretations that address their questions. For example, a clinician may need to know whether a patient is at sufficient risk for having active tuberculosis to warrant respiratory isolation. The answer to the question may be spread around the clinical repository in chest radiographs, laboratory tests, medication histories, vital signs, and physician's notes. Translating from these raw data to the interpretation (at risk or not) is a difficult and laborious task. The hypothesis of this proposal is that data mining techniques can be applied to a real-time clinical repository to discover knowledge and generate accurate clinical interpretations, and that these interpretations can be automated. The project differs from earlier machine learning studies in its emphasis on a real clinical repository and the use of natural language processing to supply coded clinical data. The specific aims are: (l) Select clinical domains--Several clinical domains with interesting, non-trivial clinical problems will be selected. Problems for which a gold standard answer can or has been assembled for a retrospective cohort will be chosen. (2) Prepare raw clinical data for mining--The raw data from a clinical repository will be transformed into a structure that facilitates data mining. The data will be flattened, pivoted, summarized, and mapped as needed for the domains. Narrative data will be coded using the MedLEE natural language processor. The preparation process will be automated. (3) Use data mining algorithms to discover knowledge- Several data mining algorithms will be applied to the selected clinical domains. Algorithms will include decision tree generation, rule discovery, neural networks, nearest neighbor, logistic regression, and composite algorithms (for variable reduction). The algorithms will be trained on a training set for each domain, and their predictive accuracy will be measured and compared to each other and to expert-written rules. The performance of human experts writing rules using manual data mining visualization techniques (which does not require an explicit training set) will also be measured. (4) Study the dependence of data mining on the training set--The performance of data mining algorithms depends on the data used the train them. The sensitivity of the algorithms to noise (inaccurate data), missing data, and training set size will be measured. (5) Use the discovered knowledge to generate real-time interpretations-- The output of the algorithms (decision tree, rules, neural network equation, or logistic regression equation, but not nearest neighbor) along with the necessary data preparation steps will be encoded in Arden Syntax Medical Logic Modules. They will be run against the clinical repository to verify that the interpretation can be automated in real time. (6) Disseminate the methods and results--The methods and results will be disseminated via publications and a Web site, and tools will be made available.  n/a",DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES,6538211,R01LM006910,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information dissemination', ' information system analysis', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2002,403006,-0.010353924837840783
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6530779,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2002,144484,0.005636612795160531
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6526728,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2002,372289,-0.009308378641367486
"Cluster Comparison Methods & the NCI Expression Dataset There is a significant commercial and academic need for new tools that provide quantitative cluster comparison metrics. It is important for pharmaceutical and biotechnology companies to be able to critically evaluate the utility of using different clustering techniques on large high dimensional datasets, in order to make the most informed decisions based upon the clustering results. We propose to evaluate and build bluster comparison metrics, integrating them with high dimensional visualization techniques, so that not only an overall scope, but the cluster distributions can be compared in an intuitive visual fashion. In carrying out our analysis, we will focus on the NCI (approximately 1,400) compound, subset, 118 known mechanism of action compound gene expression dataset analyzed by Scherf, et.al (2000). IN A FOLLOW ON Phase II SBIR Proposal, we will create a robust software package for commercial release where cluster comparison metrics are integrated with the most valuable visualization tools we identify in the Phase I research. PROPOSED COMMERCIAL APPLICATIONS: The Specific Aims of this Phase I proposal will allow us to create new tools where cluster comparison metrics are integrated with high dimensional visualization techniques, so that not only an overall score, but the cluster distributions can be compared in an intuitive visual fashion. We will use the publicly available NCI DIS compound subset, gene expression dataset of Scherf, e.g. al. (2000) to carry out these aims, as ell as data mine this dataset for new discoveries. n/a",Cluster Comparison Methods & the NCI Expression Dataset,6484325,R43CA096179,"['artificial intelligence', ' cancer information system', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' informatics', ' information retrieval', ' mathematics']",NCI,"ANVIL INFORMATICS, INC.",R43,2002,98438,-0.024674633337595012
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6538226,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2002,66954,-0.007549113122618147
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6490198,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2002,331492,-0.024593166278739947
"Diagnostic Logic and Adaptive Assessment for Psychiatry  DESCRIPTION (provided by applicant):  This SBIR Phase I application proposes to extend an existing multi-tier Internet client-server system (""CliniMetricar"") by integrating active diagnostic logic.  CliniMetrica currently provides systematic psychiatric assessment using the World Health Organization (WHO) Schedules for Clinical Assessment in Neuropsychiatry (SCAN, version 2.1). CliniMetrica automatically indexes digital audio recording of assessment interviews, and was developed with SBIR support from NIMH Digital video functions are currently being implemented and tested with additional NIMH support, as are functions to support remote psychometrics for the training and monitoring of interviewers.  The CliniMetrica system is increasing in sophistication and functionality, and has applications for clinical research (e.g., clinical trials) as well as for more routine clinical practice.  The current lack of fully developed and tested diagnostic functionality is a major gap.  A number of potential customers have requested integrated DSM-IV and/or lCD-10 diagnostic results to be automatically linked to assessments.  In order to meet this market need, we propose to implement the DSM-IV and ICD-10 nosologic systems as two classification knowledge bases that a logic engine will process to generate diagnostic results.  These logical functions are difficult to implement and manipulate with procedural languages (e. g. C + +). so the use of a logic engine provides significant technical benefits.  This diagnostic logic version of CliniMetrica is referred to as ""CliniMetrica-Dx.""  To assist raters in thorough examination and to support adaptive assessment, a user interface coupled to logic processing modules will allow tracking the diagnostic status of a subject (the sets of true, false, partially true, or partially false DSM-IV and lCD-10 diagnoses during assessment.  The logic engine will dynamically generate assessment item subsets (currently from the SCAN) needed to rule-in or rule-out DSM-IV and lCD-10 diagnoses. During the assessment, presentation of these symptom sets to the assessor can further guide the interview. By narrowing the ""search space,"" the efficiency of assessments will be increased. By formalizing the search, reliability and validity can be enhanced In addition, to provide support for nosologic research and development, in Phase II the system will include mappings between DSM-IV and lCD-10, and between current and future versions of the DSM and lCD.  The CliniMetrica-Dx system as an application framework will generalize to other clinical psychiatric assessment instruments such as the SCID, to adaptive self-report systems, and to other medical specialties.  It also will have applications for education and training.   n/a",Diagnostic Logic and Adaptive Assessment for Psychiatry,6549963,R43MH066434,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted diagnosis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' diagnosis design /evaluation', ' disease /disorder classification', ' human subject', ' indexing', ' mental disorder diagnosis', ' psychometrics']",NIMH,"MEDICAL DECISION LOGIC, INC.",R43,2002,94169,0.0001295370903699103
"Smart Power Assistance Module for Manual Wheelchairs    DESCRIPTION (provided by applicant): We propose to use power assistance as the basis for a Smart Power Assistance Module (SPAM) that provides independent power assistance to the right and left rear wheels of a manual wheelchair. The SPAM will detect obstacles near the wheelchair, and modify the forces applied to each wheel to avoid obstacles.  For individuals with visual impairments that are unable to walk with a long cane or walker, the SPAM will provide safe travel by assisting the user to avoid obstacles. This research will build on the investigative team's previous experience with power assistance for manual wheelchairs and obstacle avoidance for power wheelchairs and rollators. Extensive outside evaluation of the SPAM will be provided throughout the course of the project by clinicians active in wheelchair seating and mobility.         n/a",Smart Power Assistance Module for Manual Wheelchairs,6581049,R43EY014490,"['artificial intelligence', ' assistive device /technology', ' biomedical device power system', ' biomedical equipment development', ' clinical research', ' computer program /software', ' computer system design /evaluation', ' field study', ' human subject', ' medical rehabilitation related tag', ' vision aid', ' vision disorders']",NEI,AT SCIENCES,R43,2002,249727,-0.01951787284112941
"Digital Elevation Models for Population Estimates Research exploring the feasibility of deriving population estimates from remotely sensed data demonstrates that objects in the urban landscape can be identified and incorporated into a population estimates system based on the housing unit method. Nonetheless, this research also reveals shortcomings in the technology producing the input files used in the automatic detection of objects. The problem involves the assumption and techniques used when converting high resolution images into digital elevation models (DEM). DEM files serve as input to the programs used in the detection of housing units. Efforts to correctly identify housing units are time-consuming and error-prone without clear and distinct DEMs. The objectives of this Phase I SBIR application are to further refine strengthen and test the software employed in transforming satellite and aerial imagery into digital elevation model (DEMs). DEM files serve as input to the programs used in the detection of housing units. Efforts to correctly identify housing units are time-consuming and error-prone without clear and distinct DEMs. The objectives of this Phase I SBIR application are to further refine, strengthen and test the software employed in transforming satellite and aerial imagery into digital elevation models. Specific goals of this Phase I proposal include: 1) modifying and coding new assumptions into the DEM software, 2) testing the accuracy and reliability of the digital elevation code on new sub1, aerial imagery, and 3) designing a new GUI for use in the pre- processing phase of DEM building. PROPOSED COMMERCIAL APPLICATIONS: The commercial value of this specific research is best understood when viewed as part of a larger effort to produce an automated system for deriving population estimates of user defined areas based on current, remotely sensed data. Such a system will serve a wide range of commercial interests seeking ""up-to-the-minute"" counts and measures of population and housing change. n/a",Digital Elevation Models for Population Estimates,6443114,R43HD041774,"['altitude', ' artificial intelligence', ' bioimaging /biomedical imaging', ' computer data analysis', ' computer graphics /printing', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' environment', ' geographic site', ' human population distribution', ' mathematical model', ' population survey', ' urban area']",NICHD,"SENECIO SOFTWARE, INC.",R43,2002,99979,-0.008991375916031007
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6520265,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,318270,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6520265,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,724904,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6520265,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,848720,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6649647,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2002,232118,0.0012861963955300068
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,1,-0.015064029507833447
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,183410,-0.015064029507833447
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,8329,-0.015064029507833447
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,58260,-0.015064029507833447
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6421732,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2002,384388,-0.0029438762910813194
"Using Narrative Data to Enrich the Online Medical Record  DESCRIPTION (provided by applicant):    Narrative information is vital to health care, because it enables physicians to synthesize the raw facts and provide a context and interpretation for them.  Electronic medical record systems contain a wealth of clinical data, but typically lack the clinical narrative found in paper records, e.g., the patient history and progress notes.  Numerous barriers prevent the timely acquisition of narrative data, and most computer systems are unable to use such information productively.  Current approaches offer a tradeoff, capture of rich clinical data that lacks structure (using transcription services or speech technology), versus entry of structured data that lacks flexibility and expressiveness (using template systems).  Natural language processing can integrate these approaches by allowing physicians full freedom of expression while producing structured documents that preserve the richness and enable further computer processing.   This proposal seeks to capture and structure narrative in the online medical record in order to improve entry time, completeness, information content and consistency of clinical documentation.  The specific aims of this proposal are:  1) Maintain the continuity of the medical record; a lengthy medical record requires significant time to review and digest.  Many facts from past narratives remain true in the present or persist with minor changes.  By automatically bringing these facts forward into the current narrative, the system can reduce the time to enter the document, and improve the completeness of documentation by maintaining continuity of what is known about a patient; 2) Integrate the medical record:  Electronic medical records contain a vast amount of data.  However, most of these data are raw facts.  By helping the physician to connect, interpret and summarize these facts, the system can improve the usefulness of the information in the record, and reduce the time to enter documents by performing some syntheses automatically; and 3) Harmonize the medical record; the multidisciplinary nature of health care creates the potential for the differing perspectives and interpretations in the medical record, and even contradictions.  By bringing possible discrepancies to the attention of the physician, the system can help resolve the inconsistencies.     n/a",Using Narrative Data to Enrich the Online Medical Record,6535939,R01LM007268,"['automated medical record system', ' clinical research', ' computer data analysis', ' data collection methodology /evaluation', ' human data', ' information systems', ' medical records', ' online computer', ' primary care physician', ' vocabulary']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2002,395391,-0.03718850613939348
"Dynamic Language Modeling for Transcription Systems The high cost of data entry is a critical issue that has challenged the evolution of computerized patient record systems. Development of dynamic language models is proposed for significantly improving the cost performance of medical transcription systems. The innovative use of speech recognition, computer telephony integration, and the Internet is proposed for the management and transcription of physician dictation. Results of Phase I research demonstrate significant cost savings to healthcare organizations. Our goal in Phase II is to apply multiple dynamic language models to both improve accuracy and the robustness of the system. We propose to create a physician specific mapping of historical transcriptions to their spoken counterparts. We then propose to explore different methodologies for building language models for specific physician work-type combinations using a database of processed historical transcriptions based on dictations from over 1,500 physicians. In addition, the output of the recognition system will be processed by a natural language processing engine to transform it into a formatted, styled draft transcription for review and editing by a transcriptionist. Our unique approach integrates seamlessly into a physician's workflow and does not require the alteration of physician work patterns. We expect this research and development will result in a commercially viable transcription system that significantly reduces costs associated with medical transcription. eScription has obtained three paying pilot customers with whom we are working closely with to develop this system. These customers have/will provide eScription with textual data, audio data, and medical transcriptionists who will test the final system. All have expressed a keen interest in becoming corporate partners for Phase III. Two are currently using our prototype system in their production environments today. We are submitting this grant request to partially cover the cost of constructing and testing the system. PROPOSED COMMERCIAL APPLICATIONS: eScription focuses on alleviating significant healthcare cost pressures associated with transcription of medical dictation. We apply new technologies such as speech recognition, computer telephony and Internet communications, which are not commonly used for medical transcription. We will directly sell our software products and services to Integrated Delivery Networks (IDNs) and to Transcription Services Companies.  n/a",Dynamic Language Modeling for Transcription Systems,6528412,R44LM006930,"['computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' language', ' medical records', ' speech recognition', ' telemedicine', ' vocabulary development for information system']",NLM,"ESCRIPTION, INC.",R44,2002,325555,-0.037296757884209
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6533705,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2002,688297,-0.023969591249837104
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6489015,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2002,1329187,-0.025116160918481754
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6512665,P01EB000216,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2002,2120168,-0.037445039011905996
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6525584,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2002,128860,-0.010062594005838093
"DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES A real-time clinical repository contains a wealth of detailed information useful for clinical care, research, and administration.  In their raw form, however, the data are difficult to use there is too much volume, too much detail, missing values, and inaccuracies. Clinicians, researchers, and administrators require higher level interpretations that address their questions. For example, a clinician may need to know whether a patient is at sufficient risk for having active tuberculosis to warrant respiratory isolation. The answer to the question may be spread around the clinical repository in chest radiographs, laboratory tests, medication histories, vital signs, and physician's notes. Translating from these raw data to the interpretation (at risk or not) is a difficult and laborious task. The hypothesis of this proposal is that data mining techniques can be applied to a real-time clinical repository to discover knowledge and generate accurate clinical interpretations, and that these interpretations can be automated. The project differs from earlier machine learning studies in its emphasis on a real clinical repository and the use of natural language processing to supply coded clinical data. The specific aims are: (l) Select clinical domains--Several clinical domains with interesting, non-trivial clinical problems will be selected. Problems for which a gold standard answer can or has been assembled for a retrospective cohort will be chosen. (2) Prepare raw clinical data for mining--The raw data from a clinical repository will be transformed into a structure that facilitates data mining. The data will be flattened, pivoted, summarized, and mapped as needed for the domains. Narrative data will be coded using the MedLEE natural language processor. The preparation process will be automated. (3) Use data mining algorithms to discover knowledge- Several data mining algorithms will be applied to the selected clinical domains. Algorithms will include decision tree generation, rule discovery, neural networks, nearest neighbor, logistic regression, and composite algorithms (for variable reduction). The algorithms will be trained on a training set for each domain, and their predictive accuracy will be measured and compared to each other and to expert-written rules. The performance of human experts writing rules using manual data mining visualization techniques (which does not require an explicit training set) will also be measured. (4) Study the dependence of data mining on the training set--The performance of data mining algorithms depends on the data used the train them. The sensitivity of the algorithms to noise (inaccurate data), missing data, and training set size will be measured. (5) Use the discovered knowledge to generate real-time interpretations-- The output of the algorithms (decision tree, rules, neural network equation, or logistic regression equation, but not nearest neighbor) along with the necessary data preparation steps will be encoded in Arden Syntax Medical Logic Modules. They will be run against the clinical repository to verify that the interpretation can be automated in real time. (6) Disseminate the methods and results--The methods and results will be disseminated via publications and a Web site, and tools will be made available.  n/a",DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES,6391286,R01LM006910,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information dissemination', ' information system analysis', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2001,396315,-0.010353924837840783
"UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE   DESCRIPTION (adapted from the Abstract):                                             With the explosion of medical information accessible via the Internet, there         is a growing need for development of better access to the online medical             literature databases through user-friendly systems and interface.  The               proliferation of online information and the diversity of interfaces to data          collections has led to a medical information gap between medical researchers         and the accessibility of medical literature databases.  Users who need access        to such information must visit a variety of sources, which can be both               excessively time consuming and potentially dangerous if the information is           needed for treatment decisions.  In addition, information generated by using         existing search engines is often too general or inaccurate.  Particularly            frustrating is that simple queries can result in an excessive number of              documents retrieved - too many to search through to determine which are and          which are not relevant.                                                                                                                                                   The goal of this research is to extend a bridge across the medical information       gap by creating easy-to-use interfaces to medical literature databases based         on UMLS-enhanced Semantic Parsing and Personalized Medical Agent (PMA):                                                                                                   (1)  UMLS-enhanced Semantic Parsing: Our first goal will be to combine noun          phrasing and co-occurrence analysis techniques recently developed by The             University of Arizona Artificial Intelligence Lab (AI Lab) for the NSF-funded        Illinois Digital Library Initiative (DLI) project with existing components           found in the Unified Medical Language System (UMLS) developed by NLM.                                                                                                     (2)  Personalized Medical Agent: The second goal will be to develop a dynamic,       intelligent medical agent interface to assist searchers in effortlessly              locating documents and summarizing topics in the documents.  The interface is        particularly suited for busy physicians.                                                                                                                                  n/a",UMLS ENHANCED DYNAMIC AGENTS TO MANAGE MEDICAL KNOWLEDGE,6258188,R01LM006919,"['abstracting', ' artificial intelligence', ' cancer information system', ' computer system design /evaluation', ' human data', ' information retrieval', ' information system analysis', ' literature citation', ' semantics', ' vocabulary development for information system']",NLM,UNIVERSITY OF ARIZONA,R01,2001,140274,0.005636612795160531
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6391275,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,215487,-0.022995567901191918
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6388359,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2001,362594,-0.009308378641367486
"Functional Genomics Software   DESCRIPTION (Applicant's abstract): A substantial commercial potential exists        for software tools that allow a biomedical research scientist to use genomic         data to form experimentally testable hypotheses. These will be used to exploit       genomic sequence data to understand the aetiology of disease, to improve             diagnostic tools, and to develop more effective therapies. The Master Catalog,       a commercial product developed jointly by EraGen Biosciences and the Benner          laboratory at the University of Florida, provides a convenient framework for         implementing heuristics that do this. The Master Catalog is a naturally              organized database that contains evolutionary trees, multiple sequence               alignments, and reconstructed evolutionary intermediates for all of the              proteins in the GenBank database. The Benner laboratory has developed and            anecdotally tested heuristics that date events in the molecular history,             provide evidence for and against functional recruitment within a protein             family, detect distant homologs, associate individual residues important for         functional changes with a crystal structure, find metabolic and regulatory           pathways, and correlate events in the molecular record with the history of life      on Earth. This Phase I proposal seeks to validate a set of these heuristics          more broadly to determine their suitability for database-wide application. In        Phase II, we will implement these within the Master Catalog, and launch a            commercial bioinformatics product to support functional analysis of genomic          databases.                                                                           PROPOSED COMMERCIAL APPLICATION:  In its present version, the Master Catalog is a successful commercial product within a  niche: ""best in class"" of bioinformatics databases.  Adding a validated set of heuristics  for extracting functional information from genome databases will make it the software  of choice for most functional genomics work, and be a central tool in the pharmaceutical/  biotechnology industries.  Academic versions and student versions will find markets in most  universities.                                                                                      n/a",Functional Genomics Software,6337786,R41HG002331,"['artificial intelligence', ' biochemical evolution', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' functional /structural genomics', ' informatics', ' molecular biology information system', ' nucleic acid sequence']",NHGRI,"ERAGEN BIOSCIENCES, INC.",R41,2001,96855,-0.034180196847630503
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6343026,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2001,376147,-0.024593166278739947
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6340157,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2001,68753,-0.007549113122618147
"LATENT SEMANTIC INDEXING IN SUPPORT OF DATA RETRIEVAL We propose to extend the successful work we have achieved with                   statistically based indexing and retrieval systems, by incorporating             semantic structures which accommodate the modifying attributes of clinical       conCepts. Patient data is rarely limited to a single axis of meaning or          detail, and retrieval for application in quality improvement, decision           support, or epidemiologic research, demands Consistent information               struCture. This proposal will invoke the knowledge and tool suites of the        UMLS Specialist Lexicon, the SGML markup and recognition capabilities of         the TextMachine application, extensions to our locally developed CliniCal        Query Language, and layer these enhancements upon our core techniques for        statistically based indexing and retrieval of patient data. We commit            these activities to remain compliant with emerging standards for medical         concept representation arising from the Canon efforts and the                    standardization processes at ANSI-HISPP, CEN TC251 and the CPRI                  initiatives.                                                                      n/a",LATENT SEMANTIC INDEXING IN SUPPORT OF DATA RETRIEVAL,6487570,R01LM005416,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' computer assisted medical decision making', ' human data', ' indexing', ' information retrieval', ' semantics', ' statistics /biometry', ' vocabulary development for information system']",NLM,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",R01,2001,166368,-0.011186148742545783
"Neural Network System for Detection of EEG Microsleeps   DESCRIPTION (Verbatim from the Applicant's Abstract): A software system based        on Artificial Neuro-fuzzy hybrid technology will be developed for automatic          detection of microsleep events from EEG data. The software system will be            designed for used as a model-free and rule-free classification tool that             achieves generalization power through learning from examples.   The development of the software system will require a Graphical User Interface       for data example selection, frequency-analytic preprocessing of EEG raw data,        feature extraction for microsleep characterization, design and training of           neural networks for single EEG channels, and a fuzzy system for contextual           combination of network response for multiple EEG channels to a single system         response.                                                                                                                       The training and testing of the neural networks will be based on a database of       visually scored examples of microsleep and non-microsleep events from                electrophysiological data, which will be randomly divided into training,             validation and test sets.                                                                                 The performnance of the software system will be evaluated based on the               false-positive and false-negative rate for the microsleep detection using data       examples unknown to the system. The agreement rate between the combined network      response and results from visual and conventional automatic scoring will be          used as additional evaluation parameter.        PROPOSED COMMERCIAL APPLICATION: The software system will be an attractive tool for researchers, medical and technical  personal, industrial engineers. It enables the user to quantify alertness/sleepiness  in studies on sleep disorders, shiftwork, drug effects and fatigue countermeasures.  It will help reduce time-consuming visual scoring by human experts. In addition, it  will widen our knowledge about the rapid transition events (microsleeps) between  wake and sleep and can contribute to the development of alertness monitor systems.                                                                                                                                                                                                                                                                                                      n/a",Neural Network System for Detection of EEG Microsleeps,6338195,R43NS039711,"['artificial intelligence', ' biomedical automation', ' computational neuroscience', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' electroencephalography', ' electrophysiology', ' human data', ' neural information processing', ' sleep']",NINDS,"CIRCADIAN TECHNOLOGIES, INC.",R43,2001,93457,-0.016327632788693064
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6495900,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,35096,0.0012861963955300068
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6351629,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2001,106893,-0.008229127945131547
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6387173,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,309000,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6387173,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,818937,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6387173,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2001,824000,0.0012861963955300068
"Dynamic Language Modeling for Transcription Systems The high cost of data entry is a critical issue that has challenged the evolution of computerized patient record systems. Development of dynamic language models is proposed for significantly improving the cost performance of medical transcription systems. The innovative use of speech recognition, computer telephony integration, and the Internet is proposed for the management and transcription of physician dictation. Results of Phase I research demonstrate significant cost savings to healthcare organizations. Our goal in Phase II is to apply multiple dynamic language models to both improve accuracy and the robustness of the system. We propose to create a physician specific mapping of historical transcriptions to their spoken counterparts. We then propose to explore different methodologies for building language models for specific physician work-type combinations using a database of processed historical transcriptions based on dictations from over 1,500 physicians. In addition, the output of the recognition system will be processed by a natural language processing engine to transform it into a formatted, styled draft transcription for review and editing by a transcriptionist. Our unique approach integrates seamlessly into a physician's workflow and does not require the alteration of physician work patterns. We expect this research and development will result in a commercially viable transcription system that significantly reduces costs associated with medical transcription. eScription has obtained three paying pilot customers with whom we are working closely with to develop this system. These customers have/will provide eScription with textual data, audio data, and medical transcriptionists who will test the final system. All have expressed a keen interest in becoming corporate partners for Phase III. Two are currently using our prototype system in their production environments today. We are submitting this grant request to partially cover the cost of constructing and testing the system. PROPOSED COMMERCIAL APPLICATIONS: eScription focuses on alleviating significant healthcare cost pressures associated with transcription of medical dictation. We apply new technologies such as speech recognition, computer telephony and Internet communications, which are not commonly used for medical transcription. We will directly sell our software products and services to Integrated Delivery Networks (IDNs) and to Transcription Services Companies.  n/a",Dynamic Language Modeling for Transcription Systems,6404288,R44LM006930,"['computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' language', ' medical records', ' speech recognition', ' telemedicine', ' vocabulary development for information system']",NLM,"ESCRIPTION, INC.",R44,2001,673495,-0.037296757884209
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6449653,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2001,658874,-0.023969591249837104
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6230917,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2001,1291932,-0.025116160918481754
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6375859,P01CA051198,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2001,2071652,-0.037445039011905996
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6385653,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2001,127154,-0.010062594005838093
"DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES A real-time clinical repository contains a wealth of detailed information useful for clinical care, research, and administration.  In their raw form, however, the data are difficult to use there is too much volume, too much detail, missing values, and inaccuracies. Clinicians, researchers, and administrators require higher level interpretations that address their questions. For example, a clinician may need to know whether a patient is at sufficient risk for having active tuberculosis to warrant respiratory isolation. The answer to the question may be spread around the clinical repository in chest radiographs, laboratory tests, medication histories, vital signs, and physician's notes. Translating from these raw data to the interpretation (at risk or not) is a difficult and laborious task. The hypothesis of this proposal is that data mining techniques can be applied to a real-time clinical repository to discover knowledge and generate accurate clinical interpretations, and that these interpretations can be automated. The project differs from earlier machine learning studies in its emphasis on a real clinical repository and the use of natural language processing to supply coded clinical data. The specific aims are: (l) Select clinical domains--Several clinical domains with interesting, non-trivial clinical problems will be selected. Problems for which a gold standard answer can or has been assembled for a retrospective cohort will be chosen. (2) Prepare raw clinical data for mining--The raw data from a clinical repository will be transformed into a structure that facilitates data mining. The data will be flattened, pivoted, summarized, and mapped as needed for the domains. Narrative data will be coded using the MedLEE natural language processor. The preparation process will be automated. (3) Use data mining algorithms to discover knowledge- Several data mining algorithms will be applied to the selected clinical domains. Algorithms will include decision tree generation, rule discovery, neural networks, nearest neighbor, logistic regression, and composite algorithms (for variable reduction). The algorithms will be trained on a training set for each domain, and their predictive accuracy will be measured and compared to each other and to expert-written rules. The performance of human experts writing rules using manual data mining visualization techniques (which does not require an explicit training set) will also be measured. (4) Study the dependence of data mining on the training set--The performance of data mining algorithms depends on the data used the train them. The sensitivity of the algorithms to noise (inaccurate data), missing data, and training set size will be measured. (5) Use the discovered knowledge to generate real-time interpretations-- The output of the algorithms (decision tree, rules, neural network equation, or logistic regression equation, but not nearest neighbor) along with the necessary data preparation steps will be encoded in Arden Syntax Medical Logic Modules. They will be run against the clinical repository to verify that the interpretation can be automated in real time. (6) Disseminate the methods and results--The methods and results will be disseminated via publications and a Web site, and tools will be made available.  n/a",DISCOVERING AND APPLYING KNOWLEDGE IN CLINICAL DATABASES,6031325,R01LM006910,"['Internet', ' artificial intelligence', ' clinical research', ' computer assisted medical decision making', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' information dissemination', ' information system analysis', ' vocabulary development for information system']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2000,433383,-0.010353924837840783
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6185231,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,213046,-0.022995567901191918
"ADVANCED DIAGNOSTIC LOGIC FOR PSYCHIATRY   DESCRIPTION: (Verbatim from the Applicant's Abstract): This Phase I project          proposes to assess the feasibility of an advanced diagnostic logic system            (Diagnostica) to support the clinical assessment process for diagnosis in            psychiatry. A working prototype of the diagnostic rules in the American              Psychiatric Association Diagnostic and Statistical Manual (DSM-IV) uses and          artificial intelligence engine (""XSB"") to implement the logic of DSM-IV along        with and an interactive graphical user interface to allow a user to add              information and understand conclusions reached by the system. The Phase I            programming objectives are to make Diagnostica ready for commercial use by           improving its graphical user interface, and finalizing implementation of its         logical rules. The resulting system will be a practical tool in clinical             settings, and relies on computer science innovations that have preciously            neither been explored nor applied in the domain of medical reasoning. With the       emergence of decision support systems, the need for better quality diagnostic        information is becoming increasingly apparent. This has been due, in part, to        the complexity of diagnostic processes and the emphasis on support of financial      processes. Within mental health, the DSM-IV provides both a model and a              standard for making diagnoses. A software component that provides flexible,          complete, and efficient application of this standard is of great value. The          innovation of Diagnostica relies on the sophistication of its modeling of            DSM-IV rules, and it's flexibility in applying those rules. Diagnostica will         automatically track the status of the information entered and allow users to         tie up 'loose ends' in documenting the proof of diagnoses formally. AS example,      the user may indicate that a set of diagnoses in 'believed true' without             specifying the symptoms needed to make the diagnoses formally ( a procedure          used routinely in clinical practice). the application will track whatever            'residual' data this is necessary in order to complete formal diagnoses, while       leaving the option of when, or if, to complete the process up to the user.                                                                                                Phase II objectives include: (1) extending Diagnostica to provide other              software applications needing diagnostic decision support services, and              specifically to link Diagnostica to the World Health Organization Schedules for      Clinical Assessment in Neuropsychiatry (SCAN); (2) addressing logical modeling       of time and creating an effective user interface for repeated assessment; (3)        incorporating probabilistic information about sets of symptoms based on              empirical information initially obtained in Phase I; and (4) developing and          testing ""belief revision"" functions to changes in knowledge stemming from            repeated clinical assessment.                                                        PROPOSED COMMERCIAL APPLICATION:                                                                                     Computerization of diagnostic logic for clinical use can improve the quality of      mental health services by efficient standardization of assessment and through        motivating and making more practical the creation of data bases which can be         used for clinical quality improvement and knowledge discovery.                                                                                                            n/a",ADVANCED DIAGNOSTIC LOGIC FOR PSYCHIATRY,6210194,R43MH059420,"['artificial intelligence', ' computer assisted diagnosis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' diagnosis design /evaluation', ' interactive multimedia', ' mental disorder diagnosis', ' psychiatry']",NIMH,"MEDICINE RULES, INC.",R43,2000,98441,-0.01891494019794204
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6031661,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2000,354198,-0.009308378641367486
"IMIA WG6 CONFERENCE The basic science of representing patient events, findings, interventions, and outcomes in a semantically consistent and logically reproducible way is medical concept representation.  It embodies principles of linguistics, logic, computer science, cognition, biology and clinical medicine to undertake this highly multidisciplinary activity. Much of this work is undertaken in experimental settings, which hypothesize practical extensions to existing models, and test their utility against standardized retrieval sets or clinical usability environments. The proposed conference intends to continue the tradition of the International Medical Informatics Association (IMIA), Working Group 6 on Medical Concept Representation, to provide a forum for the academic discussion of problems, issues, theories, and applications of natural language processing, knowledge representation, terminology development, and concept coordination to biomedicine and healthcare.  the proposed tracks at this time are: 1. Natural Language Processing  2. Clinical Classifications 3. Cognitive Evaluations  4. Terminology Models  5. Maintenance and Uptake Strategies.  n/a",IMIA WG6 CONFERENCE,6027283,R13LM006899,"['informatics', ' international health /scientific organization', ' meeting /conference /symposium', ' travel']",NLM,MAYO CLINIC ROCHESTER,R13,2000,20000,-0.03322113116103759
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,-0.0024266363072172033
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,-0.0024266363072172033
"KNOWLEDGE BASES FOR STRUCTURED CARDIOVASCULAR REPORTING Healthcare has lagged behind other industries in automating the storage and retrieval of information. While billing, blood testing, and other services are widely computerized, the bulk of patient clinical information is not. The central barrier to coding clinically useful data - patient historical, test, and procedural results - is the absence of effective knowledge frameworks for entry and review of clinical data. For this reason, sofiware for storage and retrieval of patient data remains suboptimal. This project focuses on methods for recording a specific subset of patient data: results of cardiovascular tests, Although only a subset of clinical knowledge, cardiovascular procedure reporting is typical of the larger problem of how knowledge is handled. Moreover, there is an unmet market demand for cardiology reportrng tools. In Phase I we will develop a methodology for creating and maintaining the knowledge bases needed for structured entry of cardiovascular data. We will apply and hone this methodology by creating two important knowledge bases: echocardiography and cardiac catheterization. Together with the methodology we use, these two developments will dovetail into a larger Phase II  project of systematically addressing knowledge-representation and structured data entry for cardiology. Beyond Phase II  we will apply our methodology to other branches of medicine. PROPOSED COMMERCIAL APPLICATION: This research will provide a means to optimize structured recording of patient test results in a computer-searchable format improving the process of procedural reporting, and facilitating implementation of an electronic medical record.  n/a",KNOWLEDGE BASES FOR STRUCTURED CARDIOVASCULAR REPORTING,6141030,R43HL062806,"['artificial intelligence', ' cardiovascular disorder diagnosis', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' echocardiography', ' heart catheterization', ' human data', ' informatics']",NHLBI,"CYBERPULSE, LLC",R43,2000,107000,-0.015981710649859534
"DEVELOPMENT OF SPATIAL SOFTWARER This SBIR project will develop software for identifying and correcting spatial patterns in data for a wide range of alcohol-related phenomenon including alcohol consumption, problematic outcomes, and treatment modalities. Identifying and correcting statistical relationships in spatially configured data sets would be invaluable to alcohol-related research, the overall health community, and even to most social scientists (and biomedical researchers). Ecological models or models with locational components that provide unbiased estimates and increased predictive performance enhance the researcher' ability to identify new patterns within alcohol-related phenomenon. While spatial analysis has been widely researched and is a proven statistical technique, commercially available software with reasonable diagnostics and commonly used regression techniques does not yet exist. This phase I project addresses this need and will pursue three objectives: (1) Research and increase the capabilities of the current software package, (2) Design interfaces easily useable (friendly) for alcohol researchers, and (3) Improve the speed and efficiency of the core code. The proposed software development will provide powerful diagnostic and corrective tool in the analysis of mapped data describing relationships between space and alcohol- related phenomenon. PROPOSED COMMERCIAL APPLICATIONS: The need for identifying and adjusting for spatial autocorrelation in alcohol- related data sets is huge (see page 21) and so there is a large market for the proposed statistical software. The proposed package is expected to provide an easy to use, speedy, and comprehensive tool relative to current packages.  n/a",DEVELOPMENT OF SPATIAL SOFTWARER,6073824,R43AA012373,"['alcoholism /alcohol abuse', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' mathematics', ' statistics /biometry']",NIAAA,S-THREE DEVELOPMENT,R43,2000,129935,-0.037429936114265136
"PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution.  n/a",PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY,6182183,R21GM059398,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer simulation', ' computer system design /evaluation', ' electron crystallography', ' electron density', ' molecular biology information system', ' physical model', ' protein structure', ' structural biology']",NIGMS,TEXAS ENGINEERING EXPERIMENT STATION,R21,2000,101500,-0.058947975994710355
"KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA Abstractions of time-stamped clinical data are useful for planning               therapy, for monitoring therapy, and for creating high-level summaries of        time-oriented clinical databases.  Temporal abstractions also support            explanations by an intelligent patient-record system and can be used for         representation of the goals and intentions of clinical guidelines and            protocols.                                                                                                                                                        We propose to reengineer and expand the scope of the RESUME system, a            prototype computer program that implements the knowledge-based temporal-         abstraction method, a conceptual and computational framework that we have        developed for abstraction of time-stamped clinical data into clinically          meaningful interval-based concepts. RESUME has been evaluated with highly        encouraging results in several clinical areas. We will address the               practical and theoretical issues of representation, acquisition,                 maintenance, and reuse of temporal-abstraction knowledge. Our specific           aims are defined by a four-step research plan:                                                                                                                    1. We will define formally the knowledge requirements for five                   computational modules (mechanisms) we employ, thus facilitating the              acquisition, maintenance, reuse, and sharing of the required knowledge.                                                                                           2. We will enhance, expand, and redesign five computational temporal-            abstraction mechanisms:                                                          (a) Automatic formation of meaningful contexts for interpretation of             clinical data.                                                                   (b) Classification of clinical data that have equivalent time stamps into        higher-level concepts.                                                           (c) Temporal inference (e.g., the join of certain interval-based clinical        abstractions into longer ones).                                                  (d) Interpolation between temporally disjoint clinical abstractions,             including a development of a probabilistic representation and semantics.         (e) Matching of predefined and runtime temporal patterns, given time-            stamped data and conclusions.                                                                                                                                     3. We will develop a tool for automated acquisition, from expert                 physicians, of temporal-abstraction knowledge, using techniques from the         PROTEGE-II project for designing knowledge-based systems.                                                                                                         4. We will validate and evaluate our methodology and its implementation.         (a) We will assess the value of the knowledge-acquisition tool in several        experiments.                                                                     (b) We will validate the performance of the computational mechanisms in          the domain of therapy of patients who have insulin-dependent diabetes by         collaboration with expert endocrinologists.                                      (c) We will evaluate the overall framework within EON, a project in which        researchers are implementing an integrated architecture for protocol-based       care.                                                                             n/a",KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA,6185217,R29LM006245,"['abstracting', ' artificial intelligence', ' computer assisted medical decision making', ' computer program /software', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' time resolved data']",NLM,STANFORD UNIVERSITY,R29,2000,121082,-0.01075553195351199
"WAVELET-BASED AUTOMATED CHROMOSOME IDENTIFICATION Commercial automated karyotyping instruments have improved to the point where the major factor limiting throughput is the time required for operator correction of chromosome classification errors. An improvement in chromosome classification accuracy would significantly increase the value of these instruments in cytogenetics labs. The goal of this project is to develop and commercialize significantly improved chromosome measurement and classification techniques for automated karyotyping. Currently the best-performing chromosome classification approach uses Weighted Density Distribution (WDD) features [11] to quantify the banding pattern of the chromosomes. These are computed as inner products between the banding profile and a set of WDD basis functions. The particular set of 1unctions originally proposed by Granum [11,38] has come into widespread use. In Phase I we showed that better function sets exist and that our new approach can find better WDD features than the best currently used. We have an innovative wavelet-based method for generating WDD functions and a chromosome classification testbed which supports large scale classification experiments. We propose to conduct a thorough, methodical search for better performing basis functions in Phase II. Phase III will incorporate the technology into PSI's PowerGene automated karyotyping instruments. PROPOSED COMMERCIAL APPLICATIONS: When the new chromosome classification technology is qualified for routine application, it will be incorporated into PSI's Powergene products, both in new systems sold and as an upgrade to existing systems.  n/a",WAVELET-BASED AUTOMATED CHROMOSOME IDENTIFICATION,6173267,R44CA076896,"['artificial intelligence', ' biomedical automation', ' biomedical equipment development', ' chromosomes', ' computer program /software', ' cytogenetics', ' density', ' genetic mapping', ' genetic techniques', ' human genetic material tag', ' human tissue', ' image processing']",NCI,"ADVANCED DIGITAL IMAGING RESEARCH, LLC",R44,2000,366807,-0.005245087603414616
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6071498,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2000,364001,-0.024593166278739947
"LATENT SEMANTIC INDEXING IN SUPPORT OF DATA RETRIEVAL We propose to extend the successful work we have achieved with                   statistically based indexing and retrieval systems, by incorporating             semantic structures which accommodate the modifying attributes of clinical       conCepts. Patient data is rarely limited to a single axis of meaning or          detail, and retrieval for application in quality improvement, decision           support, or epidemiologic research, demands Consistent information               struCture. This proposal will invoke the knowledge and tool suites of the        UMLS Specialist Lexicon, the SGML markup and recognition capabilities of         the TextMachine application, extensions to our locally developed CliniCal        Query Language, and layer these enhancements upon our core techniques for        statistically based indexing and retrieval of patient data. We commit            these activities to remain compliant with emerging standards for medical         concept representation arising from the Canon efforts and the                    standardization processes at ANSI-HISPP, CEN TC251 and the CPRI                  initiatives.                                                                      n/a",LATENT SEMANTIC INDEXING IN SUPPORT OF DATA RETRIEVAL,6338548,R01LM005416,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' computer assisted medical decision making', ' human data', ' indexing', ' information retrieval', ' semantics', ' statistics /biometry', ' vocabulary development for information system']",NLM,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",R01,2000,48148,-0.011186148742545783
"TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES DESCRIPTION (Adapted from the applicant's abstract):                                                                                                              The proposed research will build, refine, and test in operational use, a set     of software tools designed to help support, maintain, and iteratively            revalidate computer-based clinical guidelines as they evolve over time.  The     project will focus on the domain of childhood immunization, and will build       upon IMM/Serve, a childhood immunization forecasting program that takes as       input a child's immunization history, and produces recommendations as to         which vaccinations are due and which vaccinations should be scheduled next.      The effort required to modify and validate such a program as the clinical        field evolves over time is a challenging task.  It will be extremely             important to have a robust set of tools to assist in this process.  Partial      prototype versions of certain of these tools already exist.                                                                                                       1.  The project will refine and extend computer-based tools for immunization     knowledge maintenance.  These tools will include:  a) IMM/Def, a program         which automatically generates the rule-based logic for the most complex          portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program          which automatically generates a set of test cases to help test the kernel        logic.  The project will also develop an organized set of strategies for         immunization test case generation, and implement those strategies in the         refined version of IMM/Test.                                                                                                                                      2.  The project will build a Web site to support immunization knowledge          maintenance.                                                                                                                                                      3.  The project will keep a detailed record of all modifications and             customization of the knowledge, and will represent all the variations of the     knowledge using a standardized format such as GLIF, the Guideline                Interchange Format being developed as a standard for exchanging guidelines       between sites.                                                                                                                                                    4.  The project will link IMM/Serve to a database designed to hold               IMM/Serve's analysis of a set of cases, so that the resulting package can be     used as a tool to perform compliance assessment.                                                                                                                  5.  A set of evaluation studies will be carried out to help assess the           efficacy of the tools and to help improve their functionality.                    n/a",TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES,6185229,R01LM006682,"['Internet', ' artificial intelligence', ' computer assisted medical decision making', ' computer assisted patient care', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' human data', ' immunization', ' information systems', ' medical records', ' pediatrics']",NLM,YALE UNIVERSITY,R01,2000,317318,-0.008697027711001322
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6151393,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2000,103781,-0.06718265817438919
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6132622,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,300000,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6132622,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,538812,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6132622,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,800000,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6323962,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,186611,0.0012861963955300068
"THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE We propose to create the Stanford PharmacoGenetic Knowledge Base (PharmGKB), an integrated data resource to support the NIGMS Pharmacogenetic Research Network and Database Initiative.  This initiative will focus on how genetic variation contributes to variation in the response to drugs, and will produce data from a wide range of sources.  The PharmGKB will therefore interlink genomic, molecular, cellular and clinical information about gene systems important for modulating drug responses.  The PharmGKB is based on a powerful hierarchical data representation system that allows the data model to change as new knowledge is learned, while ensuring the security and stability of the data with a relational database foundation.  Our proposal defines an interactive process for defining a data model, creating automated systems for data submission, integrating the PharmGKB with other biological and clinical data resources, and creating a robust interface to the data and to the associated analytic tools. Finally, we outline a research plan that uses the PharmGKB to (1) address difficult data modeling challenges that arise in the course of building the resource, (2) study the user interface requirements of a database with such a wide range of information sources, and (3) model and analyze the structural variations of proteins to shed light on the molecular consequences of genetic variation.  The PharmGKB will respect the absolute confidentiality of genetic information from individuals.  n/a",THE STANFORD PHARMACOGENETICS KNOWLEDGE BASE,6344145,U01GM061374,"['artificial intelligence', ' biomedical resource', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' cooperative study', ' drug interactions', ' drug metabolism', ' gene expression', ' genetic polymorphism', ' informatics', ' information dissemination', ' interactive multimedia', ' molecular biology information system', ' online computer', ' pharmacogenetics']",NIGMS,STANFORD UNIVERSITY,U01,2000,141192,0.0012861963955300068
"PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE The broad, tong-term objective of this Program Project Grant is to develop an effective imaging-based information and health care delivery system to support clinical practice, research, and education. The specific aims of the grant are to: (1) evolve PACS into an effective infrastructure that promotes the objectification of subjective patient clinical symptoms, (2) develop methods for improving the characterization of medical data through structured data collection, natural language processing of medical reports (NLP) and parametric summarization for medical images, (3) provide flexible, patient -specific presentation methods of medical images, timelines, and structured medical data. The objectification, intelligent access, and flexible presentation of medical data provide better information, which will facilitate the evidence-based practice of medicine and enhance research and evaluation. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently selected imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NP for text of parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Five integrated projects employ novel techniques to address specific elements of the system. Intelligently secreted imaging protocols are used to objectify patient symptoms. Well-defined information units capture and structure diverse forms of data, whether directly or indirectly through NLP for text or parametric summarization for images. Patient medical records are correlated with medical literature by content. Timelines organize the data into a format that allow medical events, their dependencies, and conditional trends to be easily visualized (Project 3). Scenario-based proxies provide up-to-date access to relevant medical information. Relaxation broadens queries to medical information when exact matches are not found. Software toolkits and user models enable user-, and domain-, and task-specific customizations. The hardware independent architecture will facilitate access to the system across different platforms and software subsystems. Together, they form a unique infrastructure that provides broad and intelligently customized access to well-defined structured data and up-to-date literature. This, in addition to patient-specific relevant data, expert opinion, and similar cases with known outcome, will promote the evidence-based practice of medicine. Evaluation of the impact of the proposed system will focus on technical measures; process of care; and patient and physician satisfaction. The evaluation will also explore the relationship between process changes and specific outcomes, particularly short-term health related quality of life. Although a formal cost-effectiveness study is not proposed, the foundation is laid for these measurements when these PACs technologies mature. These measurements will be facilitated by recording resource utilization, determining of imaging-based episodes of care, and counter- specific information related to a chief complaint.  n/a",PACS INFRASTRUCTURE TO SUPPORT-BASED MEDICAL PRACTICE,6094628,P01CA051198,"['automated medical record system', ' health care facility information system', ' radiology', ' telemedicine']",NCI,UNIVERSITY OF CALIFORNIA LOS ANGELES,P01,2000,1926078,-0.037445039011905996
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6180399,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2000,150497,-0.010062594005838093
"Deep learning for population genetics Project Summary The revolution in genome sequencing technologies over the past 15 years has created an explosion of population genomic data but has left in its wake a gap in our ability to make sense of data at this scale. In particular, whereas population genetics as a field has been traditionally data-limited, the massive volume of current sequencing means that previously unanswerable questions may now be within reach. To capitalize on this flood of information we need new methods and modes of analysis.  In the past 5 years the world of machine learning has been revolutionized by the rise of deep neural networks. These so-called deep learning methods offer incredible flexibility as well as astounding improvements in performance for a wide array of machine learning tasks, including computer vision, speech recognition, and natural language processing. This proposal aims to harness the great potential of deep learning for population genetic inference.  In recent years our group has made great strides in using supervised machine learning for population genomic analysis (reviewed in Schrider and Kern 2018). However, this work has focused primarily on using more traditional machine learning methods such as random forests. As we argue in this proposal, DNA sequence data are particularly well suited for modern deep learning techniques, and we demonstrate that the application of these methods can rapidly lead to state-of-the-art performance in very difficult population genetic tasks such as estimating rates of recombination. The power of these methods for handling genetic data stems in part from their ability to automatically learn to extract as much useful information as possible from an alignment of DNA sequences in order to solve the task at hand, rather than relying on one or more predefined summary statistics which are generally problem-specific and may omit information present in the raw data.  In this proposal we lay out a systematic approach for both empowering the field with these tools and understanding their shortcomings. In particular, we propose to design deep neural networks for solving population genetic problems, and incorporate successful networks into user-friendly software tools that will be shared with the community. We will also investigate a variety of methods for estimating the uncertainty of predictions produced by deep learning methods; this area is understudied in machine learning but of great importance to biological researchers who require an accurate measure of the degree of uncertainty surrounding an estimate. Finally, we will explore the impact of training data misspecification—wherein the data used to train a machine learning method differ systematically from the data to which it will be applied in practice. We will devise techniques to mitigate the impact of such misspecification in order to ensure that our tools will be robust to the complications inherent in analyzing real genomic data sets. Together, these advances have the potential to transform the methodological landscape of population genetic inference. Project Narrative Deep learning has revolutionized such disparate fields as computer vision, natural language processing, and speech recognition. In this proposal we aim to harness the great potential of deep learning for population genetic inference. We will design, implement, and apply novel deep learning methods and provide open source software for others to both use and build upon, thereby producing valuable tools for the genetics researchers at large.",Deep learning for population genetics,9976348,R01HG010774,"['Algorithms', 'Area', 'Biological', 'Biology', 'Classification', 'Code', 'Communities', 'Computer Vision Systems', 'Computer software', 'DNA Sequence', 'Data', 'Development', 'Ensure', 'Floods', 'Genetic', 'Genetic Recombination', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Image', 'Lead', 'Learning', 'Left', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Natural Language Processing', 'Natural Selections', 'Nature', 'Performance', 'Population', 'Population Explosions', 'Population Genetics', 'Process', 'Program Development', 'Publishing', 'Research Personnel', 'Sequence Alignment', 'Software Tools', 'Techniques', 'Technology', 'Training', 'Trees', 'Uncertainty', 'Ursidae Family', 'Work', 'base', 'computational chemistry', 'convolutional neural network', 'deep learning', 'deep neural network', 'design', 'empowered', 'flexibility', 'genetic information', 'genome sequencing', 'genomic data', 'infancy', 'innovation', 'learning classifier', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'network architecture', 'neural network', 'neural network architecture', 'next generation', 'novel', 'open source', 'random forest', 'recurrent neural network', 'research and development', 'speech recognition', 'statistics', 'stem', 'success', 'supervised learning', 'tool', 'tool development', 'user friendly software']",NHGRI,UNIVERSITY OF OREGON,R01,2020,529154,-0.015782970408148893
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9979659,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'MeSH Thesaurus', 'Measures', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'large scale data', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'public repository', 'specific biomarkers']",NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2020,467177,-0.032639617800900446
"Clinical and Informatics Research in Medical Terminologies A. Extraction of information from drug labels using natural language processing Drug package inserts (drug labels) are a comprehensive, up-to-date and authoritative source of drug information that is publicly available. To unleash the knowledge in the drug labels, they need to be transformed into standardized data structure and encoded in standard terminologies. Only then can the knowledge be used to drive applications such as clinical decision support. This collaborative project with the FDA uses natural language processing (NLP) and machine learning to extract information from the drug labels and create mappings to standard terminologies. The output will support the FDAs drug label indexing initiative to increase the usefulness of drug labels. In collaboration with FDA, I have hosted a challenge for extracting drug-drug interaction information from drug labels for the second year through NISTs Text Analysis Conference (TAC2019).    B. Use of medical terminologies to support clinical research  Medical terminologies and common data elements are important tools to allow sharing of clinical research data. I have studied their application in data sharing involving HIV-infected patients.  C. Creating maps between commonly used terminologies Mapping provides a solution to the problem caused by the use of multiple coding systems for the same kind of information. One example is the use of SNOMED CT and ICD-10-CM for coding medical diagnosis and problems. Using various computational methods supplemented by expert review, I have developed maps between SNOMED CT and the different flavors and versions of ICD codes. This will help to facilitate data re-use and data integration. I have also studied the potential benefits of using maps in data encoding. I am also studying various algorithmic approaches to create mappings between SNOMED CT and ICD-10-PCS, including lexical matching, ontological alignment and indirect mapping. On request from National Committee on Vital and Health Statistics, I have studied the new ICD-11 and compared it to ICD-10 and ICD-10-CM. An ongoing study is comparing the newly-released International Classification of Health Interventions (ICHI, created by WHO) with other medical procedure terminologies.  D. Use of deep learning in terminology research Deep learning techniques have resulted in breakthroughs in many areas. I have started a new line of research to employ deep learning techniques (e.g., word and graph embeddings) to tasks such as semantic relatedness measurement and mapping.   E. Facilitating adoption of terminology standards According to the Meaningful Use and subsequent Improving Interoperability incentive programs, SNOMED CT and RxNorm are terminologies required for the certification of electronic health record systems. I have studied the practical barriers of adoption of these terminologies and created useful resources to help with implementation. I studied the usage pattern of SNOMED CT terms in the problem lists of large health care providers and published a list of the most commonly used terms as the CORE Problem List Subset of SNOMED CT. The CORE subset is not only a useful resource for SNOMED CT implementers, it is also frequently used for terminology research and other purposes, and cited in multiple publications. RxTerms is another resource that I have developed to overcome data entry problems with RxNorm. n/a",Clinical and Informatics Research in Medical Terminologies,10268077,ZIALM010013,"['Adoption', 'Algorithms', 'Area', 'Certification', 'Classification', 'Clinical Informatics', 'Clinical Research', 'Code', 'Collaborations', 'Common Data Element', 'Communication', 'Computing Methodologies', 'Data', 'Data Reporting', 'Data Science', 'Data Sources', 'Diagnosis', 'Drug Interactions', 'Drug Labeling', 'Electronic Health Record', 'Graph', 'HIV', 'Health', 'Health Personnel', 'ICD-10-CM', 'Information Retrieval', 'International', 'International Classification of Disease Codes', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Intervention', 'Knowledge', 'Label', 'Machine Learning', 'Maps', 'Measurement', 'Medical', 'Medical Research', 'Methods', 'Natural Language Processing', 'Ontology', 'Output', 'Patient Care', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Procedures', 'Publications', 'Publishing', 'Research', 'Resources', 'Retrieval', 'SNOMED Clinical Terms', 'Semantics', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Terminology', 'Text', 'big-data science', 'care outcomes', 'clinical decision support', 'data integration', 'data reuse', 'data sharing', 'data standards', 'data structure', 'deep learning', 'improved', 'incentive program', 'indexing', 'information organization', 'insight', 'interoperability', 'lexical', 'statistics', 'structured data', 'symposium', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2020,610192,-0.0333407251451986
"Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement PROJECT SUMMARY/ABSTRACT Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing. Project Narrative Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing",Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement,10149058,R24MH117179,"['Address', 'Administrative Supplement', 'Archives', 'Artificial Intelligence', 'Award', 'BRAIN initiative', 'Benefits and Risks', 'Consent Forms', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Data Set', 'Ensure', 'Ethics', 'Foundations', 'Funding', 'Future', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Subject Research', 'International', 'Investments', 'Laws', 'Legal', 'Light', 'Machine Learning', 'Magnetic Resonance Imaging', 'Neurosciences', 'Parents', 'Policies', 'Privacy', 'Process', 'Regulation', 'Research', 'Research Subjects', 'Risk', 'Security Measures', 'Series', 'Software Tools', 'Solid', 'Surveys', 'Techniques', 'United States', 'United States National Institutes of Health', 'data archive', 'data privacy', 'data sharing', 'design', 'human subject', 'human subject protection', 'machine learning algorithm', 'neuroethics', 'neuroimaging', 'novel', 'prevent', 'privacy protection', 'research study', 'sharing platform', 'sound', 'stem']",NIMH,STANFORD UNIVERSITY,R24,2020,126592,-0.017739233298991738
"Natural language processing for precision medicine and clinical and consumer health question The Repository for Informed Decision Making, Clinical Question Answering and Consumer Health Question Answering projects are addressing the above objectives by developing knowledge-based and machine learning approaches to extraction and structuring of information in biomedical literature and other types of text (such as clinical notes and registered clinical trials) for the following types of information: 1) the diseases and conditions; 2) the numbers, co-morbidities, and socio-demographic characteristics of study subjects/participants, such as species, gender, smoking status and alcohol consumption; 3) the therapeutic and diagnostic interventions; 4) the study and publication types; 5) the end-points and the outcomes of the studies; 6) drug interactions and 7) adverse drug reactions.  In FY2020, we have developed a number of new approaches to facilitate understanding information requests sent to NLM customer services and long queries submitted to MedlinePlus search engine. Information requests sent to customer services are often several paragraphs long and provide the background and context that the customers believe will help understand their needs. For example, customers often describe several generations of their families affected by a disease and ask if their children will have it. The long MedlinePlus queries consist of one or two sentences and are often formed as questions. Both of these request forms are usually ungrammatical and rife with misspellings, abbreviations and informal language. We have developed a spellchecker for consumer language that is performing adequately on the misspellings important to understanding of the needs. After correcting spelling, our system employs three modules: a knowledge-based and a supervised machine learning method to understand the main points of the request, such as the disease or a drug of interest and the type of information about it. The systems extract the main points, which we found are sufficient to automatically search MedlinePlus and find authoritative and relevant pages for 65% of the requests. The third approach is to find similar questions that already have authoritative answers, e.g., provided by NIH institutes. In FY2020, the prototype consumer health question answering system https://chiqa.nlm.nih.gov/ was used to create data collection for automatic summarization of answers find in individual documents.  Our clinical question answering system is based on the framework for asking well-formed questions developed by the evidence-based medicine experts. Their analysis showed that presenting a clinical information need as four-part question frame: patient characteristics/problem; planned intervention; comparison; and desired outcome, helps formulate search engine queries that lead to relevant results. We developed methods for automatic extraction of question frames from information requests, automatic query formulation and automatic extraction of answers from retrieval results. The LHC CQA1.0 system extracts the bottom-line advice from biomedical publications and aligns the question frames and the answers to find the best answer. The CQA 1.0 system is currently used to provide bottom-line for retrieved images in the LHC Open-i system and to provide summaries of the biomedical articles in the LHC Open Summarizer.   In FY2020, our systems were used to study how to quickly address information needs of professionals, policymakers and the public. We organized two community-wide evaluations, for which our systems served as baselines. One evaluation,  TREC-COVID focuses on finding documents containing answers about Covid-19, and the second evaluation takes it further to answer questions asked by various stakeholders. n/a",Natural language processing for precision medicine and clinical and consumer health question,10269687,ZIALM010009,"['Abbreviations', 'Address', 'Affect', 'Alcohol consumption', 'COVID-19', 'Characteristics', 'Child', 'Clinical', 'Clinical Trials', 'Communities', 'Data Collection', 'Decision Making', 'Diagnostic', 'Disease', 'Drug Interactions', 'Evaluation', 'Evidence Based Medicine', 'Family', 'Formulation', 'Gender', 'General Population', 'Generations', 'Growth', 'Health', 'Image', 'Individual', 'Institutes', 'Intervention', 'Language', 'Lead', 'Literature', 'Machine Learning', 'MedlinePlus', 'Methods', 'Natural Language Processing', 'Outcome', 'Outcome Study', 'Participant', 'Patients', 'Pharmaceutical Preparations', 'Publications', 'Resources', 'Retrieval', 'Review Literature', 'Role', 'Services', 'Smoking Status', 'Source', 'Structure', 'Study Subject', 'System', 'Text', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Update', 'adverse drug reaction', 'base', 'clinical decision support', 'comorbidity', 'coronavirus disease', 'interest', 'knowledge base', 'machine learning method', 'novel strategies', 'precision medicine', 'prototype', 'repository', 'search engine', 'sociodemographics', 'spelling', 'study characteristics', 'supervised learning', 'systematic review']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2020,799199,-0.020006427177710364
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9990898,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'machine learning method', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'structured data', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2020,291873,-0.046094762044782314
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10057158,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'structured data', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2020,795873,-0.03220385873884066
"Scientific Computing The Office of Scientific Computing Services (OSCS) provides scientific and technical expertise in advanced biomedical computing and clinical informatics in support of the NIH Intramural Research Program; supports, maintains, and provides access to advanced supercomputing platforms for use by the NIH biomedical research community; works with NIH Institutes and Centers to support the use of mathematical and computer science approaches to advance biological and biomedical research efforts in medical informatics, data visualization, natural language processing, data mining, biological and medical database integration, and related areas; and develops novel computational methodologies and centralized database solutions to address and advance emerging biomedical research initiatives and challenges. OSCS is comprised of three functional groups:  (1) The Cloud Computing Services group manages NIHs cloud computing infrastructure and provides associated technical and scientific support for NIH research programs; supports NIH-wide data science strategic planning initiatives and implements plans for modernizing the NIH-funded biomedical data science ecosystem; operates the NIH Science and Technology Research Infrastructure for Discovery, Experimentation, and Sustainability (STRIDES) Initiative, through which NIH and NIH-funded researchers can access cloud-based technologies and related services from STRIDES partners to apply to their research; and provides training and other services to researchers, data owners, and research support staff across NIH to ensure optimal use of available tools and technologies.  (2) The Scientific Application Services group develops advanced algorithms and data visualization applications that are comprehensive and extensible; implements known solutions, algorithms, or methods to quickly and efficiently meet the biomedical imaging and informatics needs of NIH intramural collaborators; applies or develops novel systems, applications, algorithms, models, and machine learning techniques to efficiently deliver trusted data analysis; and collaborates with NIH intramural researchers to support biomedical informatics and data science services across research, clinical, and operational entities to shorten the path from data to insight.  (3) The High Performance Computing Services group plans, manages, supports, and operates NIH's core enterprise-wide, high-performance computational environment in support of NIH intramural research; develops and supports biomedical and life science application programs, associated biomedical databases, programming languages, and tools; provides training and technical expertise for NIH intramural staff relating to NIH high-performance computational resources and scientific applications; and  researches new technology developments in high-performance computing, life science applications, biomedical databases, and high-performance storage and network archiving for use by NIH. n/a",Scientific Computing,10273044,ZIHCT000278,"['Address', 'Adoption', 'Algorithms', 'Archives', 'Area', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Clinical Informatics', 'Cloud Computing', 'Communities', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Science', 'Databases', 'Ecosystem', 'Ensure', 'Environment', 'Funding', 'Genomics', 'High Performance Computing', 'Image Analysis', 'Infrastructure', 'Institutes', 'Intramural N.I.H. Research Support', 'Intramural Research Program', 'Machine Learning', 'Mathematics', 'Medical', 'Medical Informatics', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Natural Language Processing', 'Performance', 'Programming Languages', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Science', 'Services', 'Strategic Planning', 'Supercomputing', 'System', 'Technical Expertise', 'Techniques', 'Technology', 'Training', 'Trust', 'United States National Institutes of Health', 'Work', 'bioimaging', 'biological research', 'biomedical data science', 'biomedical informatics', 'central database', 'cloud based', 'computer science', 'computing resources', 'data mining', 'data visualization', 'drug discovery', 'functional group', 'imaging informatics', 'insight', 'mathematical sciences', 'new technology', 'novel', 'programs', 'scientific computing', 'structural biology', 'technology development', 'tool']",CIT,CENTER  FOR INFORMATION TECHNOLOGY,ZIH,2020,4927130,-0.034602616720437816
"Abiotic-Biotic Interfaces for Ophthalmology Symposium ABSTRACT This proposal seeks funding to support a symposium, Abiotic-Biotic Interfaces for Ophthalmology (ABI), which will bring together recognized world experts in clinical, research, vision science, engineering, industrial and pharmaceutical communities as well as junior investigators (i.e., young faculty and those in training) to discuss the current state of ABI, ranging from bioelectronic implantable and wearable devices, to nanoscale scaffolds for stem cell and gene therapies. Given the multidisciplinary nature of this field, it is essential to bring together researchers and clinicians with varying levels of expertise across many domains related to ABI to advance the progress of this novel field, identify challenges of advancement, and develop a strategic action plan to overcome these challenges. The timing to have such a symposium to further the application of implantable and/or wearable bioengineered systems in ophthalmology is now as we focus on precision and personalized medicine and leverage the revolution in deep learning artificial intelligence algorithms. Through symposium talks, sessions, and discussions we will cover the fundamentals and also identify innovative and cutting-edge strategies and methodologies to accelerate the rate of major discoveries and development of novel therapeutics. The specific aims of this symposium are: Specific Aim 1. To bring together both established and junior investigators representing a broad range of disciplines to discuss cutting edge research in this novel field, catalyze the development of cross-disciplinary and translational approaches to advance abiotic-biotic interfaces for ophthalmology, and identify gaps in knowledge and barriers to advancement. We will identify research questions and develop an agenda to guide future research that is consistent with the objectives and interests of NEI. Specific Aim 2. Develop a junior investigator program to motivate a diverse group of students and junior investigators to pursue research careers in vision science and ophthalmologic therapeutic development, who will ultimately submit grant proposals to NEI solicitations and contribute to the scientific literature. Specific Aim 3. Develop a strategic action plan to set priorities for future studies that will encourage inter-agency collaborations (e.g., NEI, NSF, DARPA, etc.). This is critical because often certain engineering tasks are best suited to be supported by NSF or DARPA whereas the biological testing of the engineered systems lends itself to funding from NEI. Hence such inter-agency or cross-agency efforts can help leverage the funding to develop sophisticated abiotic-biotic systems NARRATIVE This meeting is the first on this topic dedicated to the broad use of implantable and/or wearable bioelectronics for ophthalmological applications. It is anticipated that the strategic action plan will significantly impact the field by greatly accelerating the translation of basic science and engineering research findings to stimulate the development of novel treatments and improve clinical practice. Key topics include visual restoration, drug and gene delivery, and sensing intraocular pressure. This meeting will foster training and development of future leaders in this emerging field and promote collaboration and exchange of knowledge and ideas among junior and established investigators.",Abiotic-Biotic Interfaces for Ophthalmology Symposium,10070800,R13EY031988,"['Algorithms', 'Applications Grants', 'Artificial Intelligence', 'Basic Science', 'Biological Testing', 'Biomedical Engineering', 'Cellular Phone', 'Clinical Research', 'Collaborations', 'Communities', 'Computer software', 'Contact Lenses', 'Custom', 'Data', 'Development', 'Devices', 'Diagnosis', 'Discipline', 'Disease', 'Drug Delivery Systems', 'Electronics', 'Engineering', 'Eye', 'Faculty', 'Fostering', 'Funding', 'Future', 'Gene Delivery', 'Glass', 'Industrialization', 'Intraocular lens implant device', 'Knowledge', 'Literature', 'Medicine', 'Methodology', 'Nature', 'Neural Retina', 'Ophthalmology', 'Optics', 'Pharmacologic Substance', 'Physiologic Intraocular Pressure', 'Physiological', 'Research', 'Research Personnel', 'Route', 'Scientific Inquiry', 'Scientist', 'Senior Scientist', 'Students', 'System', 'Time', 'Training', 'Translations', 'Virtual and Augmented reality', 'Visual', 'base', 'career', 'clinical practice', 'deep learning', 'gene therapy', 'implantable device', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'meetings', 'multidisciplinary', 'nanoscale', 'neural network', 'novel', 'novel therapeutics', 'personalized medicine', 'portability', 'precision medicine', 'programs', 'restoration', 'scaffold', 'stem cell therapy', 'symposium', 'therapeutic development', 'translational approach', 'vision science', 'wearable device']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R13,2020,42465,-0.0338581804308978
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10058463,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data reuse', 'data sharing', 'data visualization', 'data warehouse', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,627034,-0.007527745169109905
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9847973,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data cleaning', 'data ecosystem', 'data reuse', 'data sharing', 'data standards', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2020,489919,0.007136903464686255
"Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication Patterns in Twitter data have revolutionized understanding of public health events such as influenza outbreaks. While researchers have begun to examine messaging related to substance use on Twitter, this project will strengthen the use of Twitter as an infoveillance tool to more rigorously examine nicotine, tobacco, and cancer- related communication. Twitter is particularly suited to this work because its users are commonly adolescents, young adults, and racial and ethnic minorities, all of whom are at increased risk for nicotine and tobacco product (NTP) use and related health consequences. Additionally, due to the openness of the platform, searches are replicable and transparent, enabling large-scale systematic research. Therefore, our multidisciplinary team of experts in diverse relevant fields—including public health, behavioral science, computational linguistics, computer science, biomedical informatics, and information privacy and security—will build upon our previous research to develop and validate structured algorithms providing automated surveillance of Twitter’s multifaceted and continuously evolving information related to NTPs. First, we will qualitatively assess a stratified random sample of relevant NTP-related tweets for specific coded variables, such as the message’s primary sentiment and other key information of potential value (e.g., whether a message involves buying/selling, policy/law, and cancer-related communication). Tweets will be obtained directly from Twitter using software we developed that leverages a comprehensive list of Twitter-optimized search strings related to NTPs. Second, we will statistically determine what message characteristics (e.g., the presence of certain words, punctuation, and/or structures) are most strongly associated with each of the coded variables for each search string. Using this information, we will create specialized Machine Learning (ML) algorithms based on state-of-the-art methods from Natural Language Processing (NLP) to automatically assess and categorize future Twitter data. Third, we will use this information to provide automatic assessment of current and future streaming data. Time series analyses using seasonal Auto-Regressive Integrated Moving Averages (ARIMA) will determine if there are significant changes over time in volume of messaging related to each specific coded variables of interest. Trends will be examined at the daily, weekly, and monthly level, because each of these levels is potentially valuable for intervention. To maximize the translational value of this project, we will partner with public health department stakeholders who are experts in streamlining dissemination of actionable trends data. In summary, this project will substantially advance our understanding of representations of NTPs on social media—as well as our ability to conduct automated surveillance and analysis of this content. This project will result in important and concrete deliverables, including open-source algorithms for future researchers and processes to quickly disseminate actionable data for tailoring community- level interventions. For this project, we gathered a team of public health researchers and computer scientists to leverage the power of Twitter as a novel surveillance tool to better understand communication about nicotine and tobacco products (NTPs) and related messages about cancer and cancer prevention. We will gather a random sample of Twitter messages (“tweets”) related to NTPs and examine them in depth and use this information to create specialized computer algorithms that can automatically categorize future Twitter data. Then, we will examine changes over time related to attitudes towards and interest in NTPs, as well as cancer-related discussion around various NTPs, which will dramatically improve our ability to better understand Twitter as a tool for this type of surveillance.",Leveraging Twitter to Monitor Nicotine and Tobacco Cancer Communication,10111658,R01CA225773,"['Adolescent', 'Affect', 'Alcohol or Other Drugs use', 'Algorithms', 'Attitude', 'Behavioral', 'Behavioral Sciences', 'Cancer Control', 'Categories', 'Characteristics', 'Cigarette', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Computers', 'County', 'Data', 'Disease Outbreaks', 'Electronic cigarette', 'Epidemiologic Methods', 'Event', 'Food', 'Football game', 'Future', 'Gold', 'Health', 'Health Care Costs', 'Individual', 'Influenza A Virus, H1N1 Subtype', 'Intervention', 'Laws', 'Linguistics', 'Literature', 'Malignant Neoplasms', 'Marketing', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Names', 'Natural Language Processing', 'Nicotine', 'Outcome', 'Pattern', 'Policies', 'Privacy', 'Process', 'Public Health', 'Public Opinion', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk', 'Sampling', 'Scientist', 'Security', 'Specificity', 'Structure', 'Techniques', 'Testing', 'Time', 'Time Series Analysis', 'Tobacco', 'Tobacco use', 'Twitter', 'Work', 'automated analysis', 'base', 'biomedical informatics', 'cancer prevention', 'computer program', 'computer science', 'computerized tools', 'data standards', 'data streams', 'ethnic minority population', 'geographic difference', 'hookah', 'improved', 'influenza outbreak', 'interest', 'machine learning algorithm', 'mortality', 'multidisciplinary', 'nicotine use', 'novel', 'open source', 'phrases', 'prospective', 'racial minority', 'social', 'social media', 'software development', 'statistics', 'time use', 'tobacco products', 'tool', 'trend', 'vaping', 'young adult']",NCI,UNIVERSITY OF ARKANSAS AT FAYETTEVILLE,R01,2020,491992,-0.01521201482146519
"SBIR Phase I- Topic 410 - Cancer Clinical Trials Recruitment and Retention Tools for Participant Engagement.  Many clinical trials fail to meet their accrual and retention goals, which leads to delays, early termination, or inability to draw conclusions at trial completion due to loss of statistical power. NCI wants to enhance clinical trials recruitment and retention by developing tools that could enhance communication between participants and study staff. In this Phase 1 tool development application we address the NCI interest in simplified informed consent documents that enhance personal communication during the informed consent process. In this Phase I proposal we leverage natural language processing technology and our teams prior work on the Informed Consent Ontology (ICO) to improve the language in consent documents related specifically to permissions granted by a research participant. n/a",SBIR Phase I- Topic 410 - Cancer Clinical Trials Recruitment and Retention Tools for Participant Engagement. ,10265762,5N91020C00017,"['Address', 'Authorization documentation', 'Clinical Trials', 'Communication', 'Comprehension', 'Consent Forms', 'Goals', 'Grant', 'Health', 'Informed Consent', 'Language', 'Natural Language Processing', 'Ontology', 'Participant', 'Personal Communication', 'Phase', 'Process', 'Research', 'Small Business Innovation Research Grant', 'Technology', 'Work', 'base', 'cancer clinical trial', 'improved', 'interest', 'prototype', 'recruit', 'tool', 'tool development', 'user-friendly']",NCI,"MELAX TECHNOLOGIES, INC.",N43,2020,400000,-0.03140914957111861
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,10136941,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'learning classifier', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2020,746725,-0.0016654730253907936
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,-0.019937461886953247
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9961522,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2020,15000,-0.029585486882065356
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10237828,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Information Retrieval', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2020,500000,-0.008646994991683917
"HEAL: New Chemical Structures for Pain, Addiction and Overdose Targets ELN Extraction -> Data Curation -> Reaction Analytics Dashboard Prototype A workflow was developed and executed for extraction and curation of chemical reaction data from NCATSs eLN. A subset of non-proprietary data was selected for detailed curation and annotation with the goal of creating a dataset suitable for machine learning purposes. The goal is to create a reaction analytics dashboard to better guide reaction development and more effectively enable target molecule synthesis.  Finalized installation of Reaction Screening platform Installation of an inhouse reaction screening platform consisting of a dual liquid- and solid-dispensing workstation provided by UnChained Laboratories and a large glovebox provided by MBraun for preparing, conducting, and processing high-throughput reaction screening experiments under completely inert conditions.  Chemical Inventory -> EDA, curation and annotation completed In preparation for seamless integration of reaction preparation for automated synthesis, the complete chemical inventory was downloaded, analyzed, curated, and annotated for the purposes of modeling of a next generation inventory management system for chemical synthesis.  Patent drafts initiated on SPE and Batch Evaporator The vision of the ASPIRE project is to provide a homogenous integration link between the regular laboratory and a fully integrated, nearly-autonomous system. To accomplish this, smaller-scale projects that address gaps in the medicinal chemistry workflow have been implemented to redesign the tools that chemists use at the bench allowing translation to automatic setup and execution.  Such tools include a solid phase extractor to facilitate workup of reactions to tools that permit direct and scalable evaporation from select containers such as final product scintillation vials and the reaction vessels themselves.  Strateos CRADA -> Indigo SAT NCATS initiated a collaboration with Strateos and in partnership with Eli Lilly to further the development of a flagship reactor system to be featured in both the Lilly Life Sciences Studio and the NCATS ASPIRE facility currently under development. In addition to providing technical implementation and automated function of the Indigo reactor system, the project team intends to work collaboratively to lower the barriers involved in automating chemical synthesis in general.  IOT - NCATS Laboratory Dashboard (Azure integration and blockchain pilot started with BurstIQ) A significant component of the ASPIRE project is to create a sustainable ecosystem of integrated technologies that realizes the goal of creating Real-Time Translational Science by accessing and leveraging capabilities offered by modern cloud-computing technologies into more secure, effective, and scalable data sharing strategies and implementations. To this end, we have successfully integrated instrument access through the Azure clouding computing services and initiated a collaboration with IQBurst to  study the viability of leveraging blockchain technologies as more effective means of managing the data life-cycle.  Flow Chemistry exploration in space (CASIS) NCATS is currently engaged in a four-way collaboration with CASIS, SpaceTango,  and Boston University to study flow chemistry in micro-gravity environments on the International Space Station. The study will initially involve studying the execution of iterative peptide synthesis. One of the first targets to be explored will be a four-membered cyclic peptide with known activity as a kappa opioid receptor antagonist with potential for development as a analgesic with lower addiction liability than currently available treatments.  Kebotix Collaboration NCATS collaborated with Kebotix, a technology platform company for new chemicals and materials, to enhance productivity of NCATS high-throughput experimentation (HTE) via artificial intelligence-driven optimization of assay conditions for biosynthesis inhibitors. Instead of testing a set of predefined parameters as in the traditional DOE approach, Kebotix uses adaptive optimization to gain more information using fewer experiments and less materials. By employing an advanced proprietary AI-driven optimization algorithm, Kebotix found suitable conditions and optimal assay performance with only 55 measurements with up to 20 running simultaneously. This output was sufficient for the Kebotix algorithm to identify and test the global optimum of the full 294 DOE experiments with a high probability (>95%). NCATS achieved a five-fold reduction in costs for lab supplies and HTE run-time, reduced from 49 hours to just 9 hours.  This collaboration serves as a test case for integrating existing NCATS screening technology to operate in an autonomous fashion while being driven by AI/ML techniques.  SLAS automated chemistry session NCATS featured prominently in this year's annual SLAS conference (Society for Laboratory Automation and Screening) both as presenters of NCATS research and as key contributors to the organization of the conference with Sam Michael serving as track chair for the Automation and High-Throughput Technologies program, as well as chair for the  Open Source Automation session. Further, Alex Godfrey served as session chair for the Automating Chemistry in the Age of AI session. Multiple presentations throughout the conference including both the opening and closing keynote presentations highlighted the impact of powerful machine-learning algorithms on the development of new medicines and the strides being made in automating chemical synthesis to improve the efficiency with which chemical space is explored - both key objectives woven into the ASPIRE program aimed at improving and accelerating the development of pharmacotherapies to treat pain, addiction, and overdose .  Synthetic Biomolecular Diversity Initiative This signature program involving synthetic biomolecular diversity and on-demand biologics has been initiated on the ASPIRE platform. The Design-Build-Test-Learn"" cycle will be applied to ongoing and new projects at NCATS. This effort will allow NCATS to explore macromolecular diversity and novel biological systems for translational science and research applications. This effort will also help to identify any gaps in the Design-Build-Test-Learn workflow in these labs and implement new automation/ BIO-CAD solutions to close such gaps. A working group consisting of biologists, chemists, informaticians, automation specialists and strategic alliance experts has been established to explore opportunities. A new seminar series has been initiated to engage biologists and identify potential collaborators, products, and companies of interest in areas of DNA printing, cell-free translation, and peptide or protein therapeutics using rapidly evolving synthetic biology technologies. Two collaborations have been initiated in the last 3 months with 1) Mytide Pharmaceuticals for automated peptide synthesis and 2) Center for Advanced Sensor Technology at University of Maryland Baltimore County for exploring the manufacture of therapeutic proteins in milligram to gram quantities using cell-free lysate systems. These technologies can employ computational tools for structure and synthetic route predictions along with controlling synthesis and purification in automated self-contained systems for GMP-like materials for in-vivo testing. These automated systems on the ASPIRE platform can be expanded to other macromolecular diversity collections such as synthetic oligonucleotides, carbohydrates and polysaccharides, fatty acid analogs ,and hybrid macromolecule-small molecule conjugates for therapeutic applications. n/a","HEAL: New Chemical Structures for Pain, Addiction and Overdose Targets",10259366,ZIATR000344,"['3-Dimensional', 'Address', 'Affect', 'Age', 'Algorithms', 'Anabolism', 'Analgesics', 'Animal Model', 'Area', 'Artificial Intelligence', 'Automation', 'Baltimore', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Sciences', 'Biology', 'Boston', 'Carbohydrates', 'Cell model', 'Cells', 'Chemical Structure', 'Chemicals', 'Chemistry', 'Clinical Trials', 'Cloud Computing', 'Collaborations', 'Collection', 'Cooperative Research and Development Agreement', 'County', 'Cyclic Peptides', 'DNA', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Ecosystem', 'Environment', 'Equipment and supply inventories', 'Goals', 'Government', 'Health', 'Hour', 'Human', 'Human Biology', 'Hybrids', 'Infrastructure', 'Interdisciplinary Study', 'International', 'Intervention', 'Intuition', 'Laboratories', 'Laboratory Chemicals', 'Lead', 'Learning', 'Legal patent', 'Life Cycle Stages', 'Link', 'Liquid substance', 'Machine Learning', 'Maryland', 'Measurement', 'Medicine', 'Microfluidics', 'Microgravity', 'Modeling', 'Modernization', 'Oligonucleotides', 'Opiate Addiction', 'Opioid Antagonist', 'Organic Chemistry', 'Output', 'Overdose', 'Pain', 'Patients', 'Peptide Synthesis', 'Peptides', 'Performance', 'Pharmaceutical Chemistry', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Pharmacotherapy', 'Phase', 'Polysaccharides', 'Preparation', 'Printing', 'Probability', 'Process', 'Production', 'Productivity', 'Professional Organizations', 'Public Health', 'Reaction', 'Reagent', 'Reproducibility', 'Research', 'Research Personnel', 'Risk', 'Route', 'Running', 'Science', 'Scientist', 'Secure', 'Series', 'Services', 'Societies', 'Solid', 'Specialist', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Tissue Microarray', 'Tissues', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Universities', 'Vial device', 'Vision', 'Work', 'addiction', 'base', 'biological systems', 'bioprinting', 'blockchain', 'chemical reaction', 'chemical synthesis', 'computerized tools', 'cost', 'dashboard', 'data curation', 'data management', 'data sharing', 'design', 'drug candidate', 'effective therapy', 'efficacy study', 'evaporation', 'experimental study', 'fatty acid analog', 'first-in-human', 'high throughput screening', 'high throughput technology', 'human model', 'improved', 'in vivo evaluation', 'indigo dye', 'induced pluripotent stem cell', 'inhibitor/antagonist', 'innovation', 'innovative technologies', 'instrument', 'interest', 'kappa opioid receptors', 'machine learning algorithm', 'macromolecule', 'milligram', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'open source', 'opioid epidemic', 'opioid misuse', 'opioid use', 'opioid use disorder', 'preclinical efficacy', 'programs', 'prototype', 'safety study', 'screening', 'sensor technology', 'small molecule', 'space station', 'symposium', 'synthetic biology', 'technological innovation', 'therapeutic protein', 'tool', 'working group']",NCATS,NATIONAL CENTER FOR ADVANCING TRANSLATIONAL SCIENCES,ZIA,2020,2389171,-0.018908506626843034
"Image-guided robot for high-throughput microinjection of Drosophila embryos PROJECT SUMMARY This proposal is submitted in response to the NIH Development of Animal Models and Related Biological Materials for Research (R21) program. The proposal develops an image-guided robotic platform that performs the automated delivery of molecular genetic tools and non-genetically encoded reagents such as chemical libraries, fluorescent dyes to monitor cellular processes, functionalized magnetic beads, or nanoparticles into thousands of Drosophila embryos in a single experimental session. The proposed work builds on recent engineering innovations in our collaborative group which has developed image-guided robotic systems that can precisely interface with single cells in intact tissue. The two Specific Aims provide for a systematic development of the proposed technologies. AIM 1 first engineers a robotic platform (‘Autoinjector’) that can scan and image Drosophila embryos in arrays of egg laying plates. We will utilize machine learning algorithms for automated detection of embryos, followed by thresholding and morphology analysis to detect embryo centroids and annotate injection sites. In AIM 2, we will utilize microprocessor-controlled fluidic circuits for programmatic delivery of femtoliter to nanoliter volumes of reagents into individual embryos. We will quantify the efficacy of the Autoinjector by comparing the survival, fertility, and transformation rates of transposon or PhiC31-mediated transgenesis to manual microinjection datasets. Finally, we will demonstrate the efficient delivery of sgRNAs and mutagenesis in the presence of Cas9. This project fits very well within the goals of the program by engineering a novel tool for producing and improving animal models. The Autoinjector will accelerate Drosophila research and empower scientists to perform novel experiments and genome-scale functional genomics screens that are currently too inefficient or labor intensive to be conducted on a large scale and may additionally enable other novel future applications. PROJECT NARRATIVE This proposal develops a technology platform that will enable automated microinjection of molecular genetic tools and non-genetically encoded tools such as chemical libraries, fluorescent dyes, functionalized magnetic beads, or nanoparticles, into thousands of Drosophila embryos in a single experimental session. The successful development of this technology will empower Drosophila biologists to perform screens and develop new applications that are currently too inefficient or labor intensive to contemplate and will accelerate research into the function of the nervous system and the molecular and genetic underpinnings of numerous diseases in this important animal model.",Image-guided robot for high-throughput microinjection of Drosophila embryos,9989196,R21OD028214,"['Animal Model', 'Biocompatible Materials', 'Biological Assay', 'Caliber', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Collection', 'Computer Vision Systems', 'Cryopreservation', 'Data Set', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Drosophila melanogaster', 'Embryo', 'Engineering', 'Expenditure', 'Exploratory/Developmental Grant', 'Fertility', 'Fluorescent Dyes', 'Future', 'Gene Transfer Techniques', 'Genetic', 'Goals', 'Guide RNA', 'Image', 'Individual', 'Injections', 'Investigation', 'Laboratories', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Mediating', 'Methods', 'Microinjections', 'Microprocessor', 'Microscope', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Morphology', 'Motivation', 'Mutagenesis', 'Needles', 'Nervous System Physiology', 'Performance', 'Process', 'Reagent', 'Research', 'Resources', 'Robot', 'Robotics', 'Scanning', 'Scientist', 'Signaling Molecule', 'Site', 'Space Perception', 'System', 'Technology', 'Tissues', 'Transgenes', 'Transgenic Organisms', 'United States National Institutes of Health', 'Work', 'animal model development', 'automated algorithm', 'base', 'biological research', 'cost', 'egg', 'experience', 'experimental study', 'functional genomics', 'gene product', 'genetic manipulation', 'genome-wide', 'image guided', 'improved', 'innovation', 'machine learning algorithm', 'magnetic beads', 'mutant', 'mutation screening', 'nanolitre', 'nanoparticle', 'novel', 'novel strategies', 'programs', 'response', 'robotic system', 'screening', 'small molecule libraries', 'stem', 'technology development', 'tool']",OD,UNIVERSITY OF MINNESOTA,R21,2020,222618,-0.03138953387787271
"Harmonized Data-Derived Resources for the Alzheimer's Disease and Related Dementias Community Current data harmonization work has been underway to maximize the utility of both datasets and analytics workflows across ADRD domains. We have also attempted to improve and standardize compute infrastructure to accomplish these tasks. All publicly available genomics data in the ADRD space is currently being aggregated and standardized as per Nalls et al 2019. Clinical data from longitudinal ADRD studies has been accessed as is being harmonized to mirror work represented in Iwaki et al 2019. Additionally, we have been initiating testing of hybrid cloud infrastructure to support this work, maximizing the power of google cloud and the NIH's biowulf compute resource to ensure efficient analytics.  Using these approaches, we have begun to develop several web-based applications relevant to ADRD research. In particular, we are creating a platform for the curation, sharing, and integration of multi-modal -omics data to gain mechanistic insights for early-stage ADRD drug development. We are also developing a web-based ADRD inference engine to store and harmonize clinical trajectories in ADRD for NIH using clinical data. We plan to curate all available/shareable data on ADRD and healthy brain aging in a uniform and harmonized manner to create a biobank for the ADRD research community. n/a",Harmonized Data-Derived Resources for the Alzheimer's Disease and Related Dementias Community,10251666,ZIAAG000534,"['Address', 'Adopted', 'Alzheimer&apos', 's disease related dementia', 'Area', 'Artificial Intelligence', 'Biological', 'Cells', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Communities', 'Complex', 'Data', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Distal', 'Ensure', 'Ethics', 'Genomics', 'Growth', 'Home environment', 'Human', 'Hybrids', 'Image', 'In Situ', 'Infrastructure', 'Institution', 'Investigation', 'Machine Learning', 'Mediation', 'Medical Genetics', 'Methods', 'Nature', 'Online Systems', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Speed', 'Standardization', 'Testing', 'Time', 'United States National Institutes of Health', 'Vision', 'Work', 'aging brain', 'biobank', 'computing resources', 'data access', 'data harmonization', 'data integration', 'data integrity', 'data management', 'data privacy', 'data sharing', 'data visualization', 'deep learning', 'design', 'drug development', 'genomic data', 'improved', 'insight', 'interest', 'molecular imaging', 'multimodality', 'next generation']",NIA,NATIONAL INSTITUTE ON AGING,ZIA,2020,2764516,-0.009429415932782915
"NLM Scrubber: NLM's Software Application to De-identify Clinical Text Documents Narrative clinical reports contain a rich set of clinical knowledge that could be invaluable for clinical research. However, they may also contain personally identifiable information (PII) that make those clinical reports classified as PHI, which is associated with use restrictions and risks to privacy. Computational de-identification seeks to remove all instances of PII in such narrative text in order to produce de-identified documents, which would no longer be classified as PHI and can be used in clinical research with fewer constraints and with almost no risk to privacy. Computational de-identification uses artificial intelligence methods including pattern recognition and computational linguistic techniques to recognize words and other alphanumeric tokens denoting PII (e.g., names, addresses, and telephone and social security numbers) in the text, and replace them with labels such as NAME and ADDRESS. In this way, both patient privacy is protected and clinical knowledge is preserved.  After exploring existing de-identification tools, the U.S. National Library of Medicine (NLM) began developing a new software application called NLM Scrubber, which is capable of de-identifying many types of clinical reports with high accuracy. The software design is based on both deterministic and probabilistic artificial intelligence methods utilizing large dictionaries of personal names, addresses, and organizations. The application accepts narrative reports in plain text or in HL7 format. When the input reports are formatted as HL7 messages, the application software leverages patient information embedded in HL7 segments to find such information in the text portion of the HL7 message.  NLM Scrubber has been downloaded by a number of organizations for testing and use, including IBM, Google, Fred Hutch Cancer Research Center, Harvard Medical School, Florida International University, University of Bristol, University College Dublin, and Oak Ridge National Laboratory. National Cancer Institute (NCI) along with the state cancer registries working with NCI have also been voiced their interests in using NLM Scrubber to de-identify narrative pathology reports in Surveillance, Epidemiology and End Results (SEER) database.  Our current focus is making NLM-Scrubber answer various needs of clinical scientists and clinical data managers without a deep understanding of the underlying technology. In this term, we started moving our codebase from Perl to a C++/Python-based system, which will provide us more flexibility, higher performance and scalability for future functionalities.  While NLM-Scrubber can be used for de-identifying all clinical reports repository-wide, it can also be used in various modes tailored to the user and their context, including on-demand cohort-specific de-identification and de-identification with patient and provider identifiers. NLM-Scrubber enables clinical scientists, the users of de-identified data, to be part of the process and shape the de-identification output based on their needs. n/a",NLM Scrubber: NLM's Software Application to De-identify Clinical Text Documents,10268072,ZIALM010002,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Clinical', 'Clinical Data', 'Clinical Research', 'Computational Linguistics', 'Computer software', 'Databases', 'Dictionary', 'Florida', 'Future', 'Goals', 'Guidelines', 'Health', 'International', 'Knowledge', 'Label', 'Laboratories', 'Laws', 'Methods', 'NCI Center for Cancer Research', 'Names', 'National Cancer Institute', 'Output', 'Pathology Report', 'Patients', 'Pattern Recognition', 'Performance', 'Personally Identifiable Information', 'Policies', 'Privacy', 'Process', 'Provider', 'Pythons', 'Regulation', 'Reporting', 'Risk', 'SEER Program', 'Scientist', 'Shapes', 'Social Security Number', 'Software Design', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Text', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Voice', 'base', 'cohort', 'college', 'data de-identification', 'flexibility', 'interest', 'medical schools', 'neoplasm registry', 'patient privacy', 'preservation', 'repository', 'tool']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2020,845090,-0.05124909669635966
"Conceptualizing Actionability in Clinical Genomic Screening Project Summary/Abstract. Clinical genomic sequencing (CGS) produces large amounts of data, much of which is hard to characterize or may have a negligible influence on health. The concept of actionability is commonly used to help separate information that may be useful from information that is likely irrelevant for patients. Actionability directs attention to whether genomic information warrants action and reflects its initial development as a strategy to augment diagnosis and treatment in sick patients. As CGS expands towards healthy populations in primary care settings, actionability is still widely embraced despite little consensus regarding its definition and use. Because this ambiguity could become an obstacle to the successful implementation of clinical genomic sequencing in healthy populations, greater clarity about this concept is necessary. The proposed research will fulfill this need by characterizing the emergence and varied meanings of actionability in clinical genomics, focusing on clinical genomics' transition into primary care settings. By identifying underlying values and assumptions related to actionability, this research will push beyond definitional disputes and provide a deeper framework for assessing how genetic information is valued. The specific aims are: 1. Identify and characterize, through in-depth interviews, how genomics experts and primary care providers conceptualize what makes genomic information actionable for healthy populations. 2. Identify and characterize, through a natural language processing (NLP) analysis of published literature, how the concept of actionability emerged, spread, and is used throughout clinical genomics. 3. Convene a workshop with genomics experts, primary care providers, and ELSI scholars to produce a white paper on actionability and the ethical, effective integration of CGS into primary care, guided by the results from Aims 1 and 2. This K99/R00 Pathway to Independence Award includes a highly-structured, mentored training program that will support the candidate's goal to become an independent, mixed-methods ELSI investigator focused on assessing the value of genomic information. To achieve this career goal, the candidate will: 1. Receive training in genetic and genomic science to facilitate collaboration with genomics care teams and make scientifically accurate policy recommendations 2. Build new methodological skills in biomedical informatics and natural language processing to conduct generalizable research 3. Publish and engage with scientific and medical audiences to have a more direct impact on future guidelines and policies. 4. Develop a collaborative and interdisciplinary research network. This training will include coursework, guided readings, network building, and sustained mentorship by a highly-qualified team of faculty with expertise in ELSI research, bioethics, clinical genomics, biomedical informatics, and the history and sociology of medicine. This training will prepare the candidate to transition to an independent ELSI investigator focused on ethical issues related to the actionability of genomic health information – an ELSI research priority in Genetic and Genomic Healthcare. Project Narrative. This K99/R00 Pathway to Independence Award will prepare the candidate to become an independent, mixed-methods ELSI researcher pursing a research program on ethical issues related to the actionability of genomic information. The study examines the values and assumptions underlying conceptualizations of the actionability of genomic information for healthy populations. Results of the study will contribute to the ethical and effective implementation of genomic sequencing into care for healthy populations.",Conceptualizing Actionability in Clinical Genomic Screening,10054993,K99HG010905,"['American', 'Award', 'Bioethics', 'Caring', 'Clinical', 'Collaborations', 'Consensus', 'Data', 'Development', 'Diagnosis', 'Disease', 'Disputes', 'Educational workshop', 'Ethical Issues', 'Ethics', 'Faculty', 'Future', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Healthcare', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Laboratories', 'Level of Evidence', 'Literature', 'Medical', 'Medical Genetics', 'Medical Sociology', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Natural Language Processing', 'Outcome', 'Paper', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Penetrance', 'Policies', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Provider', 'Publishing', 'Qualitative Methods', 'Reading', 'Recommendation', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Priority', 'Risk', 'Science', 'Severities', 'Structure', 'Surveys', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Variant', 'biomedical informatics', 'care providers', 'career', 'clinical implementation', 'directed attention', 'genetic information', 'genome sciences', 'health management', 'innovation', 'lifestyle intervention', 'medical schools', 'prevent', 'primary care setting', 'programs', 'screening', 'skills']",NHGRI,UNIVERSITY OF PENNSYLVANIA,K99,2020,103353,-0.024173762391934955
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,10016840,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data curation', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'large scale data', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2020,330502,-0.005875502975969051
"A decentralized macro and micro gene-by-environment interaction analysis of substance use behavior and its brain biomarkers   Abstract Wide-spread data sharing has started to permeate the brain imaging community from funders to researchers. However, in recent years there have also been some concerns raised regarding ethical issues related to privacy and data ownership among others. In the parent award we are leveraging and extending a privacy preserving decentralized data sharing platform called COINSTAC to perform a study of gene-by-environmental effects by pooling together data from across the world, some of which is unable to be openly shared. In this supplement we will study various bioethical issues related to different data sharing strategies. This will include calculating risk scores from existing data to evaluate the effectiveness of machine learning to potentially reidentify from similar or different data types, a detailed survey of various policy makers and stakeholders including researchers, federal employees, IRB members, and more, and finally the development of a forward looking white paper addressing both privacy, policy, and regulatory aspects which attempts to frame the various aspects that arise in the contact of the spectrum of data sharing approaches including fully open, ‘trust’ based via data usage agreements, privacy preserving via tools like COINSTAC, and more. The outcomes of this supplement will provide a useful guide for the field going forward and also provide initial data necessary to develop a larger scale project on these topics going forward. Narrative The era of big data, open science, and deep learning is upon us, and data sharing can be done in various ways ranging from fully open to privacy preserving to fully closed. However bioethical issues related to risk, privacy, and data sharing strategies have not been well studied in the context of combined brain imaging, genomics, and macro-environmental data and can be especially sensitive in the context of information such as substance use. In this supplement we will quantify risk levels, survey a broad community of stakeholders, and develop recommendations for the field going forward.",A decentralized macro and micro gene-by-environment interaction analysis of substance use behavior and its brain biomarkers  ,10131528,R01DA049238,"['Address', 'Adolescence', 'Adolescent', 'Age', 'Agreement', 'Alcohol or Other Drugs use', 'Anxiety', 'Award', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Big Data', 'Bioethical Issues', 'Biological Markers', 'Brain', 'Brain imaging', 'Brain scan', 'China', 'Climate', 'Communities', 'Complex', 'Computer software', 'Consumption', 'Country', 'Data', 'Data Protection', 'Decentralization', 'Development', 'Disease', 'Dropout', 'Economic Burden', 'Employee', 'Environment', 'Environmental Risk Factor', 'Epidemiology', 'Ethical Issues', 'Ethnic group', 'Europe', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heritability', 'Household', 'Image', 'Income', 'India', 'Individual', 'Institutional Review Boards', 'International', 'Intervention', 'Learning', 'Legal', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Mental Depression', 'Modeling', 'Movement', 'Neurobiology', 'Neurons', 'Neurosciences', 'Outcome', 'Ownership', 'Paper', 'Parents', 'Physiological', 'Policies', 'Policy Maker', 'Population', 'Population Density', 'Privacy', 'Quality Control', 'Race', 'Recommendation', 'Regulation', 'Research Personnel', 'Risk', 'Scanning', 'Smoking', 'Source Code', 'Structure', 'Surveys', 'System', 'Time', 'Trust', 'Twin Multiple Birth', 'Update', 'Urbanization', 'Visualization', 'adolescent substance use', 'base', 'cloud based', 'cognitive development', 'cohort', 'computerized tools', 'cost', 'data access', 'data exchange', 'data sharing', 'deep learning', 'drinking', 'early life stress', 'effectiveness evaluation', 'epidemiologic data', 'gene environment interaction', 'genetic profiling', 'human subject', 'imaging genetics', 'insight', 'member', 'neuroimaging', 'open data', 'open source', 'peer', 'privacy preservation', 'psychiatric symptom', 'relating to nervous system', 'rural area', 'sex', 'sharing platform', 'statistics', 'tool', 'tool development', 'urban area', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,149834,-0.02571833962890119
"Nathan Shock Center of Excellence in Basic Biology of Aging OVERALL PROJECT SUMMARY This application is for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington and affiliated institutions. This Center has over the past 25 years provided key resources in support of investigators who study the biology of aging. This application continues a theme that emphasizes outreach and service to the broadest community of investigators in the gerosciences. Of proximal relevance is the characterization of aging-related phenotypes of longevity and healthspan. As our Center services must be easily accessible to outside users, our Longevity and Healthspan Core (Core E) focuses on invertebrate assays, many of them novel. Two other Resources Cores focus on the high dimensional assessments that are closely related to aging phenotypes: Protein Phenotypes of Aging (Core C) and Metabolite Phenotypes of Aging (Core D). Sophisticated computational and bioinformatic tools for data analysis and optimal insight are provided by the Artificial Intelligence and Bioinformatics Core F. Each of these four Resource Cores is led by highly respected experts in that field, including Michael MacCoss and Judit Villen (Core C), Daniel Promislow (Core D), Matt Kaeberlein and Maitreya Dunham (Core E) and Su-In Lee (Core F). Each will push the envelope of appropriate technologies, developing new state-of-the art approaches for assessments that are the most applicable to gerontology and making them accessible to the national aging community. The Research Development Core (Core B) will continue to support pilot and junior faculty studies, with a firm focus on outreach of service to the national geroscience constituency. The Administrative and Program Enrichment Core (Core A) supports administrative management, an external advisory panel, courses, and data sharing and dissemination. Core A’s program of seminars and symposia will continue a focus on sponsorship and organization of national courses, meetings and pre-meetings, as well as workshops in the fields allied to our Resource Core Services. In coordination with other Nathan Shock Centers, we will support a new Geropathology Research initiative. UW NATHAN SHOCK CENTER OVERALL - PROJECT NARRATIVE We apply for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington, which has for 25 years provided key resources supporting investigators who study the biology of aging. The overarching goal of this Center is to have a positive impact on the field by accelerating research discovery and providing research support for investigators nationally and internationally, particularly junior investigators in the process of building their own research programs. We will accomplish this goal through six cores that function synergistically together: four Resource Cores with particular expertise in protein (Core C) and metabolite (Core D) phenotypes of aging, invertebrate longevity and healthspan phenotypes (Core E) and artificial intelligence and bioinformatics (Core F), along with a Research Development Core (Core B) that supports external pilot projects and junior faculty studies, and an Administrative and Program Enrichment Core (Core A) that supports administrative management, an external advisory panel, sponsorship and organization of national meetings and pre-meetings, courses, workshops and seminars, and, in coordination with other Nathan Shock Centers, a Geropathology Research initiative.",Nathan Shock Center of Excellence in Basic Biology of Aging,10042617,P30AG013280,"['Aging', 'Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'Biology of Aging', 'Collaborations', 'Communication', 'Communities', 'Consult', 'Data', 'Data Analyses', 'Development', 'Educational workshop', 'Environment', 'Experimental Designs', 'Faculty', 'Genes', 'Genetic study', 'Gerontology', 'Geroscience', 'Goals', 'Growth', 'Informatics', 'Institution', 'International', 'Invertebrates', 'Leadership', 'Longevity', 'Methodology', 'Methods', 'Microfluidics', 'Molecular Genetics', 'Office of Administrative Management', 'Pathway interactions', 'Phenotype', 'Philosophy', 'Pilot Projects', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Research', 'Research Activity', 'Research Personnel', 'Research Support', 'Resources', 'Robotics', 'Services', 'Shock', 'Statistical Data Interpretation', 'Technology', 'Transcript', 'Universities', 'Variant', 'Washington', 'bioinformatics tool', 'career development', 'cell age', 'computerized tools', 'data dissemination', 'data sharing', 'healthspan', 'high dimensionality', 'insight', 'meetings', 'metabolomics', 'multiple omics', 'novel', 'outreach', 'outreach services', 'programs', 'protein metabolite', 'research and development', 'symposium', 'tool', 'trait']",NIA,UNIVERSITY OF WASHINGTON,P30,2020,962037,-0.02104382904596229
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,10002330,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data curation', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2020,653481,-0.008547886947396952
"Medical Genomics Unit Analytic Projects This year has been focused on the establishment of the Medical Genomics Unit (MGU). All of these activities involve the analysis of data (primarily clinical data) pertinent to a variety of genetic disorders.  First, we have recruited several key individuals to be part of the MGU.  These include: a staff scientist with specific expertise in deep learning and related analytic and statistical techniques as applied to clinical/phenotypic as well as genomic data; a genetic counselor with experience in collaborative research and clinical practice in both laboratory and patient-facing areas.  The staff scientist started in August; the genetic counselor will likely start in September.  A trainee in the laboratory genetics/genomics program (who is also a trained geneticist/dysmorphologist) also joined in August; another clinician will likely join in the winter.  Second, we have had met with and discussed potential collaborations with a variety of intramural and extramural scientists and clinicians, with the goal of helping work with useful and sufficiently sized datasets to achieve our research goals.  These datasets involve a variety of image-based and medical record-based sources.  Third, we have, with the successful onboarding of our initial MGU member other than the PI, started to set up analytic pipelines using example external/publicly-available data for testing and training purposes.  We have also started to assemble and collate publicly available datasets (eg, of images of genetic conditions) for initial analyses.  Fourth, and related to the previous points, we have prepared our materials for our quadrennial board of scientific counselors (BSC) review.  Since the MGU is newly established, we view this as a unique and well-timed opportunity to solicit feedback on our research plan.  Finally, as exercises related to the above activities, we have participated in several data-related activities regarding COVID-19.  These involve the type of manual data cleaning and collection and analyses that will form part of the MGU work; these steps are necessary for the application of the planned deep learning activities. n/a",Medical Genomics Unit Analytic Projects,10267128,ZIAHG200405,"['Area', 'Biological', 'COVID-19', 'Clinical Data', 'Collaborations', 'Computerized Medical Record', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Disease', 'Electronic Health Record', 'Exercise', 'Extramural Activities', 'Feedback', 'Genetic', 'Genetic Diseases', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Individual', 'Investigation', 'Laboratories', 'Manuals', 'Medical', 'Medical Records', 'Methods', 'Modeling', 'Patients', 'Professional counselor', 'Research', 'Scientist', 'Source', 'Techniques', 'Testing', 'Training', 'Work', 'base', 'clinical phenotype', 'clinical practice', 'data cleaning', 'deep learning', 'experience', 'flexibility', 'genetic counselor', 'genomic data', 'imaging genetics', 'improved', 'insight', 'learning strategy', 'member', 'multiple data types', 'novel', 'programs', 'recruit']",NHGRI,NATIONAL HUMAN GENOME RESEARCH INSTITUTE,ZIA,2020,125662,-0.01390741085134159
"Georgia Clinical & Translational Science Alliance (GaCTSA) EFFECTIVE ALLOCATION OF TEST CENTERS FOR COVID-19 USING MACHINE LEARNING AND  ADAPTIVE SAMPLING ABSTRACT A critical task in managing and dealing with COVID-19 in communities is to perform diagnostic and/or antibody tests to identify diseased individuals. This information is critical to public health officials to estimate prevalence and transmission, and to effectively plan for required resources such as ICU beds, ventilators, personal protective equipment, and medical staff. Additionally, information on the number of infected people can be used to develop probabilistic and statistical models to estimate the reproduction number of the disease, and to predict the likely spatial and temporal trajectories of the outbreak. This provides vital information for planning actions and preparing policies and guidelines for social-distancing, school closures, remote work, community lockdown, etc. Despite the importance of diagnostic testing and identification of the positive cases, broad-scale testing is a challenging task particularly due to the limited number of test kits and resources. Our proposed research focuses on the development machine learning-based allocation strategies for determining the optimal location of COVID-19 test centers, including mobile and satellite centers, to minimize the local and global prediction uncertainties, maximize geographic coverage, associated with projections of spatio-temporal outbreak trajectories, and to improve efficient identification of diseased cases. EFFECTIVE ALLOCATION OF TEST CENTERS FOR COVID-19 USING MACHINE LEARNING AND  ADAPTIVE SAMPLING NARRATIVE Diagnostic and antibody tests for COVID-19 can provide invaluable information on prevalence and transmission of the disease. However, due to limited test capacity, broad-scale testing is currently not feasible. Consequently, there is a pressing need for a systematic and data-driven approach to defining testing strategies, in particular, determining the number and location of satellite and mobile testing centers (e.g., drive-through test locations). Our research program develops machine learning approaches to effectively allocate test centers for COVID-19 at the city, county, and state levels to accurately and reliably estimate the disease prevalence and its trajectory for resource planning and policy making, and to efficiently identify cases for treatment.",Georgia Clinical & Translational Science Alliance (GaCTSA),10158891,UL1TR002378,"['Active Learning', 'Antibodies', 'Area', 'Beds', 'Biology', 'COVID-19', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Cities', 'Clinical Sciences', 'Communities', 'Contracts', 'County', 'Data', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Ecology', 'Ensure', 'Epidemic', 'Epidemiology', 'Equipment', 'Federal Government', 'Future', 'Geography', 'Goals', 'Guidelines', 'Hybrids', 'Individual', 'Local Government', 'Location', 'Machine Learning', 'Medical Staff', 'Methods', 'Modeling', 'Monitor', 'Neighborhood Health Center', 'Neurology', 'Pattern', 'Performance', 'Policies', 'Policy Making', 'Population', 'Prevalence', 'Process', 'Public Health', 'Readiness', 'Reproduction', 'Research', 'Research Project Grants', 'Resistance', 'Resources', 'Sampling', 'Scheme', 'Schools', 'Series', 'Social Distance', 'Statistical Models', 'Testing', 'Time', 'Translational Research', 'Uncertainty', 'Update', 'Ventilator', 'Virus', 'Work', 'base', 'case finding', 'disease transmission', 'environmental justice', 'evidence base', 'experience', 'flexibility', 'improved', 'metropolitan', 'multidisciplinary', 'novel', 'pandemic disease', 'programs', 'racial and ethnic', 'racial diversity', 'sociodemographics', 'socioeconomics', 'spatiotemporal', 'transmission process']",NCATS,EMORY UNIVERSITY,UL1,2020,225579,-0.020222416088001096
"DIGITAL HEALTH SOLUTIONS FOR COVID-19:  COVID COMMUNITY ACTION AND RESEARCH ENGAGEMENT (COVID-CARE) The goal of this project is to develop mobile applications, data integrations, and validated machine learning algorithms to identify COVID-19 and differentiate it from the flu, and to perform contact tracing using Wi-Fi technologies.  Vibrent Health will accomplish this goal by enhancing their Vibrent Digital Health Solutions Platform (DHSP) implementation to large-scale pilot populations among diverse user groups.  The project will focus on validating the technology’s performance, usability, and reliability in refinement of analytics to generate predictive algorithms for infection.  The platform is intended to support individual, organizational, community, and societal-level decision-making in the COVID-19 pandemic response.  The first objective involves innovation to develop a technology that can differentiate between COVID-19 and flu (or other respiratory illness).  The second objective involves the development and testing of a Wi-Fi-based contact tracing tool using George Mason University’s enterprise Wi-Fi system.  The third objective involves the development of a full technical integration approach and strategy to support data exchange.  Data collected under this project will be deidentified and securely transmitted to an NIH data hub. n/a",DIGITAL HEALTH SOLUTIONS FOR COVID-19:  COVID COMMUNITY ACTION AND RESEARCH ENGAGEMENT (COVID-CARE),10274145,5N91020C00038,"['Action Research', 'COVID-19', 'COVID-19 pandemic', 'Caring', 'Communities', 'Community Actions', 'Contact Tracing', 'Data', 'Decision Making', 'Development', 'Goals', 'Health', 'Individual', 'Infection', 'Performance', 'Population', 'Secure', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Universities', 'base', 'coronavirus disease', 'data exchange', 'data hub', 'data integration', 'digital', 'flu', 'innovation', 'machine learning algorithm', 'mobile application', 'prediction algorithm', 'respiratory', 'response', 'tool', 'usability', 'wireless fidelity']",NCI,"VIGNET, INC.",N01,2020,1098256,-0.006433110164328083
"Automated Data Collection on Antimicrobial Use in Dogs and Cats in a Tertiary Hospital and Private Practices Judicious antimicrobial use in veterinary medicine is important because improper antimicrobial use can contribute to the evolution of antimicrobial resistance in bacterial pathogens, which makes subsequent use of these drugs less effective in both human and veterinary medicine. There is very little on-the-ground information about veterinary clinicians’ antimicrobial use (AMU) practices in companion animal practice in the US. veterinary medicine. To improve our understanding of antimicrobial use in dogs and cats, we propose to create a nationwide digital surveillance system to collect critical AMU data using existing electronic practice information management systems (PIMS) in collaboration with veterinary industry partners. The system will automatically harvest AMU and patient data from digital PIMS. The proposed system will harvest data collected in routine veterinary examinations from existing PIMS systems and therefore will not require any additional effort from practitioners to participate in the program. Natural language processing, a machine learning method used to classify unstructured text, will be used to review electronic medical records to determine patients’ diagnosis. We aim to prototype the system in our native digital PIMS at North Carolina State University’s College of Veterinary Medicine Teaching hospital. We will then enroll additional private veterinary practices, including general practice, specialty hospitals, and emergency clinics, as sentinels and collect the same detailed PIMS data from a more representative set of clinics. Working closely with the sentinel clinics will provide a deep understanding of how our system operates in private clinics, and in the final stage we aim to expand the fully automated system to PIMS nationwide. The combination of sentinel clinics with the nationwide survey of clinics will create a powerful broad and deep surveillance system for antimicrobial use in veterinary clinics. A broad suite of AMU parameters will be estimated from this data, and the results reported to the FDA in an annual report. Additionally, we will share the data with other researchers through an web-based portal and GitHub repositories. This system will provide the critical data and analysis to understand veterinary AMU in the US. NARRATIVE Veterinarians play a central role in protecting animal and human health by preserving the efficacy of the antibiotics that their use for their patients. We have created a partnership among a public university, private veterinary hospitals, and a leading industry partner to collect information on how antibiotics are being used in cat and dog practices across the country with no disruption to the participating hospitals. The data will support FDA’s commitment to promoting antimicrobial stewardship.",Automated Data Collection on Antimicrobial Use in Dogs and Cats in a Tertiary Hospital and Private Practices,10166402,U01FD007057,[' '],FDA,NORTH CAROLINA STATE UNIVERSITY RALEIGH,U01,2020,199999,-0.020107569380478568
"Scalable tools to effectively translate genomic discoveries into the clinic PROJECT SUMMARY We are in the midst of a genomic revolution; more than 250,000 human genomes have been sequenced, generating over a petabase of genomic data. While these new data hold great promise to impact health, there is a disconnect between genomic discovery and clinical care. Providers frequently misinterpret genomic information, patients often don't understand their own test results, and genomic information about disease risk is infrequently shared between patients and family members. Importantly, ineffective communication and data misinterpretation has devastating consequences- including unnecessary organ removal, missed disease prevention opportunities, and premature death. We are addressing these genomic care gaps by developing and testing tools that optimize the integration of whole-exome and whole-genome sequencing (WES, WGS) for general clinical practice. My vision for improving genomic medicine is based on my work within multidisciplinary consortia and addresses the National Human Genome Research Institute's priority research area of improving the effectiveness of healthcare. In the proposed work we will test the effectiveness of a multilevel genomic e-Health intervention in cancer (Aim 1). Our intervention 1) educates physicians and patients about genomics, 2) enables direct-to-patient return-of- results, 3) provides physicians with patient-specific results and resources for interpretation, and 4) facilitates sharing of genomic results within families. We hypothesize that intervention use will result in higher rates of uptake of high-quality, genetically guided care. We will test our hypothesis in a randomized controlled trial among academic and community physicians who use WES for their patients. Next, we will use an iterative process, with stakeholder engagement, to adapt and pilot test our tool for Spanish and Mandarin speaking patients and for patients who have diabetes (Aim 2). Finally, we will create and assess new, moderated, social networks as a platform for genomic information sharing (Aim 3). Our hypothesis is that providers, patients and family members will engage with the genomic information sharing social networks and find them to be highly useful. Our general approach includes 1) creating the secure social networks, 2) integrating the networks into our e-Health intervention, and 3) using complementary methods, such as interviews and natural language processing, to assess stakeholders' network-related attitudes and network information quality. If successful, we will be well positioned to widely disseminate our e-Health tools. In sum, this work stands to transform how people obtain, process and share genomic information in the context of clinical care. Our tools reconceive genetic communication to allow for multi-directional flow of information, connects multiple stakeholders with one another, and integrates high-quality dynamic web-based resources to improve genomic care. In creating and deploying tools that both respond to and leverage the complexities of our information environment, we intend to transform genomic research and clinical practice. PROJECT NARRATIVE/ RELEVANCE OF PROJECT TO RESEARCH AND PUBLIC HEALTH Widespread utilization of genomic sequencing in medicine creates an urgent need to educate providers and patients. Currently, providers frequently misinterpret genomic information and patients often don't understand their own test results. In order to address this critical need, we propose to design and test multiple e-Health communication tools that will help providers and patients to better understand genomic data, lead to higher quality patient care, and facilitate genomic information sharing within families.",Scalable tools to effectively translate genomic discoveries into the clinic,10003373,R35HG010721,"['Address', 'Area', 'Attitude', 'Caring', 'Cessation of life', 'Clinic', 'Communication', 'Communication Tools', 'Community Physician', 'Data', 'Diabetes Mellitus', 'Effectiveness', 'Environment', 'Excision', 'Family', 'Family member', 'Genetic', 'Genome', 'Genomic medicine', 'Genomics', 'Health', 'Healthcare', 'Human Genome', 'Information Networks', 'Intervention', 'Interview', 'Lead', 'Malignant Neoplasms', 'Medicine', 'Methods', 'National Human Genome Research Institute', 'Natural Language Processing', 'Organ', 'Patient Care', 'Patients', 'Physicians', 'Positioning Attribute', 'Process', 'Provider', 'Public Health', 'Randomized Controlled Trials', 'Research', 'Research Priority', 'Resources', 'Secure', 'Social Network', 'Sum', 'Test Result', 'Testing', 'Translating', 'Vision', 'Work', 'base', 'clinical care', 'clinical practice', 'design', 'disorder prevention', 'disorder risk', 'eHealth', 'effectiveness testing', 'exome', 'genome sequencing', 'genomic data', 'genomic platform', 'improved', 'multidisciplinary', 'online resource', 'premature', 'tool', 'uptake', 'whole genome']",NHGRI,BECKMAN RESEARCH INSTITUTE/CITY OF HOPE,R35,2020,540906,-0.01644266594863978
"Clinical Research Education in Genome Science (CREiGS) Project Summary/Abstract  The sensitivity and availability of omic technologies have enabled the genomic, transcriptomic and proteomic characterization of disease phenotypes, at the tissue and even the single cell level. This has allowed development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. Patients of providers who have participated in these educational initiatives also benefit as it allows for more rapid integration of genomic study findings into the clinical care setting. Thus, in response to PAR-19-185, we propose to develop and implement the Clinical Research Education in Genome Science (CREiGS) program that will not only focus on the analysis of genomic data, but also on gene-expression data, the integration of these two data types, as well as introductory theory and application of statistical and machine learning methods. Specifically we propose to accomplish the following specific aims: 1. Develop and successfully implement the online and in-person phases of CREiGS to increase the methodologic ingenuity by which researchers tackle important genomics-related clinical problems. 2. Establish a Diversity Recruitment External Advisory Board to ensure that the most effective strategies are employed to recruit URM doctoral students, postdoctoral fellows, and faculty from academic institutions nationwide into CREiGS. 3. Enhance the dissemination phase of CREiGS by packaging and uploading the asynchronous lectures and the online critical thinking/problem solving assessments with solutions for publicly available, online teaching resources. 4. Implement effective methods to evaluate the efficacy of CREiGS by examining:1) the participants' grasp of the CREiGS core competencies, 2) the clarity and quality of the curriculum, 3) program logistics and operation, and 4) the participants' short-term and long-term success attributed to participation in CREiGS. In summary, we posit that CREiGS will provide participants with a solid foundation in genomics science to answer complex, clinical questions. We believe that CREiGS supports the mission of the NHGRI by providing researchers with rigorous training to “accelerate medical breakthroughs that improve human health.” Project Narrative The sensitivity and availability of omic technologies have allowed for the development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. The overall goal of the Clinical Research Education in Genome Science program is to increase the methodologic ingenuity of students, postdoctoral fellows, and faculty from academic institutions nationwide through a solid foundation in genomics science to answer complex, clinical research questions and improve patient care.",Clinical Research Education in Genome Science (CREiGS),9934567,R25HG011021,"['Area', 'Biomedical Research', 'Cells', 'Clinical', 'Clinical Data', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Critical Thinking', 'Data', 'Data Analyses', 'Development', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Hour', 'Human', 'Hybrids', 'Institution', 'Knowledge', 'Logistics', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Human Genome Research Institute', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Persons', 'Phase', 'Phenotype', 'Play', 'Postdoctoral Fellow', 'Problem Solving', 'Proteomics', 'Provider', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Role', 'Single Nucleotide Polymorphism', 'Solid', 'Statistical Methods', 'Students', 'Technology', 'Tissues', 'Training', 'Translational Research', 'Treatment outcome', 'Underrepresented Minority', 'Underserved Population', 'Update', 'cancer therapy', 'clinical care', 'clinical efficacy', 'computerized tools', 'data integration', 'data management', 'disease phenotype', 'disorder subtype', 'doctoral student', 'education research', 'genetic analysis', 'genome sciences', 'genomic data', 'grasp', 'health disparity', 'improved', 'individual patient', 'innovation', 'lectures', 'machine learning method', 'operation', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'response', 'statistical and machine learning', 'success', 'theories', 'therapy development', 'tool', 'transcriptomics', 'treatment optimization', 'virtual']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R25,2020,161662,-0.04612207738752487
"Highly Portable and Cloud-Enabled Neuroimaging Research: Confronting Ethics Challenges in Field Research with New Populations Project Summary / Abstract  This 4-year Neuroethics R01 based at the University of Minnesota (UMN) will convene a national Working Group of top neuroethics, neurolaw, and neuroscience experts to conduct empirical research and generate evidence-based consensus recommendations for the ethical conduct of population research using highly portable, cloud-enabled MRI in new and diverse populations in field settings. NIH is supporting the development of both high-field portable MRI (3U01EB025153-02S2, PI: Garwood), and ultra- low field MRI (P41EB015896, PI: Rosen). As portable MRI develops quickly, guidance is urgently needed on unresolved ethical, legal, and social issues (ELSI). This R01 project builds on two NIH Administrative Supplements that have preliminarily identified the most pressing unresolved ELSI issues: (1) informed consent; (2) data security and privacy; (3) establishing local capacity to interpret and communicate neuroimaging data; (4) extensive reliance on cloud-based artificial intelligence (AI) for data analysis; (5) potential bias of interpretive algorithms in diverse populations; (6) return of research results and incidental (or secondary) findings to research participants; and (7) responding to participant requests for access to their data.  Building on this preliminary work, Aim 1 will utilize survey research to inform a systematic Working Group (WG) process described in Aim 2. In Aim 1a, we will survey the U.S. general public, including over- sampling of rural, older adult, non-Hispanic African American, Hispanic/Latino, and economically disadvantaged respondents, to probe likely research use cases, issues they raise, potential solutions, and willingness to participate in research. In Aim 1b, we will survey expert stakeholders to elicit views on current/future research use cases and how to address the ELSI challenges. Expert stakeholders will be from 5 key groups: (1) researchers utilizing brain MRI and scientists developing new MRI technology; (2) neuroethics and legal scholars; (3) industry stakeholders; (4) leaders in regulatory agencies and standard-setting organizations; and (5) leaders in patient advocacy organizations. Aim 2 builds on Aim 1 to generate evidence-based consensus guidance on the ethical conduct of research in the field using highly portable, cloud-enabled neuroimaging. In Aim 2a, we will use a modified Delphi method to elicit initial WG views on issue priorities, research use cases, and potential recommendations, and will develop an Annotated Bibliography. In Aim 2b, the WG will pursue a structured process of analysis and consensus building that is well-established in bioethics and law, in order to identify best practices and formulate recommendations informed by the Aim 1 work. In Aim 2c, we will solicit feedback on our recommendations from expert readers and through a major public conference. Project products will include: an online Annotated Bibliography, WG consensus recommendations, individual targeted articles, published empirical analyses, a webcast public conference, a symposium issue of a peer-reviewed journal, online access to our work, and wide dissemination. Project Narrative This innovative 4-year project based at the University of Minnesota will convene a national Working Group of top neuroethics, neurolaw, and neuroscience experts to conduct empirical research and generate evidence- based consensus recommendations for the ethical conduct of research using highly portable, cloud-enabled MRI in new and diverse populations in field settings. Highly-portable MRI, a transformative technology supported by the NIH BRAIN Initiative, will allow researchers to conduct population-based neuroscience research, including racial and ethnic minorities, rural, and socioeconomically disadvantaged populations that are currently underrepresented in neuroimaging research, and will accelerate research on brain biomarkers for neurodegeneration. The project team will address fundamental challenges in field-based neuroimaging research such as informed consent, data privacy, and return of results; produce Working Group consensus recommendations and targeted individual articles; publish empirical analyses; create a symposium issue of a peer-reviewed journal presenting project publications; create a publicly accessible Annotated Bibliography; produce a webcast and videotaped public conference; build an online portal offering access to our work; and conduct wide dissemination.",Highly Portable and Cloud-Enabled Neuroimaging Research: Confronting Ethics Challenges in Field Research with New Populations,10035136,RF1MH123698,"['Address', 'Administrative Supplement', 'African American', 'Algorithms', 'Artificial Intelligence', 'BRAIN initiative', 'Bibliography', 'Bioethics', 'Biological Markers', 'Brain', 'Computer software', 'Consensus', 'Consultations', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Development', 'Economically Deprived Population', 'Elderly', 'Empirical Research', 'Ethical Issues', 'Ethics', 'Feedback', 'Foundations', 'General Population', 'Group Processes', 'Hispanics', 'Individual', 'Industry', 'Informed Consent', 'International', 'Interview', 'Journals', 'Latino', 'Laws', 'Legal', 'Magnetic Resonance Imaging', 'Measures', 'Mechanics', 'Methods', 'Minnesota', 'Nerve Degeneration', 'Neurology', 'Neurosciences', 'Neurosciences Research', 'Not Hispanic or Latino', 'Participant', 'Patient Representative', 'Patient advocacy', 'Peer Review', 'Policies', 'Population', 'Population Heterogeneity', 'Population Research', 'Process', 'Professional Organizations', 'Publications', 'Publishing', 'Radiology Specialty', 'Reader', 'Recommendation', 'Research', 'Research Design', 'Research Personnel', 'Respondent', 'Review Literature', 'Rural', 'Sampling', 'Scientist', 'Source', 'Structure', 'Subgroup', 'Surveys', 'Technology', 'Testing', 'Travel', 'United States National Institutes of Health', 'Universities', 'Videotape', 'Work', 'advocacy organizations', 'base', 'cloud based', 'data privacy', 'demographics', 'disadvantaged population', 'ethical legal social implication', 'ethnic minority population', 'evidence base', 'health disparity', 'innovation', 'meetings', 'neuroethics', 'neuroimaging', 'population based', 'portability', 'racial minority', 'socioeconomic disadvantage', 'symposium', 'web portal', 'willingness', 'working group']",NIMH,UNIVERSITY OF MINNESOTA,RF1,2020,25577,-0.010882268279533966
"Highly Portable and Cloud-Enabled Neuroimaging Research: Confronting Ethics Challenges in Field Research with New Populations Project Summary / Abstract  This 4-year Neuroethics R01 based at the University of Minnesota (UMN) will convene a national Working Group of top neuroethics, neurolaw, and neuroscience experts to conduct empirical research and generate evidence-based consensus recommendations for the ethical conduct of population research using highly portable, cloud-enabled MRI in new and diverse populations in field settings. NIH is supporting the development of both high-field portable MRI (3U01EB025153-02S2, PI: Garwood), and ultra- low field MRI (P41EB015896, PI: Rosen). As portable MRI develops quickly, guidance is urgently needed on unresolved ethical, legal, and social issues (ELSI). This R01 project builds on two NIH Administrative Supplements that have preliminarily identified the most pressing unresolved ELSI issues: (1) informed consent; (2) data security and privacy; (3) establishing local capacity to interpret and communicate neuroimaging data; (4) extensive reliance on cloud-based artificial intelligence (AI) for data analysis; (5) potential bias of interpretive algorithms in diverse populations; (6) return of research results and incidental (or secondary) findings to research participants; and (7) responding to participant requests for access to their data.  Building on this preliminary work, Aim 1 will utilize survey research to inform a systematic Working Group (WG) process described in Aim 2. In Aim 1a, we will survey the U.S. general public, including over- sampling of rural, older adult, non-Hispanic African American, Hispanic/Latino, and economically disadvantaged respondents, to probe likely research use cases, issues they raise, potential solutions, and willingness to participate in research. In Aim 1b, we will survey expert stakeholders to elicit views on current/future research use cases and how to address the ELSI challenges. Expert stakeholders will be from 5 key groups: (1) researchers utilizing brain MRI and scientists developing new MRI technology; (2) neuroethics and legal scholars; (3) industry stakeholders; (4) leaders in regulatory agencies and standard-setting organizations; and (5) leaders in patient advocacy organizations. Aim 2 builds on Aim 1 to generate evidence-based consensus guidance on the ethical conduct of research in the field using highly portable, cloud-enabled neuroimaging. In Aim 2a, we will use a modified Delphi method to elicit initial WG views on issue priorities, research use cases, and potential recommendations, and will develop an Annotated Bibliography. In Aim 2b, the WG will pursue a structured process of analysis and consensus building that is well-established in bioethics and law, in order to identify best practices and formulate recommendations informed by the Aim 1 work. In Aim 2c, we will solicit feedback on our recommendations from expert readers and through a major public conference. Project products will include: an online Annotated Bibliography, WG consensus recommendations, individual targeted articles, published empirical analyses, a webcast public conference, a symposium issue of a peer-reviewed journal, online access to our work, and wide dissemination. Project Narrative This innovative 4-year project based at the University of Minnesota will convene a national Working Group of top neuroethics, neurolaw, and neuroscience experts to conduct empirical research and generate evidence- based consensus recommendations for the ethical conduct of research using highly portable, cloud-enabled MRI in new and diverse populations in field settings. Highly-portable MRI, a transformative technology supported by the NIH BRAIN Initiative, will allow researchers to conduct population-based neuroscience research, including racial and ethnic minorities, rural, and socioeconomically disadvantaged populations that are currently underrepresented in neuroimaging research, and will accelerate research on brain biomarkers for neurodegeneration. The project team will address fundamental challenges in field-based neuroimaging research such as informed consent, data privacy, and return of results; produce Working Group consensus recommendations and targeted individual articles; publish empirical analyses; create a symposium issue of a peer-reviewed journal presenting project publications; create a publicly accessible Annotated Bibliography; produce a webcast and videotaped public conference; build an online portal offering access to our work; and conduct wide dissemination.",Highly Portable and Cloud-Enabled Neuroimaging Research: Confronting Ethics Challenges in Field Research with New Populations,10035136,RF1MH123698,"['Address', 'Administrative Supplement', 'African American', 'Algorithms', 'Artificial Intelligence', 'BRAIN initiative', 'Bibliography', 'Bioethics', 'Biological Markers', 'Brain', 'Computer software', 'Consensus', 'Consultations', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Development', 'Economically Deprived Population', 'Elderly', 'Empirical Research', 'Ethical Issues', 'Ethics', 'Feedback', 'Foundations', 'General Population', 'Group Processes', 'Hispanics', 'Individual', 'Industry', 'Informed Consent', 'International', 'Interview', 'Journals', 'Latino', 'Laws', 'Legal', 'Magnetic Resonance Imaging', 'Measures', 'Mechanics', 'Methods', 'Minnesota', 'Nerve Degeneration', 'Neurology', 'Neurosciences', 'Neurosciences Research', 'Not Hispanic or Latino', 'Participant', 'Patient Representative', 'Patient advocacy', 'Peer Review', 'Policies', 'Population', 'Population Heterogeneity', 'Population Research', 'Process', 'Professional Organizations', 'Publications', 'Publishing', 'Radiology Specialty', 'Reader', 'Recommendation', 'Research', 'Research Design', 'Research Personnel', 'Respondent', 'Review Literature', 'Rural', 'Sampling', 'Scientist', 'Source', 'Structure', 'Subgroup', 'Surveys', 'Technology', 'Testing', 'Travel', 'United States National Institutes of Health', 'Universities', 'Videotape', 'Work', 'advocacy organizations', 'base', 'cloud based', 'data privacy', 'demographics', 'disadvantaged population', 'ethical legal social implication', 'ethnic minority population', 'evidence base', 'health disparity', 'innovation', 'meetings', 'neuroethics', 'neuroimaging', 'population based', 'portability', 'racial minority', 'socioeconomic disadvantage', 'symposium', 'web portal', 'willingness', 'working group']",NIMH,UNIVERSITY OF MINNESOTA,RF1,2020,1571577,-0.010882268279533966
"What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives,10162151,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Participant', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,100000,-0.04263429141828635
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,10017004,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'clinical center', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'mHealth', 'machine learning method', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2020,725375,-0.025630263706124434
"S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450 This application is requesting funds to purchase the Aperio™ GT-450 digital pathology slide scanner from Leica Biosystems. The requested instrumentation will be located in the Pathology and Biobanking Core of the Lester and Sue Smith Breast Center at Baylor College of Medicine (BCM). The predominant use of the Aperio scanner will be research-based whole slide imaging (WSI) and analysis of patient specimens, patient-derived xenograft (PDX) cancer models, and pre-clinical investigations on various animal- and cell-line model systems. All user projects have large sample cohorts that require high throughput, high-resolution scanning and image analysis. High capacity and improved scanning with dynamic focusing makes the GT-450 microscope scanner well-suited and the most cost-effective for use in the proposed projects. An underlying theme in the studies selected for Aperio scanner-supported services integrates novel biomarker and molecular pathway discovery with spatial morphological characterization, a necessary process to investigate heterogeneity in disease states. This instrument leverages high-throughput scanning capability with open-source, fully customizable machine-learning analytics to meet the evolving needs of investigators at Baylor College of Medicine, in particular faculty groups studying mechanisms of cancer cell dynamics and the development of new therapeutic targets. Expansion of systems biology and precision medicine research is an essential component of the college’s strategic roadmap. The Aperio GT-450 is critically needed as we modernize our laboratory offerings and capabilities; the acquisition of this digital scanner will strengthen existing research programs underway and establish new, collaborative research opportunities and directions within Baylor College of Medicine and surrounding institutions. To address the growing demand for integrating quantitative spatial assessment of biomarkers with molecular pathway discovery, we request funding for the Leica Aperio GT-450 digital microscope scanner. This instrument will facilitate research that seeks to better understand molecular mechanisms of tumorigenesis in the context of its spatial environment and will be critical for the development of clinically correlative biomarkers for next generation precision medicine research initiatives at Baylor College of Medicine.",S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450,9940426,S10OD028671,"['Animals', 'Biological Models', 'Breast', 'Cancer Model', 'Cell Line', 'Development', 'Disease', 'Faculty', 'Funding', 'Grant', 'Heterogeneity', 'Image Analysis', 'Institution', 'Laboratories', 'Machine Learning', 'Medicine', 'Microscope', 'Modernization', 'Molecular', 'Morphology', 'Pathology', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Slide', 'Specimen', 'Systems Biology', 'Xenograft procedure', 'base', 'biobank', 'cancer cell', 'clinical investigation', 'cohort', 'college', 'cost effective', 'digital', 'digital pathology', 'improved', 'instrument', 'instrumentation', 'new therapeutic target', 'novel marker', 'open source', 'pre-clinical', 'precision medicine', 'programs', 'whole slide imaging']",OD,BAYLOR COLLEGE OF MEDICINE,S10,2020,477043,-0.022756152871053297
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,-0.033151334434797
"Developing novel technologies that ensure privacy and security in biomedical data science research Data science holds the promise of enabling new pathways to discovery and can improve the understanding, prevention and treatment of complex disorders such as cancer, diabetes, substance abuse, etc., which are significantly on the rise. The promise of data science can be fully realized only when collected data can be collaboratively shared and analyzed. However, the widespread increases in healthcare data breaches due to inappropriate access as well as the increasing number of novel privacy attacks restrict institutions from sharing data. Indeed, in some cases, the results of the analysis can themselves lead to significant privacy harm. The success of the data commons depends on ensuring the maximal access to data, subject to all of the patient privacy requirements including those mandated by legislation, and all of the constraints of the organization collecting the data itself. While there are existing solutions that can solve parts of the problem, there are significant challenges in truly incorporating these into comprehensive working solutions that are usable by the biomedical research community, and new challenges brought on by modern techniques such as deep learning. The long-term goal of this research is to develop technologies that can holistically enable data sharing while respecting privacy and security considerations and to ensure that they are implemented in existing platforms that have widespread acceptance in the research community. Towards this, the objective of this project is to develop complementary solutions for risk inference, distributed learning, and access control that can enable different modalities of data sharing. The problems studied are general in nature and will evolve depending on research successes and new impediments that arise. The proposed program of research is significant since lack of access to biomedical data can lead to fragmentation of care, resulting in higher economic and social costs, and is a significant impediment to biomedical research. The project will result in open-source, freely available software tools that will be integrated into widely used data collection, cohort identification, and distributed analytics platforms. There are several ongoing collaborations that will serve as initial pilot customers to provide use cases, identify the requirements, evaluate results, and in general validate the developed solutions. Project Narrative Statement of Relevance to Public Health Being able to ensure privacy and security while enabling data sharing and analysis is critical to pave the way forward for public health research and improve our understanding of diseases. The proposed work will address the challenges that impede the use of data across all of the different modalities of data sharing. The integration into existing platforms will ensure that the developed models, tools, and solutions directly impact the research community and improve public health interventions.",Developing novel technologies that ensure privacy and security in biomedical data science research,9851602,R35GM134927,"['Address', 'Biomedical Research', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Science', 'Diabetes Mellitus', 'Disease', 'Economics', 'Ensure', 'Goals', 'Healthcare', 'Institution', 'Lead', 'Learning', 'Malignant Neoplasms', 'Modality', 'Modeling', 'Modernization', 'Nature', 'Pathway interactions', 'Prevention', 'Privacy', 'Public Health', 'Research', 'Risk', 'Security', 'Software Tools', 'Statutes and Laws', 'Substance abuse problem', 'Techniques', 'Technology', 'Work', 'biomedical data science', 'care fragmentation', 'cohort', 'cost', 'data sharing', 'deep learning', 'improved', 'new technology', 'novel', 'open source', 'patient privacy', 'programs', 'public health intervention', 'public health research', 'social', 'success', 'tool']",NIGMS,RUTGERS THE STATE UNIV OF NJ NEWARK,R35,2020,382108,-0.026396940350982986
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9888390,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2020,1932440,-0.02546562253978306
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9902428,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science Core', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2020,1492946,-0.034130186709346345
"N3C & All of Us Research Program Collaborative Project Project Summary/Abstract The COVID-19 pandemic presents unprecedented clinical and public health challenges. Though institutions collect large amounts of clinical data about COVID-19 cases, these datasets individually might not be diverse enough to draw population level conclusions. Also, statistical, machine learning, and causal analyses are most successful with large-scale data beyond what is available in any given organization. To tackle this problem, NCATS introduced the National COVID Cohort Collaborative (N3C), an open science, community-based initiative to share patient level data for analysis. The initiative requires participating institutions to share information about their COVID-19 patients in a standard-driven way, including demographics, vital signs, diagnoses, laboratory results, medications, and other treatments. The data from multiple institutions will be merged and consolidated, and access will be provided to investigators through a centralized analytical platform. The COVID-19 data sharing collaboration with the N3C initiative offers a mechanism to initiate collaborations with other NIH sponsored data sharing programs, such as the All of Us Research Program (AoURP). This administrative supplement will support efforts to clean and standardize data at VCU, and to transfer it to the N3C data repository. The supplement will also assist in introducing new services at the Wright Center to support our investigators to use the N3C resources. It will also enable collaboration with the AoURP by establishing a pipeline to collect and transmit consented patients' EHR data and by building on existing community outreach pathways to recruit additional participants for the AoURP. The project will be overseen by the PI/Executive Committee and supervised by the Director of Research Informatics. Procedures and services developed at our local CTSA hub will be shared and disseminated to the CTSA network. Project Narrative NIH/NCATS has been working on the National COVID Cohort Collaborative (N3C), which aims to build a centralized national data resource to be used by the research community to study the COVID-19 pandemic and identify potential treatments as the pandemic continues to evolve. The COVID-19 data sharing collaboration with the N3C initiative also offers a mechanism to initiate collaborations with the All of Us Research Program (AoURP). This administrative supplement will support the creation and management of a data extraction and transfer pipeline to the N3C and AoURP data repositories from VCU.",N3C & All of Us Research Program Collaborative Project,10217339,UL1TR002649,"['Administrative Supplement', 'All of Us Research Program', 'COVID-19', 'COVID-19 pandemic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Outreach', 'Consent', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Ecosystem', 'Effectiveness', 'Funding Opportunities', 'Goals', 'Health', 'Health Status', 'Individual', 'Informatics', 'Infrastructure', 'Institution', 'Laboratories', 'Outcomes Research', 'Participant', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Procedures', 'Public Health', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Services', 'Supervision', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'base', 'biomedical informatics', 'clinical center', 'cohort', 'coronavirus disease', 'data resource', 'data sharing', 'data standards', 'data warehouse', 'demographics', 'design', 'improved', 'informatics infrastructure', 'innovation', 'large scale data', 'multi-site trial', 'network informatics', 'open data', 'pandemic disease', 'parent grant', 'programs', 'recruit', 'response', 'statistical and machine learning', 'tool']",NCATS,VIRGINIA COMMONWEALTH UNIVERSITY,UL1,2020,346608,-0.007580507615834965
"A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth This project provides a data science framework and a toolbox of best practices for systematic and reproducible data-driven methods for validating and deriving RDoC constructs with relevance to psychopathology. Despite recent advances in methods for data-driven constructs, results are often hard to reproduce using samples from other studies. There is a lack of systematic statistical methods and analytical design for enhancing reproducibility. To fill this gap, we will develop a data science framework, including novel scalable algorithms and software, to derive and validate RDoC constructs. Although the proposed methods will generally apply to all RDoC domains and constructs, we focus specifically on furthering understanding of the RDoC domains of cognitive control (CC) and attention (ATT) constructs implicated in attention deficit disorder (ADHD) and obsessive-compulsive disorder (OCD). Our application will use multi-modal neuroimaging, behavioral, and clinical/self-report data from large, nationally representative samples from the on Adolescent Brain Cognitive Development (ABCD) study and multiple local clinical samples with ADHD and OCD. Specifically, using the baseline ABCD samples, in aim 1, we will apply and develop methods to assess and validate the current configuration of RDoC for CC and ATT using confirmatory latent variable modeling. We will implement and develop new unsupervised learning methods to construct new computational-driven, brain-based domains from multi-modal image data. In Aim 2, We will introduce network analysis (via Gaussian graphical models) to characterize heterogeneity in the interrelationship of RDoC measurements due to observed characteristics (i.e., age and sex). We will further model the heterogeneity of the population due to unobserved characteristics by introducing the data-driven precision phenotypes, which are the subgroup of participants with similar RDoC dimensions. We propose a Hierarchical Bayesian Generative Model and scalable algorithm for simultaneous dimension reduction and identify precision phenotypes. The model also serves as a tool to transfer information from the community sample ABCD to local clinical enriched studies. In aim 3, we will utilize the follow-up samples from ABCD and local clinical enriched data sets to validate the results from Aims 1 and 2 and assess the clinical utility of the precision phenotypes in predicting psychological development in follow-up time. Our project will provide a suite of analytical tools to validate existing RDoC constructs and derive new, reproducible constructs by accounting for various sources of heterogeneity. To advance the understanding of psychopathology using dimensional constructs of measurements from multiple units of analysis, we propose reproducible statistical framework for validating and deriving RDoC constructs with relevance to psychopathology. We will use multi-modal neuroimaging, behavioral and clinical/self-report data from multiple samples to develop this framework. The design of our study consists of analyzing large, nationally representative samples, validating the results in local clinically enriched samples, and transfer information from the large community samples to local clinical samples.",A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth,10058921,R01MH124106,"['11 year old', 'Accounting', 'Adolescent', 'Age', 'Algorithmic Software', 'Algorithms', 'Attention', 'Attention Deficit Disorder', 'Base of the Brain', 'Behavioral', 'Brain', 'Characteristics', 'Child', 'Chronology', 'Clinical', 'Clinical Data', 'Communities', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Dimensions', 'Ensure', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Goals', 'Heterogeneity', 'Image', 'Knowledge', 'Learning', 'Link', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Obsessive-Compulsive Disorder', 'Participant', 'Pathway Analysis', 'Patient Self-Report', 'Phenotype', 'Population Heterogeneity', 'Prediction of Response to Therapy', 'Psychological Transfer', 'Psychopathology', 'Reproducibility', 'Reproducibility of Results', 'Research Domain Criteria', 'Sampling', 'Source', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'Time', 'Variant', 'Youth', 'age effect', 'analytical tool', 'autoencoder', 'base', 'biological sex', 'cognitive control', 'cognitive development', 'deep learning', 'design', 'follow up assessment', 'follow-up', 'high dimensionality', 'independent component analysis', 'insight', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'multimodality', 'network models', 'neuroimaging', 'novel', 'psychologic', 'response', 'sex', 'tool', 'unsupervised learning']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2020,710101,-0.017689762455756958
"Meta-analysis in human brain mapping This is the competing renewal of R01MH074457-13, which sustains the BrainMap Project (www.brainmap.org). The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data. To date, the BrainMap Project has designed and populated two coordinate-based databases: 1) a task-activation repository (TA DB); and, 2) a voxel-based morphometry repository (VBM DB). The TA DB contains >17,200 experiments, collectively representing > 78,000 subjects and > 110 task- activation paradigms. The VBM DB contains > 3,100 experiments, collectively representing > 81,000 subjects with > 80 psychiatric, neurologic and developmental disorders with ICD-10 coding. The BrainMap Project has created, optimized and validated an integrated pipeline of multi-platform (Javascript), open-access tools to curate (Scribe), filter and retrieve (Sleuth), analyze (GingerALE), visualize (Mango) and interpret analysis output (BrainMap meta-data plugins for Mango). Several network-modeling approaches have been applied to BrainMap data -- MACM, independent components analysis (ICA), graph theory modeling (GTM), author-topic modeling (ATM), structural equation modeling (SEM), and connectivity-based parcellation (CBP) – but none are yet pipeline components. Utilization of these CBMA resources is substantial: BrainMap software, data and meta-data have been used in > 825 peer-reviewed publications. Of these, > 350 were published within the current funding period (April 2015-March 2019; brainmap.org/pubs). In this competing renewal, four tool- development aims are proposed, each of which extends this high-impact research resource. Aim 1. Database Expansion. BrainMap data repositories will be expanded. Aim 2. Meta-analytic Network Modeling. Network modeling will be added to the BrainMap pipeline. Aim 3. Large-Scale Simulations, Comparisons and Validations. Data simulations, characterizations and validations will be performed. Aim 4. Meta-data Inferential tools. Tools for mining BrainMap’s location-linked meta-data will be expanded. Data Sharing Plan. BrainMap data, meta-data, pipeline tools, and templates created by whole-database modeling (e.g., ICA and ATM network masks) are shared at BrainMap.org. Of all new data entries, more than half are contributed by BrainMap users, i.e., community data sharing via BrainMap.org. For community-coded entries, the BrainMap team provides curation and quality control. Comprehensive database images (database dumps) are available to tool developers through Collaborative Use Agreements. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data  sets, metadata, computational tools, and related resources that enable coordinate-­based meta-­analyses  (CBMA), meta-­analytic connectivity modeling (MACM), meta-­data informed interpretation (“decoding”) of  imaging results, and meta-­analytic priors for mining (including machine learning) primary (per-­subject)  neuroimaging data.    ",Meta-analysis in human brain mapping,10056029,R56MH074457,"['Agreement', 'Area', 'Brain', 'Brain Mapping', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Set', 'Databases', 'Disease', 'Educational workshop', 'Equation', 'Functional disorder', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Institution', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Java', 'Link', 'Location', 'Machine Learning', 'Mango - dietary', 'Masks', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Output', 'Peer Review', 'Plug-in', 'Publications', 'Publishing', 'Quality Control', 'Research Domain Criteria', 'Resources', 'Rest', 'Site', 'Software Framework', 'Specificity', 'Structure', 'Training', 'Universities', 'Validation', 'base', 'candidate marker', 'computerized tools', 'data pipeline', 'data sharing', 'data warehouse', 'design', 'developmental disease', 'experimental study', 'graph theory', 'independent component analysis', 'interest', 'large scale simulation', 'morphometry', 'nervous system disorder', 'network architecture', 'network models', 'neuroimaging', 'neuropsychiatric disorder', 'repository', 'simulation', 'tool', 'tool development']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R56,2020,543396,-0.010265053985840553
"Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets PROJECT SUMMARY/ABSTRACT Recent advances in high-resolution mass spectrometry (HRMS) instrumentation have not been fully leveraged to upgrade the information content of metabolomics datasets obtained from stable isotope labeling studies. This is primarily due to lack of validated software tools for extracting and interpreting isotope enrichments from HRMS datasets. The overall objective of the current application is to develop tools that enable the metabolomics community to fully leverage stable isotopes to profile metabolic network dynamics. Two new tools will be implemented within the open-source OpenMS software library, which provides an infrastructure for rapid development and dissemination of mass spectrometry software. The first tool will automate tasks required for extracting isotope enrichment information from HRMS datasets, and the second tool will use this information to group ion peaks into interaction networks based on similar patterns of isotope labeling. The tools will be validated using in-house datasets derived from metabolic flux studies of animal and plant systems, as well as through feedback from the metabolomics community. The rationale for the research is that the software tools will enable metabolomics investigators to address important questions about pathway dynamics and regulation that cannot be answered without the use of stable isotopes. The first aim is to develop a software tool to automate data extraction and quantification of isotopologue distributions from HRMS datasets. The software will provide several key features not included in currently available metabolomics software: i) a graphical, interactive user interface that is appropriate for non-expert users, ii) support for native instrument file formats, iii) support for samples that are labeled with multiple stable isotopes, iv) support for tandem mass spectra, and v) support for multi-group or time-series comparisons. The second aim is to develop a companion software that applies machine learning and correlation-based algorithms to group unknown metabolites into modules and pathways based on similarities in isotope labeling. The third aim is to validate the tools through comparative analysis of stable isotope labeling in test standards and samples from animal and plant tissues, including time-series and dual-tracer experiments. A variety of collaborators and professional working groups will be engaged to test and validate the software, and the tools will be refined based on their feedback. The proposed research is exceptionally innovative because it will provide the advanced software capabilities required for both targeted and untargeted analysis of isotopically labeled metabolites, but in a flexible and user-friendly environment. The research is significant because it will contribute software tools that automate and standardize the data processing steps required to extract and utilize isotope enrichment information from large-scale metabolomics datasets. This work will have an important positive impact on the ability of metabolomics investigators to leverage information from stable isotopes to identify unknown metabolic interactions and quantify flux within metabolic networks. In addition, it will enable entirely new approaches to study metabolic dynamics within biological systems. PROJECT NARRATIVE The proposed research is relevant to public health because it will develop novel software tools to quantify and interpret data from stable isotope labeling experiments, which can be used to uncover relationships between metabolites and biochemical pathways. These tools have potential to accelerate progress toward identifying the causes and cures of many important diseases that impact metabolism.",Tools for Leveraging High-Resolution MS Detection of Stable Isotope Enrichments to Upgrade the Information Content of Metabolomics Datasets,10002192,U01CA235508,"['Address', 'Algorithms', 'Animals', 'Biochemical Pathway', 'Biological', 'Communities', 'Companions', 'Complement', 'Computer software', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Environment', 'Feedback', 'Infrastructure', 'Ions', 'Isotope Labeling', 'Isotopes', 'Knowledge', 'Label', 'Letters', 'Libraries', 'Machine Learning', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolism', 'Methods', 'Modeling', 'Network-based', 'Outcome', 'Pathway interactions', 'Pattern', 'Plants', 'Process', 'Public Health', 'Publishing', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Series', 'Software Tools', 'Stable Isotope Labeling', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Tracer', 'Validation', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'data standards', 'experience', 'experimental study', 'file format', 'flexibility', 'improved', 'innovation', 'instrument', 'instrumentation', 'metabolic abnormality assessment', 'metabolic phenotype', 'metabolic profile', 'metabolomics', 'novel', 'novel strategies', 'open source', 'operation', 'stable isotope', 'tandem mass spectrometry', 'tool', 'user-friendly', 'working group']",NCI,VANDERBILT UNIVERSITY,U01,2020,427122,-0.009631316623124933
"Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design PROJECT SUMMARY/ABSTRACT Molecular simulation is a powerful tool to predict the properties of biomolecules, interpret biophysical experiments, and design small molecules or biomolecules with therapeutic utility. However, a number of obstacles have impeded the development of quantitative, cloud-scale research workﬂows involving biomolecular simulation. Two main ob- stacles are the insufﬁcient accuracy of current atomistic models for biomolecules and small molecule therapeutics and the lack of interoperability in simulation toolchains used in both academic and industrial biomolecular research. Our original R01, “Open Data-driven Infrastructure for Building Biomolecular Force Fields for Predictive Bio- physics and Drug Design,” seeks to solve the ﬁrst problem. It helps fund our effort, the Open Force Field Initiative (https://openforceﬁeld.org) to develop open, extensible, and shared software and data infrastructure, implementing statistically robust methods of parameterizing force ﬁelds and choosing new force ﬁelds in a statistically sound manner. This work is designed to create not just a new generation of force ﬁelds, but an open technology to continue advancing force ﬁeld science. However, even with improved molecular models, putting together complete workﬂows of biomolecular simulations involves interfacing substantial numbers of different tools. However the majority of the existing molecular simulation workﬂows are mutually incompatible, with differing representations of the molecular models. The Open Force Field Initiative effort already includes the development of molecular data structures that we can ex- port into existing molecular simulation tools. We propose to extend the existing scope of our R01 to create an extensible common molecular simulation representation and translators to and from this representation. Such a set of tools will immediately make it signiﬁcantly easier to combine the disparate workﬂows developed for different sets of molecular simulation tools. Researchers will be able to set up and build the biophysical simulations using their usual tools, but run and analyze them with currently incompatible tools, enabling better matching of computational resources and methods to problems. It will help avoid trapping in a single software framework, and enable combinations of functionalities previously impossible without substantial developer time and effort. We will (Aim 1) work with partners to generalize our modular, extensible object model for representing parameterized biomolecular systems in a manner that accommodates the force ﬁeld terms currently supported by most popular biomolecular simulation packages. We will engineer it to be extensible to advanced interaction forms, such as polarizability and other multibody terms, and machine learning models for intermolecular forces. We will (Aim 2): enable easy conversion between components of molecular simulation workﬂows by allowing other molecular simulation packages to easily store their representations in this data model, developing converters that can import/export this object model to multiple popular ﬁle formats, focusing initially on OpenMM, AMBER, CHARMM, and GROMACS. We will demonstrate the utility of this interface in cloud-ready workﬂows. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life carry out their functions and to design new medications. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms. This supplement will make it much easier for molecular simulation workﬂows to interoperate with each other in large-scale workﬂows.",Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design,10166314,R01GM132386,"['Affinity', 'Binding', 'Biophysics', 'COVID-19', 'Collaborations', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA', 'Development', 'Drug Design', 'Ecosystem', 'Engineering', 'Funding', 'Generations', 'Human', 'Individual', 'Industrialization', 'Infrastructure', 'Language', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Motion', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Problem Solving', 'Property', 'Proteins', 'Pythons', 'RNA', 'Readability', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Science', 'Scientist', 'Software Framework', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'Writing', 'biomaterial interface', 'computing resources', 'data infrastructure', 'data modeling', 'design', 'experimental study', 'file format', 'improved', 'interoperability', 'molecular modeling', 'open data', 'simulation', 'small molecule', 'small molecule therapeutics', 'software infrastructure', 'sound', 'structured data', 'tool']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,225000,-0.014705096042026742
"IEEE International Symposium on Biomedical Imaging (ISBI) 2020 Project Summary This R13 application will provide travel support for competitively selected U.S.-based students, postdoctoral fellows, and junior faculty to present their work at the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 to be held April 4-8, 2020 at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa. ISBI is a scientific conference dedicated to mathematical, algorithmic, and computational aspects of biological and biomedical imaging, across all scales of observation (from microscopic to whole-body imaging) and is sponsored by both the IEEE Signal Processing Society (SPS) and the IEEE Engineering in Medicine and Biology Society (EMBS). It attracts approximately 600-700 attendees each year involved in biomedical imaging research and development from academic institutions, government laboratories, and private companies. Since its inception in 2002, ISBI has become a leading international conference bringing together researchers from diverse algorithmic fields, applications, modalities, and size scales, to facilitate cross-fertilization of ideas. ISBI, like other IEEE SPS and EMBS conferences, requires submission and review of four-page papers which are peer-reviewed much like journal articles. In addition to oral and poster presentations of peer-reviewed papers in multiple tracks during the main conference, ISBI 2020 will also include special student events, tutorials, a “clinical day” emphasizing multi-disciplinary presentations/collaborations, plenary talks, workshops, and onsite grand challenges. Recipients of the travel awards will be competitively selected based on need and scientific excellence. U.S.- based students, postdoctoral fellows, and junior faculty with accepted papers will be eligible to apply. We will be particularly supportive in providing travel awards to women and under-represented groups to help increase the diversity of the attendees. We anticipate that the travel awards will provide sufficient funding to the awardees to make the cost-benefit ratio for attendance extremely favorable. Through their attendance at ISBI, the awardees will benefit through their exposure to the simultaneous breadth and depth of topics offered at ISBI (presented by a mixture of leaders in the field as well as those early in their careers), their experience of presenting their work at an international conference, and their interactions and discussions with other attendees and leaders in the field. The conference as a whole will also benefit by not only enabling the high-quality work of the attendees to be presented but by also enabling an increased attendance of (and discussions/ideas/interactions with) U.S.-based students, postdoctoral fellows, and early career faculty with diverse backgrounds. Project Narrative This application requests funds to provide travel support for students, postdoctoral fellows, and/or early- career faculty to attend and participate in the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 conference, to be held at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa, April 4-8, 2020. The conference covers many of the mathematical and computational aspects of biological and biomedical imaging problems of high relevance to human health, and hence is of high relevance to the interests of the National Institutes of Health.",IEEE International Symposium on Biomedical Imaging (ISBI) 2020,9914410,R13EB029304,"['Address', 'Algorithms', 'Appointment', 'Area', 'Artificial Intelligence', 'Award', 'Biological', 'Biological Models', 'Biology', 'Biomedical Computing', 'Breeding', 'Budgets', 'Cities', 'Clinical', 'Collaborations', 'Complement', 'Computational algorithm', 'Computer Models', 'Costs and Benefits', 'Data', 'Educational workshop', 'Engineering', 'Event', 'Exposure to', 'Faculty', 'Fertilization', 'Funding', 'Future', 'Generations', 'Goals', 'Government', 'Grant', 'Growth', 'Health', 'Human', 'Image', 'Image Analysis', 'Imaging problem', 'Institution', 'International', 'Iowa', 'Laboratories', 'Location', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Microscopic', 'Modality', 'Modeling', 'Oral', 'Paper', 'Participant', 'Peer Review', 'Postdoctoral Fellow', 'Privatization', 'Recommendation', 'Request for Applications', 'Research', 'Research Personnel', 'Societies', 'Statistical Models', 'Students', 'Training', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'Visualization', 'Women&apos', 's Group', 'Work', 'authority', 'base', 'bioimaging', 'body system', 'career', 'computerized tools', 'cost', 'early-career faculty', 'experience', 'graduate student', 'imaging modality', 'innovation', 'interest', 'journal article', 'mathematical algorithm', 'meetings', 'member', 'multidisciplinary', 'physical model', 'posters', 'programs', 'reconstruction', 'research and development', 'signal processing', 'student participation', 'success', 'supportive environment', 'symposium', 'whole body imaging']",NIBIB,UNIVERSITY OF IOWA,R13,2020,5000,-0.05071631887155105
"IEEE International Symposium on Biomedical Imaging (ISBI) 2020 Project Summary This R13 application will provide travel support for competitively selected U.S.-based students, postdoctoral fellows, and junior faculty to present their work at the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 to be held April 4-8, 2020 at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa. ISBI is a scientific conference dedicated to mathematical, algorithmic, and computational aspects of biological and biomedical imaging, across all scales of observation (from microscopic to whole-body imaging) and is sponsored by both the IEEE Signal Processing Society (SPS) and the IEEE Engineering in Medicine and Biology Society (EMBS). It attracts approximately 600-700 attendees each year involved in biomedical imaging research and development from academic institutions, government laboratories, and private companies. Since its inception in 2002, ISBI has become a leading international conference bringing together researchers from diverse algorithmic fields, applications, modalities, and size scales, to facilitate cross-fertilization of ideas. ISBI, like other IEEE SPS and EMBS conferences, requires submission and review of four-page papers which are peer-reviewed much like journal articles. In addition to oral and poster presentations of peer-reviewed papers in multiple tracks during the main conference, ISBI 2020 will also include special student events, tutorials, a “clinical day” emphasizing multi-disciplinary presentations/collaborations, plenary talks, workshops, and onsite grand challenges. Recipients of the travel awards will be competitively selected based on need and scientific excellence. U.S.- based students, postdoctoral fellows, and junior faculty with accepted papers will be eligible to apply. We will be particularly supportive in providing travel awards to women and under-represented groups to help increase the diversity of the attendees. We anticipate that the travel awards will provide sufficient funding to the awardees to make the cost-benefit ratio for attendance extremely favorable. Through their attendance at ISBI, the awardees will benefit through their exposure to the simultaneous breadth and depth of topics offered at ISBI (presented by a mixture of leaders in the field as well as those early in their careers), their experience of presenting their work at an international conference, and their interactions and discussions with other attendees and leaders in the field. The conference as a whole will also benefit by not only enabling the high-quality work of the attendees to be presented but by also enabling an increased attendance of (and discussions/ideas/interactions with) U.S.-based students, postdoctoral fellows, and early career faculty with diverse backgrounds. Project Narrative This application requests funds to provide travel support for students, postdoctoral fellows, and/or early- career faculty to attend and participate in the IEEE International Symposium on Biomedical Imaging (ISBI) 2020 conference, to be held at the Coralville Marriott Hotel & Conference Center near the University of Iowa, Iowa City, Iowa, April 4-8, 2020. The conference covers many of the mathematical and computational aspects of biological and biomedical imaging problems of high relevance to human health, and hence is of high relevance to the interests of the National Institutes of Health.",IEEE International Symposium on Biomedical Imaging (ISBI) 2020,9914410,R13EB029304,"['Address', 'Algorithms', 'Appointment', 'Area', 'Artificial Intelligence', 'Award', 'Biological', 'Biological Models', 'Biology', 'Biomedical Computing', 'Breeding', 'Budgets', 'Cities', 'Clinical', 'Collaborations', 'Complement', 'Computational algorithm', 'Computer Models', 'Costs and Benefits', 'Data', 'Educational workshop', 'Engineering', 'Event', 'Exposure to', 'Faculty', 'Fertilization', 'Funding', 'Future', 'Generations', 'Goals', 'Government', 'Grant', 'Growth', 'Health', 'Human', 'Image', 'Image Analysis', 'Imaging problem', 'Institution', 'International', 'Iowa', 'Laboratories', 'Location', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Microscopic', 'Modality', 'Modeling', 'Oral', 'Paper', 'Participant', 'Peer Review', 'Postdoctoral Fellow', 'Privatization', 'Recommendation', 'Request for Applications', 'Research', 'Research Personnel', 'Societies', 'Statistical Models', 'Students', 'Training', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'Visualization', 'Women&apos', 's Group', 'Work', 'authority', 'base', 'bioimaging', 'body system', 'career', 'computerized tools', 'cost', 'early-career faculty', 'experience', 'graduate student', 'imaging modality', 'innovation', 'interest', 'journal article', 'mathematical algorithm', 'meetings', 'member', 'multidisciplinary', 'physical model', 'posters', 'programs', 'reconstruction', 'research and development', 'signal processing', 'student participation', 'success', 'supportive environment', 'symposium', 'whole body imaging']",NIBIB,UNIVERSITY OF IOWA,R13,2020,10000,-0.05071631887155105
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domainagnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10057127,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2020,609068,-0.03595378852018383
"Immersive Virtual Environment Testing Area A wide range of services are provided by the Immersive Virtual Environment Testing Area (IVETA) and its staff including, scientific consultation, technological consultation and support, data collection, study administration, data preparation and analysis, and arranging necessary scientific materials. The IVETA also engages in communication and education about virtual reality and related technologies to the research and health care communities, and to the public. During this reporting year, five Social and Behavioral Research Branch-initiated research projects, and one project initiated within the wider NIH have been served by the IVETA team. The IVETA team has also conducted capacity-building research.   These include:   - A study assessing the influence of patient race/ethnicity and socio-economic status on physician decision-making related to personalized medicine (PI: Bonham): Scientific and technological consultation, data collection, data coding and management - A similar clinical personalized medicine study in a nursing context (PIs: Calzone and Brennan): Scientific and technical consultation, planning and procurement - A project related to encouraging communication about genetic risk among families (PI: Koehly): virtual reality environment planning and consultation, study design consultation - A study related to the influence of genomic information on child feeding (PI; Persky): Data coding and analysis - A study of an intervention to communicate about gene-environment interaction concepts (PI; Persky): virtual environment design consultation and research   - A study about ADHD intervention (PI: Shaw): consultation  - A study about treatments for alcohol use disorder (PI: Leggio): consultation and design   We have also played a more minor role in supporting data analysis in several other projects and analyses.     In addition to directly serving the research needs of the Social and Behavioral Research Branch and other NIH colleagues, the IVETA team also conducts research and assessments to expand its own capabilities. During this reporting year the IVETA team has undertaken a study to assess the feasibility, utility, and added value of olfactory stimuli within a virtual reality-based tool to study parent feeding behavior. We are also undertaking application of machine learning approaches to analyze physical movement behavior in VR. n/a",Immersive Virtual Environment Testing Area,10267140,ZIDHG200384,"['3-Dimensional', 'Area', 'Attention deficit hyperactivity disorder', 'Behavior', 'Behavioral Research', 'Child', 'Clinical', 'Code', 'Communication', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Decision Making', 'Devices', 'Discipline of Nursing', 'Education', 'Environment Design', 'Ethnic Origin', 'Family', 'Feeding behaviors', 'Genetic Risk', 'Genomics', 'Head', 'Healthcare', 'Image', 'Industry', 'Intervention', 'Intervention Studies', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Minor', 'Movement', 'National Human Genome Research Institute', 'Parents', 'Participant', 'Patients', 'Physicians', 'Physiology', 'Play', 'Positioning Attribute', 'Preparation', 'Public Health', 'Race', 'Reporting', 'Research', 'Research Design', 'Research Methodology', 'Research Project Grants', 'Resources', 'Role', 'Services', 'Shapes', 'Socioeconomic Status', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visual', 'alcohol abuse therapy', 'alcohol use disorder', 'base', 'clinical center', 'computer generated', 'design', 'digital', 'feeding', 'gene environment interaction', 'head mounted display', 'healthcare community', 'innovation', 'olfactory stimulus', 'personalized medicine', 'remote location', 'social', 'tool', 'virtual environment', 'virtual library', 'virtual reality', 'virtual reality environment', 'virtual world']",NHGRI,NATIONAL HUMAN GENOME RESEARCH INSTITUTE,ZID,2020,289510,-0.019704923483527925
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Contact PD/PI: Panettieri, Reynold Alexander Project Summary/Abstract Overview Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Page 337 Project Summary/Abstract Contact PD/PI: Panettieri, Reynold Alexander New Jersey Alliance for Clinical and Translational Science (NJ ACTS) Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health.",New Jersey Alliance for Clinical Translational Science: NJ ACTS,9890029,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'Neighborhood Health Center', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical database', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'experience', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2020,4640627,-0.022649819121712007
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Contact PD/PI: Panettieri, Reynold Alexander Project Summary/Abstract Overview Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Page 337 Project Summary/Abstract Contact PD/PI: Panettieri, Reynold Alexander New Jersey Alliance for Clinical and Translational Science (NJ ACTS)  Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health. Page 338 Project Narrative",New Jersey Alliance for Clinical Translational Science: NJ ACTS,10201004,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'Neighborhood Health Center', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical database', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2020,1482000,-0.022570990025747587
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,-0.03368779528798831
"Academy of Aphasia Research and Training Symposium PROJECT SUMMARY/ABSTRACT The annual Academy of Aphasia meeting is the premier conference for researchers in the field of language processing and aphasia. Since the first meeting in 1963, this international meeting has brought together an interdisciplinary group of linguists, psychologists, neurologists, and speech-language pathologists to discuss the latest research in the field of aphasia, including theoretical, clinical, and rehabilitation aspects of this language disorder. The topics at the conference range widely but almost always cover all aspects of language processing including phonological processing, lexical-semantic processing, syntactic processing, orthographic processing, bilingualism, computational modeling, non-invasive and invasive brain imaging, language recovery, neuroplasticity, and rehabilitation. In this proposal, we aim to include two special initiatives that will take place during the annual academy of aphasia conference. The first initiative involves a formal mentoring program for young investigators entering the field of aphasia research. In this program, selected student/post-doctoral fellows from interdisciplinary backgrounds who are first authors at the conference are paired with a mentor. This mentor will provide specific feedback about the fellow's presentation and general mentorship to the fellow about research and academic careers. This program is currently occurring as part of the conference and has been growing at the annual meeting with very positive feedback. Additionally, a formal mentoring meeting will allow a structured format for discussion about a career in aphasia research. The second initiative will be a three hour seminar (New Frontiers in Aphasia Research) that covers the background and approach of a state of the art methodology (e.g., fNIRS, graph theoretical metric, machine learning approaches) that has an application to the study of aphasia. These workshops will be recorded and, consequently, uploaded to the academy website/youtube channel for dissemination to aphasia researchers and the public. This workshop will allow conference attendees to understand the conceptual and methodological aspect of a particular scientific approach that can be implemented in their study of aphasia. Given the highly interdisciplinary nature of aphasia research, these workshops will bridge the communication between aphasia researchers and scientists and experts who have developed new approaches to study the brain. This meeting already allows a valuable opportunity for cross-pollination of research ideas and will now provide a platform for the training the next generation of scientists interested in pursuing the nature of aphasia and associated language disorders in adults. PROJECT NARRATIVE Approximately 100,000 individuals suffer from aphasia each year. The academy of aphasia is an organization of clinicians, scientists and practitioners who study this communication disorder and develop interventions to treat individuals with aphasia. The members comprise a very interdisciplinary group of linguists, psychologists, speech and language clinicians, neurologists and neuroscientists. The annual academy of aphasia meeting is the premier venue to share state of the art methodologies for application in the study of aphasia to improve the research in the development of diagnosis and treatment approaches to alleviate aphasia. This conference is also the ideal venue to educate and train the next generation of aphasia researchers who are well trained from a theoretical, technical and clinical standpoint and are committed to expand the impact of research in aphasia.",Academy of Aphasia Research and Training Symposium,9944489,R13DC017375,"['Academy', 'Adult', 'Aphasia', 'Brain', 'Brain imaging', 'Chicago', 'Clinical', 'Collaborations', 'Committee Membership', 'Communication', 'Communication impairment', 'Computer Models', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Discipline', 'Educational workshop', 'Feedback', 'Fellowship', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Governing Board', 'Grant', 'Graph', 'Hour', 'Image', 'Individual', 'International', 'Intervention', 'Language', 'Language Disorders', 'Learning', 'Lesion', 'Linguistics', 'Machine Learning', 'Manuscripts', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Mission', 'National Institute on Deafness and Other Communication Disorders', 'Nature', 'Neurologic', 'Neurologist', 'Neuronal Plasticity', 'Neurosciences', 'Non-aphasic', 'Orthography', 'Outcome', 'Pathologist', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychologist', 'Psychology', 'Publishing', 'Reading', 'Recovery', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Support', 'Research Training', 'Rest', 'Retrieval', 'Scientist', 'Secure', 'Speech', 'Speech Pathologist', 'Speech Perception', 'Stroke', 'Structure', 'Students', 'Symptoms', 'Techniques', 'Testing', 'Time', 'Training', 'Training and Education', 'Transcranial magnetic stimulation', 'Travel', 'Videotape', 'Work', 'Writing', 'base', 'bilingualism', 'career', 'career networking', 'experience', 'frontier', 'improved', 'improved outcome', 'interest', 'language processing', 'lexical', 'meetings', 'member', 'neuroimaging', 'next generation', 'novel', 'novel strategies', 'outcome forecast', 'phonology', 'programs', 'relating to nervous system', 'semantic processing', 'success', 'symposium', 'syntax', 'tenure track', 'web site']",NIDCD,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R13,2020,39939,-0.04948843815967116
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9894759,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Information Retrieval', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Medicine', 'Methods', 'Mining', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'machine learning method', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'structured data', 'tool', 'unstructured data']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2020,269500,-0.01829767591492799
"ShapeWorks in the Cloud Project Summary This application is submitted in response to NOT-OD-20-073 as an administrative supplement to the parent award R01AR076120 titled: ""Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches."" The form (or shape) of anatomies is the clinical language that describes abnormal mor- phologies tied to pathologic functions. Quantifying such subtle morphological shape changes requires parsing the anatomy into a quantitative description that is consistent across the population in question. For more than 100 years, morphometrics has been an indispensable quantitative tool in medical and biological sciences to study anatomical forms. But its representation capacity is limited to linear distances, angles, and areas. Sta- tistical shape modeling (SSM) is the computational extension of classical morphometric techniques to analyze more detailed representations of complex anatomy and their variability within populations The parent award ad- dresses existing roadblocks for the widespread adoption of SSM computational tools in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM) and its associated suite of open-source software tools, ShapeWorks. ShapeWorks enables learning population-level shape representation via automatic dense placement of homologous landmarks on image segmentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread applicability and impact to medicine and biology are hindered by computational barriers that most existing shape modeling packages face. The goal of this supplement award is to provide supplemental support for Aim 3 of the parent award to leverage best practices in software development and advances in cloud computing to enable researchers with limited computational resources and/or large-scale cohorts to build and execute custom SSM workﬂows us- ing remote scalable computational resources. To achieve this goal, we have developed a plan to enhance the design, implementation, and cloud-readiness of ShapeWorks and augmented our scientiﬁc team to add senior, experienced software engineers/developers who have extensive experience in professional programming, code refactoring, and scientiﬁc computing. This award will provide our team with the support necessary to (Aim 1) de- sign ShapeWorks as a collection of modular and reusable services, (Aim 2) decouple ShapeWorks services from explicitly encoded data sources, and (Aim 3) refactor ShapeWorks to scale efﬁciently on the cloud. All software development will be performed in adherence to software engineering practices and design principles, including coding style, documentation, and version control. The proposed efforts will be released as open-source software in a manner consistent with the principles of reproducible research and the practices of open science. Our long- term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein in addition to the parent award will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. The impact and scientiﬁc value of ShapeWorks have been recognized in a range of applications, including psychology, biological phenotyping, car- diology, and orthopedics. If funded, this supplement will provide support to revise, refactor, and redeploy Shape- Works to take advantage of new cloud computing paradigms, to be robust, sustainable, scalable, and accessible to a broader community, and to address the growing need for shape modeling tools to handle large collections of clinical data and to obtain sufﬁcient statistical power for large shape studies.",ShapeWorks in the Cloud,10166337,R01AR076120,"['Address', 'Adherence', 'Administrative Supplement', 'Adoption', 'Anatomy', 'Applied Research', 'Architecture', 'Area', 'Award', 'Biological', 'Biological Sciences', 'Biology', 'Cardiology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cloud Computing', 'Cloud Service', 'Code', 'Collection', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computer Models', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Databases', 'Disabled Persons', 'Documentation', 'Environment', 'Face', 'Funding', 'Goals', 'Image', 'Imagery', 'Language', 'Learning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'Occupations', 'Online Systems', 'Orthopedics', 'Parents', 'Pathologic', 'Phenotype', 'Population', 'Privatization', 'Psychology', 'Readiness', 'Reproducibility', 'Research', 'Research Personnel', 'Running', 'Scientist', 'Services', 'Shapes', 'Software Design', 'Software Engineering', 'Software Tools', 'Source Code', 'Speed', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'base', 'cohort', 'computational platform', 'computerized tools', 'computing resources', 'data management', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'large datasets', 'model development', 'open data', 'open source', 'particle', 'response', 'scientific computing', 'shape analysis', 'software development', 'statistics', 'tool', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,210000,-0.01863815404461152
"Nathan Shock Center for Excellence in Basic Biology of Aging OVERALL—PROJECT SUMMARY Healthspan is a complex trait, influenced by many interacting polymorphic alleles and environmental factors that may accelerate or delay aging, reduce or increase disease risk, and/or promote extended lifespan. Thus, assessing the role of genetic variation in aging requires an experimental strategy capable of modeling the genetic and biological complexity of human populations while allowing for efficient identification and validation of candidate genes. With this proposal, the JAX NSC seeks support to further develop and disseminate the next generation of genetic, phenotyping, and information resources necessary to enable a systems-wide approach to understanding healthy aging. Over the past 15 years, The JAX NSC has transformed aging research both at JAX and across the geroscience community, providing central resources to support investigators that have resulted in 26 peer-reviewed publications in the last funding period. The Center has developed nascent regional and national resources for aging research, including aging mouse resources and tissues that support our numerous collaborations and external researchers. All JAX NSC data and tools are publicly disseminated on the Mouse Phenome Database and the JAX NSC website, thus ensuring that the resources generated and expertise acquired through the Center is readily available to the aging research community. In this renewal, we will advance towards our goal by providing unique resources, tools, and support to geroscience investigators while leveraging JAX's unparalleled expertise in the large-scale identification and functional validation of complex polygenic traits in mice. We will do this by providing effective Center administration and enhancing the utility of JAX NSC resources throughout the aging community (Aim 1); expanding the research focus on aging, healthspan and age-related diseases through a robust Research Development Core (Aim 2); increasing the diversity of mouse resources available for aging research, including a new study to, for the first time, investigate the effect of genetic variation on cellular senescence and treatment with senolytic drugs (Aim 3); strengthening the data and computational and support available to the aging community (Aim 4); expanding the use of machine learning technologies in interpretation of aging pathologies (Aim 5). The Center will be led by a highly experienced team of Principal Investigators and Core Leaders who, with oversight from an External Advisory Board, will provide effective management to facilitate the goals and objectives of the Center. The Center will leverage unparalleled institutional resources, facilities and expertise of The Jackson Laboratory, a globally renowned institution for mouse genetics research, to enhance its goals and the utility of the resources it generates for the aging research community. OVERALL—PUBLIC HEALTH RELEVANCE Human aging is influenced by genetic factors, whereby differences in longevity as well as changes in health and disease risk with time are linked to variation in individuals' genetic codes. The Jackson Laboratory Nathan Shock Center will develop resources to encourage the use of a wider range of mouse models in aging research. Resources—including aged mouse models that mirror human genetic variation, metabolic and microbiome data, and methods to reveal genetic factors tied to human aging—will be available to the scientific community, accelerating research to understand and ultimately prolong healthy human aging.",Nathan Shock Center for Excellence in Basic Biology of Aging,10045024,P30AG038070,"['Advisory Committees', 'Aging', 'Alleles', 'Animals', 'Biological', 'Biology of Aging', 'Candidate Disease Gene', 'Cell Aging', 'Collaborations', 'Communities', 'Complex', 'Computer Assisted', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Educational workshop', 'Ensure', 'Environmental Risk Factor', 'Funding', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Models', 'Genetic Research', 'Genetic Variation', 'Geroscience', 'Goals', 'Health', 'Heart', 'Histologic', 'Human', 'Human Genetics', 'Image Analysis', 'Inbred Strain', 'Individual', 'Information Resources', 'Institution', 'Joints', 'Laboratories', 'Leadership', 'Link', 'Liver', 'Longevity', 'Lung', 'Machine Learning', 'Maps', 'Mentorship', 'Metabolic', 'Methods', 'Mus', 'Pathology', 'Peer Review', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Pilot Projects', 'Polygenic Traits', 'Population', 'Principal Investigator', 'Process', 'Protocols documentation', 'Publications', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Shock', 'Statistical Methods', 'Structure', 'System', 'Technology', 'The Jackson Laboratory', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Validation', 'Variant', 'Visit', 'age related', 'aged', 'animal tissue', 'behavioral phenotyping', 'candidate validation', 'career development', 'data dissemination', 'data management', 'data tools', 'disorder risk', 'experience', 'healthspan', 'healthy aging', 'insight', 'microbiome', 'mouse genetics', 'mouse model', 'next generation', 'novel', 'open source', 'phenome', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'senescence', 'symposium', 'tool', 'trait', 'user-friendly', 'web site']",NIA,JACKSON LABORATORY,P30,2020,1069526,-0.02454649478572749
"Query Log Analysis for Improving User Access to NCBI Web Services Over the last decade, the online search for biological information has progressed rapidly and has become an integral part of any scientific discovery process. Today, it is virtually impossible to conduct R&D in biomedicine without relying on the kind of Web resources developed and maintained by the NCBI. Indeed, each day millions of users search for biological information via NCBIs updated online PubMed system. However, finding data relevant to a users information need is not always easy. Improving our understanding of the growing population of Entrez users, their information needs and the way in which they meet these needs opens opportunities to improve information services and information access provided by NCBI. The unfortunate arrival of SARS-CoV-2 and the COVID-19 pandemic has led to unprecedented focused biomedical research and new opportunities to distribute the information learned.  Tools to aid searching PubMed are query suggestion, expansion, and spelling correction. Dedicated best match algorithms aid navigational queries by ignoring minor errors and aid informational searches using machine learning to combine relevant signals such as article popularity, publication date and type, and query-document relevance score. Additional valuable aids including identifying related articles and author name disambiguation.  Historically, an important part of MedLINE search has been the MeSH terms assigned to each article. These were critical when only titles were available, or there were only a few articles on a given topic. But with the growth of MEDLINE, the cost of humans assigning MeSH terms has become increasingly prohibitive.   As part of a trans-NLM initiative called NLM Labs, we investigated the value of MeSH terms assigned to each article, in contrast to the value of the MeSH vocabulary for synonymy. We measured that a MeSH term assigned to an article only leads to a click for a small percentage of total queries. When a query leads to multiple articles, an article that did not need MeSH for retrieval is more likely to be clicked by a small margin.   Because manual MeSH assignment is costly, there are projects that investigate the use of automation for increasing its efficiency. Assignments using the FullMeSH tool benefit from using the full text of an article, not just the title and abstract. Not only does it use the full text, it uses Learning-to-Rank to weight the different sections of the article in the most valuable manner. It performs several percentage points better than other state of the art methods.  TermVariants is a collection of synonyms developed via morphology and token distribution. To improve these synonyms, we used similarity of word embeddings to produce a probability of synonymy. This allows us to identify pairs of words that while typographically similar, have very different meanings. Examples include mushrooming vs. mushrooms and mineralizer vs. minerals.  When queries in PubMed return a large, incomprehensible set of documents, PDC, a probabilistic distributional clustering algorithm, can group the articles into titled subtopics. For example, the articles returned by the query suicide are grouped into topics such as assisted suicide, attempted suicide, prevention and control of suicide, and military personnel. Related phrases can further clarify the subtopic.  Our work is directly visible in the new PubMed in several ways. One is that the default Best Match sort order used by the new PubMed is based on a Learning to Rank algorithm we recently developed. While author disambiguation has been available for some time, it now also uses ORCID which is available in a growing number of articles. It can distinguish even articles that do not themselves include the ORCID. SARS-CoV-2 has been a shock to the entire world. The medical research community has responded with a flood of studies on COVID-19 covering prevention, diagnosis, treatment and other areas. LitCovid provides quick, direct access to these articles. The articles are separated by broad area and can be searched for more specific articles.  To understand this collection better, we applied NER and NLP tools, and other tools mentioned above, to provide an overview. We identified bioentities such as diseases, internal body organs, symptoms and co-morbidities. Their relationship to COVID-19 was determined via co-occurrence. We also automatically clustered articles by topic. We then recognized emerging topics and their growth. These tools could be used to more fully understand any collection of articles. n/a",Query Log Analysis for Improving User Access to NCBI Web Services,10261212,ZIALM000001,"['2019-nCoV', 'Algorithms', 'Area', 'Assisted Suicide', 'Automation', 'Biological', 'Biomedical Research', 'COVID-19', 'COVID-19 pandemic', 'Collection', 'Communities', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Floods', 'Goals', 'Growth', 'Human', 'Information Services', 'Internet', 'Learning', 'Link', 'MEDLINE', 'Machine Learning', 'Manuals', 'MeSH Thesaurus', 'Measures', 'Medical Research', 'Methods', 'Military Personnel', 'Minerals', 'Minor', 'Molecular Biology', 'Morphology', 'Names', 'Organ', 'Population', 'Prevention', 'Probability', 'Process', 'PubMed', 'Publications', 'Research', 'Retrieval', 'Shock', 'Signal Transduction', 'Suggestion', 'Suicide', 'Suicide attempt', 'Suicide prevention', 'Symptoms', 'System', 'Text', 'Time', 'Update', 'Vocabulary', 'Weight', 'Work', 'base', 'comorbidity', 'cost', 'improved', 'navigation aid', 'online resource', 'phrases', 'research and development', 'spelling', 'tool', 'virtual', 'web services']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIA,2020,1701108,-0.01717990438712632
"Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Efforts to include behavioral measures in large-scale studies as envisioned by precision medicine are hampered by the time and expertise required. Paper-and-pencil tests currently dominating clinical assessment and neuropsychological testing are plainly unfeasible. The NIH Toolbox contains many computerized tests and clinical assessment tools varying in feasibility. Unique in the Toolbox is the Penn Computerized Neurocognitive Battery (CNB), which contains 14 tests that take one hour to administer. CNB has been validated with functional neuroimaging and in multiple normative and clinical populations across the lifespan worldwide, and is freely available for research. Clinical assessment tools are usually devoted to specific disorders, and scales vary in their concentration on symptoms that are disorder specific. We have developed a broad assessment tool (GOASSESS), which currently takes about one hour to administer. These instruments were constructed, optimized and validated with classical psychometric test theory (CTT), and are efficient as CTT allows. However, genomic studies require even more time-efficient tools that can be applied massively.  Novel approaches, based on item response theory (IRT) can vastly enhance efficiency of testing and clinical assessment. IRT shifts the emphasis from the test to the items composing it by estimating item parameters such as “difficulty” and “discrimination” within ranges of general trait levels. IRT helps shorten the length of administration without compromising data quality, and for many domains leads to computer adaptive testing (CAT) that further optimizes tests to individual abilities. We propose to develop and validate adaptive versions of the CNB and GOASSESS, resulting in a neurocognitive and clinical screener that, using machine learning tools, will be continually optimized, becoming shorter and more precise as it is deployed. The tool will be in the Toolbox available in the public domain. We have item-level information to perform IRT analyses on existing data and use this information to develop CAT implementations and generate item pools for adaptive testing. Our Specific Aims are: 1. Use available itemwise data on the Penn CNB and the GOASSESS and add new tests and items to generate item pools for extending scope while abbreviating tests using IRT-CAT and other methods. The current item pool will be augmented to allow large selection of items during CAT administration and add clinical items to GOASSESS. New items will be calibrated through crowdsourcing. 2. Produce a modular CAT version of a neurocognitive and clinical assessment battery that covers major RDoC domains and a full range of psychiatric symptoms. We have implemented this procedure on some CNB tests and clinical scales and will apply similar procedures to remaining and new tests as appropriate. 3. Validate the CAT version in 100 individuals with psychosis spectrum disorders (PS), 100 with depression/anxiety disorders (DA), and 100 healthy controls (HC). We will use this dataset to implement and test data mining algorithms that optimize prediction of specific outcomes. All tests, algorithms and normative data will be in the toolbox. Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan Narrative Large scale genomic studies are done in the context of precision medicine, and for this effort to benefit neuropsychiatric disorders such studies should include behavioral measures of clinical symptoms and neurocognitive performance. Current tools are based on classical psychometric theory, and we propose to apply novel approaches of item response theory to develop a time-efficient adaptive tool for assessing broad neurocognitive functioning and psychopathology. The tool will be available in the public domain (NIH Toolbox) and will facilitate incorporation of psychiatric disorders into the precision medicine initiative.",Creating an adaptive screening tool for detecting neurocognitive deficits and psychopathology across the lifespan,9920211,R01MH117014,"['Algorithms', 'Anxiety', 'Anxiety Disorders', 'Assessment tool', 'Behavior', 'Biological Markers', 'Calibration', 'Characteristics', 'Classification', 'Clinical', 'Clinical Assessment Tool', 'Clinical assessments', 'Cognitive', 'Collection', 'Complex', 'Computers', 'Data', 'Data Compromising', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discrimination', 'Disease', 'Environmental Risk Factor', 'Feedback', 'Female', 'Genomics', 'Hour', 'Individual', 'Internet', 'Internet of Things', 'Intervention Studies', 'Length', 'Link', 'Longevity', 'Machine Learning', 'Measures', 'Medicine', 'Mental Depression', 'Mental disorders', 'Methods', 'Molecular Genetics', 'Moods', 'Neurocognitive', 'Neurocognitive Deficit', 'Neuropsychological Tests', 'Neurosciences', 'Outcome', 'Paper', 'Pathway interactions', 'Performance', 'Phenotype', 'Population', 'Precision Medicine Initiative', 'Preparation', 'Preventive Intervention', 'Procedures', 'Psychiatry', 'Psychometrics', 'Psychopathology', 'Psychotic Disorders', 'Public Domains', 'Research', 'Research Domain Criteria', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Severities', 'Speed', 'Structure', 'Symptoms', 'Tablets', 'Testing', 'Time', 'Translational Research', 'United States National Institutes of Health', 'Validation', 'base', 'behavior measurement', 'cognitive performance', 'computerized', 'crowdsourcing', 'data mining', 'data quality', 'digital', 'genomic variation', 'improved', 'individualized prevention', 'instrument', 'male', 'mobile computing', 'neuroimaging', 'neuropsychiatric disorder', 'novel', 'novel strategies', 'open source', 'precision medicine', 'protective factors', 'psychiatric symptom', 'response', 'symptom cluster', 'theories', 'tool', 'trait', 'validation studies']",NIMH,UNIVERSITY OF PENNSYLVANIA,R01,2020,709525,-0.011010621701897406
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,10019388,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2020,381282,-0.011381059089660589
"Better Understanding and Handling of Tautomerism One motivation of our tautomerism-related work is thus to use all tools at our disposal, chemoinformatics analyses, QM computations, experimental work, and systematic extraction of results from literature, to provide a scientific footing for the recommendations how to improve handling of tautomerism in InChI V2 - instead of just holding a vote in the Working Group. While prototropic tautomerism rules are the only ones currently implemented as the standard rule set in CACTVS, and all tautomeric transformations covered by InChI (as default or by option) are prototropic, ring-chain (RC) tautomerism is well-known and widespread. Nevertheless, and somewhat surprisingly, very little in terms of RC rules was available in chemoinformatics until recently. Based on Baldwin's well-known set of rules to predict the relative facility of ring forming reactions, we developed a set of 11 rules describing RC tautomerism. The rules were encoded in SMIRKS line notation, the chemical transform extension of the chemical structure line notation SMILES, developed by Daylight Chemical Information Systems, Inc., just like the currently 20 individual rules in CACTVS for describing prototropic tautomerism are encoded. A number of modifications were applied to Baldwin's rule set, which, after all, were rules for ring-closure in general, not for RC tautomerism in specific. Foremost, ring closure and opening reactions involving a tetrahedral electrophilic carbon thus leading to breakage of a single bond would cause a loss of atoms to the molecule, violating the definition of tautomerism. Adding these new RC rules to the existing standard prototropic rules in CACTVS, we applied this combined rule set to the ""poster child"" of RC tautomerism: warfarin. This anticoagulant drug, in wide use for decades, can theoretically exist in solution in 40 distinct tautomeric forms. We investigated all these tautomers with computational approaches (relative energies calculated at the B3LYP/6-311G+ level of theory) and recorded NMR (13C and 1H) spectra. We introduced an intuitive and graphical network for tautomers and their interconversion paths, which for warfarin contained 11 tautomers and 17 tautomeric transformations between them allowed by our rules. We then applied the combined RC and prototropic rule set to an entire database: the Aldrich Market Select (AMS) database of (then) 6 million screening samples and building blocks. We found over 30,000 cases where two or more AMS products were declared by our rules to be just different tautomeric forms of the same compound. 1H and 13C NMR analysis of 166 such tautomer pairs (plus a few triplets) we purchased from the AMS were performed to determine whether the chemoinformatics transforms had accurately predicted what was the same ""stuff in the bottle"" as determined by NMR. Essentially all prototropic transforms for which examples in the AMS existed (some of the ""rarer"" types of tautomerism had no such ""conflict pairs"" in the AMS) were confirmed. Some of the RC transforms were found to be too ""aggressive"", i.e. to equate structures with one another that were different compounds according to the NMR analyses. This paper received an Editor's Choice selection in the Journal of Chemical Information and Modeling. In order to provide additional experimental data for tautomerism-related analyses and chemoinformatics work, we have created a database based on data extracted from experimental literature. This database consists of 1,873 entries which belong to n-tuples of tautomers studied in a particular set of experimental conditions (pH, solvent, temperature, technique), adding up to 3,898 records since the average of n is slightly 2. The data were extracted from 73 publications, many of them reviews, taken from a selection of 200 papers provided to the contractor company that did the initial extraction (Parthys Reverse Informatics), out of about 900 papers we identified in literature searches that might contain useful data for this purpose. Each tautomer (or tuple, as appropriate) is annotated with Structural information: SMILES, InChI, InChIKey, NCI/CADD Identifiers; ""Prevalence"" data: measured ratios, interconversion rates, relative energies etc.; Condition data: solvent, temperature, pH etc. (if given); Method data: NMR, UV spectroscopy, IR spectroscopy etc.; Reference data: Bibliographic information. To the best of our knowledge, such as tautomer database does not exist elsewhere, certainly not in the public domain. A new web service - called Tautomerizer - was created to apply and test the transforms we have compiled from the above database and literature for the Redesign of Handling of Tautomerism in InChI(Key) V.2. The set of transforms compiled in the context of this project has meanwhile grown to its final number of 86, which are also being added to the Tautomerizer. The phase of initiating and then making a decision in the IUPAC Working Group about the final set of transforms to be recommended for InChI V2 has been started. Exploratory coding for adding some of the 86 rukes to the current InChI code (v.1.05) were successful for 6 rules. Work on a second-level analysis of tautomerism based on quantum-mechanical calculations and subsequent Deep Learning approaches has been started. Also, X-ray crystallography on a subset of the small molecules mentioned above has been performed. Several manuscripts about this project have been published or are under preparation. n/a",Better Understanding and Handling of Tautomerism,10262460,ZIABC011785,"['Anticoagulants', 'Appearance', 'Area', 'Azides', 'Bibliography', 'Book Chapters', 'Carbon', 'Charge', 'Chemical Structure', 'Chemicals', 'Chemistry', 'Child', 'Code', 'Computer software', 'Conflict (Psychology)', 'Contractor', 'Cyclization', 'Data', 'Databases', 'Environment', 'Equilibrium', 'Eye', 'Hydrogen', 'Individual', 'Informatics', 'Intuition', 'Journals', 'Lead', 'Literature', 'Manuscripts', 'Measures', 'Mechanics', 'Methods', 'Modification', 'Molecular Weight', 'Motivation', 'Movement', 'Organic Chemistry', 'Paper', 'Phase', 'Preparation', 'Prevalence', 'Property', 'Protons', 'Public Domains', 'Publications', 'Publishing', 'Reaction', 'Recommendation', 'Records', 'Sampling', 'Solvents', 'Spectrum Analysis', 'Structure', 'System', 'Techniques', 'Temperature', 'Terminology', 'Testing', 'Tetrazoles', 'Triplet Multiple Birth', 'Variant', 'Voting', 'Warfarin', 'Work', 'X-Ray Crystallography', 'base', 'catalyst', 'chemical information system', 'deep learning', 'foot', 'improved', 'information model', 'migration', 'posters', 'quantum', 'quantum computing', 'screening', 'single bond', 'small molecule', 'structural biology', 'tautomer', 'theories', 'tool', 'web services', 'web site', 'working group']",NCI,DIVISION OF BASIC SCIENCES - NCI,ZIA,2020,213364,-0.005302537692382959
"The University of Iowa Clinical and Translational Science Award ABSTRACT The Institute for Clinical and Translational Science (ICTS) at the University of Iowa (UI) was established by the Board of Regents to realize three objectives – first, to lead the development of translational science at the UI; second, to advance translational science as a distinct academic discipline; and third, to disseminate capacities in translational science across the State of Iowa. This mandate enabled us to tackle large problems affecting translational science that required institutional solutions, such as transforming regulatory processes for human subjects research, developing an informatics infrastructure for integrating electronic medical record and other health care data, establishing bi-directional relationships with community organizations, and revitalizing the pipeline of well trained clinical and translational researchers. Iowa is a rural state, which brings special health care needs and challenges. We have used these rural considerations as a catalyst for driving our approach to clinical and translational research pushing our teams to develop strategies to engage rural populations of all ages and backgrounds and to create new approaches that overcome the geographic barriers in a rural state. We are capitalizing on our established community practice networks of family physicians, clinics, school nurses and pharmacists. We utilize e Health/ e Learning platforms in novel ways and will test the efficacy of these new methods of engagement. As we move research “Beyond Our Borders,” we have created methods to capture real-time, real-life data from the home and to correlate this environmentally specific, comprehensive data to human performance. The ICTS is engaging with other CTSA hubs and national CTR systems to empirically test different approaches and to develop the evidence base of proven strategies for accelerating translation that can be more broadly disseminated. Though distance and rurality drive our approaches, the strategies that we develop are simply new and potentially better ways to generate broad representation and improved participation by patients, healthcare teams and academicians. Through our local, state and national partnerships, UI and the ICTS are poised to move clinical and translational discovery rapidly into healthcare practice in a variety of clinical settings. PROJECT NARRATIVE The mission of the University of Iowa Institute for Clinical and Translational Science (ICTS) is to accelerate translational science through programs to develop the translational workforce, promote the engagement of community members and other stakeholders, to promote research integration across the lifespan, and to catalyze innovative clinical and translational research. The ongoing development of collaborative data-based infrastructure and services is a central tenant to achieving this mission. The goals of this proposal are: for Iowa to join the N3C partnership, contributing data in an effective method that evolves and improves over time, to enable Iowa researchers to leverage the N3C data resource in their research and to leverage the N3C framework for other areas of research in the future.",The University of Iowa Clinical and Translational Science Award,10201104,UL1TR002537,"['Affect', 'Age', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Clinics and Hospitals', 'Communities', 'Community Practice', 'Computerized Medical Record', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Development', 'Discipline', 'Disease', 'E-learning', 'Emergency Situation', 'Family Physicians', 'Future', 'Genomics', 'Geography', 'Goals', 'Health', 'Health system', 'Healthcare', 'Home environment', 'Human', 'Human Subject Research', 'Image', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Iowa', 'Knowledge', 'Lead', 'Life', 'Longevity', 'Machine Learning', 'Medical Care Team', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Patient Participation', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacists', 'Phenotype', 'Positioning Attribute', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rural', 'Rural Health', 'Rural Population', 'School Nursing', 'Services', 'System', 'Testing', 'Time', 'Training', 'Transfer Agreement', 'Translational Research', 'Translations', 'Universities', 'Work', 'base', 'biomedical informatics', 'catalyst', 'clinical decision support', 'cohort', 'collaborative approach', 'community organizations', 'coronavirus disease', 'data acquisition', 'data enclave', 'data exchange', 'data resource', 'data sharing', 'data warehouse', 'efficacy testing', 'evidence base', 'health management', 'improved', 'informatics infrastructure', 'innovation', 'interest', 'member', 'novel', 'novel strategies', 'pandemic disease', 'phenotypic data', 'programs', 'rurality', 'social determinants', 'support tools', 'translational scientist']",NCATS,UNIVERSITY OF IOWA,UL1,2020,98933,-0.02593376635396911
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an extensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spatial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high-resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",10148333,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human BioMolecular Atlas Program', 'Human body', 'Image', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Visualization', 'Work', 'base', 'cell type', 'data exploration', 'data standards', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2020,1000000,-0.04843161308721878
"Carolina Population Center PROJECT SUMMARY/ABSTRACT The Carolina Population Center requests infrastructure support that will advance population dynamics research at CPC by increasing research impact, innovation, and productivity, supporting the development of junior scientists, and reducing the administrative burden on scientists. Infrastructure support will advance science in three primary research areas: Sexuality, Reproduction, Fertility, and Families; Population, Health, and the Environment; and Inequality, Mobility, Disparities, and Well-Being. Much of the research at CPC draws on large publicly available longitudinal data sets that our faculty have designed and collected, including the National Longitudinal Study of Adolescent to Adult Health, the China Health and Nutrition Survey, newer surveys associated with the Transfer Project, and the Study of the Tsunami Aftermath and Recovery, all of which will continue to be important in work related to our primary research areas over the next five years. These projects embody several themes that have guided research at CPC since the Center's inception. These themes, which will continue to shape our work, are the importance of life course processes and longitudinal data, multi-level processes and measurement of context, interventions and natural experiments as means of learning about causal processes, and the relevance of sociodemographic variables such as age, gender, race- ethnicity, and socioeconomic status for disparities in health and well-being. By embedding these themes, our projects provide data that enable us to address barriers that otherwise impede progress in the population sciences generally, and in our primary research areas in particular. We request support for three cores which in combination will provide an institutional infrastructure that will push populations dynamics research forward by empowering CPC faculty to tackle challenging questions using state of the art measurement techniques and methods. The Administrative Core plans activities that maintain a stimulating intellectual community, streamlines administrative processes so that scientists can focus on research, coordinates activities of the Cores so that services are offered efficiently, and communicates information about research and data more broadly. The Development Core supports early stage investigators and other faculty with exciting new ideas through multiple mechanisms: workshops, access to technical expertise in measurement, and seed grants. The Research Services Core enables scientists to address complex and important population research issues by providing access to state-of-the-art research tools and professional support for programming, survey development, and analysis. NARRATIVE This project will provide infrastructure support for a cutting edge program of research on population dynamics at the Carolina Population Center. Research at the Center will analyze state-of-the art data to address fundamental questions regarding fertility, adolescent health, and links between the environment and health. Special attention will be paid to factors creating health disparities.",Carolina Population Center,10005569,P2CHD050924,"['Address', 'Adolescent', 'Adopted', 'Adult', 'Age', 'Applications Grants', 'Area', 'Attention', 'Biological Markers', 'China', 'Cognitive', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Vision Systems', 'Creativeness', 'Data', 'Data Collection', 'Development', 'Diffuse', 'Educational workshop', 'Environment', 'Ethnic Origin', 'Extramural Activities', 'Faculty', 'Family', 'Fertility', 'Fostering', 'Funding', 'Gender', 'Genetic', 'Grant', 'Hand', 'Health', 'Health Surveys', 'Home environment', 'Inequality', 'Infrastructure', 'Intervention', 'Journals', 'Learning', 'Life Cycle Stages', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mainstreaming', 'Measurement', 'Mentors', 'Methods', 'Natural experiment', 'Nutrition Surveys', 'Personal Satisfaction', 'Phase', 'Policy Making', 'Population', 'Population Dynamics', 'Population Research', 'Population Sciences', 'Postdoctoral Fellow', 'Process', 'Production', 'Productivity', 'Publishing', 'Race', 'Recovery', 'Reproduction', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Schools', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Seeds', 'Services', 'Sexuality', 'Shapes', 'Socioeconomic Status', 'Structure', 'Students', 'Surveys', 'Talents', 'Teacher Professional Development', 'Technical Expertise', 'Techniques', 'Training Programs', 'Tsunami', 'Universities', 'Work', 'adolescent health', 'career', 'collaborative environment', 'cost', 'data access', 'design', 'empowered', 'experience', 'faculty support', 'health disparity', 'innovation', 'interdisciplinary collaboration', 'longitudinal dataset', 'novel strategies', 'population health', 'privacy protection', 'programs', 'research and development', 'response', 'sociodemographic variables', 'success', 'tool']",NICHD,UNIV OF NORTH CAROLINA CHAPEL HILL,P2C,2020,774402,-0.018030985018751416
"Central Sequencing Initiative Summary of ongoing projects organized by topic:   I. CLINICAL DIAGNOSTICS AND CONSULTATION  A. CLIA reports in CRIS. A central deliverable for our initiative is the clinical analysis and CLIA reporting into the CC medical record. This has been completed for approximately 600 patients since January 2019 and is a major accomplishment.  B. Operational development and refinement. Given the scale of our initiative, we dedicate significant attention to optimizing the efficiency of our workflow and anticipating potential disruptions related to policy adjustment or special circumstances. Specifically, this year we have increased automation of data entry into reporting and tracking processes, developed a new rapid turnaround time workflow for rare cases with high clinical urgency, and developed new CRIMSON workflows related to the new requirement for each patient to have a documented patient visit plan from a physician specifying sequencing prior to order activation.  C. Integration of medical geneticist in suite of consultation offerings. Recognizing room for improvement in our clinical consultation service for complex cases, we've started collaborating with a NIAID-NIAMS medical geneticist and are working concurrently with the NHGRI genetics consult service so we can continue to provide educational opportunities to genetics fellows. This has proven to be a clinically valuable service in multiple cases with complex clinical presentations or clinically significant molecular diagnoses outside the immune system.  D. Reanalysis. Clinical reanalysis of exome data is known to be an important source of new diagnoses over time. We are implementing a limited re-analysis workflow to be alerted to recent publications on variants in our database, which in rare cases provides sufficient evidence to merit a new molecular diagnosis.   E. Childrens collaboration. The CSI works with Gigi Notarangelo to recruit young patients from Children National Health System (site-PI: Mike Keller). The CSI protocol is the first protocol to be formally submitted under the new reliance agreement, paving the way for future studies and opening up a referral source of very young research participants who cannot typically be seen at the CC.   II. DISCOVERY  A. New gene disease discovery. The CSI is contributing to at least three ongoing projects with DIR investigators characterizing new gene-disease relationships. There is a significant opportunity to further exploit this data for discovery.  B. Collaboration with NHGRI on UTR and mosaic variants.  We have an ongoing collaboration with NHGRI to evaluate and develop more effective approaches for detection of clinically relevant variants in the untranslated regions of genes and clinically relevant mosaic variation.  C. Collaboration with multiple groups on computable phenotypic data. The CSI has built a highly valuable dataset of genomic data associated with detailed clinical records, manually coded with relevant phenotypic terms.  The integration of computable phenotypic and laboratory data into genomics is an area of great interest across the field.  We are working with NLM for more efficient text mining approaches, the NIAID epidemiological unit on phenotypic modeling for machine learning in large datasets from other health centers, and other extramural collaborators on tailoring phenotypic data analysis approaches for Mendelian disorders of the immune system.  D. Collaboration with The Genotype Ascertainment Cohort (TGAC), hosted by NHGRI. The CSI seeks to model genomic data sharing approaches that optimize both patient confidentiality and research productivity. In addition to the required deposition into dbGAP, CSI contributes data to TGAC. Exome and genome data from patients in contributing cohorts, including CSI, are available to view in aggregate by DIR researchers. This project is designed to enable further study of individuals via genomic ascertainment without prior knowledge of phenotype.  CSI staff has also participated in the review board for patient access requests.   III. SOCIAL AND BEHAVIORAL RESEARCH, POLICY  A. Negative results comprehension. The CSI is studying patient perceptions and understanding of the inconclusive negative exome results released in the medical record without specific counseling. The objective of this substudy is to use survey and interview data to better understand how well patients understand their negative exome sequencing results and to identify patient characteristics associated with poor understanding. Modification of our policy or specific educational interventions may follow if needed.  B. Secondary findings follow up collaboration. The CSI is collaboratively studying the clinical follow up of secondary findings with researchers at NHGRI.  There is some evidence that a minority of participants do not seek recommended follow up care for the potentially life threatening disorders which are returned on CSI as secondary findings.  The objective of this substudy is to better understand patient cognitive, emotional, and behavioral reactions to receiving a secondary finding, over time, including the motivators and barriers to clinical follow up.  C. Participating in facilitated discussions about policies on genetic research in humans at NIAID DIR. These discussions covered issues about quality of data, quality of analysis, and return of results, primarily focusing on the latter. n/a",Central Sequencing Initiative,10274166,ZICAI001244,"['Address', 'Agreement', 'Area', 'Attention', 'Automation', 'Behavioral', 'Behavioral Research', 'Caring', 'Characteristics', 'Child', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Complex', 'Comprehension', 'Confidentiality of Patient Information', 'Consent', 'Consult', 'Consultations', 'Counseling', 'Data', 'Data Analyses', 'Data Discovery', 'Databases', 'Deposition', 'Detection', 'Development', 'Diagnosis', 'Diagnostic Services', 'Disease', 'Education', 'Educational Intervention', 'Emotional', 'Enrollment', 'Epidemiology', 'Evaluation', 'Extramural Activities', 'Feedback', 'Future', 'Genes', 'Genetic', 'Genetic Counseling', 'Genetic Research', 'Genome', 'Genomics', 'Genotype', 'Health', 'Health system', 'Human', 'Immune System Diseases', 'Immune system', 'Individual', 'Infrastructure', 'Institutes', 'Interview', 'Knowledge', 'Laboratories', 'Language', 'Life', 'Machine Learning', 'Manuals', 'Medical', 'Medical Records', 'Medicine', 'Mendelian disorder', 'Minority', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Diagnosis', 'Mosaicism', 'National Human Genome Research Institute', 'National Institute of Allergy and Infectious Disease', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Participant', 'Patients', 'Perception', 'Phenotype', 'Physicians', 'Policies', 'Process', 'Productivity', 'Protocols documentation', 'Publications', 'Reaction', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Robotics', 'Rotation', 'SNP array', 'Secure', 'Services', 'Site', 'Source', 'Specific qualifier value', 'Students', 'Surveys', 'Testing', 'Time', 'United States National Institutes of Health', 'Untranslated Regions', 'Variant', 'Visit', 'Work', 'clinical care', 'clinical center', 'clinical diagnostics', 'clinically relevant', 'clinically significant', 'cohort', 'computable phenotypes', 'data sharing', 'database of Genotypes and Phenotypes', 'design', 'exome', 'exome sequencing', 'follow-up', 'genetic counselor', 'genomic data', 'interest', 'large datasets', 'lectures', 'meetings', 'pediatric patients', 'phenotypic data', 'recruit', 'sample collection', 'social', 'text searching']",NIAID,NATIONAL INSTITUTE OF ALLERGY AND INFECTIOUS DISEASES,ZIC,2020,2768476,-0.03377949506740368
"Functional MRI Core Facility Space Utilization:  The Functional Magnetic Resonance Imaging Facility (FMRIF) currently occupies approximately 5000 sq ft of space in Building 10, divided between the B1level scanner bays, control rooms and electronics/machine rooms for 3TA/3TB, 3TC, and the Siemens 7T-Classic, (about 1800 sq ft, 1100 sq ft and 1300 sq ft respectively) and office space within the Nuclear Magnetic Resonance (NMR) center. On the first floor are the Functional MRI Facility and the Section on Functional Imaging Methods suites (approximately 800 sq ft total) for office space and shared conference space for all staff employed full-time by the facility.  Staff:  The FMRIF staff consists of: the facility director, four staff scientists to keep the scanners running, six MRI technologists, an information technology specialist, and an administrative laboratory manger.  Investigators:  The functional MRI facility supports the research of over 30 Principal Investigators translating to over 300 researchers overall. Over 70 research protocols are active and making use of FMRIF scanners. Each scanner has scheduled operating hours of 105 hours per week.  Papers published using the core:  A strong measure of the utility of a core facility is the quantity and quality of scientific papers published by investigators using the facility. We have kept careful records of papers published and their corresponding citations, such that we have been able to create a core facility h-index.   Since its inception in 2000 until August 2020, a total of 1,222 peer-reviewed publications from intramural investigators have used data acquired in the FMRIF core facility. The total is distributed among 700 papers from NIMH, 345 papers from NINDS, and 148 from the other institutes. These papers have been cited a total of 140,641 times for a combined h-index of 185. In other words, 185 papers using the FMRIF have been cited at least 185 times.   Core Projects of the Staff Scientists:  Linqing Li  Dr. Li helped providing service for a TMS-fMRI coil test and safety verification and application. The project was interrupted early this year due to pandemic, however, last month, the TMS group requested another service for resuming this procedure. He has restored and updated the procedure for the coil installation and MRI protocol evaluation on phantom.   Dr. Li has also been very active scientifically. He has been collaborating with Dr. Yuhui Chai on high resolution functional pulse sequence development. He has also been continuing his ongoing development on DANTE-prepared EPI for both imaging blood volume changes as well as towards baseline CMRO2 mapping.   Vinai Roopchansingh  Dr. Roopchansingh has continued the development of a system that enables neuro-feedback data the GE and Siemens systems.  This system has been made publicly available at https://pypi.org/project/afniRTI/  He has also received, tested, and deployed several versions of research EPI prototype sequences from GE including a multi-band EPI prototype being used by SFIM on FMRIF 3TB.  Recently, he implemented a multi-band, multi-echo enabled prototype from GE.    FY 2020 involved multiple larger purchasing acquisitions.  He recently completed the procurement for a maintenance agreement for all of our GE scanners, which will provide coverage for the next 5 years.  His is also in the process of procuring a 32-channel head coil from Nova Medical for us, and in the midst of the procurement for a commercial option to replace our current web platform that makes DICOM data available to our users.  It should be noted that Dr. Roopchansingh's response to COVID-19-related scheduling and management of the technologists as research slowly returns to the FMRIF has been exemplary. He formulated a return to work plan, and personally oversees scanning requests and technologist schedules to make sure operations ramp up smoothly and safely.  Sean Marrett Dr. Marrett collaborated on pulse sequence debugging, testing and  a data collection using VASO methods and variants with Dr. Laurentius Huber of  Maastricht University  with the goal of whole-brain layer-specific MRI. Data was collected on both the FMRIF-7T (VB17) as well as the more modern NMRF-7T (VE12). This included the tests of the MAGEC-VASO methods.   FMRIF was given approval for supplementary funding for an upgrade of our current FMRIF-7T (now more than 10 years old in design) , and a 2nd  a  new, modern human 7T system  which will replace one of our GE MR750 3T scanners. Dr. Marrett  has been the main point of contact for these new 7T projects.   Dr. Marrett deployed an eye-tracker and screen assembly for the 3T systems in collaboration with the Section on Instrumentation and Dr. Danny Pine.  FMRIF continues to work closely with Dr. Peter Molfese (CMN/SFIM) to support concurrent high-density EEG data collection.     FMRIF was given approval and supplementary funding for a new scientific imaging database. This project was initiated by Dr. Marrett and is now being led by Dr. Roopchansingh.  This will be integrated with long-planned improvements to our data storage area network (SAN) that is the backbone of the FMRIF computing and data archiving network and will increase the capacity and speed of access to storage significantly.    Andy Derbyshire  Dr. Derbyshire continues to be the FMRIF lead in the FMRIF/NIBIB insertable head gradient project.  In the last year this has included planning/construction and, increasingly early experimental work.  The upgrades to the chilled water supply and modifications to the penetration panels etc. of 3TB were completed to accommodate the insertable head gradient (Pierpaoli/NIBIB).  The insertable head gradient unit was delivered at the end of 2019 with the first installation and demonstration of the unit being performed by the Brian Rutt's team (Stanford/U. Western Ontario) in early 2020.  The insertable head-gradient provides 120mT/m max gradient with 1200T/m/s slew rate (to be compared with 50mT/m 200T/m/s clinical gradients) allowing a single slice of 3-echo, non-accelerated EPI at 2mm resolution to be acquired in less than 100ms.  The 10kW of heat generated in the coil by running this sequence continuously is being successfully removed by the cooling system comprising chilled water through hollow conductors.     He  provided support installing and maintaining the Siemens Pulse Sequence development environment for the various platforms within the MR Center, including support for the FMRIF7T (VB17), NMRF7T (VE12U), 3TD/Skyra (VD13A and VE11C) and Prisma (VD13D) platforms as well as assistance with pulse-sequence programming related issues.   He is continuing to work with Pascal Sati (Riech/NINDS) in the development of their susceptibility contrast 3D-EPI pulse sequence for detection of the central vein in MS lesions.    He is the FMRIF representative on the NMR Safety Committee. We support for the NMRF Human Safety Class (which includes educating users about MR Safety in general as well as the Rules and Polices for human scanning in the NMR Center).   The Safety Class team is developing a new on-line refresher course to help meet the requirement that all NMR Center users re-certify their safety training at least every two years.  We have also, in response user demand during the COVID-19/max-telework situation, adapted the course to provide virtual Safety Classes which are now being regularly scheduled.  Beyond teaching the safety class sessions, the NMR Safety Class team is actively involved in developing the polices within the NMR Center to meet the changes and new standards being introduced for Human MRI scanning.     In June 2020, Dr. Derbyshire assisted with the planning and running of the Gadgetron 2020 online summer course, including teaching a small session. n/a",Functional MRI Core Facility,10266650,ZICMH002884,"['10 year old', '3-Dimensional', 'Agreement', 'Area', 'Atlases', 'Award', 'Biological Markers', 'Blood Volume', 'Brain', 'COVID-19', 'Central Vein', 'Charge', 'Chills', 'Clinical', 'Collaborations', 'Communities', 'Computer software', 'Core Facility', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Detection', 'Development', 'Digital Imaging and Communications in Medicine', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Electroencephalography', 'Electronics', 'Environment', 'Equilibrium', 'Evaluation', 'Extramural Activities', 'Eye', 'Feedback', 'Financial compensation', 'Floor', 'Functional Imaging', 'Functional Magnetic Resonance Imaging', 'Funding', 'Goals', 'Head', 'Hour', 'Human', 'Image', 'Individual', 'Information Technology', 'Infrastructure', 'Institutes', 'International', 'Interruption', 'Iron', 'Laboratories', 'Lamivudine', 'Language', 'Lead', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Measures', 'Medical', 'Methodology', 'Methods', 'Modernization', 'Modification', 'Motion', 'Multiple Sclerosis Lesions', 'Myelin', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Noise', 'Nuclear Magnetic Resonance', 'Ontario', 'Paper', 'Peer Review', 'Penetration', 'Persons', 'Physiologic pulse', 'Police', 'Predisposition', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Publishing', 'Ramp', 'Records', 'Research', 'Research Personnel', 'Research Support', 'Resolution', 'Resources', 'Role', 'Running', 'Safety', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Sensory', 'Services', 'Slice', 'Specialist', 'Speed', 'Structure', 'System', 'Testing', 'Time', 'Traction', 'Training', 'Translating', 'United States National Institutes of Health', 'Universities', 'Update', 'Variant', 'Vertebral column', 'Vision', 'Water', 'Water Supply', 'Work', 'base', 'computerized data processing', 'data archive', 'data sharing', 'deep learning', 'density', 'design', 'functional magnetic resonance imaging/electroencephalography', 'imaging facilities', 'imaging modality', 'improved', 'indexing', 'instrumentation', 'multimodality', 'neurofeedback', 'neuroimaging', 'neuroregulation', 'operation', 'pandemic disease', 'prisma', 'programs', 'prototype', 'response', 'symposium', 'telework', 'trend', 'virtual', 'web platform']",NIMH,NATIONAL INSTITUTE OF MENTAL HEALTH,ZIC,2020,5421173,-0.018602349361718874
"Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome Addiction is a highly complex disease with risk factors that include genetic variants and differences in development, sex, and environment. The long term potential of precision medicine to improve drug treatment and prevention depends on gaining a much better understanding how genetics, drugs, brain cells, and neuronal circuitry interact to influence behavior. There are serious technical barriers that prevent researchers and clinicians from incorporating more powerful computational and predictive methods in addiction research. The purpose of the NIDA P30 Core Center of Excellence in Omics, Systems Genetics, and the Addictome is to empower and train researchers supported by NIH, NIDA, NIAAA, and other federal and state institutions to use more quantitative and testable ways to analyze genetic, epigenetic, and the environmental factors that influence drug abuse risk and treatment. In the Transcriptome Informatics and Mechanisms research core we assemble and upgrade hundreds of large genome (DNA) and transcriptome (RNA) datasets for experimental rodent (rat) models of addiction. In the Systems Analytics and Modeling research core, we are using innovative systems genetics methods (gene mapping) to understand the linkage between DNA differences, environmental risks such as stress, and the differential risk of drug abuse and relapse. Our Pilot core is catalyzing new collaborations among young investigator in the field of addiction research. In sum the Center is a national resource for more reproducible research in addiction. We are centralizing, archiving, distributing, analyzing and integrating high quality data, metadata, using open software systems in collaboration with many other teams of researchers. Our goal is to help build toward an NIDA Addictome Portal that will include all genomic research relevant to addiction research. PROJECT NARRATIVE The NIDA Core Center of Excellence in Omics, Systems Genetics, and the Addictome (OSGA) provides genomic and computational support to a large number of research scientists working on mechanisms and treatment of addiction. The two main research cores of OSGA are providing support for transcriptome, epigenome, and metagenome studies of rat models of addiction at many levels of analysis. We are also creating open access tools and a powerful web portal to catalyze more effective and replicable use of massive datasets generated by programs in addiction biology and treatment.","Overall NIDA Core ""Center of Excellence"" in Transcriptomics, Systems Genetics and the Addictome",9929423,P30DA044223,"['Archives', 'Bayesian Modeling', 'Behavior', 'Behavioral', 'Bioinformatics', 'Biology', 'Biometry', 'Cellular Assay', 'Chromosome Mapping', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consult', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Drug Interactions', 'Drug abuse', 'Educational workshop', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Epigenetic Process', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Hybrids', 'Image', 'Informatics', 'Institution', 'Joints', 'Leadership', 'Machine Learning', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'National Institute of Drug Abuse', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences Research', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Population', 'Prevention', 'Proteome', 'Publications', 'Publishing', 'Quantitative Genetics', 'Quantitative Trait Loci', 'RNA', 'Rattus', 'Relapse', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Rodent', 'Role', 'Scientist', 'Site', 'Standardization', 'Statistical Models', 'Stress', 'Sum', 'System', 'Systems Analysis', 'Testing', 'Training', 'Translations', 'United States National Institutes of Health', 'Update', 'Variant', 'Visualization', 'Work', 'addiction', 'base', 'behavior influence', 'brain cell', 'career', 'cohort', 'computerized tools', 'computing resources', 'data archive', 'data integration', 'data modeling', 'data tools', 'data warehouse', 'deep learning', 'digital imaging', 'drug relapse', 'epigenome', 'experience', 'genetic analysis', 'genetic variant', 'genomic variation', 'graphical user interface', 'health record', 'high dimensionality', 'improved', 'innovation', 'insight', 'metagenome', 'mouse model', 'multiple omics', 'neurogenomics', 'neuronal circuitry', 'novel', 'precision medicine', 'prevent', 'programs', 'ranpirnase', 'rat genome', 'repository', 'sex', 'single cell analysis', 'software systems', 'tool', 'transcriptome', 'transcriptomics', 'web portal']",NIDA,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,P30,2020,754121,-0.019293968202007322
"A Multigenerational Longitudinal Panel for Aging Research Summary/Abstract This project will construct a Multigenerational Longitudinal Panel (IPUMS-MLP) of unprecedented scale and scope. Using cutting-edge automatic record linkage technology and drawing on complete count U.S. census data available from IPUMS for the period 1850 to 1940, the project will construct millions of individual life histories and trace millions of families over multiple generations. This infrastructure will provide the most comprehensive view of long-run changes in life-course dynamics available for any place in the world and will transform our understanding of processes of population aging. The work will require significant innovation and new technical infrastructure to accommodate the massive scale of the database. These data will allow investigators to directly observe changes in aging processes and life-course transitions during the period in which U.S. society was being transformed by industrialization, urbanization, immigration, demographic transition, and economic collapse. Investigators will be able to follow individuals over time to evaluate the impact of early-life conditions on later outcomes, trace life-course transitions into adulthood and old age, and observe family change over multiple generations. IPUMS-MLP will enrich existing aging surveys by providing data on multiple generations of forebears of survey respondents; likewise, it will enrich existing historical databases by enabling them to connect with descendants across multiple generations. Leveraging billions of dollars of federal investments in census data and transactional records from a variety of administrative sources, this project is a highly cost-effective use of scarce resources to develop shared infrastructure for research, education, and policy-making on health and aging. Project Narrative The proposed work is directly relevant to the core mission of the Population and Social Processes branch of NIA: the new data will advance fundamental knowledge about the causes and consequences of changes in health and well-being of the older population and will support research on the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. For example, the data will enable examinations of the impact of lead exposure to late onset Alzheimer’s disease, the socioeconomic and health effects of early-life income support, intergenerational transmission of health and wellbeing over multiple generations, and the impact of early-life cognitive capacity on later-life health and economic outcomes.",A Multigenerational Longitudinal Panel for Aging Research,9984216,R01AG057679,"['Adult', 'Age', 'Aging', 'Big Data', 'Censuses', 'Characteristics', 'Communities', 'Custom', 'Data', 'Data Security', 'Databases', 'Demographic Transitions', 'Economics', 'Education', 'Elderly', 'Exposure to', 'Family', 'Family member', 'Future', 'Genealogy', 'Generations', 'Health', 'Household', 'Immigration', 'Income', 'Individual', 'Industrialization', 'Infrastructure', 'Institution', 'Investments', 'Knowledge', 'Late Onset Alzheimer Disease', 'Life', 'Life Cycle Stages', 'Link', 'Metadata', 'Methods', 'Military Personnel', 'Mission', 'Names', 'Neighborhoods', 'Older Population', 'Outcome', 'Personal Satisfaction', 'Persons', 'Policy Making', 'Population', 'Population Process', 'Process', 'Public Policy', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Resources', 'Respondent', 'Running', 'Sampling', 'Selection Bias', 'Social Processes', 'Social Security', 'Societies', 'Source', 'Surveys', 'Technology', 'Time', 'Urbanization', 'War', 'Weight', 'Woman', 'Work', 'aging population', 'base', 'cognitive capacity', 'cost effective', 'data curation', 'data integration', 'data quality', 'economic outcome', 'experience', 'health economics', 'improved', 'innovation', 'intergenerational', 'lead exposure', 'life history', 'longitudinal dataset', 'machine learning algorithm', 'machine learning method', 'novel', 'parallel processing', 'social', 'socioeconomics', 'structured data', 'tool', 'transmission process']",NIA,UNIVERSITY OF MINNESOTA,R01,2020,675029,-0.025441118310533112
"NIMH MEG Core Facility Hardware: The Magnetoencephalography (MEG) Core operates a state-of-the-art 275 channel MEG system (CTF MEG). A complete assortment of stimulus delivery and subject response equipment is interfaced to the MEG system. In FY20, acquired and began training users on the use of a BrainSight neuronavigation unit to enable real-time co-registration of MEG localization fiducial markers and MRI scans.  Construction on the MEG laboratory space to enable the installation of the helium recycling system was completed, and we expect to have the system installed at some time in September 2020. This system will eliminate the reliance of the lab on the procurement of liquid helium, a scarce non-renewable natural resource. After an initial break-even point of several years, this system is estimated to save the US federal government approximately $100,000 per year.  The MEG Core Facility has continued its development of a novel optically pumped magnetometer (OPM) system. Due to the COVID-19 global pandemic, delivery of the initial 19 channel system acquired using IRP funds was delayed. The manufacturer has supplied a 7-channel test system, with which we were able to acquire initial measurements, which was unfortunately interrupted by the NIH shift to maintenance-only mode in early 2020. We have, however, been able to restart our collaboration with George Dold and the Section on Instrumentation in construction of a sensor fixture and calibration device, and we expect to begin testing in late FY20, early FY21.  Finally, the MEG Core Facility continues ongoing assessment of all Core Facility computing infrastructure, and we have initiated acquisition of a specialized workstation containing GPU processors for machine learning and other computational projects.   Software: A variety of software for data analysis is maintained and supported by the Core, including proprietary CTF code, beamformer source reconstruction software (the SAM suite) written in-house, which interfaces with AFNI (NIMH Scientific and Statistical Computing Core) and Freesurfer, Fieldtrip, and MNE-Python. In addition, the MEG Core Facility frequently writes custom scripts to integrate stimulus and response data with the MEG dataset. The Core also actively develops new functionality within the SAM software suite, which will be released online at our website (https://megcore.nih.gov). The release version of the software, including innovations such as a patch beamformer, automated alignment, and FreeSurfer integration is planned for FY21. To supplement the SAM software suite, the MEG Core Facility has begun writing a modularized set of python scripts to automate all states of MEG data pre-processing and analysis. Due to the modular nature of these scripts, they can rapidly be customized for new studies. The MEG Core Facility has been implementing these scripts using data acquired as part of the NIMH research volunteer study (NCT03304665), and will begin working with individual Core Facility users to automate their analyses.   The MEG Core facility also continues to assist investigators in setting up MEG software and ensures that all software is available on shared resources (The NIH High Performance Computing (HPC) center).   Education and Training: As noted earlier, the field of MEG is relatively small compared to the MRI community, and there are fewer well-established methods. The analysis of complex tasks designed to test innovative hypotheses requires unique approaches. The MEG Core Facility staff can leverage its wealth of experience to support and train investigators on these tasks. One-on-one training and support are provided upon request, and accounts for a significant portion of the scientific staffs time. No formal courses have been held in the MEG Core Facility in FY20; a course is planned for FY21 to coincide with release of the SAM suite of data analysis tools.   Support of the Larger MEG Community: As an effort to foster collaboration and communication across North American research and clinical MEG laboratories, the MEG Core Facility held the MEG-North America Workshop on the NIH campus in FY20 (November 2019). The first day of the meeting included several working groups, running concurrently with a full day hackathon. Notably, this was the first ever MEG-focused hackathon. The hackathon included a full-day workshop on the Human Neocortical Neurosolver (HNN) software package, and a team determining the ease at which a variety of software packages could analyze MEG data from a repository in BIDS format, which is becoming the global standard for shared neuroimaging data. The scientific sessions featured two keynotes, nine symposium lectures, reports from working groups, and a poster session. We continued to receive feedback from investigators that they find MEG-North America workshops valuable and would attend again. While original plans were to hold the meeting again in late 2021, the COVID-19 pandemic forced the delay of BioMag, the premier international meeting for biomagnetic imaging and MEG to late 2021. To avoid overlap, the next MEG-North America will be postponed until 2022. (either late FY22 or early FY23).   During FY18, an amendment was crafted to the NIMH protocol Recruitment and Characterization of Healthy Research Volunteers for NIMH Intramural studies (NCT03304665), to add an optional MEG study. We began scanning volunteers in January 2019, and 68 recordings have now been completed to date. This multimodal dataset is a unique and valuable resource for the scientific community. We are currently in collaboration with the data science and sharing team to convert all data to the BIDS standard format, and we expect to have the full dataset shared on the NIH-supported OpenNeuro repository.   Scientific Contribution and Collaboration: The primary focus of the MEG Core Facility is to facilitate and enable the science of other NIMH investigators. However, based upon scientific interests, MEG Core Facility scientists frequently collaborate with other investigator to analyze data and test novel hypotheses. In FY18, core personnel have collaborated extensively with the labs of Carlos A. Zarate, Jr. (ETPB) and Karen F. Berman (CTNB) to gain insights into the pathophysiology of major depressive disorder (MDD) and schizophrenia. Details regarding these collaborations are available in the attached project bibliography and in forthcoming publications.  The MEG core facility has recently completed a unique project to demonstrate the ability of MEG to record activity from deep sources in the brain, carried out under NIMH protocol NCT00397111. While many investigators still believe recording from deep/subcortical sources is not feasible, through the use of a system such as the CTF system with axial gradiometers, combined with beamforming techniques, recording signals from these sources is possible. A pilot study using both reward task (which robustly activates the basal ganglia), and a fine motor control task (which activates the cerebellum) has been completed, and manuscripts are in preparation. Currently, the reward task is being used by Dr. Argyris Stringaris's group in a population of depressed adolescents.   The final scientific contribution of the MEG Core Facility is the support of the work of NIH intramural scientists. A list of manuscripts acquired using the Core Facility resources appears below:  Gregory MD, Kippenhan JS, Callicott JH, Rubinstein DY, Mattay VS, Coppola R, Berman KF. Sequence Variation Associated with SLC12A5 Gene Expression is Linked to Brain Structure and Function in Healthy Adults. Cerebral Cortex 29(11):4654-4661 (2019). n/a",NIMH MEG Core Facility,10264687,ZICMH002889,"['Adult', 'Amendment', 'American', 'Basal Ganglia', 'Bibliography', 'Brain', 'COVID-19', 'COVID-19 pandemic', 'Calibration', 'Cerebellum', 'Cerebral cortex', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Consultations', 'Core Facility', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Educational workshop', 'Ensure', 'Equipment', 'Experimental Designs', 'Federal Government', 'Feedback', 'Fostering', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Funding', 'Gene Expression', 'Goals', 'Helium', 'High Performance Computing', 'Human', 'Human Resources', 'Image', 'Individual', 'Infrastructure', 'International', 'Interruption', 'Knowledge', 'Laboratories', 'Link', 'Liquid substance', 'MRI Scans', 'Machine Learning', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Maintenance', 'Major Depressive Disorder', 'Manufacturer Name', 'Manuscripts', 'Measurement', 'Methods', 'Mission', 'National Institute of Mental Health', 'Natural Resources', 'Nature', 'Neuronavigation', 'North America', 'Optics', 'Pilot Projects', 'Population', 'Preparation', 'Protocols documentation', 'Publications', 'Pump', 'Pythons', 'Recycling', 'Reporting', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Response to stimulus physiology', 'Rewards', 'Running', 'Scanning', 'Schizophrenia', 'Science', 'Scientist', 'Signal Transduction', 'Source', 'Statistical Computing', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Training Support', 'Training and Education', 'United States National Institutes of Health', 'Variant', 'Work', 'Writing', 'analytical method', 'base', 'child depression', 'computerized data processing', 'data acquisition', 'data quality', 'data sharing', 'design', 'experience', 'hackathon', 'innovation', 'insight', 'instrumentation', 'interest', 'investigator training', 'lectures', 'meetings', 'motor control', 'multimodal data', 'neocortical', 'neuroimaging', 'neurophysiology', 'novel', 'pandemic disease', 'posters', 'reconstruction', 'recruit', 'repository', 'response', 'scientific computing', 'sensor', 'symposium', 'tool', 'volunteer', 'web site', 'working group']",NIMH,NATIONAL INSTITUTE OF MENTAL HEALTH,ZIC,2020,1400218,-0.006774376673595472
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),10019333,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip Prosthesis', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee Prosthesis', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'clinical center', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2020,629384,-0.004863360062034312
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9962426,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Pooling', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'multiple data types', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,323659,-0.03032356316404862
"Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes PROJECT SUMMARY Overview: We will extend and develop implementations of foundational methods for analyzing populations of attributed connectomes. Our toolbox will enable brain scientists to (1) infer latent structure from individual connectomes, (2) identify meaningful clusters among populations of connectomes, and (3) detect relationships between connectomes and multivariate phenotypes. The methods we develop and extend will naturally overcome the challenges inherent in connectomics: high-dimensional non-Euclidean data with multi-level nonlinear interactions. Our implementations will comply with the highest open-source standards by: providing extensive online documentation and extended tutorials, hosting workshops to demonstrate our tools on an annual basis, and merging our implementations into commonly used packages such as scikit-learn [1], scipy [2], and networkx [3]. All of the code we develop is open source. We strive to ensure that our code is shared in accordance with the strictest guiding principles. We chose to implement these algorithms in Python due to its wide adoption in the neuroscience and data science fields. In particular, many other neuroscience tools applicable to connectomics, including NetworkX DiPy, mindboggle, nilearn, and nipy, are also implemented in Python. This will enable researchers to chain our analysis tools onto pre-existing pipelines for data preprocessing and visualization. Nonetheless, we feel that sharing our code in our own public repositories is insufficient for global reach. We have also begun reaching out to developers of the leading data science packages in python, including scipy, sklearn, networkx, scikit-image, and DiPy. For each of those packages, we have informal approval to begin integrating algorithms that we have developed. Those packages are collectively used by >220,000 other packages, so merging our algorithms into those packages will significantly extend our global reach. All researchers investigating connectomics, including all the authors of the 24,000 papers that mention the word “connectome”, will be able to apply state-of-the-art statistical theory and methods to their data. Currently, we have about 150 open source software projects on our NeuroData GitHub organization. Collectively, these projects get about 2,000 downloads and >11,000 views per month. As we incorporate additional functionality as described in this proposal, we expect far more researchers across disciplines and sectors will utilize our software. 20 ​ ​​ ​ ​​ Project Narrative Connectomes are an increasingly important modality for characterizing the structure of the brain, to complement behavior, genetics, and physiology. We and others have developed foundational statistical theory and methods over the last decade for the analysis of networks, networks with edge, vertex, and other attributes, and populations thereof, with preliminary implementations of those tools that we leverage in our laboratory for various application papers. In this project, we will extend our package, called graspy, to be of professional quality, implementing key functionality to include (1) estimating latent structure from attributed connectomes, (2) identifying meaningful clusters among populations of connectomes, and (3) detecting relationships between connectomes and multivariate phenotypes, such as behavior, genetics, and physiology. 18",Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes,10012519,RF1MH123233,"['Adoption', 'Algorithms', 'Behavioral Genetics', 'Brain', 'Code', 'Coin', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Development', 'Discipline', 'Documentation', 'Educational workshop', 'Ensure', 'Foundations', 'Funding', 'Genes', 'Human', 'Image', 'Individual', 'Journals', 'Laboratories', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modality', 'Modernization', 'Motivation', 'Neurosciences', 'Paper', 'Pathway Analysis', 'Phenotype', 'Physiology', 'Population', 'Population Analysis', 'Population Study', 'Property', 'PubMed', 'Publishing', 'Pythons', 'Research Personnel', 'Scientist', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Study', 'Structure', 'Telecommunications', 'Testing', 'Visualization', 'Work', 'brain research', 'connectome', 'data pipeline', 'design', 'high dimensionality', 'high standard', 'open source', 'public repository', 'software development', 'theories', 'tool', 'user-friendly']",NIMH,JOHNS HOPKINS UNIVERSITY,RF1,2020,1246005,-0.007927809919025796
"National Biomedical Information Services In FY20, NLM significantly enhanced and expanded its National Biomedical Information Services.  Added 1.2 million citations to PubMed (now with 31 million+ citations to biomedical journal articles), and launched a new PubMed platform with mobile friendly design and AI-powered intuitive search.  Added 560,000 articles to PubMed Central (PMC), which now provides free public access to 6.3 million full text journal articles. Launched a pilot program to make full-text preprints available and searchable via PMC and PubMed, concentrating first on SARS-CoV-2 and COVID-19 related preprints.   Partnered with White House Office of Science and Technology Policy and publishers to make a growing collection of more than 75,000 published coronavirus articles freely available in machine readable formats via the PMC Public Health Emergency Collection to support machine learning research.  Launched, with university and industry partners, the COVID-19 Open Research Dataset (CORD-19), which provides openly available, full-text, machine readable pre-print and peer-reviewed coronavirus articles for machine analysis.  Created LitCOVID, a curated literature hub for scientific information on 35,000+ SARS-CoV-2 articles in PubMed.   Added 35,000+ new clinical research studies and 6,500 new results summaries to ClinicalTrials.gov (now 356,000 studies and 45,000 results summaries), including 4,000 clinical studies related to COVID-19, 2,800 of which are listed by WHO. Implemented new procedures to post results information within 30 days of submission and solicited public input to inform ongoing modernization effort.   Provided reference information on environmental health and toxicology via TOXNET, including links to training materials and resources in NLMs PubChem and Bookshelf. Added to PubChem information on chemical compounds used in SARS-CoV-2 clinical trials and found in COVID-19-related Protein Database structures.   Provided trusted consumer health information via MedlinePlus (medical tests, drugs and supplements, healthy recipes, and videos, 700+ resource links in 60 languages); added content on effects of genetic variation on human health.   Developed evaluation plan to assess community needs for health services research information; continued to provide health services research information for researchers, practitioners, and policy makers.   Provided drug information via NLMs Drug Information Portal, DailyMed (drug labeling information from package inserts for 126,000 drugs), LiverTox (clinical, diagnostic and research information and case registry on liver injury due to drugs, herbals and dietary supplements), LactMed (effects of 1,500 drugs, dietary supplements, and diagnostic agents on breastfeeding mothers and their nursing infants); and ChemID (1,900 new chemical items added with 400+ drugs related to COVID-19).   Added 420 million genetic sequences to GenBank (which contains all publicly available DNA sequences), 40 million records (a 19% increase) to RefSeq (database of reference sequences including genomic, transcript, and protein) and 100,000 human genome sequence variants to ClinVar (archive of reports of the relationships among human variations and phenotypes). Launched a new search page for betacoronavirus gene sequences, including the latest outbreak data.   Completed the transition of 36.4 petabytes of public and controlled-access genetic sequence data in Sequence Read Archive (SRA) to secure commercial clouds under the NIH STRIDES initiative to accelerate genomics research.   Participated in SARS-CoV-2 Sequencing for Public Health Emergency Response, Epidemiology and Surveillance (SPHERES) consortium to coordinate U.S. efforts to provide publicly accessible SARS-CoV-2 sequence data in GenBank and SRA.  Streamlined data submission and integrated data validation to support epidemiology (12,000 total SARS-CoV-2 sequences including 10,000+ complete genomes in GenBank; and 27,000+ SARS-CoV-2 sequence read datasets submitted to SRA).   Established COVID-19 Genome Sequence Dataset, providing scientists free cloud-based access to SARS-CoV-2 SRA data via the NIH STRIDES Initiative.   Processed genome sequence data for 197,000 samples via the Pathogen Detection pipeline to identify sources of human illnesses such as Salmonella, E. coli, and Listeria. Provided real-time US foodborne pathogen surveillance system used by FDA to support 660+ actions intended to protect consumers from foodborne illness. Provided access to antimicrobial resistance data for 590,000+ pathogens via AMRFinderPlus.   Coordinated clinical data standards for HHS, provided tools for exchanging, analyzing, and facilitating interoperability of machine-readable clinical health data, and expedited content to help COVID-19 response: added 316,000 implantable devices to AccessGUDID database to support certified EHR interoperability requirements.   Released SNOMED CT US Edition (3,800 new concepts, 40 relating to COVID-19), including concepts for capturing COVID-19 testing data, as requested by the CDC; released SNOMED CT International Edition interim version with WHO official naming for COVID-19 concepts.   Supported continued expansion of LOINC (1,200 new terms for lab tests and observations, including 160 COVID-19-related); updated RxNorm (drug terminology resource) to include select investigational drugs related to pandemic and edited 10,000+ hand sanitizer entries supplied by the FDA; and enabled access to 660 COVID-19 related value sets.   Added seven new Supplementary Concept Records related to Medical Subject Headings (MeSH) for SARS-CoV-2 and COVID-19 treatment and detection.  Contributed to trans-NIH work to make COVID-19 specific survey items publicly available and identify core common data elements related to COVID-19 and social determinants of health.   Via the Network of the National Library of Medicine (8,200+ libraries, information centers, and other organizations): increased engagement nationwide to promote diversity, equity, and inclusion, e.g., by  educating people in communities traditionally underrepresented in biomedical research about precision medicine, digital literacy, and the NIH All of Us Research Program. Projects and programs covering health disparities priority areas included: health literacy; rural, urban and medically underserved areas; and population groups including ethnic and minority communities, LGBTQ, children, teens, adults, and seniors.  Trained public library staff to use high-quality health information; trained health sciences librarians in research data management and data science; and encouraged citizen science. Conducted 4,000 activities reaching 1.5 million participants (398 organizations led projects; citizen science activities resulted in 160 new citations to 77 preventive health and wellness articles on Wikipedia that received almost 2 million views).   Combined contemporary and historical inquiry to advance biomedical knowledge via: 37 NLM traveling exhibitions (265,000 visitors); online exhibitions (590,000 visitors), including two new exhibitions linking to 5,200 pages of digitized collection materials; three host venues virtually hosting The Politics of Yellow Fever exhibition; and selecting six Michael E. DeBakey Fellows in the History of Medicine who will conduct research using NLM historical collections when conditions allow them onsite.  Expanded Data Science NLM Training program: mentoring program with capstone project and practical skills classes; trained 26 staff in 60-hour intensive Data Science Fundamentals course. Has become a model for data science workforce development across government. n/a",National Biomedical Information Services,10261250,ZIHLM200888,"['2019-nCoV', 'Adult', 'African American', 'All of Us Research Program', 'American Indians', 'Antimicrobial Resistance', 'Archives', 'Area', 'Bioinformatics', 'Biomedical Research', 'Biotechnology', 'Breast Feeding', 'COVID-19', 'COVID-19 pandemic', 'Centers for Disease Control and Prevention (U.S.)', 'Chemicals', 'Child', 'ClinVar', 'Clinical', 'Clinical Data', 'Clinical Practice Guideline', 'Clinical Research', 'Clinical Trials', 'Collection', 'Common Core', 'Common Data Element', 'Communication', 'Communities', 'Computational Biology', 'Coronavirus', 'DNA Sequence', 'Data', 'Data Science', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnostic', 'Diagnostics Research', 'Disease Outbreaks', 'Drug Labeling', 'Emergency response', 'Environmental Health', 'Epidemiologic Monitoring', 'Epidemiology', 'Escherichia coli', 'Evaluation', 'Family', 'Genbank', 'General Population', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Government', 'Guidelines', 'Hand', 'Health', 'Health Professional', 'Health Sciences', 'Health Services Needs', 'Health Services Research', 'Health Technology', 'Healthcare', 'Herbal supplement', 'Hispanics', 'History of Medicine', 'Hour', 'Human', 'Human Genome', 'Imagery', 'Information Centers', 'Information Dissemination', 'Information Services', 'Information Systems', 'Infrastructure', 'International', 'Intramural Research Program', 'Intuition', 'Investigational Drugs', 'Knowledge', 'Language', 'Latino', 'Lesbian Gay Bisexual Transgender Queer', 'Librarians', 'Libraries', 'Link', 'Listeria', 'Literature', 'Logical Observation Identifiers Names and Codes', 'Machine Learning', 'MeSH Thesaurus', 'Medical', 'Medical Informatics', 'Medically Underserved Area', 'MedlinePlus', 'Mentors', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Mothers', 'Names', 'Nursing infant', 'Package Insert', 'Participant', 'Pathogen detection', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Policies', 'Policy Maker', 'Politics', 'Population Group', 'Positioning Attribute', 'Preventive', 'Private Sector', 'Procedures', 'Process', 'Protein Structure Databases', 'Proteins', 'PubChem', 'PubMed', 'Public Health', 'Public Health Informatics', 'Public Sector', 'Publishing', 'Readability', 'Recipe', 'Records', 'Registries', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Role', 'SNOMED Clinical Terms', 'Salmonella', 'Sampling', 'Science', 'Scientist', 'Secure', 'Software Tools', 'Source', 'Speed', 'Strategic Planning', 'Support System', 'Surveys', 'System', 'Technology', 'Teenagers', 'Terminology', 'Testing', 'Text', 'Time', 'Toxicology Data Network', 'Training', 'Training Programs', 'Transcript', 'Translating', 'Travel', 'Trust', 'Underrepresented Populations', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Validation', 'Variant', 'Work', 'Workforce Development', 'Yellow Fever', 'bioinformatics tool', 'biomedical information system', 'citizen science', 'clinical diagnostics', 'cloud based', 'data archive', 'data management', 'data standards', 'data submission', 'data tools', 'database of Genotypes and Phenotypes', 'design', 'dietary supplements', 'digital', 'diverse data', 'environmental toxicology', 'ethnic minority population', 'exhibitions', 'foodborne illness', 'foodborne pathogen', 'genome database', 'genomic data', 'health data', 'health disparity', 'health information technology', 'health literacy', 'health training', 'implantable device', 'improved', 'industry partner', 'interoperability', 'journal article', 'language processing', 'literacy', 'liver injury', 'minority communities', 'outreach', 'pandemic disease', 'pathogen', 'petabyte', 'precision medicine', 'preservation', 'programs', 'public health emergency', 'research and development', 'research study', 'research to practice', 'response', 'rural underserved', 'scientific computing', 'skills', 'social health determinants', 'tool', 'urban underserved', 'virtual']",NLM,NATIONAL LIBRARY OF MEDICINE,ZIH,2020,340214502,-0.013358305998599768
"University of Buffalo Clinical and Translational Science Institute The Buffalo Translational Consortium (BTC), which includes the University at Buffalo (UB) health sciences schools, the major healthcare institutions in our region, four key research institutes and five influential community partners, have embarked on a comprehensive strategic plan to build a strong foundation for clinical and translational research in response to our community needs. Buffalo is the second most populous city in New York State and has a rich cultural history. The proportion of underrepresented minorities in Buffalo in 2018 (50%) parallels that projected for the US in 2050, making Buffalo a microcosm of what the US will look like in 30 years. A similar proportion of our population experiences health disparities. The vision for our CTSA hub is to perform innovative research across the translational spectrum to improve the health of our community and the nation. We will develop, test and share novel approaches to engage difficult-to-engage populations and reduce health disparities in our community, which represents a “population of the future”. Guided by our vision, the CTSA has catalyzed a transformation of our environment since our CTSA was first funded in August 2015 with remarkable growth in clinical and translational research. Further, in just the past year, the UB medical school has moved into a spectacular new building and our clinical partner, Kaleida Health, the largest healthcare system in the region, opened the new Oishei Children’s Hospital, both on the Buffalo Niagara Medical Campus and connected to the Clinical and Translational Research Center devoted entirely to clinical and translational research that opened in 2012. This rapid and continuing trajectory of growth in healthcare and research in the region has resulted in a new 21st century Academic Health Center with healthcare, medical education and clinical and translational research on one campus in the heart of Buffalo, creating a foundation to enhance the impact of our CTSA even further. While launching our CTSA, we have prioritized participation in the national consortium through hosting and testing Innovation Labs as a team science tool, working with multiple hubs on initiatives to solve translational research barriers and sharing tools that we have developed with the CTSA consortium, including novel health informatics tools. Our CTSA has five ambitious but achievable aims, including: 1) Accelerate innovative translational research with teams that engage communities, regional stakeholders and the national consortium; 2) Train an excellent, diverse workforce to advance translation of discoveries; 3) Enhance inclusion of special populations across the lifespan and difficult-to-engage populations; 4) Streamline clinical research processes focusing on quality and efficiency with emphasis on multisite studies; 5) Develop, test and share biomedical informatics tools to integrate data from multiple sources to speed translation. Guided by our vision to perform research to improve the health of our community and the nation, we will continue our momentum to expand translational research, train our diverse workforce, streamline processes, engage our community, and actively contribute to the national consortium. The University at Buffalo Clinical and Translational Science Institute (CTSI) is the coordinating center of the Buffalo Translational Consortium, which includes the region's premier research, educational and clinical institutions with influential community partners. The vision of the CTSI is to perform innovative clinical and translational research to reduce health disparities and improve the health of our community and the nation. We engage our community as research partners to create a shared environment to bring discoveries in the laboratory, clinic and community to benefit individual and public health.",University of Buffalo Clinical and Translational Science Institute,10053435,UL1TR001412,"['Achievement', 'Address', 'Adopted', 'African American', 'Buffaloes', 'Center for Translational Science Activities', 'Cities', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Health', 'County', 'Coupled', 'Cultural Backgrounds', 'Data', 'Diverse Workforce', 'Ensure', 'Environment', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Healthcare', 'Healthcare Systems', 'Heart', 'Image', 'Imaging technology', 'Individual', 'Influentials', 'Informatics', 'Institutes', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Life Expectancy', 'Longevity', 'Medical', 'Medical Education', 'Medical center', 'Methods', 'Natural Language Processing', 'New York', 'Outcomes Research', 'Participant', 'Pediatric Hospitals', 'Phenotype', 'Population', 'Poverty', 'Process', 'Program Development', 'Prospective Studies', 'Public Health', 'Public Health Informatics', 'Recording of previous events', 'Recruitment Activity', 'Refugees', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Schools', 'Science', 'Sensitivity and Specificity', 'Site', 'Speed', 'Strategic Planning', 'System', 'Testing', 'Training', 'Translational Research', 'Translations', 'Underrepresented Minority', 'Universities', 'Vision', 'Work', 'Workforce Development', 'base', 'biomedical informatics', 'clinical center', 'clinical data warehouse', 'community partnership', 'data sharing', 'education research', 'experience', 'health care disparity', 'health disparity', 'imaging genetics', 'improved', 'informatics tool', 'innovation', 'interoperability', 'medical schools', 'multidisciplinary', 'multiple data sources', 'named group', 'novel', 'novel strategies', 'recruit', 'response', 'sharing platform', 'skills', 'social health determinants', 'structured data', 'tool', 'translational impact', 'translational pipeline', 'translational scientist', 'unstructured data']",NCATS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,UL1,2020,4118079,-0.020078855255189834
"Mixed Reality System for STEM Education and the promotion of health-related careers Project Summary/Abstract Proposed is a system to combine and leverage the advantages of existing medical props with interactive media to provide engaging and cooperative group STEM learning experiences. Significance: The PowerPoint lecture style has become the standard method for teaching groups of students. Unfortunately, this style does not emphasize student-instructor or student-student instruction, and in fact seems to have made students even less engaged than before. Broad agreement exists in the field of science education that more engaging pedagogies benefit students in introductory classes. A variety of teaching aids, for example plastic medical props and mannequins are available to support more engaging learning exercises. Despite their substantial benefits, physical props are fundamentally limited as they are primarily static (e.g. fixed coloration, disease depiction), their internal structures (with limited exceptions) often bear little resemblance to actual human anatomy, and they are passive objects. Hypothesis: A system which can provide more engaging interaction with physical props will be able to improve student retention and increase interest in STEM related subjects. Specific Aims: To prove the feasibility of the proposed system in Phase I IDL will 1) Determine stakeholder requirements through round table discussions; 2) Create prototype system hardware & software to augment learning with physical props; and 3) Validate the prototype system through a pilot study. The overall Phase I effort will demonstrate the ability of the proposed system to augment learning with physical props. In the Phase II effort IDL will ready the system for commercialization by 1) Developing production-quality software, hardware, and user interfaces; 2) Developing a set of comprehensive curricula for the system; and 3) Validating the system through human subject testing. Project Narrative Passive learning methods, i.e. PowerPoint lectures, have become the standard method for teaching groups of students topics including Anatomy and Physiology in spite of broad agreement in the field of science education that more engaging pedagogies benefit students in introductory classes. A variety of teaching aids, for example plastic medical props and mannequins are available to support more engaging learning; however, these props are fundamentally limited.",Mixed Reality System for STEM Education and the promotion of health-related careers,9997967,R44GM130247,"['3-Dimensional', 'Agreement', 'Algorithmic Software', 'Anatomy', 'Biological', 'Biological Sciences', 'Collaborations', 'Color', 'Computer Vision Systems', 'Computer software', 'Computers', 'Development', 'Disease', 'Disease Progression', 'Dissection', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Environment', 'Exercise', 'Hand', 'Health', 'Health Promotion and Education', 'Hour', 'Human', 'Hybrids', 'Image', 'Instruction', 'Intervention', 'Learning', 'Location', 'Manikins', 'Medical', 'Minnesota', 'Modeling', 'Participant', 'Phase', 'Physiological', 'Physiology', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Role', 'Sampling', 'Science, Technology, Engineering and Mathematics Education', 'Scientist', 'Slide', 'Small Business Innovation Research Grant', 'Structure', 'Students', 'Support Groups', 'System', 'Teaching Method', 'Testing', 'Time', 'Training', 'Universities', 'Ursidae Family', 'animation', 'career', 'college', 'commercialization', 'design', 'digital media', 'experience', 'flexibility', 'graphical user interface', 'guided inquiry', 'hands-on learning', 'human subject', 'improved', 'innovation', 'instructor', 'interactive tool', 'interest', 'learning strategy', 'lectures', 'machine vision', 'mid-career faculty', 'mixed reality', 'pedagogy', 'prototype', 'retention rate', 'science education', 'software systems']",NIGMS,"INNOVATIVE DESIGN LABS, INC.",R44,2020,698435,-0.03794394493335142
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,-0.009776627206582919
"South Carolina Clinical & Translational Research Institute (SCTR) PROJECT SUMMARY – SCTR INSTITUTE Parent Award UL1-TR001450 Since 2009, the South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina (SC) by creating a Learning Health System that supports high- quality clinical and translational research (CTR) and fosters collaboration and innovation. Headquartered at the Medical University of South Carolina (MUSC), SCTR has engaged stakeholders and created statewide partnerships to improve care and address social determinants of health across SC. However, greater than 75% of SC is rural, and all 46 counties contain areas designated as medically underserved, so health disparities remain an issue. Over the next five years, SCTR will strengthen its outreach to these medically underserved areas through collaboration with the Clemson University Health Extension Program and the MUSC Telehealth Center of Excellence. With a focus on implementation and dissemination as well as discovery, we will develop and demonstrate innovative technologies and outreach to improve the health of our stakeholders. We will build on prior successes and introduce innovative approaches to expand CTR across SC through the following aims: Aim 1. Extend and enhance high-quality, innovative, flexible curricula and training experiences for all levels of the CTR workforce, with particular emphasis on enhancing workforce heterogeneity and team science. Aim 2. Engage a diverse group of stakeholders as active partners in CTR to address health care priorities while enhancing the scientific knowledge base about collaboration and engagement. Aim 3. Promote greater inclusion across the full translational spectrum of research by engaging investigators from many disciplines and patient populations from diverse demographic backgrounds and geographic areas. Aim 4. Develop, demonstrate and disseminate innovative methods and processes to address barriers and accelerate the translation of research discoveries to improvements in human health that can be generalized to a variety of practice settings. Aim 5. Enhance the conduct of translational research through the development of secure and innovative informatics and digital health solutions, tools and methodologies that affect every aspect of CTR. SCTR’s vision is to be a major force in facilitating the translation of innovative science into practice to address the health priorities of the citizens of SC and beyond. To achieve this vision, SCTR’s mission is to catalyze the development of methods and technologies that lead to more efficient translation of biomedical discoveries into interventions that improve individual and public health. SCTR will serve as the statewide academic home for CTR, one that is well-integrated with SC’s healthcare systems and provides essential support for innovative, efficient, multidisciplinary research and research training. We will work within SCTR, with our partners across SC and with the CTSA Consortium to realize this vision. PROJECT NARRATIVE The South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina by creating a Learning Health System characterized by strong training and infrastructure resources that stimulate collaboration and innovation as a means to accelerate the translation of biomedical research discoveries into human health improvements. A major focus of this application is to strengthen SCTR’s statewide collaborations through innovative partnerships and initiatives with an emphasis on rural and medically underserved communities where significant health disparities exist. We will develop, demonstrate and disseminate innovative ways to train a diverse workforce and address barriers to translational research, and we will continue to collaborate across the CTSA Consortium to maximize impact and improve the health of the nation.",South Carolina Clinical & Translational Research Institute (SCTR),10241088,UL1TR001450,"['2019-nCoV', 'Address', 'Administrative Supplement', 'Affect', 'Area', 'Award', 'Biomedical Research', 'COVID-19', 'COVID-19 pandemic', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials Network', 'Clinical and Translational Science Awards', 'Clinical effectiveness', 'Collaborations', 'Communities', 'County', 'Databases', 'Development', 'Discipline', 'Diverse Workforce', 'Educational Curriculum', 'Electronic Health Record', 'Emergency Situation', 'Environment', 'Fast Healthcare Interoperability Resources', 'Fostering', 'Geographic Locations', 'Health', 'Health Priorities', 'Health Sciences', 'Health system', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Home environment', 'Human', 'Individual', 'Informatics', 'Information Retrieval', 'Interdisciplinary Study', 'Intervention', 'Lead', 'Learning', 'Link', 'Medical', 'Medically Underserved Area', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Natural Language Processing', 'Outcome', 'Parents', 'Patients', 'Phenotype', 'Population', 'Population Heterogeneity', 'Process', 'Public Health', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Rural', 'Science', 'Secure', 'South Carolina', 'Support System', 'Technology', 'Terminology', 'Text', 'Training', 'Training and Infrastructure', 'Translating', 'Translational Research', 'Translations', 'Universities', 'Vision', 'Work', 'base', 'biomedical informatics', 'cohort', 'coronavirus disease', 'cost effective', 'data access', 'data enclave', 'data interoperability', 'data modeling', 'data sharing', 'digital', 'expectation', 'experience', 'flexibility', 'health disparity', 'improved', 'innovation', 'innovative technologies', 'interest', 'knowledge base', 'medically underserved', 'member', 'method development', 'outreach', 'parent grant', 'patient population', 'practice setting', 'programs', 'response', 'rural underserved', 'social health determinants', 'success', 'synergism', 'telehealth', 'tool', 'translational pipeline', 'web services']",NCATS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,UL1,2020,100000,-0.029807104227121975
"Development of a visual-to-tactile conversion system for automating tactile graphic generation process PROJECT SUMMARY/ABSTRACT There are an estimated 23.7 million people who are blind or visually-impaired (BVI) in the U.S. and 285 million globally. Of this population, 30% do not travel independently outside of their home, only ~11% have a bachelor’s degree, and more than 70% are unemployed. The goal of this SBIR effort is to develop a novel system, which performs principled down-sampling and translation of visual information from digital documents into tactile equivalents. Timely access to information is one of the biggest challenges for BVI people. While access to textual information has largely been solved via screen reading software (e.g., JAWS or VoiceOver), very little progress has been made in making graphical information accessible. Although few assistive technology (AT) devices aim provide non-visual graphical access, they suffer from several shortcomings including high cost, limited portability, lack of multi-purpose, and inability to present information in a real-time context. Importantly, a common underlying problem across all extant approaches is that they require intensive human effort for producing or authoring tactile (and/or multimodal) graphics, which leads to high production costs and significant delays in the time between when the accessible materials are needed, and when they are actually delivered, adversely impacting BVI individuals in K-12 schools, colleges, and workplace settings. To address this long-standing problem, UNAR Labs aims to develop a novel system, which will automatically down-sample and translate visual graphical information into an intuitive tactile equivalent that can be used in tactile embossers. Building upon eight years of empirical research, this Phase I SBIR effort will prove the technical feasibility and functional viability of a prototype system for automating visual-to-tactile graphic conversion process and using the output in embossers. Two specific aims will guide this Phase I project: (1) to develop a prototype of an automated system for performing visual-to-tactile conversion without human intervention, and (2) to assess the technical feasibility and functional utility of the system through a rigorous human study. Success in this effort will provide a robust automated system for tactile graphic generation and promote empowerment of millions of BVI individuals by supporting increased educational attainment, proliferation of vocational opportunities, and enhancing overall quality of life for BVI people. PROJECT NARRATIVE Lack of equitable and timely access to information among persons who are blind or visually impaired (BVI) is key to realizing an inclusive world for all as it alleviates a known impediment that is hugely detrimental to their success in activities affecting quality of life and socio-economic status. The proposed innovation presents a first- of-its kind on-demand visual-to-tactile translation system, which will fully automate the tactile graphic generation process using bio-inspired sensory substitution rules and will instantly deliver the translated information for use in tactile embossers. Successful completion of this project will significantly reduce tactile graphic production costs and preparation time, and will promote empowerment of millions of BVI individuals by supporting increased educational attainment, vocational opportunities, and overall better quality of life.",Development of a visual-to-tactile conversion system for automating tactile graphic generation process,10008494,R43EY031628,"['Access to Information', 'Address', 'Adoption', 'Affect', 'Bachelor&apos', 's Degree', 'Benchmarking', 'Braille Display', 'Characteristics', 'Cognitive', 'Computer software', 'Computers', 'Data', 'Development', 'Devices', 'Elements', 'Empirical Research', 'Evaluation', 'Floor', 'Generations', 'Goals', 'Graph', 'Home environment', 'Human', 'Individual', 'Information Retrieval', 'Intervention', 'Intuition', 'Maine', 'Maps', 'Nature', 'Output', 'Performance', 'Persons', 'Phase', 'Plant Roots', 'Population', 'Preparation', 'Process', 'Production', 'Productivity', 'Psychophysics', 'Quality of life', 'Readability', 'Reading', 'Research', 'Route', 'Sampling', 'Schools', 'Self-Help Devices', 'Sensory', 'Small Business Innovation Research Grant', 'Socioeconomic Status', 'Software Framework', 'Support System', 'System', 'Tactile', 'Text', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Unemployment', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'braille', 'college', 'cost', 'data modeling', 'deep learning', 'digital', 'empowerment', 'human study', 'innovation', 'multimodality', 'multisensory', 'novel', 'operation', 'portability', 'prototype', 'success', 'touchscreen', 'usability', 'visual information', 'visual learning']",NEI,"UNAR LABS, LLC",R43,2020,300000,-0.02260572593547963
"Tufts Clinical and Translational Science Institute (N3C Supplement) PROJECT SUMMARY Tufts Clinical and Translational Science Institute (Tufts CTSI) is based on the conviction that authentic involvement of the entire spectrum of clinical and translational research (CTR) is critical to fulfilling the promise of biomedical science for meeting the public's needs. This includes not only from translation from bench to bedside (T1 translation), but also, crucially for having health impact, translation into effective clinical practice (T2), care delivery and public health (T3), and health policy (T4). Advances on all of these fronts is increasingly dependent on making effective use of scientific data from multiple domains. The COVID-19 global emergency presents both an immediate challenge and an opportunity to progress on important data sharing aims emphasized by NIH. In response, NCATS and the Centers for Translational Science Award (CTSA) hubs, several HHS agencies, and other partnering organizations have committed to developing a next-generation repository for clinical data related to COVID-19, the National COVID Cohort Collaborative (N3C), as a means of accelerating global research into the disease and aiding the development of diagnostics, therapeutics, and effective vaccines. The N3C initiative's goal of improving the efficiency and accessibility of analyses with clinical data is consistent with the primary informatics objectives of Tufts CTSI, which am to reduce barriers to the integration of healthcare and research by providing innovative systems, data repositories, and analytical tools, and by enabling greater exchange and collaboration through interoperability, standardization, and resource sharing. In- line with shared objectives, in this supplement we seek to contribute to the N3C initiative as a data provider and thought partner through the following specific aims: (1) continue to play an important role providing tools and resources for N3C's analytics platform; and (2) ensure Tufts CTSI's Informatics Program has sufficient staff and technical resources to continue to provide COVID-specific patient data from our hub to the N3C repository. PROJECT NARRATIVE An integrated, continuously updated data repository and a platform of putting powerful analytics capabilities at the disposal of the scientific community can generate insights into COVID-19 and accelerate the development of effective treatments and vaccines to counter the disease. In this project, Tufts Clinical and Translational Science Institute plans to contribute to this goal by providing carefully structured clinical data and innovative informatics tools to the National COVID Cohort Collaborative (N3C), an initiative demonstrating a novel approach for collaborative pandemic data sharing.",Tufts Clinical and Translational Science Institute (N3C Supplement),10172199,UL1TR002544,"['Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communities', 'Data', 'Development', 'Diagnostic', 'Disease', 'Emergency Situation', 'Ensure', 'Environment', 'Funding', 'Goals', 'Health', 'Health Care Research', 'Health Policy', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Mission', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Play', 'Provider', 'Public Health', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Risk', 'Role', 'Science', 'Secure', 'Speed', 'Standardization', 'Structure', 'System', 'Time', 'Translational Research', 'Translations', 'Treatment Efficacy', 'United States National Institutes of Health', 'Update', 'Vaccines', 'analytical tool', 'base', 'bench to bedside', 'care delivery', 'clinical data warehouse', 'clinical decision support', 'clinical practice', 'cohort', 'collaborative approach', 'convict', 'coronavirus disease', 'data integration', 'data sharing', 'data warehouse', 'effective therapy', 'health management', 'improved', 'informatics tool', 'innovation', 'insight', 'interoperability', 'meetings', 'next generation', 'novel strategies', 'pandemic disease', 'programs', 'repository', 'response', 'social determinants', 'support tools', 'tool']",NCATS,TUFTS UNIVERSITY BOSTON,UL1,2020,100000,-0.01786210463688921
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,-0.03249507495669233
"Modeling the Incompleteness and Biases of Health Data Modeling the Incompleteness and Biases of Health Data Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. Existing efforts for missing health data imputation often focus on only cross-sectional correlation (e.g., correlation across subjects or across variables) but neglect autocorrelation (e.g., correlation across time points). Moreover, they often focus on modeling incompleteness but neglect the biases in health data. Modeling both the incompleteness and bias may contribute to better understanding of health data and better support clinical decision making. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets. Aim 1 introduces the MICA framework to jointly consider cross-sectional correlation and auto-correlation. In Aim 2, we will augment MICA to be bias-aware (hence BAMICA) to account for biases stemmed from multiple roots such as healthcare process and use them as features in imputing missing health data. This augmentation is achieved by a novel recurrent neural network architecture that keeps track of both evolution of health data variables and bias factors. In Aim 3, we will supplement unstructured clinical notes to structured health data for modeling incompleteness and biases using a novel architecture of graph neural network on top of memory network. We will apply graph neural networks to process clinical notes in order to learn proper representations as input to the memory networks for imputation and downstream predictive modeling tasks. Depending on the clinical problem and data availability, not all modules may be needed. Thus our proposed BAMICA framework is designed to be flexible and consists of selectable modules to meet some or all of the above needs. In summary, our proposal bridges a key knowledge gap in jointly modeling incompleteness and biases in health data and utilizes unstructured clinical notes to supplement and augment such modeling in order to better support predictive modeling and clinical decision making. We will demonstrate generalizability by experimenting on four large clinical and cohort study datasets, and by scaling up to the eMERGE network spanning 11 institutions nationwide. We will disseminate the open-source framework. The principled and flexible framework generated by this project will bring significant methodological advancement and have a direct impact on enhancing discovery from health data. Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets.",Modeling the Incompleteness and Biases of Health Data,9941499,R01LM013337,"['Adoption', 'Algorithms', 'Architecture', 'Awareness', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Collection', 'Communities', 'Computer software', 'Critical Care', 'Data', 'Data Collection', 'Data Set', 'Dependence', 'Derivation procedure', 'Development', 'Diagnostic', 'Diagnostic tests', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Evolution', 'Functional disorder', 'General Hospitals', 'Goals', 'Graph', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Hour', 'Individual', 'Inpatients', 'Institution', 'Intuition', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Learning', 'Measurement', 'Medical', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Procedures', 'Process', 'Protocols documentation', 'Regimen', 'Research', 'Research Personnel', 'Resources', 'Role', 'Schedule', 'Structure', 'Symptoms', 'System', 'Test Result', 'Testing', 'Time', 'Training', 'Validation', 'clinical decision support', 'clinical decision-making', 'data mining', 'data quality', 'design', 'experimental study', 'flexibility', 'health care service utilization', 'health data', 'improved', 'lifetime risk', 'machine learning algorithm', 'neglect', 'neural network', 'neural network architecture', 'novel', 'open source', 'patient population', 'personalized diagnostics', 'personalized therapeutic', 'predictive modeling', 'recurrent neural network', 'scale up', 'social health determinants', 'stem', 'structured data', 'text searching', 'tool', 'trait']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,348397,-0.015439019485825706
"WASHINGTON UNIVERSITY SCHOOL OF MEDICINE UNDIAGNOSED DISEASES NETWORK CLINICAL SITE 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",WASHINGTON UNIVERSITY SCHOOL OF MEDICINE UNDIAGNOSED DISEASES NETWORK CLINICAL SITE,10124937,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Models', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data dissemination', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2020,150000,-0.018815596009635928
"Development and Validation of a Collaborative Web/Cloud-Based Dosimetry System for Radiopharmaceutical Therapy. Project Summary  Radiopharmaceutical therapy (RPT) – the use of targeted radionuclides to deliver radiation specifically to cancer cells and their microenvironment – is a fundamentally different approach to cancer therapy that is growing, with a substantial number of large and small pharmaceuticals companies developing products in this area and radionuclide producers making substantial investments in scaling up production. This is especially true in the area of alpha emitters. The dosimetric evaluation of therapeutic radiopharmaceuticals is a key requirement for regulatory approval and optimal administration of RPTs, especially in combination with external beam radiation therapy. This project will provide a cloud-based dosimetry software service, delivered through a web-browser, that includes the full complement of methods needed for dosimetry in the context of obtaining regulatory approval of RPTs and, ultimately, for optimal clinical delivery. Providing this in a cloud-based system will enable a variety of models for selling the service that do not require a large up-front capital investment for clinics or radiopharmaceutical developers. It also will provide access to expert advice, customization, and dosimetry services, and allow for collaboration between developers, dosimetry experts, and clinical sites. To accomplish the goal of developing this cloud-based web-browser-delivered RPT dosimetry software service, we propose the following specific aims: (1) Design, develop and implement a web/cloud-based integrated software system for treatment planning of RPT therapy; (2) design and implement a full server-side framework for subscription, authentication, and granting collaborative privileges for the various processes and data in the dosimetry pipeline; (3) optimize and adapt the four most computationally intensive processes for a multi-processor cloud-based compute environment; (4) apply and evaluate the toolchain developed in aims 1-3 to phantom, simulated and existing patient data. Successful completion of this project will produce a cloud-based software system delivered to the user via a web browser that provides an integrated, streamlined, robust, state-of-the-art system for RPT treatment planning. This system would enable a collaborative approach to multi-center clinical trials and eventually to clinical delivery of optimally dosed RPT. Projective Narrative  Radiopharmaceutical therapy is an emerging cancer therapy modality involving the targeted delivery of radiation to tumors using tumor-targeting molecules. The dosimetric evaluation of the therapeutic radiopharmaceuticals is a key requirement for regulatory approval and optimal administration of RPTs. This project seeks to develop a cloud-based software system delivered to the user via a web browser that provides an integrated, streamlined, robust, state-of-the-art system for RPT treatment planning; and enables a collaborative approach to multi-center clinical trials and eventually to the clinical delivery of optimally dosed RPT.",Development and Validation of a Collaborative Web/Cloud-Based Dosimetry System for Radiopharmaceutical Therapy.,10019481,R44CA213782,"['3-Dimensional', 'Architecture', 'Area', 'Big Data', 'Biological', 'Businesses', 'Capital', 'Client', 'Clinic', 'Clinical', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Complement', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Collection', 'Development', 'Dose', 'Dose-Rate', 'Environment', 'External Beam Radiation Therapy', 'Generations', 'Goals', 'Grant', 'Growth', 'Health', 'Individual', 'Infrastructure', 'Internet', 'Investments', 'Licensing', 'Methods', 'Modality', 'Modeling', 'Multi-Institutional Clinical Trial', 'Online Systems', 'Pathway interactions', 'Patients', 'Pharmacologic Substance', 'Phase', 'Privacy', 'Process', 'Production', 'Radiation', 'Radioisotopes', 'Radiopharmaceuticals', 'Research Personnel', 'Running', 'Services', 'Side', 'Site', 'Small Business Innovation Research Grant', 'Software Tools', 'System', 'Systemic disease', 'Translating', 'Treatment Protocols', 'Uncertainty', 'Validation', 'Vendor', 'Work', 'analysis pipeline', 'base', 'cancer cell', 'cancer therapy', 'clinical application', 'clinical research site', 'cloud based', 'collaborative approach', 'computing resources', 'cost effective', 'data de-identification', 'data sharing', 'deep learning', 'design', 'dosimetry', 'encryption', 'experience', 'image reconstruction', 'image registration', 'imaging Segmentation', 'interest', 'medical specialties', 'multicore processor', 'neoplastic cell', 'precision medicine', 'prototype', 'quantitative imaging', 'radiation delivery', 'reconstruction', 'scale up', 'single photon emission computed tomography', 'software systems', 'targeted delivery', 'therapeutic evaluation', 'tool', 'treatment planning', 'treatment strategy', 'tumor', 'web services']",NCI,"RADIOPHARMACEUTICAL IMAGING AND DOSIMETRY, LLC",R44,2020,162019,-0.032948618511819364
"Microdata for Analysis of Early Life Conditions, Health, and Population Project Summary This project will enhance and develop the only source of data that provides core information about work, education, income, and migration for the entire U.S. population. This unique resource allows us to observe today's late-life population when they were young. This allows a prospective view of the impact of early-life environments and socioeconomic status on health and well-being in later life. The massive database describes the characteristics of all 133 million persons who resided in the United States in 1940. The data have been available for only a brief period, but are already having a profound impact on scientific research. They are the primary data source for at least 19 sponsored projects, including nine funded by NIH and seven funded by NSF. Through May 2017, 104 investigators had produced 95 papers based on the data, including 22 articles, four books, three PhD dissertations, and 66 working papers. This project will improve the quality and usability of the database by correcting transcription errors; improving the coding, editing, and allocation of key variables; and introducing new data dissemination tools designed to simplify access to the data. The project will undertake three major activities to meet these objectives. 1. Incorporate new verified census information on name, age, sex, family relationship, race, marital status,  birthplace, and residence five years ago. 2. Improve coding, editing, and allocation for geographic variables, migration, occupation, and numerically-  coded variables such as income. 3. Apply new technologies to democratize access to the data through new data access tools and a virtual data  enclave for a restricted version of the data that contains names. This project is a highly cost-effective use of scarce resources to develop shared infrastructure for research, education, and policy-making on health and aging. The proposed improvement of data quality is urgent, and will provide a resource of unprecedented power for understanding the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. Narrative This project will develop data for prospective analysis of the impact of early-life circumstances and environment on health outcomes in late life, including Alzheimer's disease. The proposed improvement of data quality is urgent, and will provide a resource of unprecedented power for understanding the effects of public policies, social institutions, and environmental conditions on the health, well-being, and functioning of people, both over the life course and in their later years. The proposed work is directly relevant to the core mission of the Population and Social Processes branch of NIA, since the data will allow us to observe today's late-life population when they were young.","Microdata for Analysis of Early Life Conditions, Health, and Population",9903193,R01AG041831,"['Age', 'Aging', 'Alzheimer&apos', 's Disease', 'American', 'Arbitration', 'Books', 'Censuses', 'Characteristics', 'Church of Jesus Christ of Latter-day Saints', 'Code', 'County', 'Data', 'Data Sources', 'Databases', 'Demography', 'Doctor of Philosophy', 'Economics', 'Education', 'Elderly', 'Ensure', 'Environment', 'Family Relationship', 'Funding', 'Future', 'Genealogy', 'Genetic Transcription', 'Geography', 'Handwriting', 'Health', 'Income', 'Institution', 'Investments', 'Journals', 'Life', 'Life Cycle Stages', 'Link', 'Machine Learning', 'Marital Status', 'Methods', 'Mission', 'Names', 'Occupational', 'Occupations', 'Outcome', 'Paper', 'Personal Satisfaction', 'Persons', 'Policy Making', 'Population', 'Population Process', 'Privatization', 'Process', 'Public Policy', 'Publications', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Social Processes', 'Socioeconomic Status', 'Sociology', 'Source', 'Techniques', 'Technology', 'Time', 'United States', 'United States National Institutes of Health', 'Work', 'arm', 'base', 'blind', 'cost', 'cost effective', 'data access', 'data dissemination', 'data enclave', 'data quality', 'design', 'experimental study', 'improved', 'migration', 'new technology', 'prospective', 'residence', 'sex', 'social', 'tool', 'usability', 'virtual']",NIA,UNIVERSITY OF MINNESOTA,R01,2020,662054,-0.02626679347660012
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9977220,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Models', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data dissemination', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2020,550000,-0.018815596009635928
"Estimating Mediation Effects in Prevention Studies The purpose of this competing continuation grant proposal is to develop, evaluate and apply  methodological and statistical procedures to investigate how prevention programs change outcome  variables. These mediation analyses assess the link between program effects on the constructs targeted  by a prevention program and effects on the outcome. As noted by many researchers and federal  agencies, mediation analyses identify the most effective program components and increase  understanding of the underlying mechanisms leading to changing outcome variables. Information from  mediation analysis can make interventions more powerful, more efficient, and shorter. The P. I. of this grant received a one-year NIDA small grant and four multi-year grants to develop and evaluate mediation  analysis in prevention research. This work led to many publications and innovations. The proposed  five-year continuation focuses on the further development and refinement of exciting new mediation  analysis statistical developments. Four statistical topics represent next steps in this research and include  analytical and simulation research as well as applications to etiological and prevention data. The work expands on our development of causal mediation and Bayesian mediation methods that hold great promise for mediation analysis. In Study 1, practical causal mediation and Bayesian mediation analyses  for research designs are developed and evaluated. This approach will clarify methods and develop  approaches for dealing with violation of testable and untestable assumptions. Study 2 investigates  important measurement issues for the investigation of mediation. This work will focus on methods to identify critical facets of mediating variables, approaches to understanding whether mediators and  outcomes are redundant, and develop methods for studies with big data. Study 3 continues the development and evaluation of new longitudinal mediation methods for ecological momentary assessment data and other studies with massive data collection. These new methods promise to more accurately model change over time for both individuals and groups of individuals. Study 4 develops methods to  uncover subgroups in mediation analysis including causal mediation methods, multilevel models, and new  approaches based on residuals for identifying individuals for whom mediating processes differ in  effectiveness from other individuals. For each study, we will investigate unique issues with mediation analysis of prevention data including methods for small N and also massive data collection (big data), the RcErLitEicVaANl rCoEle(Soeef imnsetruacstiounrse):ment for mediating mechanisms, and the application of the growing literature on  causal methods and Bayesian methods. Study 5 applies new statistical methods to data from several NIH  The project further develops a method, statistical mediation analysis, that extracts more information from  funded prevention studies providing important feedback about the usefulness of the methods. Study 6  research. Mediation analysis explains how and why prevention and treatments are successful. Mediation  disseminates new information about mediation analysis through our website and other media, by  analysis improves prevention and treatment so that their effects are greater and even cost less. communication with researchers, and publications from the project. n/a",Estimating Mediation Effects in Prevention Studies,9851457,R37DA009757,"['Address', 'Alcohol or Other Drugs use', 'Applications Grants', 'Bayesian Method', 'Behavioral Mechanisms', 'Big Data', 'Biological Models', 'Communication', 'Complex', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Ecological momentary assessment', 'Educational workshop', 'Effectiveness', 'Etiology', 'Evaluation', 'Feedback', 'Funding', 'Grant', 'Individual', 'Individual Differences', 'Intervention', 'Investigation', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Outcome', 'Persons', 'Prevention', 'Prevention Research', 'Prevention program', 'Principal Investigator', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Residual state', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Testing', 'Time', 'Translating', 'United States National Institutes of Health', 'Work', 'base', 'computer program', 'cost', 'data space', 'design', 'dynamic system', 'improved', 'innovation', 'interest', 'longitudinal design', 'model design', 'multilevel analysis', 'novel strategies', 'programs', 'simulation', 'successful intervention', 'theories', 'therapy design', 'tool', 'treatment research', 'web site']",NIDA,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R37,2020,382893,-0.032377676385178446
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9969443,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data standards', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'large datasets', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'public repository', 'repository', 'research and development', 'software development', 'software infrastructure', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,158388,-0.033854261610541045
"Alliance for Regenerative Rehabilitation Research & Training 2.0 (AR3T 2.0) OVERALL: ABSTRACT  The scope of regenerative medicine encompasses the repair, regeneration, and replacement of defective, injured, and diseased tissues and organs. The success of regenerative therapies is dependent, at least in part, on a favorable microenvironment in which the regenerative processes occur. Technological innovations and a deepened mechanistic understanding of how these microenvironmental signals influence tissue regeneration has drawn attention to the critical importance of the clinical field with foundations in the application of physical, thermal, and electrical stimuli to promote functional restoration—rehabilitation. We propose that the fields of regenerative medicine and rehabilitative science are inextricably intertwined, an intersection of disciplines that we and others have termed Regenerative Rehabilitation. To realize the full potential of Regenerative Rehabilitation, there is a need for formalized mechanisms that promote the interaction of basic scientists with rehabilitation specialists. During the initial funding cycle, the Alliance for Regenerative Rehabilitation Research & Training (AR3T) built a national network of investigators and programs that has helped to expand scientific knowledge, expertise and methodologies across the domains of regenerative medicine and rehabilitation. This proposal seeks funding for AR3T 2.0, in which we will build on successes achieved and lessons learned over the initial period of support with the goal of being even more responsive to the needs of the greater community. Six specific aims define a framework upon which we will achieve our goals. AR3T will provide education and drive the science underlying Regenerative Rehabilitation by: 1) Providing didactic programs that expose rehabilitation researchers to cutting-edge investigations and state-of-the-art technologies in the field of regenerative medicine (Didactic Aim); 2) Cultivating collaborative opportunities between renowned investigators in the fields of regenerative medicine and rehabilitation (Collaborations Aim); 3) Coordinating a pilot funding program to support novel lines of Regenerative Rehabilitation research (Pilot Funding Aim); 4) Developing and validating technologies to advance the measurement and use of the regenerative rehabilitation programs (Technology Aim); 5) Promoting our center’s expertise to a broad community of trainees, investigators, and clinicians (Promotion Aim); 6) Carefully monitoring and evaluating the effectiveness of our program will ensure that we are successful in achieving our goals (Quality Control Aim). Administrative note: In the preparation of this proposal, we made every effort to present a comprehensive and detailed plan for achieving our goals while minimizing redundancy. Therefore, in multiple places, we refer the reader to specific components of the application, rather than repeating text. We appreciate the time and effort the reviewers devote to the evaluation of the proposals.  Sincerely, Fabrisia, Tom and Mike PROJECT NARRATIVE  Regenerative Rehabilitation is the integration of principles and approaches across the fields of rehabilitation science and regenerative medicine. The integration of these two fields will increase the efficiency of interventions designed to optimize physical functioning to the benefit of a wide range of individuals with disabilities. The Alliance for Regenerative Rehabilitation Research & Training (AR3T) 2.0 will build on the momentum gained over the first cycle of funding with the goal of continuing to illuminate and seize opportunities to expand scientific knowledge, expertise and methodologies in the domain of Regenerative Rehabilitation.",Alliance for Regenerative Rehabilitation Research & Training 2.0 (AR3T 2.0),9967689,P2CHD086843,"['Accountability', 'Activities of Daily Living', 'Age', 'Attention', 'Awareness', 'Basic Science', 'Biocompatible Materials', 'Clinical', 'Collaborations', 'Communities', 'Congenital Abnormality', 'Country', 'Data Analyses', 'Development', 'Disabled Persons', 'Discipline', 'Disease', 'Documentation', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'In Vitro', 'Incubators', 'Individual', 'Injury', 'Intervention', 'Investigation', 'Journals', 'Knowledge', 'Laboratories', 'Machine Learning', 'Marketing', 'Measurement', 'Mechanics', 'Mentors', 'Methodology', 'Methods', 'Mission', 'Monitor', 'Natural regeneration', 'Organ', 'Performance', 'Physical Function', 'Pre-Clinical Model', 'Preparation', 'Process', 'Quality Control', 'Reader', 'Regenerative Medicine', 'Rehabilitation therapy', 'Research', 'Research Design', 'Research Personnel', 'Research Training', 'Resources', 'Science', 'Scientist', 'Series', 'Signal Transduction', 'Specialist', 'Stimulus', 'Structure', 'Systems Analysis', 'Technology', 'Text', 'Time', 'Tissues', 'Training', 'Trauma', 'Treatment Efficacy', 'Update', 'career', 'effectiveness evaluation', 'falls', 'functional restoration', 'gait examination', 'healing', 'injured', 'innovation', 'interest', 'investigator training', 'multidisciplinary', 'new technology', 'novel', 'novel strategies', 'pre-clinical', 'programs', 'regenerative', 'regenerative therapy', 'rehabilitation research', 'rehabilitation science', 'repaired', 'response', 'sabbatical', 'social media', 'success', 'symposium', 'technological innovation', 'therapy design', 'tissue regeneration', 'webinar']",NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P2C,2020,1059198,-0.031558420782899436
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10050843,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk stratification', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,759918,-0.01801341748008824
"Leveraging Existing Data and Analytic Methods for Health Disparities Research Related to Aging and Alzheimer's Disease and Related Dementias (ADRD) Abstract Two of a series of in-person workshops in 2020-2022 will be hosted at Duke to provide new knowledge on how existing and recently developed analytic methods can be used for detailed population and clinical data analysis in order to make progress in understanding the causes and mechanisms of health-related disparities in Alzheimer’s disease (AD), related dementias (ADRD), and other prominent age-related diseases. The long- term goal of the series is to provide a resource focused on diffusing methodological know-how in terms of demonstrating the capabilities of newly developed methodologies, expanding on the rigor and range of application of well-established and familiar methods, promoting correct use of big health data both from a methodological and ethical prospective as well as providing a forum for experts and newcomers interested in health disparities and age-related diseases to discuss their ideas and promote their research. The pilot Duke- NIA workshop of the series held in February 2019 at Duke University was successful in drawing broad scientific interest to the topic and generated the background for the current proposal. The focus of the first workshop (planned in Winter 2020/2021) will be on demonstrating how studies using established administrative health data resources such as the Medicare claims database combined with innovative analytic approaches such as partitioning analyses, time-series based methods of projection/forecasting, and stochastic process models can be used to uncover previously overlooked and/or understudied aspects in this area of research. Specific topics to be discussed will include: i) disparities in risks and survival of AD/ADRD and other age- related diseases; ii) forecasting approaches for prevalence and mortality of AD/ADRD and other age-related diseases; iii) analysis of Medicare and other administrative claim-based data. The focus of the second workshop (planned in Winter 2021/2022) will extend this to include the health records data routinely collected in hospitals or University medical centers (e.g., the Duke Clinical Data Warehouse) and demonstrate how well- established and new analytic methods can be rigorously applied to such data to contribute to identifying the causes of persistent health disparities between specific groups of the U.S. population and narrowly defined patient strata. Specific topics will be expanded to include: i) analytic approaches to identify and quantify the contribution of treatment-related and medical care access-related factors to disparities in outcomes of AD/ADRD and other age-related diseases; ii) comorbidity, multimorbidity, treatment-related, social and genetic factors as sources of disparities in health outcomes of AD/ADRD and other age-related diseases; iii) forecasting of health outcomes and approaches for analyses of potential health interventions. The proceedings will be streamed live on the workshop website and presentations will be freely available in text and video form after the fact. Narrative Our objective is to host two in-person workshops in 2020-2021, in which research findings and evidence-based information and analytic tools for analyses of health-related disparities in Alzheimer’s disease, related dementias and other prominent age-related diseases are discussed. Specific Aims to be addressed in this project will be focused on increased collaboration and partnership in an interdisciplinary research community focused on analytic methods for large-scale population and clinic-related data, constructing a bridge between independent research subgroups, and the identification of ways to achieve synergistic effects in multidisciplinary research by combining innovative approaches developed across different research groups. Ultimately, our long-term goal is to diffuse the active use of advanced analytic methods for analyses of existing big health population datasets in health disparity research.",Leveraging Existing Data and Analytic Methods for Health Disparities Research Related to Aging and Alzheimer's Disease and Related Dementias (ADRD),10070960,R13AG069381,"['Academic Medical Centers', 'Address', 'Aging', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Area', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Databases', 'Dependence', 'Development', 'Diffuse', 'Diffusion', 'Discipline', 'Disease', 'Educational workshop', 'Epidemiology', 'Ethics', 'Ethnic Origin', 'Future', 'Generations', 'Genetic', 'Geographic Locations', 'Goals', 'Health', 'Health Services Accessibility', 'Healthcare Systems', 'Hospitals', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Knowledge', 'Machine Learning', 'Measures', 'Medical', 'Medical Care Costs', 'Medical center', 'Medicare', 'Medicare claim', 'Methodology', 'Methods', 'Modeling', 'Operative Surgical Procedures', 'Outcome', 'Participant', 'Patients', 'Persons', 'Population', 'Population Analysis', 'Prevalence', 'Process', 'Publications', 'Race', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Risk Factors', 'Series', 'Site', 'Source', 'Statistical Methods', 'Stochastic Processes', 'Stream', 'Subgroup', 'Text', 'Therapeutic', 'Time', 'Time trend', 'Universities', 'advanced analytics', 'age related', 'analytical method', 'analytical tool', 'base', 'burden of illness', 'clinical data warehouse', 'comorbidity', 'data resource', 'data warehouse', 'dementia risk', 'disorder risk', 'evidence base', 'health data', 'health disparity', 'health record', 'innovation', 'interest', 'mortality', 'multiple chronic conditions', 'news', 'population health', 'prospective', 'social', 'web site']",NIA,DUKE UNIVERSITY,R13,2020,50000,-0.04436588002111533
"Longitudinal and Intergenerational Determinants of Aging and Mortality ABSTRACT  Some of the most important open questions in aging relate to the impact of longitudinal and intergenerational factors. But documenting the role of early-life and intergenerational determinants of health and aging is limited by the dearth of large-scale micro-data containing this information. This is especially true for understudied populations such as women and minority groups.  Our research objective is to add critical information on cause of death to the new large-scale data resource, the Longitudinal, Intergenerational Family Electronic Micro-database (LIFE-M). Funded by the National Science Foundation, LIFE-M links millions of vital records (birth, marriage, and death certificates) to decennial censuses over four generations and 120 years for five states. LIFE-M is a representative sample of cohorts aging and dying in the last 25 years of the 20th century and includes crucial early-life and intergenerational information. Enhancing the LIFE-M with cause of death will facilitate path-breaking research on the relationship of longevity and cause of death with demographic, socio-economic, and early-life environmental factors for family networks across four generations.  We will achieve this objective by pursuing the following specific aims:  (1) We will use new “Smart Indexing” technology to digitize and cross-validate hand-written cause-of-death information;  (2) We will link digitized causes of death to the LIFE-M infrastructure and create extensive documentation for  this new variable for public use; and  (3) We will publicly release the cause-of-death variable and documentation with the LIFE-M dataset, meta-  data, and supporting documentation on ICPSR in 2020.  The proposed project will also have broader impacts. In addition to contributing a significant new data resource that can be added to Minnesota Population Center's historical linked censuses and the Census Longitudinal Infrastructure Project (CLIP), this project's methodological innovations in script digitization will enhance on-going and future data infrastructure initiatives. Both contributions promise to transform the research frontier in population health and aging in the United States. PROJECT NARRATIVE  This project contributes to public health knowledge by adding cause-of-death information to a new intergenerational and longitudinal dataset (LIFE-M). These data will allow much more research on the long- term determinants of health and aging, including a deeper understanding the intergenerational and early-life origins of later-life diseases and mortality in today's aging population.",Longitudinal and Intergenerational Determinants of Aging and Mortality,9829517,R01AG057704,"['Address', 'Adult', 'Affect', 'Age', 'Aging', 'Alzheimer&apos', 's Disease', 'American', 'Biogenesis', 'Birth', 'Birth Records', 'Cardiovascular Diseases', 'Cause of Death', 'Censuses', 'Cessation of life', 'Child', 'Childhood', 'Complement', 'Cost Savings', 'Data', 'Data Set', 'Databases', 'Death Certificates', 'Demography', 'Development', 'Disease', 'Documentation', 'Economics', 'Elderly', 'Enrollment', 'Environmental Risk Factor', 'Epidemiology', 'Exposure to', 'Family', 'Foundations', 'Funding', 'Future', 'Generations', 'Genetic Transcription', 'Hand', 'Handwriting', 'Health', 'Health Campaign', 'Health Resources', 'Health and Retirement Study', 'Hypertension', 'Image', 'Immigrant', 'Income', 'Individual', 'Inequality', 'Infant', 'Infrastructure', 'Lead', 'Life', 'Link', 'Longevity', 'Maiden Name', 'Malignant Neoplasms', 'Marriage', 'Medicare/Medicaid', 'Metadata', 'Methodology', 'Michigan', 'Minnesota', 'Minority Groups', 'Names', 'Pilot Projects', 'Population', 'Price', 'Process', 'Public Health', 'Recording of previous events', 'Records', 'Research', 'Research Infrastructure', 'Role', 'Sample Size', 'Sampling', 'Sanitation', 'Science', 'Subgroup', 'Surveys', 'Technology', 'Toxin', 'United States', 'Universities', 'Vaccines', 'Water', 'Woman', 'Women&apos', 's Group', 'aging population', 'base', 'cohort', 'convolutional neural network', 'cost', 'cost effective', 'data infrastructure', 'data resource', 'early-life nutrition', 'ethnic minority population', 'frontier', 'health knowledge', 'improved', 'indexing', 'innovation', 'intergenerational', 'large scale data', 'longitudinal dataset', 'machine learning algorithm', 'mortality', 'panel study of income dynamics', 'population health', 'population survey', 'programs', 'racial and ethnic', 'response', 'socioeconomics', 'suicide rate']",NIA,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,307227,-0.013330769282477914
"Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt Founded in 1965 as one of the original Intellectual and Developmental Disorders Research Centers (IDDRC), the Vanderbilt Kennedy Center (VKC) IDDRC serves as the central nexus across Vanderbilt for interdisciplinary research, communication, and training in intellectual and developmental disabilities (IDD). The VKC IDDRC serves as a trans-institutional institute that brings together over 200 faculty from 38 departments in 10 schools at Vanderbilt. The VKC’s mission to facilitate discoveries that inform best practices to improve the lives of people with IDD and their families. This mission is met by leveraging our outstanding institutional resources and support, partnering with disability communities, and capitalizing on synergistic interactions across the VKC’s federally-designated centers: the VKC IDDRC, a University Center of Excellence in Developmental Disabilities and a Leadership Education in Neurodevelopmental Disabilities program. The IDDRC as the centerpiece of the VKC is the foundational organizing structure that creates a “Center culture” wherein research and discovery permeates the VKC’s broader training and service activities, thus enhancing the translational research goals of the IDDRC. Demonstrable IDDRC success includes 976 investigator- authored publications and robust NIH funding to Vanderbilt to support IDD-related research ($52.6M in FY20). Harnessing and leveraging this trans-institutional strength to focus on unique challenges in IDD, the overarching goal of the next phase of the IDDRC is to develop precision care for IDD by providing infrastructure and scientific leadership to enable rapid translation of basic discoveries into high- impact IDD interventions and treatments. Three global Aims guide the IDDRC’s work. Aim 1 provides core services to enable and disseminate impactful research on individualizing treatments based upon the causes, mechanisms, and contributing co-morbid sequelae of IDD; Aim 2 focuses on incorporating innovative methods and approaches to enhance multidisciplinary IDD research; and Aim 3 proposes to conduct a signature research project to improve the precision use of antipsychotic medication in people with autism. Across these Aims and five Cores supported by the IDDRC (Administrative, Clinical Translational, Translational Neuroscience, Behavioral Phenotyping, and Data Sciences), three themes permeate our work: (1) recruitment of highly-skilled researchers not currently conducting IDD research (non-traditional researchers); (2) inclusion of IDD participants into research studies that currently do not include IDD (non-traditional subjects); and (3) incorporation of novel scientific approaches and methods (non-traditional approaches). Our IDDRC is ideally posed to enable rapid discovery of precision care approaches by supporting 50 investigators leading 70 research projects (15 from NICHD) and, as highlighted by the Signature Research Project, to promote and implement generative, novel, and impactful research directions, thus meeting the NICHD’s vision of applying newly evolved technologies and approaches to rapidly accelerate the prevention and/or amelioration of IDDs. PUBLIC HEALTH RELEVANCE: As a group, intellectual and developmental disabilities, including Down syndrome and autism spectrum disorder, have dramatic effects on affected people’s and their caregiver’s lives. Unfortunately, there remains a lack of understanding about what causes these disabilities and, critically, how to treat them with targeted therapies. The Vanderbilt Kennedy Center’s Intellectual and Developmental Disabilities Research Center serves as the hub for Vanderbilt’s research efforts focusing on improving the lives of people with intellectual and developmental disabilities by understanding the causes of these disorders and developing and testing therapies tailored to each individual’s precise needs.",Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt,10085550,P50HD103537,"['Academic Medical Centers', 'Affect', 'Antipsychotic Agents', 'Basic Science', 'Behavioral', 'Biomedical Research', 'Caregivers', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communication', 'Communities', 'Computerized Medical Record', 'Data', 'Data Science', 'Development', 'Developmental Disabilities', 'Diagnosis', 'Disease', 'Disease model', 'Down Syndrome', 'Education', 'Evaluation', 'Faculty', 'Family', 'Foundations', 'Funding', 'Future', 'Gap Junctions', 'Genotype', 'Goals', 'Image', 'Individual', 'Infrastructure', 'Institutes', 'Intellectual and Developmental Disabilities Research Centers', 'Intellectual functioning disability', 'Interdisciplinary Study', 'Intervention', 'Leadership', 'Longevity', 'Machine Learning', 'Medical Records', 'Methods', 'Mission', 'Modeling', 'National Institute of Child Health and Human Development', 'Neurodevelopmental Disability', 'Obesity', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Phase', 'Pilot Projects', 'Policy Research', 'Prevention', 'Problem behavior', 'Publications', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Sampling', 'Schools', 'Series', 'Services', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Universities', 'Vision', 'Weight Gain', 'Work', 'autism spectrum disorder', 'base', 'behavioral phenotyping', 'clinical translation', 'comorbidity', 'cost effective', 'developmental disease', 'disability', 'drug-induced weight gain', 'experience', 'image processing', 'implementation science', 'improved', 'individualized medicine', 'innovation', 'large datasets', 'lectures', 'meetings', 'multidisciplinary', 'novel', 'personalized approach', 'personalized care', 'personalized medicine', 'population based', 'pragmatic trial', 'predictive modeling', 'programs', 'public health relevance', 'recruit', 'research study', 'success', 'targeted treatment', 'translational neuroscience', 'trial comparing']",NICHD,VANDERBILT UNIVERSITY MEDICAL CENTER,P50,2020,1387605,-0.02182556424174728
"Biomedical Informatics Section (BIS) project support of high-priority HIV projects We interact with NIDA/IRP investigators to develop biomedical informatics applications and solutions that can access, manage, disseminate, and analyze large quantities of high-quality data. We develop, research, and/or apply computational tools to assist in the acquisition and analysis of biological, medical behavioral or health data, within a specific time frame determined to be appropriate by the NIDA. We help facilitate new system initiatives and changes to existing systems to meet legislative, regulatory, and departmental requirements within the specific time frames designated by each requirement. We conduct routine system analysis of automatic data processing resources and techniques of existing projects. We recommend, as needed, the implementation of new technologies that are efficient and timely.  We provide technical and professional support to help integrate computer systems, design or acquire computer programs, configure and support networks and streamline automated data processing within the NIDA's specified timeframe. We also integrate scientific data systems with network services and security services to foster safe and secure NIDA/IRP laboratory collaboration and for collaboration with extramural entities, when possible integrate scientific data systems with one another and ensure design for interoperable data.  We designed and deployed a mobile solution in collaboration with NIDA/IRP Archway clinic which enables studying craving and mood related to opioid and cocaine use among asymptomatic HCV+ and HCV methadone patients who have not started antiviral treatment. The smartphone-based system is capable of delivering a flexible ecological momentary assessment solution with multi-modal prompting operations as well as having enhanced integrated geolocation recording capabilities dynamically linking craving and mood to the whereabouts of the participants. Additionally, a collaborated project entitled ANCHOR: A novel Model of Hepatitis C Treatment as Anchor to Prevent HIV, Initiate Opioid Substitution Therapy, and Reduce Risky Behavior, is underway for data collection in order to evaluate a model of care for treatment of hepatitis C in people with ongoing injection drug use. These systems have been used in multiple protocols and are currently used in studies to better understand why some people are more likely to engage in HIV prevention behaviors. n/a",Biomedical Informatics Section (BIS) project support of high-priority HIV projects,10267587,ZIHDA000631,"['AIDS prevention', 'Adherence', 'Antiviral Agents', 'Archives', 'Area', 'Automatic Data Processing', 'Baltimore', 'Behavior', 'Behavioral', 'Big Data', 'Biological', 'Caring', 'Cellular Phone', 'Clinic', 'Clinical Decision Support Systems', 'Clinical Informatics', 'Clinical Research', 'Collaborations', 'Collection', 'Communication', 'Computational Science', 'Computer Security', 'Continuity of Patient Care', 'Data', 'Data Collection', 'Databases', 'Development', 'Ecological momentary assessment', 'Enrollment', 'Ensure', 'Environment', 'Extramural Activities', 'Fostering', 'Goals', 'Guidelines', 'HIV', 'HIV Seronegativity', 'HIV Seropositivity', 'HIV risk', 'Healthcare', 'Hepatitis C Therapy', 'Hepatitis C virus', 'Informatics', 'Information Management', 'Information Resources Management', 'Information Sciences', 'Information Systems', 'Information Technology', 'Infrastructure', 'Intelligence', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Laws', 'Link', 'Location', 'Machine Learning', 'Maintenance', 'Medical', 'Medical Informatics', 'Medical Records', 'Methods', 'Modeling', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Neighborhoods', 'Opioid replacement therapy', 'Outcome', 'Participant', 'Phylogenetic Analysis', 'Policies', 'Procedures', 'Process', 'Protocols documentation', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Risk Behaviors', 'Science', 'Secure', 'Security', 'Services', 'Site', 'Social Network', 'Specific qualifier value', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Training Support', 'Treatment Protocols', 'base', 'biomedical informatics', 'cocaine use', 'computer program', 'computer science', 'computer system design', 'computerized tools', 'craving', 'data interoperability', 'data mining', 'design', 'flexibility', 'handheld mobile device', 'health data', 'improved', 'injection drug use', 'methadone patient', 'mobile computing', 'multimodality', 'new technology', 'novel', 'operation', 'opioid use', 'opioid user', 'prevent', 'programs', 'support network', 'telework', 'transmission process']",NIDA,NATIONAL INSTITUTE ON DRUG ABUSE,ZIH,2020,4525708,-0.06012732228778059

text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Development of assistive self-care robot technologies for people with disabilities Towards Autonomy in Daily Living: A Formalism for Intelligent Assistive Feeding Systems  Applicant PI, Tapomayukh Bhattacharjee Overview We propose to develop a design space framework and co-design methodology for the development of assistive self-care robot technologies that are informed by the social model of disability. Our model of assistive robots in the domain of self-care considers an individual's social and environmental context, coping processes and other factors that can affect independent functioning. Our design methods utilize embedded sensing to intelligently respond to these con- siderations. We speciﬁcally focus on assistive feeding tasks, proposing a formalism that enables a robotic system to feed a person with upper-extremity disability. Our guiding principle is that human-level interaction is feasible only if the robot itself relies on human-level semantics. We im- plement this principle by relying on data to learn and develop object-dependent control policies and timing models for acquiring and transferring a bite to a user at a proper time. The system's ob- server detects world states and arbitrator invokes different control policies based on these states. The tangible result will be an intelligent assistive feeding robot whose performance can generalize to different activities, adapt to user preferences, and recover from failures. Objectives and Relevance to NIH A design framework for assistive robots would provide for- malisms that let us address the fundamental challenge of designing robots that are responsive to context of use and support assisted self-care in a variety of social settings. We combine method- ologies from human-robot interaction, cognitive science, machine learning, robotics and haptics with user studies and our formalism to address the following research questions: (Q1) Mechanics of Feeding-Control Policies: How can control policies be designed for dexterous non-prehensile manipu- lation of deformable objects such as food? (Q2) Social Aspects of Feeding-Bite Timing: How should an assistive feeding robot decide the right timing for feeding a user? (Q3) Human-in-the-Loop: How can human-directed feedback be added into the loop for an autonomous assistive feeding system?  The proposed work will allow users with upper-arm disabilities to use this system for intelli- gent assistance with daily feeding tasks. This can in turn help them increase their independence and autonomy making eating easier and more enjoyable. While we presently focus on this spe- ciﬁc application, the tools and insights we gain can generalize to the ﬁelds of robotic assistance and human-robot interaction across other activities of daily living and instrumental activities of daily living. Thus, our work is clearly motivated by the intent to improve the quality of health and life of the aging population and is very relevant to the theme of NIH. 1 Towards Autonomy in Daily Living: A Formalism for Intelligent Assistive Feeding Systems  Applicant PI, Tapomayukh Bhattacharjee  The proposed work will allow users with upper-arm disabilities to use this system for intelli- gent assistance with daily feeding tasks, potentially increasing their independence and autonomy making eating easier and more enjoyable. The long-term promise of this research is to have robots in society that are able to seamlessly and ﬂuently perform complex manipulation tasks in dynamic human environments in real homes which could impact individuals with other disabilities as well as able-bodied individuals. Through improved access to independent living and customizing to the unique needs and preferences of users, the results of this project can positively impact mil- lions of people worldwide, especially given the vast variability in our target population by being transformational in the scalability of assistive robotics for self-care. 1",Development of assistive self-care robot technologies for people with disabilities,10232054,F32HD101192,"['Activities of Daily Living', 'Address', 'Affect', 'Aging', 'Bathing', 'Bite', 'Caregiver Burden', 'Caring', 'Child', 'Cognitive Science', 'Communities', 'Complex', 'Cues', 'Custom', 'Data', 'Development', 'Disabled Persons', 'Eating', 'Emotional', 'Environment', 'Expert Systems', 'Failure', 'Family', 'Feedback', 'Food', 'Generations', 'Health', 'Home environment', 'Human', 'Improve Access', 'Independent Living', 'Individual', 'Intelligence', 'Learning', 'Life', 'Machine Learning', 'Mechanics', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Panthera leo', 'Parents', 'Performance', 'Persons', 'Play', 'Policies', 'Population', 'Process', 'Quality of life', 'Research', 'Robot', 'Robotics', 'Role', 'Self Care', 'Semantics', 'Societies', 'Sterile coverings', 'System', 'Target Populations', 'Taxonomy', 'Technology', 'Time', 'Tweens', 'United States National Institutes of Health', 'Upper Extremity', 'Upper arm', 'Work', 'aging population', 'assistive robot', 'base', 'care recipients', 'coping', 'design', 'disability', 'experience', 'experimental study', 'feeding', 'haptics', 'human subject', 'human-in-the-loop', 'human-robot interaction', 'improved', 'insight', 'instrumental activity of daily living', 'intergenerational', 'kinematics', 'patient oriented', 'peer', 'preference', 'robot assistance', 'robotic system', 'social', 'social model', 'tool']",NICHD,UNIVERSITY OF WASHINGTON,F32,2021,68562
"Postdoctoral Training in Vision Research Abstract We propose an Institutional Training Grant to train 8 Postdoctoral Fellows in basic, clinical and rehabilitation science relevant to translational vision research at the Smith-Kettlewell Eye Research Institute (SKERI). Eighteen faculty whose expertise spans the areas of spatial and binocular vision, eye movements, strabismus, central vision loss, low vision and blindness rehabilitation, computer vision and assistive technology are available to train the postdoctoral fellows. The goal of the Fellowship program is to transition postdoctoral fellows towards independent research careers by the end of their 2-year fellowship. To this end, the program encourages the Fellow to develop an independent research project in collaboration with the mentor, to test and hone these ideas, and distill them into a grant proposal. The training will also provide a solid grounding in rigor and reproducibility and in the responsible conduct of research, as well as frequent and wide-ranging seminars, journal clubs and colloquia. Because the vast majority of SKERI Faculty are full-time researchers with no teaching duties and small laboratories, the Fellows experience a great deal of direct interaction with their sponsors. In addition, the Fellows have available to them many opportunities for interaction with the rest of the Faculty, which include basic, clinician and rehabilitation researchers. These interactions are facilitated by the researchers all being housed within the same building and all working in clinically relevant vision research. The Faculty-Fellow interactions represent all areas of the research process: proposal, critique, performance, and communication of findings through the writing of papers and preparation of presentations, as well as participation in scientific and ethics seminars. Importantly, Fellows are also in frequent contact with each other through organized events, adjacent open work-spaces, and the numerous collaborations among Faculty. The Fellowship program forms a critical component of the research vitality and capacity of SKERI. Because SKERI is not a degree-granting institution, its investigators do not typically have graduate students. It is widely appreciated within the Institute that Fellows bring in new ideas and techniques to the preceptors’ laboratories. The process of training Fellows encourages Faculty to challenge old assumptions, to develop clear and concise descriptions of why a given research activity is of significance, and to expand the range of approaches to research problems. The T32 Program will significantly augment SKERI’s internally funded Rachel C. Atkinson Fellowship, C.V. Starr Scholarship Program, and individual Fellowship awards from other sources to yield an overall program size of approximately 8-10 post-doctoral fellows. Public Health Relevance The proposed postdoctoral training program in vision and rehabilitation research will have a positive and direct impact on public health. The program is designed to train fellows to gain a deep understanding of vision and visual impairment to develop novel models of visual function that provide insights into disease prevention, monitoring and treatment, interventions for binocular vision disorders, rehabilitation techniques and assistive devices. Postdoctoral fellows emerging from the program will be able to launch their own independent research careers. Because research at Smith-Kettlewell is motivated by clinical problems, it is highly likely that our fellows will make significant contributions to vision health during and after their training.",Postdoctoral Training in Vision Research,10207176,T32EY025201,"['Vision research', 'post-doctoral training']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,T32,2021,70859
"Cooperative Control Robotics and Computer Vision: Development of Semi-Autonomous Temporal Bone and Skull Base Surgery Project Summary I am an assistant professor in the department of Otolaryngology-Head and Neck Surgery at the Johns Hopkins School of Medicine, where my practice is focused on neurotology and lateral skull base surgery. I am applying for a mentored surgeon-scientist career development award (CDA) to obtain further training in robotics, deep learning and computer vision. This will further my long-term career goals of improving neurotologic surgical outcomes through novel applications of engineering methods and ultimately to investigate semi-autonomous, robotic interventions in the inner ear and skull base which are beyond the limits of the human hand alone.  Operating in the temporal bone and lateral skull base is technically demanding due to complex three- dimensional anatomy, small working spaces and delicate neurovascular structures. Many of these challenges are ideally suited to semi-autonomous surgical platforms to augment a surgeon’s skills with robotic and image- guided assistance. Despite the widespread implementation of robotic surgery and image guidance in other areas of the body, the field of neurotology has had relatively little adoption of this technology. We believe one reason for this is the precise registration needed in this field, where millimeter differences differentiate a successful from a catastrophic result. This CDA proposes expanding on my prior work investigating cooperative control robotics and virtual safety barriers by using computer vision and deep learning networks to develop highly accurate surgical image registration. This CDA aims to provide me with multi-disciplinary training in the departments of Otolaryngology, Biomedical Engineering and Computer Science. Specific training goals include: (1) Training in robotics, statistical shape modeling and computer tomography landmark segmentation, (2) Training in deep learning networks and computer vision video image registration, (3) Integrating this training, with my knowledge of temporal bone and skull base surgery to develop into an independent investigator (4) Pursue additional training in the ethical and responsible conduct of research.  The research plan addresses the hypothesis that virtual safety barriers can be accurately enforced by a cooperative control robot, and computer vision methods can be used to automate accurate placement and registration of these safety barriers. I believe that the integration of these techniques will allow for semi- autonomous surgical methods resulting in improved surgical safety and efficiency. The specific aims of the proposal are to: (1) Develop and Validate Cooperative Control Robot Enforced Virtual Safety Barriers for Cortical Mastoidectomy (2) Develop and Test Autonomous Segmentation of Lateral Skull Base Anatomy (3) Develop Video-Based, Fiducial-less, Surgical Image Registration to Detect and Update the 3-D Position of Temporal Bone Anatomy from Intraoperative Stereoscopic Microscope Video. Project Narrative The research proposed here will investigate the feasibility of using novel applications of computer vision and cooperative control robotics to enforce virtual safety barriers in neurotologic surgery. Our long-term goal is to develop semi-autonomous, robotic methods to improve the safety and efficiency of neurotologic surgery, and open the possibilities for surgical interventions which currently are beyond the abilities of the human hand alone.",Cooperative Control Robotics and Computer Vision: Development of Semi-Autonomous Temporal Bone and Skull Base Surgery,10283480,K08DC019708,"['3-Dimensional', 'Address', 'Adoption', 'Algorithms', 'Anatomy', 'Area', 'Atlases', 'Automobile Driving', 'Automobiles', 'Biomedical Engineering', 'Cadaver', 'Cochlea', 'Complex', 'Computer Vision Systems', 'Computers', 'Consumption', 'Data', 'Drug Delivery Systems', 'Dura Mater', 'Electromagnetics', 'Engineering', 'Equilibrium', 'Ethics', 'Facial nerve structure', 'Feedback', 'Future', 'Goals', 'Hand', 'Head and Neck Surgery', 'Hearing', 'Human', 'Image', 'Image-Guided Surgery', 'Intervention', 'Judgment', 'K-Series Research Career Programs', 'Knowledge', 'Labyrinth', 'Lateral', 'Left', 'Manuals', 'Mentors', 'Methods', 'Microscope', 'Modeling', 'Modernization', 'Monitor', 'Motion', 'Movement', 'Operative Surgical Procedures', 'Otolaryngology', 'Outcome', 'Patients', 'Positioning Attribute', 'Postoperative Period', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Safety', 'Scientist', 'Shapes', 'Sigmoid colon', 'Site', 'Speed', 'Structure', 'Surface', 'Surgeon', 'Surgical Instruments', 'Surgical complication', 'System', 'Tactile', 'Techniques', 'Technology', 'Temporal bone structure', 'Testing', 'Time', 'Training', 'Tremor', 'Update', 'Work', 'X-Ray Computed Tomography', 'automated segmentation', 'base', 'career', 'computer science', 'deep learning', 'digital', 'digital imaging', 'image guided', 'image registration', 'improved', 'instrument', 'learning network', 'medical schools', 'microscopic imaging', 'millimeter', 'multidisciplinary', 'neurovascular', 'novel', 'professor', 'responsible research conduct', 'robot control', 'simulation', 'skills', 'skull base', 'stereoscopic', 'success', 'surgery outcome', 'three-dimensional modeling', 'virtual', 'vision development']",NIDCD,JOHNS HOPKINS UNIVERSITY,K08,2021,191792
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,10129965,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2021,403882
"Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit SUMMARY  Many of the estimated four million adults in the U.S. with severe speech and physical impairments (SSPI) resulting from neurodevelopmental or neurodegenerative diseases cannot rely on current assistive technologies (AT) for communication. During a single day, or as their disease progresses, they may transition from one access technology to another due to fatigue, medications, changing physical status, or progressive motor dysfunction. There are currently no clinical or AT solutions that adapt to the multiple, dynamic access needs of these individuals, leaving many people poorly served. This competitive renewal, called BCI-FIT (Brain Computer Interface-Functional Implementation Toolkit) adds to our innovative multidisciplinary translational research conducted over the past 11 years for the advancement of science related to non-invasive BCIs for communication for these clinical populations. BCI-FIT relies on active inference and transfer learning to customize a completely adaptive intent estimation classifier to each user's multiple modality signals in real-time. The BCI-FIT acronym has many implications: our BCI fits to each user's brain signals; to the environment, offering relevant personal language; to the user's internal states, adjusting signals based on drowsiness, medications, physical and cognitive abilities; and to users' learning patterns from BCI introduction to expert use.  Three specific aims are proposed: (1) Develop and evaluate methods for optimizing system and user performance with on-line, robust adaptation of multi-modal signal models. (2) Develop and evaluate methods for efficient user intent inference through active querying. (3) Integrate language interaction and letter/word supplementation as input modalities in real-time BCI use. Four single case experimental research designs will evaluate both user performance and technology performance for functional communication with 35 participants with SSPI in the community, and 30 healthy controls for preliminary testing. The same dependent variables will be tested in all experiments: typing accuracy (correct character selections divided by total character selections), information transfer rate (ITR), typing speed (correct characters/minute), and user experience (UX) questionnaire responses about comfort, workload, and satisfaction. Our goal is to establish individualized recommendations for each user based on a combination of clinical and machine expertise. The clinical expertise plus user feedback added to active sensor fusion and reinforcement learning for intent inference will produce optimized multi-modal BCIs for each end-user that can adjust to short- and long-term fluctuating function. Our research is conducted by four sub-teams who have collaborated successfully to implement translational science: Electrical/computer engineering; Neurophysiology and systems science; Natural language processing; and Clinical rehabilitation. The project is grounded in solid machine learning approaches with models of participatory action research and AAC participation. This project will improve technologies and BCI technical capabilities, demonstrate BCI implementation paradigms and clinical guidelines for people with severe disabilities. PROJECT NARRATIVE The populations of US citizens with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies, as proposed in BCI-FIT. This project implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit,10213005,R01DC009834,"['Adult', 'Attention', 'Behavioral', 'Brain', 'Calibration', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Clinical assessments', 'Cognition', 'Cognitive', 'Communication', 'Communities', 'Computers', 'Custom', 'Data', 'Decision Making', 'Disease', 'Drowsiness', 'Electroencephalography', 'Engineering', 'Environment', 'Eye Movements', 'Fatigue', 'Feedback', 'Goals', 'Guidelines', 'Head Movements', 'Impairment', 'Individual', 'Informed Consent', 'Knowledge', 'Language', 'Learning', 'Letters', 'Life', 'Locked-In Syndrome', 'Machine Learning', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Motor Skills', 'Movement', 'Muscle', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Participant', 'Partner Communications', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Policies', 'Population', 'Protocols documentation', 'Psychological Transfer', 'Psychological reinforcement', 'Public Health', 'Questionnaires', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Design', 'Role', 'Science', 'Secondary to', 'Self-Help Devices', 'Sensory', 'Signal Transduction', 'Solid', 'Source', 'Speech', 'Speed', 'Supplementation', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Vocabulary', 'Workload', 'acronyms', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinical implementation', 'cognitive ability', 'community based participatory research', 'computer science', 'disability', 'experience', 'experimental study', 'improved', 'innovation', 'learning strategy', 'motor disorder', 'multidisciplinary', 'multimodality', 'neurophysiology', 'phrases', 'residence', 'response', 'satisfaction', 'sensor', 'signal processing', 'simulation', 'spelling', 'theories', 'visual tracking']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2021,915264

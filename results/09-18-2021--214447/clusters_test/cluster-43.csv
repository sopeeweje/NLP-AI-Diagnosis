text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Human Face Representation in Deep Convolutional Neural Networks The human visual system can recognize a familiar face across wide variations of viewpoint, illumination, expression, and appearance. This remarkable computational feat is accomplished by large-­scale networks of neurons. We will test a face space theory of the representations that emerge at the top layer of deep learning convolutional neural networks (DCNNs) as a model of human visual representations of faces. Computer-based face recognition has improved in recent years due to DCNNs and the easy availability of labeled training data (faces and identities) from the web. Inspired by the primate visual system, DCNNs are feed­forward artificial neural networks that can map images of faces into representations that support recognition over widely variable images. Although the calculations executed by the simulated neurons are simple, enormous numbers of computations are used to convert an image into a representation. The end result of this processing is a highly compact representation of a face that retains image detail in an invariant, identity­-specific face code. This code is fundamentally different than any representation of faces considered in vision science. This theory we test combines key components of previous face space models (similarity, learning history) with new features (imaging conditions, personal face history) in a unitary space that represents both identity and facial appearance across variable images. We will test whether this model can account for human recognition of familiar faces, which is highly robust to image variability (pose, illumination, expression). The model will also be applied to understanding long standing difficulties humans (and machines) have with faces of other races. We aim to bridge critical gaps in our knowledge of how DCNNs work, linking psychological, neural, and computational perspectives. A fundamentally new theory of face representation will alter the questions we ask about face representations in all three fields. A new focus on understanding how we (or neural networks) “perceive” a single familiar identity in widely variable images will give rise to a search for representations that gracefully merge the properties of faces with the real-­world image conditions in which they are experienced. This project presents a unique opportunity to study, manipulate, and learn from these representations, and to apply the findings to broader questions about high-­level vision from neural and perceptual perspectives. Human recognition of familiar faces is highly robust to image variability (pose, illumination, expression)—a skill that is likely due to the quality and quantity of experience we have with the faces of people we know well. Deep convolutional neural networks are modeled after the primate visual system and have made impressive gains recently on the problem of robust face recognition. Understanding the visual nature of the face “feature” codes that emerge in these networks can give insight into long-standing questions about how the human visual system can, but does not always, represent a face in a way that generalizes across images that vary widely.",Human Face Representation in Deep Convolutional Neural Networks,10129967,R01EY029692,"['Affect', 'Appearance', 'Categories', 'Code', 'Computers', 'Data', 'Data Set', 'Face', 'Face Processing', 'Familiarity', 'Human', 'Image', 'Individual', 'Internet', 'Knowledge', 'Label', 'Learning', 'Lighting', 'Link', 'Maps', 'Methods', 'Modeling', 'Nature', 'Neural Network Simulation', 'Neurons', 'Performance', 'Persons', 'Primates', 'Property', 'Published Comment', 'Race', 'Recording of previous events', 'Space Models', 'Testing', 'Training', 'Variant', 'Vision', 'Visual', 'Visual system structure', 'Work', 'artificial neural network', 'base', 'convolutional neural network', 'deep learning', 'experience', 'feedforward neural network', 'human model', 'improved', 'insight', 'neural network', 'psychologic', 'relating to nervous system', 'representation theory', 'skills', 'theories', 'vision science']",NEI,UNIVERSITY OF TEXAS DALLAS,R01,2021,361998
"Shared and specific mechanisms of auditory and visual category learning PROJECT SUMMARY/ABSTRACT The ability to learn new perceptual categories enables some of the most complex human behaviors, from speech perception to visual object recognition. Current understanding of the mechanisms involved in perceptual category learning relies on the fundamental assumption that the processes underlying such learning are shared across the senses. However, the vast majority of this work has focused on the visual modality. As a consequence, the research regarding how humans learn to group complex auditory information into categories has relied greatly on conclusions from the research in the visual domain without testing this critical assumption. However, recent evidence from the attention literature suggests that even seemingly domain-general cognitive processes, such as working memory, are accomplished via sensory-biased regions in frontal cortex. The current investigation will directly compare the computational and neural mechanisms supporting auditory and visual category learning by training the same individuals on categories in both modalities while in an fMRI scanner. Aim #1 of this investigation will identify the shared and sensory-biased circuits supporting feedback processing during auditory and visual category learning. If the neural circuits supporting perceptual category learning are shared across the modalities, it is expected that similar regions will be recruited to a similar extent during feedback processing. If instead, the neural circuits are distinct for particular modalities, it is expected that sensory-biased regions will emerge as supporting category learning for auditory and visual modalities. Aim #2 will utilize advanced machine learning techniques (multivariate pattern classification and representational similarity analyses) to characterize the emergence of category-level neural representations over the course of learning. Aim #3 will identify the functional and structural connectivity of the circuits as they contribute to perceptual category learning. The proposed research will directly test the fundamental assumption about the nature of this complex problem that affects everyday behaviors. This research has the potential to impact understanding of cases where modality- specific learning abilities might be impaired, such as phonetic learning and language-related impairments in dyslexia, autism, and specific language impairment. The proposed research will provide the training foundation to support the PI’s long-term objective of developing theories of perceptual category learning that are constrained by neurobiology and behavior and will specify the behavioral, computational, and neural mechanisms of such learning. This project presents the opportunity to directly test a critical assumption underlying understanding of perceptual category learning. The proposed research will take place in an exceptional training environment and the PI will be mentored by a team of knowledgeable and accomplished scientists. The research will provide the PI with training in functional magnetic resonance experiment design and analysis which will prepare her well for a career as an independent scientist in computational cognitive neuroscience. PROJECT NARRATIVE The proposed research will contribute to fundamental knowledge about how seemingly general-purpose cognitive systems may demonstrate modality specificity. The goal of this investigation is to characterize the differences in cognitive processing during category learning when the information comes from the auditory or visual modalities. The findings from this work may inform mechanistic approaches to understanding modality- specific deficits in language-based disorders, such as dyslexia, autism, and specific language impairment.",Shared and specific mechanisms of auditory and visual category learning,10197776,F32DC018979,"['Affect', 'Area', 'Attention', 'Auditory', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Classification', 'Cognitive', 'Complex', 'Corpus striatum structure', 'Disease', 'Dyslexia', 'Environment', 'Esters', 'Feedback', 'Finches', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hobbies', 'Human', 'Hybrids', 'Impairment', 'Individual', 'Investigation', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Magnetic Resonance', 'Mentors', 'Modality', 'Modeling', 'Nature', 'Neurobiology', 'Participant', 'Pattern', 'Process', 'Property', 'Research', 'Scientist', 'Sensory', 'Short-Term Memory', 'Specific qualifier value', 'Specificity', 'Speech', 'Speech Perception', 'Structure', 'Techniques', 'Testing', 'Theoretical model', 'Training', 'Visual', 'Work', 'auditory stimulus', 'autism spectrum disorder', 'base', 'behavior measurement', 'career', 'cognitive neuroscience', 'cognitive process', 'cognitive system', 'design', 'experimental study', 'frontal lobe', 'individual variation', 'innovation', 'learning ability', 'neural circuit', 'neuromechanism', 'object recognition', 'programs', 'recruit', 'relating to nervous system', 'response', 'sound', 'specific language impairment', 'theories', 'visual learning']",NIDCD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,F32,2021,65994
"Cortical computations underlying binocular motion integration PROJECT SUMMARY / ABSTRACT Neuroscience is highly specialized—even visual submodalities such as motion, depth, form and color processing are often studied in isolation. One disadvantage of this isolation is that results from each subfield are not brought together to constrain common underlying neural circuitry. Yet, to understand the cortical computations that support vision, it is important to unify our fragmentary models that capture isolated insights across visual submodalities so that all relevant experimental and theoretical efforts can benefit from the most powerful and robust models that can be achieved. This proposal aims to take the first concrete step in that direction by unifying models of direction selectivity, binocular disparity selectivity and 3D motion selectivity (also known as motion-in-depth) to reveal circuits and understand computations from V1 to area MT. Motion in 3D inherently bridges visual submodalities, necessitating the integration of motion and binocular processing, and we are motivated by two recent paradigm-breaking physiological studies that have shown that area MT has a robust representation of 3D motion. In Aim 1, we will create the first unified model and understanding of the relationship between pattern and 3D motion in MT. In Aim 2, we will construct the first unified model of motion and disparity processing in MT. In Aim 3, we will develop a large-scale biologically plausible model of these selectivities that represents realistic response distributions across an MT population. Having a population output that is complete enough to represent widely-used visual stimuli will amplify our ability to link to population read-out theories and to link to results from psychophysical studies of visual perception. Key elements of our approach are (1) an iterative loop between modeling and electrophysiological experiments; (2) building a set of shared models, stimuli, data and analysis tools in a cloud-based system that unifies efforts across labs, creating opportunities for deep collaboration between labs that specialize in relevant submodalities, and encouraging all interested scientists to contribute and benefit; (3) using model-driven experiments to answer open, inter-related questions that involve motion and binocular processing, including motion opponency, spatial integration, binocular integration and the timely problem of how 3D motion is represented in area MT; (4) unifying insights from filter-based models and conceptual, i.e., non-image- computable, models to generate the first large-scale spiking hierarchical circuits that predict and explain how correlated signals and noise are transformed across multiple cortical stages to carry out essential visual computations; and (5) carrying out novel simultaneous recordings across visual areas. This research also has potential long-term benefits in medicine and technology. It will build fundamental knowledge about functional cortical circuitry that someday may be useful for interpreting dysfunctions of the cortex or for helping biomedical engineers construct devices to interface to the brain. Insights gained from the visual cortex may also help to advance computer vision technology. NARRATIVE The processing of visual motion and depth information is essential for a wide variety of important human abilities, including navigating through the world, avoiding collisions, catching and grabbing objects and interpreting complex scenes. To understand how neurons in the visual cortex transform and represent the information that underlies these abilities, we aim to initiate the development of a more complete, biologically constrained and openly available computer model of motion and depth processing that will be used to guide, and to interpret and incorporate results from, primate visual neurophysiological and psychophysical experiments. Gaining an understanding of the normal function of cortical neural circuitry is an important step in building the fundamental knowledge that someday may help to improve the ability to assess dysfunctions of the cortex and may help bioengineers create devices that interface to cortical circuitry to treat disorders and overcome disabilities.",Cortical computations underlying binocular motion integration,10188534,R01EY027023,"['3-Dimensional', 'Affect', 'Architecture', 'Biological', 'Biomedical Engineering', 'Brain', 'Collaborations', 'Color', 'Complex', 'Computer Models', 'Computer Vision Systems', 'Cues', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disadvantaged', 'Discrimination', 'Disease', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Foundations', 'Frequencies', 'Functional disorder', 'Human', 'Joints', 'Knowledge', 'Link', 'Literature', 'Medicine', 'Modeling', 'Motion', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Primates', 'Production', 'Psychophysics', 'Reproducibility', 'Research', 'Role', 'Scientist', 'Signal Transduction', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Vision Disparity', 'Visual', 'Visual Cortex', 'Visual Motion', 'Visual Perception', 'area MT', 'base', 'cloud based', 'color processing', 'disability', 'experimental study', 'extrastriate visual cortex', 'fitness', 'improved', 'in vivo', 'insight', 'interest', 'neural circuit', 'neurophysiology', 'novel', 'predictive modeling', 'relating to nervous system', 'response', 'spatial integration', 'spatiotemporal', 'theories', 'tool', 'visual neuroscience', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF WASHINGTON,R01,2021,387514
"Structural and functional tests of ganglion cell damage in glaucoma This project will use a combination of structural and functional measurements to test the hypothesis that early- stage damage in human glaucoma occurs first in the inner plexiform layer (IPL) of the retina – especially its OFF sub-lamina – as suggested by murine glaucoma models. In the first Aim, we will use a novel visible-light optical coherence tomograph (VIS OCT) to study structural changes in the retina of glaucoma patients. The newly developed VIS OCT has sufficient image contrast and resolution to segment the IPL boundaries and to define sub-lamination in volumetric OCT data, something not currently possible with existing near-infrared OCT instruments. We will make comparative measurements within the IPL and between the IPL, the ganglion cell layer (GCL) and the retinal nerve fiber layer (RNFL). Because data from mouse models of glaucoma suggests that early damage occurs preferentially within the OFF sub-lamina of the IPL, we will make separate VIS OCT measurements biased for the OFF- and ON-sublaminae of the IPL and use machine learning approaches to determine whether a similar damage process can be demonstrated in human. To test whether OFF-pathway function is preferentially lost in glaucoma, we will use a novel Steady-State Visual Evoked Potential (SSVEP) paradigm that employs sawtooth increments and decrements to bias the measurement to ON vs OFF pathways, respectively, a paradigm our data suggests discriminates glaucoma from control patients. The second Aim will optimize this SSVEP measurement for testing localized areas of the visual field. The third Aim will make comparative measurements of visual-field, VIS OCT and SSVEP loss patterns in a large sample of glaucoma patients and in age- and sex-matched controls. Thickness and interface reflectivity amplitude maps derived from VIS OCT imaging of the RNFL, GCL and IPL including sublaminae will be correlated topographically with visual field defects to assess the relative sensitivity of our structural biomarkers at and near visual field locations with demonstrable losses on conventional (Humphrey) perimetry. Similarly, SSVEP responses from different locations in the visual field will be correlated topographically with visual field loss patterns and to VIS OCT losses, with special emphasis on correlating structural damage in OFF vs ON sub-laminae of the IPL with the functional correlates derived from regional decremental and incremental SSVEPs. Separately and in combination, our structural and functional measurements are designed to provide strong tests of the biological hypothesis that the OFF pathway is preferentially damaged in human glaucoma, and to reveal new biomarkers for the disease. Improving visual outcomes in glaucoma will require a better understanding of the earliest sites and processes of damage and methods to measure them quickly and accurately in patients. This project will address both needs through a combination of novel Optical Coherence Tomography and electrophysiological measurements. The new imaging and electrophysiological tests that will be developed here, either separately or together, could eventually replace conventional visual field testing which is time-consuming and unreliable.",Structural and functional tests of ganglion cell damage in glaucoma,10150874,R01EY030361,"['Address', 'Affect', 'Age', 'Animal Model', 'Area', 'Atrophic', 'Biological Assay', 'Biological Markers', 'Biological Testing', 'Clinical', 'Complex', 'Consumption', 'Data', 'Disease', 'Early Diagnosis', 'Early treatment', 'Economic Burden', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Frequencies', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Inner Plexiform Layer', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modification', 'Mus', 'Noise', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Perimetry', 'Process', 'Property', 'Resolution', 'Retina', 'Retinal Ganglion Cells', 'Rodent Model', 'Sampling', 'Scotoma', 'Severities', 'Signal Transduction', 'Site', 'Specificity', 'Speed', 'Structural defect', 'Structure', 'Synapses', 'Techniques', 'Testing', 'Thick', 'Time', 'Visible Radiation', 'Vision', 'Visual', 'Visual Fields', 'Visual evoked cortical potential', 'base', 'cell injury', 'comparative', 'contrast imaging', 'design', 'detection method', 'extrastriate visual cortex', 'field study', 'ganglion cell', 'improved', 'instrument', 'mouse model', 'novel', 'optic nerve disorder', 'response', 'retinal imaging', 'retinal nerve fiber layer', 'sex']",NEI,STANFORD UNIVERSITY,R01,2021,499625
"Novel Perceptual and Oculomotor Heuristics for Enhancing Radiologic Performance PROGRAM SUMMARY Radiological imaging is often the first step of the diagnostic pathway for many devastating diseases; thus, an erroneous assessment of “normal” can lead to death. Whereas a grayscale object in an image can be described by its first-order image statistics—such as contrast, spatial frequency, position, entropy, and orientation—none of these dimensions, by itself, indicates abnormal vs normal radiological findings. We are a highly diverse team proposing an empirical approach to determine the mixtures of the first-order statistics—the “visual textures”— that radiology experts explicitly and implicitly use to identify the locations of potential abnormalities in medical images. Our innovative approach does not rely on assumptions about which textures may or may not be im- portant to abnormality detection. Instead, we will track the oculomotor behavior of expert radiologists to deter- mine their conscious and unconscious targeting choices, and thus ascertain which textures are empirically in- formative. The ability of expert radiologists to rapidly find abnormalities suggests that they may be able to first identify them in their retinal periphery. Peripheral visual analysis skills are therefore potentially critical to radio- logic performance, despite being understudied. We will measure these skills and leverage the results to develop perceptual learning heuristics to improve peripheral abnormality texture detection. By comparing novices to ex- perts we will determine whether the first are inexpert due to a lack of sensitivity to diagnostically relevant textures (texture informativeness), or to a lack of knowledge about which textures are abnormal, or to a combined lack of both sensitivity and knowledge. Radiology also requires the acquisition of oculomotor skills through practice and optimization. Radiologic expertise thus changes the oculomotor system in predictable and detectable ways, in much the same way that an athlete’s body and brain change as a function of expertise acquisition in their sport. We will therefore analyze both the consistency between experts’ fixation choices in medical images, and the eye movement performance characteristics of experts vs novice radiologists, to create an objective oculomotor bi- omarker of radiological expertise. The differences between novices and experts will train a deep learning (DL) system, which will have human visual and oculomotor performance characteristics. Training the DL with the abnormalities identified by a panel of expert radiologists will allow it to pinpoint the possible solutions in the manner of a simulated human radiologist performing at peak accuracy, precision, and speed. The resulting rank- ordered list of possible optimal and suboptimal image-reading strategies will serve as a benchmarking tool to quantify the performance of actual clinicians and residents who read the same images, rested vs fatigued. Meas- uring the effects of both training and fatigue on radiology expertise will be a major interdisciplinary cross-cutting advance in performance assessment. Our proposal to quantify fatigue in terms of erosion of expertise represents a transformational advance towards objective fitness-for-duty and expertise measures in medicine and beyond. PROJECT NARRATIVE There are 25-32 million perceptual errors in radiological case studies worldwide each year, contributing to med- ical error, the third most common cause of death in the US. We seek to reduce detection errors in radiology with four innovations: (1) we will empirically and objectively determine the visual textures used by expert radiologists to identify abnormalities within medical images; (2) we will determine the ways in which expert radiologists use their eyes, and especially their peripheral vision, to scan images and target informative regions; (3) we will de- velop a perceptual learning paradigm to optimally train residents in both texture perception and oculomotor per- formance domains; and (4) we will construct a deep learning model bestowed with simulated human visual and oculomotor capabilities, to create a normative model of human radiological expertise. The combined results from these studies will quantify peak expert performance and be employed to track and enhance individual expertise acquisition during radiology training; thus, the proposed research will help reduce medical error and moreover provide objective fitness-for-duty measurement tools—based on quantified biomarkers—to evaluate and ame- liorate the effects of fatigue on radiologic performance.",Novel Perceptual and Oculomotor Heuristics for Enhancing Radiologic Performance,10220201,R01CA258021,"['Assessment tool', 'Benchmarking', 'Biological Markers', 'Brain', 'COVID-19', 'Cancer Detection', 'Case Study', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Clinical/Radiologic', 'Collection', 'Conscious', 'Data', 'Data Analyses', 'Databases', 'Detection', 'Diagnostic', 'Dimensions', 'Disease', 'Elements', 'Ensure', 'Entropy', 'Exposure to', 'Eye', 'Eye Movements', 'Fatigue', 'Film', 'Foundations', 'Frequencies', 'Human', 'Image', 'Incentives', 'Individual', 'Instruction', 'Knowledge', 'Lead', 'Learning', 'Location', 'Measurement', 'Measures', 'Medical Errors', 'Medical Imaging', 'Medicine', 'Modeling', 'Nature', 'North America', 'Outcome', 'Participant', 'Pathway interactions', 'Perception', 'Perceptual learning', 'Performance', 'Peripheral', 'Positioning Attribute', 'Radiologic Finding', 'Radiology Specialty', 'Reading', 'Research', 'Residencies', 'Resolution', 'Rest', 'Retina', 'Scanning', 'Societies', 'Speed', 'Sports', 'Stress', 'System', 'Testing', 'Texture', 'Thoracic Radiography', 'Time', 'Training', 'Unconscious State', 'Vision', 'Visual', 'Workload', 'X-Ray Computed Tomography', 'base', 'cancer diagnosis', 'cancer imaging', 'cohort', 'deep learning', 'design', 'experience', 'fitness', 'heuristics', 'human error', 'human model', 'improved', 'innovation', 'learning network', 'lung imaging', 'meetings', 'novel', 'oculomotor', 'oculomotor behavior', 'pandemic disease', 'patient safety', 'programs', 'radiological imaging', 'radiologist', 'sample fixation', 'shift work', 'skills', 'statistics', 'tool', 'tool development']",NCI,SUNY DOWNSTATE MEDICAL CENTER,R01,2021,646124
"Cortical visual processing for navigation Project summary Vision plays a key role in our ability to navigate through the environment, from identifying landmarks and obstacles to determining location and heading. While studies of visual cortex have provided an understanding of properties such as orientation selectivity and object recognition, much less is known about how cortical circuitry extracts and processes features from the visual scene to support navigation. In particular, there are two challenges. First, the nature of the visual stimulus is dramatically different in navigation, where the subject's movement through the world creates a complex and dynamic visual input, in contrast to standard synthetic stimuli presented to stationary subjects. Second, the types of visual features and computations that must be performed are different in navigation than in standard detection or discrimination paradigms. Our goal in this proposal is to determine how the brain extracts relevant visual features from the rich, dynamic visual input that typiﬁes active exploration, and investigate how the neural representation of these features can support visual navigation.  We will investigate this through three parallel aims, that build up from the representation of the visual scene in V1 during freely moving navigation, to the computation of speciﬁc variables needed for navigation. In our ﬁrst aim, we will measure the visual input in freely moving mice using miniature head-mounted cameras, together with neural activity in V1, to determine how neural dynamics represent the visual scene during natural navigation. In our second aim, we will use large ﬁeld-of-view two-photon imaging of multiple cortical areas, while mice navigate in a naturalistic open-world virtual reality system, to determine how visual features are represented across visual cortical areas. In our third aim, we will use 2-photon imaging in mice in a rotational arena to determine how visual input is used to dynamically update a key navigational variable: heading direction. Together, this project bridges foundational measurements in freely moving animals with mechanistic circuit investigations, to provide insights into an important aspect of visual system function. Project Narrative This project will study how the brain processes visual information to support navigation, which is important for guiding goal-directed movement through the world. The results of this work will provide knowledge about normal visual function and insights for treating impaired vision via prosthetic or assistive devices.",Cortical visual processing for navigation,10208550,R01NS121919,"['Animals', 'Area', 'Behavior', 'Behavioral', 'Brain', 'Code', 'Complex', 'Conflict (Psychology)', 'Cues', 'Data', 'Detection', 'Discrimination', 'Electrophysiology (science)', 'Environment', 'Foundations', 'Frequencies', 'Goals', 'Head', 'Hippocampus (Brain)', 'Image', 'Investigation', 'Knowledge', 'Location', 'Measurement', 'Measures', 'Modeling', 'Motion', 'Movement', 'Mus', 'Nature', 'Neural Network Simulation', 'Play', 'Population', 'Process', 'Property', 'Prosthesis', 'Rotation', 'Sampling', 'Self-Help Devices', 'Sensory', 'Signal Transduction', 'Space Models', 'Stimulus', 'Structure', 'Testing', 'Update', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Visual system structure', 'Visuospatial', 'Work', 'deep neural network', 'entorhinal cortex', 'experimental study', 'high dimensionality', 'insight', 'novel', 'object recognition', 'optic flow', 'orientation selectivity', 'relating to nervous system', 'response', 'statistics', 'synergism', 'theories', 'two-photon', 'virtual reality', 'virtual reality system', 'virtual world', 'visual information', 'visual process', 'visual processing', 'visual stimulus']",NINDS,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R01,2021,2833387
"Elucidating novel features of visual processing and physiological connectivity from retina to primary visual cortex Project Summary The use of stimuli with increasingly naturalistic properties has become critical to advance our understanding of vision. Many studies demonstrate that simple artificial stimuli (e.g. sinusoidal gratings and white noise) fail to engage nonlinearities that profoundly alter responses in the retina, lateral geniculate nucleus (LGN), and primary visual cortex (V1). A recent and striking example comes from the use of naturalistic ‘flow’ stimuli, which engage robust responses in V1 that are not predicted from responses to gratings. This gap in understanding motivates the development of a stimulus ensemble and analysis framework that produces a quantitative understanding of visual processing to increasingly naturalistic stimuli and the nonlinearities that they engage. Our objective is to understand how flow stimuli are processed from retina through visual cortex. To meet this goal, we will make neural population recordings in retina (Aims 1 & 3), LGN (Aims 1 & 3) and V1 (Aim 3) using matched experimental conditions and a unified theoretical/modeling framework to map the transformations that occur across these stages of visual processing. Our central hypothesis is that V1 transforms a discrete and heavily light-level-de- pendent retinal representation of natural stimuli into a continuous (uniform) representation that is relatively in- variant to changes in the mean luminance. This invariance places a strong constraint on the class of nonlineari- ties that transform retinal responses to those observed in LGN and V1. We test this hypothesis in three aims: (1) determine early visual processing (retina & LGN) of naturalistic flow stimuli; (2) develop an encoding manifold to capture the population activity at each processing stage and transforms from one stage to the next; (3) test the ability of the manifold description to predict the impact of light adaptation on processing flow stimuli from retina to V1. Aim 1 will yield a matched experimental dataset to an interesting and novel class of ecologically-relevant stimuli. Aim 2 will yield a quantitative framework by which to understand the transformations that occur between retina, LGN, and V1. Aim 3 will provide a platform for globally perturbing the output of the retina by switching from photopic to mesopic and scotopic conditions, and thereby compare predictions of our model to measured changes in LGN and V1 activity. The primary significance of this research is that it will provide a computationally and experimentally unified framework for understanding the transformations that occur in the processing of stim- uli across multiple stages of visual processing. The major innovations are (1) presenting visual stimuli for retinal recordings that are matched to eye movements and pupil dynamics in alert animals; (2) creating a novel analysis framework that captures the responses of neurons at all three levels and the inter-level transformations to in- creasingly complex stimuli; (3) utilizing light adaptation as a method of perturbing retinal output to test our model and the stability (invariance) of LGN and V1 responses to adapting retinal signals. The expected outcome is a data-driven model of the processing from retina to LGN and V1 that generalizes from starlight to sunlight. Project Narrative Restoring vision to the blind likely requires understanding how retinal signals are communicated to the brain and how these signals are transformed in the thalamocortical pathway. This project aims to acquire an understanding of these transformations in the context of complex and more naturalistic visual stimuli.",Elucidating novel features of visual processing and physiological connectivity from retina to primary visual cortex,10229447,R01EY031059,"['Affect', 'Animals', 'Brain', 'Collaborations', 'Complex', 'Cone', 'Data', 'Data Set', 'Development', 'Environment', 'Eye Movements', 'Future', 'Goals', 'Lateral Geniculate Body', 'Light', 'Light Adaptations', 'Machine Learning', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Movement', 'Mus', 'Neurons', 'Noise', 'Optics', 'Outcome', 'Output', 'Pathway interactions', 'Physiological', 'Population', 'Process', 'Property', 'Pupil', 'Research', 'Retina', 'Retinal Ganglion Cells', 'Rod', 'Signal Transduction', 'Stimulus', 'Structure', 'Sunlight', 'Techniques', 'Testing', 'Theoretical model', 'Variant', 'Vision', 'Visual Cortex', 'Visual system structure', 'Work', 'area striata', 'base', 'blind', 'cell type', 'computational neuroscience', 'experimental study', 'in vivo', 'innovation', 'luminance', 'multi-electrode arrays', 'novel', 'predictive modeling', 'receptive field', 'relating to nervous system', 'response', 'retinal neuron', 'sight restoration', 'stimulus processing', 'visual processing', 'visual stimulus']",NEI,DUKE UNIVERSITY,R01,2021,487568
"GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN PROJECT SUMMARY & ABSTRACT  Human locomotion through natural environments requires the coordination of all levels of the sensorimotor hierarchy, from the cortical areas involved in processing of visual information and high level planning to the subcortical and spinal structures involved in the regulation of the gait and posture. However, despite the complex neural bases of human locomotion, the output is highly regular and well organized around the basic physical dynamics and biomechanics that define the stability and energetic costs of moving a bipedal body through space. There is a rich and growing body of literature describing detailed knowledge each of the individual components of human locomotion, including neural mechanisms, muscular neuromechanics, and biomechanics. However, very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion through a complex and dynamic world. This lack of integrative research not only restricts the breadth of impact of research from these individual disciplines, but also limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to to fill this critical gap in our knowledge about human locomotion, it is necessary to develop an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world. In service of this general goal, this proposal outlines research projects aimed at specific unanswered questions about locomotion over different terrains. This proposal comprises three specific research and training aims on the visual control of locomotion over rough terrain. Aim 1 focuses on the behavioral task itself, Aim 2 investigates the sensory stimulus experienced during real-world locomotion, and Aim 3 examines the motor integration of visually specified goals into the ongoing gait cycle. Aim 1 investigates effects of changing environmental uncertainty and task demands on gaze allocation strategies during locomotion over real-world rough terrain. Aim 2 analyzes and models the visual stimulus experienced during locomotion over real-world rough terrain. Aim 3 determines how visually specified target footholds and targets are integrated into the ongoing preferred steady-state gait. Together these aims will significantly advance our understanding of how humans use vision to control their movement through the natural world, which greatly increase our ability to develop clinical diagnosis and treatment for loss of locomotor function. PROJECT NARRATIVE  Very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion. This lack of integrative research limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to fill this critical gap in our knowledge about human locomotion, this proposal develops an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world.",GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN,10224830,R00EY028229,"['3-Dimensional', 'Aging', 'Algorithms', 'Area', 'Attention', 'Behavior', 'Behavioral', 'Biomechanics', 'Clinical Treatment', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Development', 'Discipline', 'Environment', 'Eye', 'Gait', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Link', 'Literature', 'Locomotion', 'Measures', 'Mechanics', 'Mentors', 'Modeling', 'Motion', 'Motor', 'Movement', 'Muscle', 'Musculoskeletal', 'Nature', 'Neuromechanics', 'Output', 'Parkinson Disease', 'Pattern', 'Phase', 'Photic Stimulation', 'Positioning Attribute', 'Postdoctoral Fellow', 'Posture', 'Protocols documentation', 'Regulation', 'Research', 'Research Activity', 'Research Project Grants', 'Research Training', 'Services', 'Signal Transduction', 'Specific qualifier value', 'Spinal', 'Stroke', 'Structure', 'System', 'Training', 'Training Programs', 'Uncertainty', 'Vision', 'Visual', 'Visual Fields', 'Walking', 'Wireless Technology', 'area MST', 'area MT', 'base', 'clinical Diagnosis', 'cost', 'design', 'environmental change', 'experience', 'experimental study', 'foot', 'gaze', 'insight', 'instrument', 'kinematics', 'multidisciplinary', 'neuromechanism', 'neuromuscular', 'novel', 'optic flow', 'programs', 'relating to nervous system', 'response', 'sensory stimulus', 'skills', 'statistics', 'treadmill', 'treatment planning', 'visual control', 'visual information', 'visual processing', 'visual stimulus', 'visual-motor integration']",NEI,NORTHEASTERN UNIVERSITY,R00,2021,229062
"Early representation of 3D volumetric shape in visual object processing Project Summary The goal of this project is to test a novel theoretical framework for understanding how the ventral pathway subserves object vision. In the standard framework, a series of neural operations on 2D image data through many intermediate cortical stages, including area V4, leads to high-level perceptual representations, including representation of object identity, at the final stages of the ventral pathway. However, our preliminary microelectrode data from a fixating monkey show that many neurons in V4 represent volumetric (volume- enclosing) 3D shape, not 2D image patterns. These neurons respond to many different 2D images that convey the same 3D shape with different shape-in-depth cues, including shading, reflection, and refraction. They even respond preferentially to random dot stereograms that convey 3D volumetric shape with no 2D cues whatsoever. Moreover, our preliminary results with 2-photon functional imaging in anesthetized monkey V4 show that 3D shape signals are grouped by their similarity, and also group with isomorphic (same outline) 2D shape signals (which could contribute evidence to corresponding 3D shape inferences). We propose to capitalize on these preliminary data by demonstrating the prevalence of 3D shape tuning in area V4, analyzing the 3D shape coding strategies used by these neurons, and measuring how 2D and 3D shape signals are arranged at a microscopic level across the surface of V4. We expect these results to provide strong evidence that extraction of 3D shape fragments is a primary goal of V4 processing. This early extraction of 3D shape information, just two synapses beyond primary visual cortex, would suggest a competing framework for understanding the ventral pathway, in which the initial goal is to represent 3D physical structure, independent of the various 2D image cues used to infer it. In this framework, object recognition would be based on preceding information about 3D physical structure, which would explain why human object recognition is so robust to image changes, in a way that the best computational vision systems are not. The scientific impact of this work would be to divert vision experiments toward understanding representation of real world 3D structure (rather than 2D planar stimuli) and to encourage computational vision scientists to incorporate early 3D shape processing into the deep convolutional network models that are the current state of the art. Narrative This study will help explain how the human brain achieves such remarkably robust perception of visual objects. The results will help guide future rehabilitative and prosthetic strategies for patients with visual impairments, by elucidating neural strategies that could be used to bring computer vision prosthetics up to human performance levels, and by discovering specific neural signals for object information that could be replicated by electrode array implants in higher-level visual cortex.",Early representation of 3D volumetric shape in visual object processing,10173788,R01EY029420,"['3-Dimensional', '3D world', 'Area', 'Biological', 'Brain', 'Calcium Signaling', 'Code', 'Computer Vision Systems', 'Cues', 'Data', 'Electrodes', 'Functional Imaging', 'Future', 'Genetic Programming', 'Glass', 'Goals', 'Human', 'Image', 'Implant', 'Lead', 'Learning', 'Measures', 'Medial', 'Microelectrodes', 'Microscopic', 'Microscopy', 'Modeling', 'Monkeys', 'Network-based', 'Neurons', 'Ocular Prosthesis', 'Pathway interactions', 'Patients', 'Pattern', 'Performance', 'Population', 'Prevalence', 'Prosthesis', 'Rehabilitation therapy', 'Scientist', 'Series', 'Shapes', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Synapses', 'System', 'Testing', 'Texture', 'Three-dimensional analysis', 'Time', 'V4 neuron', 'Vision', 'Visual Cortex', 'Visual Perception', 'Visual impairment', 'Work', 'area V4', 'area striata', 'base', 'convolutional neural network', 'experimental study', 'individual response', 'network models', 'neurotransmission', 'nonhuman primate', 'novel', 'object perception', 'object recognition', 'operation', 'relating to nervous system', 'response', 'three dimensional structure', 'two-photon', 'virtual reality', 'visual object processing']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2021,532112
"Access to parietal action representations after stroke lesions in visual cortex PROJECT SUMMARY  The ability to recognize and use objects according to their function (e.g., fork, hammer, pencil) requires integration of visual, semantic and action knowledge across occipital, temporal and parietal areas. Left parietal regions support critical aspects of object-directed action, such as grasping and object manipulation. This research activity uses a combination of fMRI and behavioral measures in patients with ischemic strokes to early and extrastriate visual areas to test the following hypotheses: Aim 1: There is a visual pathway to the parietal grasp region (aIPS) that bypasses processing in primary visual cortex. Aim 2: Left ventral extrastriate cortex is necessary to access manipulation information for visually presented objects. Aim 3A: Ballistic grasping actions to objects in the hemianopic field are influenced by volumetric properties (size, orientation) of targets. Aim 3B: Left ventral extrastriate lesions impair object function (e.g., `scissors used to cut') and disrupt access to manipulation knowledge from visual input. The research leverages strengths of fMRI (whole brain correlational measure) and neuropsychology (causal inference) to test new hypotheses about vision and action.  `Tools' (i.e., small manipulable objects) are an excellent domain in which to address broader questions about the integration of sensory, motor and cognitive processing. This is because tool recognition and tool use require the integration of distinct sensory, motor and cognitive representations, and the neural substrates of tool processing are well described. The research program emphasizes fresh perspectives on longstanding ideas about the dorsal and ventral visual pathways, by a) undertaking the first systematic investigation of the types of information about objects that are extracted by visual pathways that bypass primary visual cortex, and by b) studying how some parietal areas depend on inputs from the ventral stream in order to access the correct action for a given object. The research activity innovates by testing hypotheses about how lesions at different stages in the cortical visual hierarchy affect downstream processing in parietal cortex, combining neural and behavioral measures to study brain damaged patients (generating causal evidence), and by combining univariate and multivariate measures to `read out' the information content of brain regions (parietal cortex) that are anatomically remote from a lesion. The research advances understanding of how lesions in one brain region disrupt computations in other parts of the brain that depend on the damaged region for their inputs, a phenomenon (`dynamic diaschisis') that applies to brain injury generally. Advancing understanding of these basic issues using causal data has broad implications for understanding how the brain selects the correct action for the correct object, and more generally for theories of conceptual organization and causal reasoning. Understanding how the brain accesses actions from visual input has implications for related fields, such as robotics, neuroprosthetics, and evidenced based approaches for rehabilitating function after brain injury. Project Narrative Functional MRI and behavioral testing are used to test hypotheses about how strokes affecting occipital and temporal cortex disrupt access to action representations in parietal cortex. This research will advance understanding of how the brain processes visual information in support of everyday actions.",Access to parietal action representations after stroke lesions in visual cortex,10130533,R01EY028535,"['Address', 'Affect', 'Alexia', 'Anatomy', 'Area', 'Ballistics', 'Behavioral Assay', 'Brain', 'Brain Injuries', 'Brain region', 'Bypass', 'Cognitive', 'Data', 'Dorsal', 'Eating', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grain', 'Imaging Device', 'Impairment', 'Investigation', 'Ischemic Stroke', 'Knowledge', 'Left', 'Lesion', 'Literature', 'Location', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Neural Pathways', 'Neuropsychology', 'Occipital lobe', 'Parahippocampal Gyrus', 'Parietal', 'Parietal Lobe', 'Participant', 'Pathway interactions', 'Patients', 'Population', 'Process', 'Property', 'Prosopagnosia', 'Reading', 'Research', 'Research Activity', 'Robotics', 'Role', 'Semantics', 'Sensory', 'Stimulus', 'Stream', 'Stroke', 'Structure of supramarginal gyrus', 'Temporal Lobe', 'Testing', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual system structure', 'Work', 'area striata', 'base', 'behavior measurement', 'behavior test', 'blind', 'cognitive development', 'evidence base', 'extrastriate', 'extrastriate visual cortex', 'fovea centralis', 'grasp', 'information processing', 'innovation', 'lens', 'multisensory', 'neuroprosthesis', 'post stroke', 'programs', 'relating to nervous system', 'sensory integration', 'theories', 'tool', 'visual information', 'visual motor', 'visual object processing', 'visual process', 'visual processing']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2021,410316
"Perceptual integration of luminance, texture and color cues for visual boundary segmentation Project Summary One of the most essential computations performed by the visual system is segmenting images into regions corresponding to distinct surfaces. This in turn requires identifying the boundaries separating image regions, a process known as boundary segmentation. Computational analyses of natural images have revealed that many visual cues are available at region boundaries, including differences in luminance, texture, and color. It is known that these cues combine for tasks like edge localization and orientation discrimination. However, it remains unclear how these various cues are weighted and combined for boundary segmentation.  In collaborative work with Canadian colleagues at McGill University in Montreal, we have developed a novel machine learning framework for characterizing human performance on boundary segmentation tasks using naturalistic micro-pattern stimuli. Our method makes use of the Filter-Rectify-Filter (FRF) model often applied to characterizing texture boundary segmentation. The major innovation of our approach is that we fit the FRF model directly to thousands of psychophysical stimulus-response observations to estimate its major defining parameters. We have recently applied this approach to investigating spatial strategies for contrast boundary segmentation and comparing competing hypotheses of how contrast modulation is integrated across orientation channels. In this grant, we propose to apply both classical psychophysical techniques and our novel machine learning methodology to understanding the computations employed to combine luminance, texture and color cues for segmentation.  In Aim 1, we focus on modeling segmentation of luminance-defined boundaries, comparing the case where each surface has uniform luminance, giving rise to a sharp edge (luminance step), to the more naturalistic case where the two surfaces have differing proportions of dark and light micro-patterns on either side of the boundary with no sharp edge (luminance texture). We will apply our machine learning methodology to test the hypothesis that different neural mechanisms may be involved in segmenting these two different kinds of luminance boundaries. In Aim 2, we ask how observers integrate first-order (luminance) and second-order (texture) cues for boundary segmentation, and if there are differences in cue combination strategies for luminance steps and luminance textures. We will also compare models embodying competing hypotheses of the underlying neural mechanisms of cue combination. In Aim 3, we extend the analyses in Aims 1 and 2 beyond simple luminance differences to include differences in color. Finally, Aim 4 is a pedagogical aim of promoting undergraduate research. Project Narrative Segmenting natural images into regions corresponding to distinct surfaces is an essential visual task, yet the underlying computations for boundary segmentation remain poorly understood. In this project, we will apply classical psychophysical techniques and our novel machine learning methodology to understand how multiple cues, including luminance, texture, and color, combine to enable boundary segmentation. We hope to develop and test computational models of boundary segmentation, with the ultimate goal of gaining insight into the underlying neural mechanisms employed for this essential natural vision task.","Perceptual integration of luminance, texture and color cues for visual boundary segmentation",10201916,R15EY032732,"['Address', 'Biological', 'Color', 'Computational Biology', 'Computer Analysis', 'Computer Models', 'Cues', 'Data', 'Discrimination', 'Educational process of instructing', 'Environment', 'Goals', 'Grant', 'Human', 'Image', 'Journals', 'Laboratories', 'Light', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Positioning Attribute', 'Process', 'Psychophysics', 'Publications', 'Research', 'Response to stimulus physiology', 'Series', 'Side', 'Source', 'Stimulus', 'Surface', 'Techniques', 'Testing', 'Texture', 'Universities', 'Vision', 'Vision research', 'Visual', 'Visual system structure', 'Work', 'computer studies', 'experience', 'innovation', 'insight', 'interest', 'laboratory curriculum', 'luminance', 'neuromechanism', 'neurophysiology', 'novel', 'pedagogy', 'undergraduate research', 'undergraduate student', 'vision science']",NEI,FLORIDA GULF COAST UNIVERSITY,R15,2021,374345
"Statistical Unsupervised Learning VF for IIHTT & ONTT Summary Current assessments of visual field testing depend on algorithms, principally developed to diagnose and monitor progression in glaucoma, or on expert descriptive categorization of deficits. The algorithms do not work well for non-glaucomatous optic neuropathies as these disorders can both improve and deteriorate. Descriptive categorizations are not readily quantifiable to assess change over time. Unsupervised statistical learning archetypal analysis is a new way to investigate glaucoma and potentially other optic neuropathies. Both idiopathic intracranial hypertension and optic neuritis are disorders that often improve and respond to therapy. Archetypal analysis of the visual fields from two NEI sponsored clinical trials on each disorder, ONTT and IIHTT, will be investigated to determine if the findings parallel the reported outcomes and effects of therapy. We will also test whether machine learning quantifiable archetypes, which are disease-associated patterns of field deficits, are similar to expert determinations, whether they are sensitive to changes in optic nerve function, and if they reveal residual optic nerve dysfunction in eyes reported to be normal by prior study criteria. Adding cases of IIH and optic neuritis from the clinic will enhance the archetypes for each disorder for use in the clinic and new studies. Narrative Analysis of the visual fields of patients with optic neuropathies using machine learning will improve the evaluation and provide objective measurement, rather than the current descriptive methods. The approach, called archetypal analysis, should improve safety monitoring during clinical trials as well as uncover residual visual field deficits not seen with other types of analyses.",Statistical Unsupervised Learning VF for IIHTT & ONTT,10192112,R21EY032522,"['Acute', 'Affect', 'Algorithms', 'Clinic', 'Clinical Trials', 'Cost Savings', 'Detection', 'Deterioration', 'Diagnosis', 'Disease', 'Evaluation', 'Event', 'Eye', 'Face', 'Frequencies', 'Functional disorder', 'Future', 'Glaucoma', 'Head', 'Injury', 'Intervention', 'Intervention Studies', 'Lead', 'Machine Learning', 'Manuals', 'Masks', 'Measurement', 'Measures', 'Methods', 'Military Personnel', 'Monitor', 'Optic Nerve', 'Optic Neuritis', 'Outcome', 'Outcome Study', 'Papilledema', 'Patients', 'Pattern', 'Perimetry', 'Physiologic Intraocular Pressure', 'Pseudotumor Cerebri', 'Reader', 'Reporting', 'Residual state', 'Safety', 'Shapes', 'Supervision', 'Testing', 'Time', 'Vision', 'Visit', 'Visual Fields', 'Weight', 'archetypal analysis', 'base', 'central visual field', 'clinical practice', 'eligible participant', 'field study', 'improved', 'longitudinal analysis', 'optic nerve disorder', 'prospective', 'response', 'statistical learning', 'successful intervention', 'treatment effect', 'treatment trial', 'trend', 'unsupervised learning']",NEI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R21,2021,331170
"An enhancement to nGoggle: a VR Goggle for Structural and Functional assessment in Glaucoma PROJECT SUMMARY  Assessments of functional loss and of structural damage to the optic nerve are typical evaluations per- formed in the diagnosis and monitoring of glaucoma. This proposal aims at developing the hardware system that will allow acquisition of fundus photographs using the nGoggle, a portable Brain-Computer Interface (BCI) which integrates a wearable, wireless, electroencephalogram (EEG) system and a head-mounted display to monitor electrical brain activities associated with visual field stimulation. In a previous project, we developed the nGoggle portable system for the assessment of visual function deficits in order to address limitations of standard automated perimetry (SAP), a traditional but subjective visual field test. We compared the diagnostic accuracies of the nGoggle and SAP in discriminating patients with glaucoma from healthy subjects and found that the BCI performed at least as well, if not better than SAP, with the additional advantage of portability and objectivity.  With rapid advancements in smartphone cameras, it is now possible to develop compact auto-focusing fundus cameras and integrate them into our neuro-monitoring nGoggle. Recently we developed an approach named Machine-to-Machine (M2M), a deep learning algorithm that was trained to analyze fundus photos and predict quantitative measurements of retinal nerve fiber layer thickness and neuroretinal rim provided by spectral domain-optical coherence tomography (SDOCT), with high correlation and agreement with the original SDOCT observations. This approach opens up the possibility of using simple fundus photographs to extract quantitative information about neural damage in glaucoma. By combining functional assessment currently provided by the nGoggle with structural assessment capability, this new system would result in a very unique device capable of portable structural and functional assessment of optic nerve damage for a multitude of eye conditions, not limited to glaucoma, such as age-related macular degeneration, retinal degenerations and non-glaucomatous optic neu- ropathies.  The specific aims of this Phase I SBIR project are (1) to incorporate a high-resolution auto-focusing fundus camera to the nGoggle head-mounted display in full compliance with ophthalmic instrument safety stand- ards and fundus camera functional standards, and (2) to demonstrate the ability of the nGoggle fundus camera to capture high-quality optical nerve images from a model eye. Successful development of this system will enable both structural and functional assessments of retinal conditions to be performed easily and inexpensively using a portable device. PROJECT NARRATIVE NGoggle Inc. proposes to develop a new wearable ophthalmic diagnostic instrument capable of performing con- current structural and functional assessment to diagnose and monitor optic neuropathies and retinal diseases. This project aims at developing a pair of compact high-resolution fundus cameras in full compliance with oph- thalmic instrument safety standards and integrating them into our neuro-monitoring nGoggle, an instrument with the demonstrated capability of estimating functional vision loss in glaucoma patients based on their electroen- cephalogram (EEG) responses evoked by visual stimuli. We also intend to demonstrate that the optic nerve images of model eyes taken by the integrated fundus cameras can yield consistent and compatible cup-to-disc ratios when compared with the measurements obtained from the same model eyes using optical coherence tomography (OCT). After successful demonstration, we plan to conduct clinical tests in the subsequent phase of this project.",An enhancement to nGoggle: a VR Goggle for Structural and Functional assessment in Glaucoma,10253661,R43EY032820,"['Address', 'Age related macular degeneration', 'Agreement', 'Algorithms', 'Anatomy', 'Blindness', 'Cellular Phone', 'Clinic', 'Clinical', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Electroencephalogram', 'Electroencephalography', 'Evaluation', 'Eye', 'Flare', 'Fundus', 'Fundus photography', 'Glaucoma', 'Healthcare', 'Image', 'Imaging technology', 'Light', 'Measurement', 'Modeling', 'Monitor', 'Names', 'Nerve', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Performance', 'Perimetry', 'Phase', 'Physiologic pulse', 'Publications', 'Reproducibility', 'Resolution', 'Resources', 'Retina', 'Retinal Degeneration', 'Retinal Diseases', 'Safety', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Structure', 'System', 'Systems Development', 'Testing', 'Thick', 'Time', 'Training', 'Vision', 'Visual Fields', 'Visual evoked cortical potential', 'Visual impairment', 'Wireless Technology', 'base', 'brain computer interface', 'brain electrical activity', 'cost', 'deep learning', 'deep learning algorithm', 'deep neural network', 'detection test', 'diagnostic accuracy', 'disability', 'field study', 'functional loss', 'gaze', 'hazard', 'head mounted display', 'innovation', 'instrument', 'loss of function', 'neural network algorithm', 'optic nerve disorder', 'patient response', 'portability', 'prevent', 'relating to nervous system', 'retinal imaging', 'retinal nerve fiber layer', 'screening', 'visual stimulus', 'wearable device']",NEI,"NGOGGLE, INC.",R43,2021,299600
"Biomechanical Analysis in Strabismus Surgery We propose 3 interrelated aims to define the biomechanics of the eye rotating (extraocular) muscles (EOMs) & optic nerve (ON) in health & visual disease, understand novel EOM actions, & characterize mechanical effects that may contribute to severe myopia. We aim to improve treatment of strabismus, misalignment of visual directions of the eyes; glaucoma & non-arteritic anterior ischemic optic neuropathy (NA-AION), both common blinding ON diseases; & high axial myopia, an ocular elongation & distortion that has become a worldwide epidemic & major cause of blindness. We propose a novel & critical nexus linking the EOMs, ON, & structure of the eye's scleral wall that we will explore using modern imaging & artificial intelligence techniques. Aim I will clarify the kinematic (motion) properties of the human eye, testing by multipositional magnetic resonance imaging (MRI) of the eyeball & EOMs the hypothesis that translational (linear) movement contributes importantly to ocular alignment. MRI will be performed during horizontal convergence & vertical eye rotation in normal people, & in patients who have common forms of strabismus including convergence insufficiency, eye crossing (esotropia), & outward ocular deviation (exotropia), both before & after corrective EOM surgery. Clarification of ocular translation is necessary to understand normal ocular motility and treat its disorders. Aim II will characterize the mechanical loading on the ON caused by eye movements. We will characterize the mechanical effects of ON tractional loading on the eyeball during horizontal & vertical eye rotations at 2 scales in living people, to test the hypothesis that such ON loading deforms it & adjacent retina & blood vessels as loading translates the eye. We propose that the resulting deformation during eye movements may create repetitive strain injury contributing to glaucoma, NA-AION, & axial myopia. In groups of subjects with the foregoing diseases, & in an equal group of matched healthy subjects, we will study mechanical effects of eye movement within the living eye by imaging its internal micro structure & blood vessels with optical coherence tomography, & outside the eyeball in the eye socket using MRI. Effects of tethering during eye movement will be studied ex vivo by precision 3D optical imaging of fresh human eye bank specimens subjected to mechanical tension on the ON that mimic effects of the eye movements imaged in the living subjects. Aim III will model the biomechanics of ocular kinematics. The constitutive mechanical properties of the non-muscular ocular & eye socket tissues will be described by finite element models (FEMs) using modern engineering methods for computational simulation to predict ocular kinematics, as well as local mechanical strains in the ON & sclera that may cause glaucoma, NA-AION, & the ocular deformities underlying extreme nearsightedness. We will determine if FEMs employing normal tissue properties can simulate normal ocular translation during horizontal & vertical rotations & convergence. By FEM simulation, we will also test the hypothesis that ocular loading by eye movement might contribute to: normal vergence, strabismus, & the effects of strabismus surgery. Relevance. Strabismus is a common clinical disorder that can cause double vision in adults and vision loss in children. Strabismus is often treated by surgical manipulation of the eye muscles, although current knowledge of their structure and function is incomplete. Proposed functional imaging and biomechanical studies of the properties of the eye muscles, eyeball, and optic nerve will improve understanding of the causes and treatment of strabismus, optic nerve diseases, and nearsightedness.",Biomechanical Analysis in Strabismus Surgery,10134346,R01EY008313,"['3-Dimensional', 'Accounting', 'Adult', 'Agreement', 'Algorithms', 'Anatomy', 'Anterior Ischemic Optic Neuropathy', 'Artificial Intelligence', 'Behavior', 'Biological Specimen Banks', 'Biomechanics', 'Blindness', 'Blood Vessels', 'Child', 'Choroid', 'Clinical', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Connective Tissue', 'Convergence Insufficiency', 'Cumulative Trauma Disorders', 'Deformity', 'Degenerative Myopia', 'Diplopia', 'Disease', 'Duct (organ) structure', 'Elements', 'Engineering', 'Epidemic', 'Equilibrium', 'Esotropia', 'Etiology', 'Exotropia', 'Eye', 'Eye Banks', 'Eye Movements', 'Failure', 'Functional Imaging', 'Gap Junctions', 'Glaucoma', 'Health', 'Human', 'Image', 'Individual', 'Knowledge', 'Lasers', 'Link', 'Magnetic Resonance Imaging', 'Matched Group', 'Measurement', 'Measures', 'Mechanics', 'Modeling', 'Modernization', 'Motion', 'Movement', 'Muscle', 'Muscle Contraction', 'Myopia', 'Normal tissue morphology', 'Ocular orbit', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Physiologic Intraocular Pressure', 'Play', 'Primary Open Angle Glaucoma', 'Property', 'Retina', 'Role', 'Rotation', 'Scanning', 'Sclera', 'Strabismus', 'Stress', 'Structure', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Traction', 'Translating', 'Translations', 'Validation', 'Variant', 'Visual', 'anatomic imaging', 'biomechanical model', 'cell motility', 'crosslink', 'digital imaging', 'ex vivo imaging', 'human tissue', 'improved', 'in vivo imaging', 'in vivo optical imaging', 'kinematics', 'mechanical load', 'mechanical properties', 'model development', 'models and simulation', 'monocular', 'neglect', 'novel', 'ocular imaging', 'optic nerve disorder', 'optical imaging', 'orbit muscle', 'predictive modeling', 'quantitative imaging', 'retina blood vessel structure', 'simulation']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2021,584907
"Neural mechanisms of active vision in the fovea Neural mechanisms of active vision in the fovea In many ways human vision is like a camera, with a lens that forms an image on a spatially arranged sensor (the retina). However, it is unlike a camera because the sensor has uneven sampling and is constantly moving with the eyes. Recent behavioral and theoretical work suggest these eye movements serve a faciliatory role in high acuity vision – where the eye movements are part of the computations and enhance spatial resolution. However, the neurophysiological mechanisms to support this facilitation remain unknown. More broadly, little is known about the neural mechanisms that integrate across the retinal motion generated by eye movements, especially in the central visual field (the fovea). This is particularly important because over 8 million Americans suffer from central vision loss due to retinal disorders. Even if the retinal signals could be repaired, it is imperative to understand how the brain reads out foveal signals to ensure recovery of high-acuity visual processing, and fixational eye movements are a part of that process. The proposed career development plan aims to address these questions by measuring visual processing in the foveal representation of primary visual cortex (V1) during natural visual behavior. This proposal uses custom high-resolution eye-tracking, a novel visual foraging paradigm, largescale neurophysiology, and state-of-the-art machine learning to make these measurements possible. The proposed research will not only generate fundamental understanding of how eye-movements facilitate visual processing, but also will integrate the experimental and theoretical tools required to support neurophysiological studies of active visual processing without a loss of rigor or detail. The candidate has extensive expertise in awake- behaving neurophysiology and computational modeling and the training plan is designed to support his further training in statistical modeling, high-resolution eye-tracking, and modern machine-learning techniques for analyzing neural population data. The primary mentor, Dr. Daniel Butts, is a world expert in statistical models of neural activity during active vision; Co-mentor, Dr. Michele Rucci, is a world leader in high-resolution eye tracking and theoretical approaches to active vision; and Co-mentor, Dr. Jude Mitchell, is a pioneer in establishing the marmoset model of visual neuroscience and an expert in neurophysiology of visual attention. Together, they will provide the guidance to establish the candidate’s transition to a successful independent research career. The goal of this proposal is to identify the impact of fixational eye movements on neural representations in visual cortex in the central visual field. Over 9 million Americans have central vision loss from age-related macular degeneration. Results from this proposal will build a fundamental understanding of how retinal signals from the fovea are processed by visual cortex during natural visual behavior.",Neural mechanisms of active vision in the fovea,10106203,K99EY032179,"['Address', 'Age related macular degeneration', 'American', 'Behavior', 'Behavioral', 'Blindness', 'Brain', 'Callithrix', 'Code', 'Cognitive', 'Computer Models', 'Custom', 'Data', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Ensure', 'Esthesia', 'Eye', 'Eye Movements', 'Foundations', 'Frequencies', 'Funding', 'Goals', 'Grant', 'Human', 'Image', 'Laboratories', 'Machine Learning', 'Measurement', 'Measures', 'Mentors', 'Modeling', 'Modernization', 'Monkeys', 'Motion', 'Neurons', 'Outcome', 'Peripheral', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Primates', 'Process', 'Recovery', 'Research', 'Resolution', 'Retina', 'Retinal Diseases', 'Role', 'Saccades', 'Sampling', 'Series', 'Signal Transduction', 'Statistical Models', 'Stream', 'System', 'Techniques', 'Testing', 'Training', 'Universities', 'V1 neuron', 'V4 neuron', 'Vision', 'Visual', 'Visual Acuity', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual attention', 'Work', 'active vision', 'area striata', 'awake', 'base', 'career', 'career development', 'central visual field', 'computerized tools', 'design', 'extrastriate', 'extrastriate visual cortex', 'flexibility', 'fovea centralis', 'interest', 'lens', 'neural model', 'neuromechanism', 'neurophysiology', 'new technology', 'novel', 'professor', 'receptive field', 'relating to nervous system', 'repaired', 'response', 'retinal imaging', 'sample fixation', 'sensor', 'skills', 'spatial vision', 'spatiotemporal', 'statistics', 'tool', 'visual information', 'visual neuroscience', 'visual process', 'visual processing', 'visual tracking']",NEI,"UNIV OF MARYLAND, COLLEGE PARK",K99,2021,116969
"Isolating and mitigating sequentially dependent perceptual errors in clinical visual search Project Summary When looking at an x-ray, radiologists are typically asked to localize a tumor (if present), and to classify it, judging its size, class, position and so on. Importantly, during this task, radiologists examine on a daily basis hundreds and hundreds of x-rays, seeing several images one after the other. A main underlying assumption of this task is that radiologists’ percepts and decisions on a current X-ray are completely independent of prior events. Recent results showed that this is not true: our perception and decisions are strongly biased by our past visual experience. Although serial dependencies were proposed to be a purposeful mechanism to achieve perceptual stability of our otherwise noisy visual input, serial dependencies play a crucial and deleterious role in the everyday task performed by radiologists. For example, an x-ray containing a tumor can be classified as benign depending on the content of the previously seen x-ray. Given the importance and the impact of serial dependencies in clinical tasks, in this proposal, we plan to (1) establish, (2) identify and (3) mitigate the conditions under which serial effects determine our percepts and decisions in tumor search tasks. In Aim 1, we will establish the presence of serial effects in four different clinically relevant domains: tumor detection, tumor classification, tumor position and recognition speed. In Aim 2, we plan to identify the specific boundary conditions under which visual serial dependence impacts tumor search in radiology. In Aim 3, once we will fully understand these boundary conditions in Aim 2, we will propose a series of task and stimulus manipulations to control and mitigate the deleterious effects of visual serial dependence on tumor search. As a result of these manipulations, visual search performance should improve in measurable ways (detection, classification, position, speed). Aim 3 is particularly crucial because it will allow us to propose new guidelines which will greatly improve tumor recognition in x-ray images, making this task even more effective and reliable. Taken together, the proposed studies in Aim 1, 2, and 3 will allow us to establish, identify, and mitigate the deleterious effect of serial dependencies in radiological search tasks, which could have a significant impact on the health and well-being of patients everywhere. ! ! ! Project Narrative Our proposal is designed to investigate the detrimental impact of visual serial dependencies in clinical settings. Serial dependencies significantly impact our perceptual experience, but little is known about their detrimental consequences when radiologists are asked to detect tumors in x-rays. Crucially, the final goal of our research project is to develop recommendations and guidelines to mitigate the negative effect of serial effects and, thus, improve diagnosis accuracy.",Isolating and mitigating sequentially dependent perceptual errors in clinical visual search,10137898,R01CA236793,"['Benign', 'Classification', 'Clinical', 'Computer Vision Systems', 'Data', 'Decision Making', 'Dependence', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Event', 'Goals', 'Guidelines', 'Health', 'Human', 'Image', 'Impairment', 'Judgment', 'Machine Learning', 'Measurable', 'Patients', 'Perception', 'Performance', 'Personal Satisfaction', 'Play', 'Positioning Attribute', 'Radiology Specialty', 'Reading', 'Recommendation', 'Reporting', 'Research Project Grants', 'Roentgen Rays', 'Role', 'Scanning', 'Series', 'Speed', 'Stimulus', 'Structure', 'Testing', 'Time', 'Training', 'Visual', 'Visual system structure', 'Work', 'base', 'clinically relevant', 'design', 'diagnostic accuracy', 'experience', 'improved', 'laboratory experiment', 'radiologist', 'tumor', 'visual search']",NCI,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2021,311975

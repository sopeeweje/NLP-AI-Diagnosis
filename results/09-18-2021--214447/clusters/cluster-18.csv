text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"POPULATION GENOMICS OF ADAPTATION Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",POPULATION GENOMICS OF ADAPTATION,9753261,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependence', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Link', 'Location', 'Machine Learning', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Techniques', 'Time', 'Variant', 'Work', 'deep neural network', 'fight against', 'genomic data', 'global health', 'human disease', 'learning strategy', 'malaria infection', 'malaria mosquito', 'malaria transmission', 'markov model', 'novel', 'recurrent neural network', 'resistance allele', 'response', 'supervised learning', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,UNIVERSITY OF OREGON,R01,2019,295000,0.043894417079007104
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9610628,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Infrastructure', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multi-task learning', 'multitask', 'new technology', 'novel', 'pandemic disease', 'predictive modeling', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2019,224899,0.04038422301295893
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell-based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off-target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,9783881,U01EB029373,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Proliferation', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'learning strategy', 'next generation', 'novel', 'novel therapeutics', 'response', 'safety assessment', 'safety testing', 'side effect', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NIBIB,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2019,611358,0.01654998677460554
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9666926,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'supervised learning', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2019,242725,0.021390296776236297
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9774249,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2019,332952,0.011304429795329859
"Selective Whole Genome Amplification - Enabling Microbial Population Genomics Microbial population genetic research has been crucial for understanding pathogen dynamics, virulence, host specificity, and many other topics; in many cases uncovering unexpected and transformative biological processes. However, conventional population genetic analyses are limited by the quantity of sequence data from each sample. The temporal, spatial, and evolutionary resolution of techniques that rely on single gene sequences or multi-locus sequence typing are often insufficient to study biological processes on fine scales, precisely the scales at which many evolutionary and mechanistic process occur. Population genomics offers a vast quantity of sequence information for inferring evolutionary and ecological processes on very fine spatial and temporal scales, inferences that are critical to understanding and eventually controlling many infectious diseases. The promise of population genomics is tempered, however, by difficulties in isolating and preparing microbes for next-generation sequencing. We have developed the selective whole genome amplification (SWGA) technology to sequence microbial genomes from complex biological specimens without relying on labor-intensive laboratory culture, even if the focal microbial genome constitutes only a miniscule fraction of the natural sample. The primary hindrance to popular adoption of SWGA for microbial genomic studies is not its effectiveness in producing samples suitable for next-generation sequencing but in the upfront investment needed to develop an effective protocol to amplify the genome of a specific microbial species. Identifying an SWGA protocol that consistently results in selective and even amplification across the target genome is currently hindered by computationally-inefficient software that can evaluate a very limited set of the potentially effective solutions. Further, this software uses marginally-effective optimality criteria as there is currently only a limited understanding of the true criteria that result in highly-selective and even amplification of a target genome. As a result, SWGA protocol development is currently costly in both time and resources. A primary goal of the proposed research is to identify the criteria that result in optimal SWGA by analyzing next- generation sequencing data with advanced machine learning techniques. These optimality criteria will be integrated into a freely-available, computationally-efficient swga development program that will reduce the upfront investment in SWGA protocol development, thus allowing researchers to address medically- and biologically-important questions in any microbial species. In the near term, this project will also generate effective SWGA protocols for four microbial species which can be used immediately to address fundamental questions in evolutionary biology, disease progression, and emerging infectious disease dynamics. From a global disease perspective, this work is imperative as the majority of microbial species cannot easily be cultured and are in danger of becoming bystanders in the genomics revolution that is currently elucidating evolutionary processes and molecular mechanisms in cultivable microbial species. Addressing many of the major outstanding questions about pathogen evolution will require analyses of populations of microbial genomes. Although population genomic studies would provide the analytical resolution to investigate evolutionary and mechanistic processes on fine spatial and temporal scales – precisely the scales at which these processes occur – microbial population genomic research is currently hindered by the practicalities of obtaining sufficient quantities of genomes to analyze. We propose to develop an innovative, cost-effective, practical, and publically-available technology to collect sufficient quantities of microbial genomic DNA necessary for next-generation microbial genome sequencing.",Selective Whole Genome Amplification - Enabling Microbial Population Genomics,9699440,R21AI137433,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Biological', 'Biological Process', 'Biology', 'Characteristics', 'Communicable Diseases', 'Complex', 'Computer software', 'Coupling', 'DNA', 'Data', 'Development', 'Disease', 'Disease Progression', 'Effectiveness', 'Emerging Communicable Diseases', 'Evolution', 'Foundations', 'Genes', 'Genetic Research', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Health', 'Human', 'Investigation', 'Investments', 'Laboratory culture', 'Machine Learning', 'Medical', 'Metaphor', 'Methods', 'Microbe', 'Microbial Genome Sequencing', 'Microsatellite Repeats', 'Molecular', 'Organism', 'Population', 'Population Analysis', 'Population Genetics', 'Process', 'Program Development', 'Protocols documentation', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Shapes', 'Specificity', 'Specimen', 'System', 'Techniques', 'Technology', 'Time', 'Virulence', 'Work', 'cost', 'cost effective', 'design', 'genetic analysis', 'genetic approach', 'host-microbe interactions', 'improved', 'innovation', 'machine learning algorithm', 'microbial', 'microbial genome', 'next generation', 'next generation sequencing', 'novel', 'pathogen', 'prevent', 'protocol development', 'vector', 'whole genome']",NIAID,UNIVERSITY OF PENNSYLVANIA,R21,2019,186101,0.04435520270226526
"Computational modeling of spatial genome organization and gene regulation PROJECT SUMMARY/ABSTRACT The three-dimensional (3D) organization of the genome plays an essential role in genome stability, gene regulation, and many diseases, including cancer. The recent development of high-throughput chromatin conformation capture (Hi-C) and its variants provide an unprecedented opportunity to investigate higher-order chromatin organization. Despite the rapidly accumulating resources for investigating 3D genome organization, our understanding of the regulatory mechanisms and functions of the genome organization remain largely incomplete. Hi-C analyses and 3D genome research are still in their early stage and face several challenges. First, high-resolution chromatin contact maps require extremely deep sequencing and hence have been achieved only for a few cell lines. Second, it is computationally challenging to complement 3D genome structure with one-dimensional (1D) genomic and epigenomic features. Third, recent studies have just begun to infer associations between chromatin interactions and genetic variants and to identify potential target genes of those variants at the genome-wide scale. Given these challenges and my unique multi-disciplinary training, my long-term research goal is to develop innovative computational and statistical methods to uncover the interplay between 3D genome structure and function. Speciﬁcally, in the next ﬁve years, I will i) develop computational approaches to enhance the resolution of existing Hi-C data and investigate ﬁne-scale 3D genome architecture as well as its spatiotemporal dynamics and ii) build scalable and interpretable machine learning models that leverage 1D epigenomic data to predict cell type-speciﬁc 3D chromatin interactions and gene expression and elucidate the function of 3D genome organization in gene regulation and human diseases. The completion of the proposed work will deepen our knowledge of 3D genome architecture as well as its functions in gene regulation and disease. PROJECT NARRATIVE The overarching mission of my research is to understand the interplay between genome architecture and gene regulation. Recent development of high-throughput chromatin conformation capture techniques has allowed us to look beyond the nucleotide sequence of DNA and investigate the principles of higher-order chromatin organization. I will develop innovative computational and statistical strategies to investigate 3D genome organization at an unprecedented scale, thereby elucidating the impacts of genome organization on gene regulation and disease.",Computational modeling of spatial genome organization and gene regulation,9798957,R35GM133678,"['3-Dimensional', 'Architecture', 'Base Sequence', 'Cell Line', 'Chromatin', 'Complement 3d', 'Computer Simulation', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Development', 'Dimensions', 'Disease', 'Face', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome Stability', 'Genomics', 'Goals', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mission', 'Modeling', 'Play', 'Research', 'Resolution', 'Resources', 'Role', 'Statistical Methods', 'Structure', 'Techniques', 'Training', 'Variant', 'Work', 'cell type', 'chromosome conformation capture', 'deep sequencing', 'epigenomics', 'genetic variant', 'genome-wide', 'human disease', 'innovation', 'multidisciplinary', 'spatiotemporal']",NIGMS,UNIVERSITY OF CALIFORNIA RIVERSIDE,R35,2019,369984,-0.005153911472360548
"Development of New Genome Editing Agents Using RNA Modifying Enzymes Komor – Project Summary/Abstract - “Development of New Genome Editing Agents Using RNA Modifying Enzymes”  While targeted genome editing, the introduction of a specific modification in genomic DNA, has the potential to allow researchers to study and better understand mechanisms of human genetic diseases, traditional genome editing methods (including CRISPR-Cas9) that rely on the initial introduction of double stranded DNA breaks (DSB) suffer from modest genome editing efficiencies as well as unwanted gene alterations (indels), particularly when attempting to correct point mutations. Recently, a class of genome editing agents called single base editors was developed that does not involve DSBs, but rather uses a dCas9-tethered single-stranded DNA (ssDNA) modifying enzyme to directly chemically modify target nucleobases within a ~5 nucleotide window determined by the protospacer. Two classes of editors have been developed that use cytosine and adenine deamination chemistries to catalyze the conversion of C•G base pairs to T•A (CBEs), and A•T base pairs to G•C (ABEs), respectively. Here we propose the development and characterization of new base editors capable of facilitating new point mutations using methylation chemistry. We have use a bioinformatic approach to identify RNA modifying enzymes that have the potential to be repurposed into new base editors, and have rationally designed mutant libraries to use with directed evolution to convert these enzymes into base editors (Aim 1). Concurrently, we are developing a machine learning program that utilizes existing ssDNA modifying enzymes to identify putative mutations that will expand the substrate scope of the identified methyltransferases to ssDNA (Aim 2). Mutations identified from both strategies will then be tested and characterized for base editing in multiple orthogonal systems (Aim 3). The successful completion of the proposed work will represent a significant addition to existing base editing technologies, and will enable researchers to cleanly and efficiently install two additional types of point mutations into the genome of living cells, allowing researchers to quickly and effectively general model systems for the study of human genetic diseases. Komor – Project Narrative - “Development of New Genome Editing Agents Using RNA Modifying Enzymes” Base editing enables high efficiency genomic point mutation introduction in a variety of cell types and has the potential to allow researchers to better study human genetic diseases. We propose transformative improvements to current base editing technologies that will expand the types of point mutations that can be introduced by base editors. The tools developed here will enable researchers to cleanly and efficiently install additional types of point mutations into the genome of living cells for the study and potential treatment of human genetic diseases.",Development of New Genome Editing Agents Using RNA Modifying Enzymes,9876634,R21GM135736,"['Adenine', 'Adoption', 'Algorithms', 'Base Pairing', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'CRISPR/Cas technology', 'Case Study', 'Cell Line', 'Cells', 'Chemicals', 'Chemistry', 'Communities', 'Cytosine', 'DNA', 'DNA Damage', 'DNA Double Strand Break', 'Deamination', 'Development', 'Directed Molecular Evolution', 'Engineering', 'Enzymes', 'Escherichia coli', 'Evolution', 'Gene Mutation', 'Generations', 'Genetic Diseases', 'Genome', 'Genomic DNA', 'Genomics', 'Human', 'Human Genetics', 'In Vitro', 'Individual', 'Inosine', 'Lesion', 'Libraries', 'Link', 'Machine Learning', 'Mammalian Cell', 'Measures', 'Mediating', 'Methods', 'Methylation', 'Methyltransferase', 'Modification', 'Mutation', 'Nucleotides', 'Pathogenicity', 'Point Mutation', 'Program Development', 'Proteins', 'Purines', 'Pyrimidine', 'RNA', 'Research Personnel', 'Single-Stranded DNA', 'Site', 'Specificity', 'System', 'Technology', 'Testing', 'Transfer RNA', 'Uracil', 'Variant', 'Work', 'adenosine deaminase', 'base', 'cell type', 'combat', 'design', 'genome editing', 'insertion/deletion mutation', 'machine learning algorithm', 'molecular dynamics', 'mutant', 'novel', 'nucleobase', 'preference', 'programs', 'tool', 'transition mutation', 'transversion mutation']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2019,205479,-0.018710374148716024
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9672444,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2019,1515934,0.028213666708367405
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,10012073,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2019,77975,0.028213666708367405
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9766330,R01HG009188,"['Affect', 'Base Pairing', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'Custom', 'DNA', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modernization', 'Modification', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Reproducibility', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'competitive environment', 'cost', 'enzyme activity', 'experimental study', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2019,468098,-0.015452775391489853
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades — such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases — have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9658531,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2019,479215,-0.004093649279005045
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9967363,R01HG009906,"['3-Dimensional', 'Address', 'CRISPR/Cas technology', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Dimensions', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Imagery', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'base', 'cell type', 'chromosome conformation capture', 'convolutional neural network', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'genome annotation', 'genome browser', 'genome-wide', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'random forest', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,102272,0.004683795753653977
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9623355,R01HG009906,"['3-Dimensional', 'Address', 'CRISPR/Cas technology', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Dimensions', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Imagery', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'base', 'cell type', 'chromosome conformation capture', 'convolutional neural network', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'genome annotation', 'genome browser', 'genome-wide', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'random forest', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,PENNSYLVANIA STATE UNIV HERSHEY MED CTR,R01,2019,284020,0.004683795753653977
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9751141,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2019,1186500,0.03759634353387125
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9789914,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2019,1500000,0.0024301540439596358
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9626416,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Infrastructure', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2019,2000000,0.029534531573562322
"Micropublications for Automating Genome Sequence Variant Interpretation from Medical Literature PROJECT SUMMARY Accurate and efficient interpretation of genomic variants for clinical decision making is predicated on ready access to useful information in the medical literature. The sheer number of potentially relevant articles that must be examined during this curation process poses a major challenge in ensuring the accuracy and reproducibility of clinical variant interpretation as it is time-consuming and the results highly user-dependent. To this end, we have developed the Mastermind Genomic Search Engine - a commercial database that automatically organizes disease, gene and variant information from the medical literature by systematically indexing millions of scientific articles. In direct comparison to manually developed databases of genetic variants, we have achieved greater than 97% concordance and accurately identified >50% more variants with an average of 3-fold more references demonstrating the effectiveness of our automated approach. Currently, Mastermind is used by over 1800 variant scientists in 25 different countries to more quickly curate literature for genetic variants in clinical settings. In response to feedback from ClinGen curators and others, the present proposal seeks to create a framework to facilitate literature curation and clinical variant interpretation activities within Mastermind by 1) prioritizing relevant references and external database entries containing content meaningful to variant classification guidelines, 2) assembling this information into a “micropublication” text format with codified data fields including population frequencies, computational predictions, reference citations and relevant sentence fragments with conclusive content, 3) allowing users to manually review, alter and augment pre-populated entries and 4) providing a platform to share and continuously update this information with other variant scientists in the Mastermind community and elsewhere. Developing tools that allow for collaborative curation in real-time at the point of interaction with source material (i.e. individual articles) will mitigate reproducibility challenges plaguing other large-scale crowd-sourced projects. In contrast to genetic variant databases of user submitted classification information and associated data, the present proposal seeks to create enhanced curation tools to fill such variant databases with more accurate and reproducible data and in a way that would promote dramatic scaling of variant curation activities including those undertaken by groups like ClinVar. To test this approach, we will work with industry partners engaged in variant curation activities to 1) determine the requisite data fields, 2) integrate external database information, 3) test the accuracy and relevance of results and the overall efficacy of the approach using hundreds of manually curated genetic variants, and 4) solicit and incorporate feedback from our development partners to iterate and refine the software features for the greatest effect. Within the $4B genome sequencing software market, there is significant commercial potential and scientific merit in bringing more automated techniques of data analysis to large-scale genome sequencing variant interpretation as described in this proposal. [Word count – 447; Line count - 30] PROJECT NARRATIVE Successful completion of the present project will contribute to the public health mission of the NIH by promoting more widespread adoption of genome sequencing by making the interpretation of genome variant data more accurate, reproducible and cost-effective in clinical and research laboratories. The community of users that can benefit from this work include geneticists, oncologists, pathologists, researchers and patients. [Two sentences]",Micropublications for Automating Genome Sequence Variant Interpretation from Medical Literature,9776985,R43HG010446,"['Address', 'Adoption', 'Algorithms', 'Authorship', 'Blinded', 'Categories', 'Classification', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Consumption', 'Country', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Effectiveness', 'Ensure', 'Feedback', 'Frequencies', 'Future', 'Genetic Databases', 'Genome', 'Genomics', 'Goals', 'Gold', 'Guidelines', 'Individual', 'Knowledge', 'Laboratory Research', 'Light', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Mission', 'Modification', 'Oncologist', 'Pathogenicity', 'Pathologist', 'Patients', 'Phase', 'Population', 'Population Database', 'Process', 'PubMed', 'Public Health', 'Published Comment', 'Publishing', 'Recommendation', 'Reproducibility', 'Research Personnel', 'Scientist', 'Semantics', 'Source', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Variant', 'Work', 'clinical decision-making', 'clinical practice', 'cost effective', 'crowdsourcing', 'design', 'genetic variant', 'genome sequencing', 'genome-wide', 'indexing', 'industry partner', 'information classification', 'literature citation', 'novel', 'peer', 'preservation', 'response', 'search engine', 'standardize guidelines', 'success', 'tool']",NHGRI,"GENOMENON, INC.",R43,2019,152946,0.00014574230436164982
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9724498,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2019,67704,0.0007431195754188314
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9693291,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2019,397125,0.032845051240561446
"Computational Methods for Next-Generation Comparative Genomics PROJECT SUMMARY Recent advances in regulatory genomics, especially 3D genome organization in cell nucleus, suggest that existing methods for cross-species comparisons are limited in their ability to fully understand the evolution of non-coding genome function. In particular, it is known that genomes are compartmentalized to distinct compartments in the nucleus such as nuclear lamina and nuclear speckles. Such nuclear compartmentalization is an essential feature of higher-order genome organization and is linked to various important genome functions such as DNA replication timing and transcription. Unfortunately, to date no study exists that directly compares nuclear compartmentalization between human and other mammals. In addition, there are no computational models available that consider the continuous nature of multiple features of nuclear compartmentalization and function, which is critical to integrate genome-wide functional genomic data and datasets that measure cytological distance to multiple compartments across species. In this project, we will develop novel algorithms and generate new datasets to directly address two key questions: (1) How to identify the evolutionary patterns of nuclear compartmentalization? (2) What types of sequence evolution may drive spatial localization changes across species? The proposed project represents the first endeavor in comparative genomics for nuclear compartmentalization. Our Specific Aims are: (1) Developing new probabilistic models for identifying evolutionary patterns of nuclear compartmentalization. (2) Identifying genome-wide evolutionary patterns of nuclear compartmentalization in primate species based on TSA-seq and Repli-seq. (3) Developing new algorithms to connect sequence features to nuclear compartmentalization through cross-species comparisons. Successful completion of these aims will result in novel computational tools and new datasets that will be highly valuable for the comparative genomics community. Integrating the new computational tools and unique datasets will provide invaluable insights into the relationship between sequence evolution and changes in nuclear genome organization in mammalian species. Therefore, the proposed research is expected to advance comparative genomics to a new frontier and provide new perspectives for studying human genome function PROJECT NARRATIVE The proposed research is relevant to public health because the outcome of the project is expected to enhance the analyses of nuclear genome organizations across primate species to better understand genome function and human biology. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Computational Methods for Next-Generation Comparative Genomics,9765970,R01HG007352,"['3-Dimensional', 'Address', 'Algorithms', 'CRISPR/Cas technology', 'Cell Nucleus', 'Cells', 'Communities', 'Complement', 'Computer Simulation', 'Computing Methodologies', 'Crete', 'Cytology', 'DNA Insertion Elements', 'DNA Replication Timing', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genetic Transcription', 'Genome', 'Genomics', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Imagery', 'Knowledge', 'Lamin Type B', 'Link', 'Machine Learning', 'Mammals', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular Profiling', 'Nature', 'Nuclear', 'Nuclear Lamina', 'Outcome', 'Pattern', 'Phenotype', 'Primates', 'Psyche structure', 'Public Health', 'Research', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Time', 'Translating', 'United States National Institutes of Health', 'Untranslated RNA', 'base', 'comparative genomics', 'computerized tools', 'frontier', 'functional genomics', 'genetic variant', 'genome-wide', 'genomic data', 'improved', 'insight', 'mental function', 'next generation', 'novel', 'predictive modeling']",NHGRI,CARNEGIE-MELLON UNIVERSITY,R01,2019,433604,0.025060484570913973
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9693289,U01HG009086,"['Accounting', 'Affect', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Validation', 'Variant', 'analysis pipeline', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic architecture', 'genetic variant', 'genome sciences', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2019,864186,0.016791894022335684
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,0.01549674194063342
"Genome Based Influenza Vaccine Strain Selection using Machine Learning No abstract available PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection using Machine Learning,10044945,R01AI116744,[' '],NIAID,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2019,147704,0.03143332658536112
"Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen The impact of fungal pathogens on human health is devastating. They infect billions of people worldwide, and kill more than 1.5 million each year. The most vulnerable are people with reduced immune function, such as those with HIV or those undergoing immune suppressing treatments for cancer or organ transplants. One of the most pervasive fungal pathogens is Candida albicans, which kills almost 40% of people suffering from bloodstream infections. Treating these infections is extremely difficult, as fungi are closely related to humans and there are very few drugs that kill the fungus without host toxicity. With the emergence of drug resistance, the development of new therapeutic strategies is now crucial. To address this important clinical need and identify new antifungal drug targets, it is critical to uncover mechanisms that enable C. albicans to cause life-threatening human disease.  We are one of the first academic labs to obtain a powerful resource that will allow us to test the function of almost every gene in the C. albicans genome. This resource includes a collection of double barcoded heterozygous mutants covering ~90% of the genome, and a collection of strains covering ~40% of the genome where the expression of the remaining wild-type allele of a gene is governed by the tetracycline-repressible promoter. This resource provides an unprecedented opportunity to identify genes that control key virulence traits such as morphogenesis. It also enables the identification of determinants of commensalism and virulence, and to further elucidate the molecular mechanisms involved. We have optimized a functional genomics platform for massively parallel analysis of fungal virulence traits using next generation sequencing to quantify the relative proportion of each barcoded strain in pooled assays. We have also optimized high- resolution image analysis of cellular morphology and structures, and assays for identifying genes important for commensalism, virulence, and interaction with host immune cells. Our studies will provide the first global analysis of C. albicans morphogenesis, commensalism, and virulence.  Our studies will: 1) develop a computational platform to predict C. albicans essential genes, and expand the collection of tetracycline-repressible conditional expression strains to cover most non-essential genes, since genes required for pathogen viability in vitro provide little insight into mechanisms of host adaptation or virulence; 2) identify novel regulators of key virulence traits such as morphogenesis; and 3) identify determinants of C. albicans host adaptation and virulence on a genome scale. This work will provide an expanded functional genomics resource to advance the field, and will leverage this resource to elucidate the genes and genetic networks governing morphogenesis and virulence. This will reveal new strategies to cripple fungal pathogens, and drug targets to improve clinical outcome. The focus of this proposal is to elucidate mechanisms governing morphogenesis, commensalism, and virulence of the fungal pathogen Candida albicans. This eukaryotic pathogen has a profound impact on human health, with a global incidence of Candida bloodstream infections of ~400,000 cases, associated with crude mortality rates of 42% and attributable mortality rates of 27%; C. albicans is the most prevalent Candida species in almost all patient populations. The central hypotheses that govern this proposal are that functional genomic analysis of C. albicans using genome-scale libraries of double barcoded heterozygous gene deletion mutants and tetracycline-repressible conditional expression strains will reveal novel circuitry governing morphogenesis, commensalism, and virulence, and that targeting this circuitry has therapeutic potential for combating life-threatening fungal infections.","Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen",9751202,R01AI127375,"['Address', 'Alleles', 'Aneuploidy', 'Antifungal Agents', 'Antifungal Therapy', 'Ascomycota', 'Biological Assay', 'Biology', 'Candida', 'Candida albicans', 'Cells', 'Cellular Morphology', 'Cellular Structures', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Cues', 'Cytolysis', 'Data Set', 'Development', 'Diploidy', 'Drug Targeting', 'Drug resistance', 'Environment', 'Essential Genes', 'Filament', 'Foundations', 'Gastrointestinal tract structure', 'Gene Deletion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Library', 'Genomic approach', 'Genomics', 'Goals', 'Growth', 'HIV', 'Health', 'Human', 'Image Analysis', 'Immune', 'Immune system', 'Immunocompromised Host', 'In Vitro', 'Incidence', 'Individual', 'Infection', 'Investments', 'Laboratories', 'Libraries', 'Life', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Morphogenesis', 'Mus', 'Mutation', 'Mycoses', 'Nosocomial Infections', 'Organ Transplantation', 'Organism', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Property', 'Publishing', 'Resistance development', 'Resolution', 'Resources', 'Sepsis', 'Solid', 'Symbiosis', 'Systemic infection', 'Testing', 'Tetracyclines', 'Therapeutic', 'Toxic effect', 'Virulence', 'Work', 'Yeast Model System', 'Yeasts', 'attributable mortality', 'cancer therapy', 'cancer transplantation', 'combat', 'computational platform', 'fitness', 'functional genomics', 'fungus', 'gene replacement', 'genome-wide', 'genomic platform', 'high resolution imaging', 'human disease', 'immune function', 'improved', 'insight', 'member', 'mortality', 'mucosal microbiota', 'mutant', 'new therapeutic target', 'next generation sequencing', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'pathogen', 'pathogenic fungus', 'patient population', 'programs', 'promoter', 'quantitative imaging', 'response', 'trait', 'unpublished works']",NIAID,UNIVERSITY OF TORONTO,R01,2019,545099,-0.00420108711667114
"Computational approaches for identifying epigenomic contexts of somatic mutations ABSTRACT During normal development, aging, and diseases such as cancer, DNA damage due to endogenous and external factors, and repair defects result in accumulation of different types of somatic mutations including single nucleotide substitutions, small InDels, copy number alterations, translocations, and ploidy changes. While a vast majority of somatic mutations in the genome are not disease drivers, their patterns of genetic changes and associated context can provide insights into past exposure to mutagens, mechanisms of DNA damage and repair defects, and extent of genomic instability, which are important for understanding disease etiology, minimizing hazardous environmental exposure, and also predicting efficacy of emerging treatment strategies such as immunotherapy. A number of mutation signatures have been identified based on local sequence contexts to address this need. But, mechanisms of DNA damage and repair preferences depend on both local sequence and epigenomic contexts, and it remains to be understood whether epigenomic contexts of emerging mutation signatures can provide critical, complementary etiological insights at a genome-wide scale, which are not apparent from sequence contexts alone. This is of fundamental importance, because (i) etiology of many of the emerging mutation signatures is currently unknown, (ii) DNA damage response and repair depends on tissue contexts, and defects in core DNA repair genes often result in cancer development in tissue-specific manner, and (iii) differences in the extent of DNA damage and repair between stem and differentiated cells within the same tissues have consequences for aging and disease incidence rates. Built logically on our previous works, we propose to develop computational approaches to determine the impact of epigenomic contexts on the patterns of somatic mutations within and across tissue types, and validate computational predictions using targeted experiments. In Aim-1, we will develop an epigenomic context preference map for emerging mutation signatures. In Aim-2, we will determine the basis of tissue-dependent differences in mutation profiles attributed to DNA repair defects. In Aim-3, we will predict the extent of cell lineage-dependent patterns of mutation accumulation from the mutational landscape of terminal cells. I am currently an early stage investigator, and the proposal is aligned with my long-term goal to identify fundamental principles of mutability and evolvability of somatic genomes. Our project will deliver novel resources and knowledge for addressing questions regarding genomic integrity during development and aging, and diseases such as cancer. ! PUBLIC HEALTH RELEVANCE: The proposed project will use computational biology approaches to determine epigenomic context preference for somatic mutations, and use that to infer tissue-dependent changes in mutation patterns. Our results will provide fundamental insights into aspects of genome maintenance, which is important for advancing our understanding of cancer etiology, reducing exposure to mutagenic factors, and also predicting efficacy of emerging treatment strategies. !",Computational approaches for identifying epigenomic contexts of somatic mutations,9737246,R01GM129066,"['Address', 'Affect', 'Aging', 'Biometry', 'Blood', 'Cancer Etiology', 'Cancer Relapse', 'Cell Differentiation process', 'Cell Line', 'Cell Lineage', 'Cells', 'Chromatin', 'Clinical', 'Computational Biology', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'Data', 'Defect', 'Development', 'Disease', 'Doctor of Philosophy', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Evolution', 'Exposure to', 'Genome', 'Genomic DNA', 'Genomic Instability', 'Genomics', 'Goals', 'Immunotherapy', 'Incidence', 'Knowledge', 'Least-Squares Analysis', 'Location', 'Maintenance', 'Malignant Neoplasms', 'Maps', 'Modeling', 'Mutagenesis', 'Mutagens', 'Mutation', 'Nuclear', 'Nucleotides', 'Pathway interactions', 'Pattern', 'Ploidies', 'Point Mutation', 'Process', 'Publishing', 'Radiation Tolerance', 'Research Personnel', 'Resources', 'Role', 'Somatic Mutation', 'Source', 'Stem cells', 'Tissues', 'Work', 'base', 'cancer genomics', 'computer framework', 'epigenomics', 'experimental study', 'genome integrity', 'genome-wide', 'human tissue', 'improved', 'insertion/deletion mutation', 'insight', 'markov model', 'medical schools', 'novel', 'preference', 'public health relevance', 'random forest', 'repaired', 'response', 'stem', 'transcriptomics', 'treatment strategy']",NIGMS,RBHS -CANCER INSTITUTE OF NEW JERSEY,R01,2019,336177,-0.037561906610610124
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9729028,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2019,1002519,0.00029604730343296703
"The Enzymatic Reader Project Summary At this point in time, it is generally understood and agreed upon that single-molecule sequencing (SMS) is the future of genomics, transcriptomics, epigenomics, and epitranscriptomics due to its significant advantages over other technologies and methods. However, in order for these advantages to be fully realized, and for SMS to become the “gold standard” sequencing approach, significant issues and hurdles must be solved and overcome. During this program, Electronic BioSciences, Inc. (EBS) aims to demonstrate a completely new and enabling SMS method that will possess the ability to directly and correctly identify individual nucleotides, including chemically modified nucleotides. During this project, we will both demonstrate the ability of this entirely new sequencing approach to sequence DNA with high accuracy (directly comparing the obtained accuracy, throughput, error mechanisms and associated rates to other SMS approaches) and correctly identify (and sequence) 5-methylcytosine (5mC) and its derivatives, at the single molecule level. At the conclusion of this Phase I project, we will have successfully demonstrated an entirely new and dramatically improved SMS approach, and reduced the associated risks involved with its full future commercial developments. There is a current need within the field of next generation sequencing (NGS) or so called third generation sequencing (TGS) for new, enabling instrumentation that is capable of high-accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats. The entirely new SMS methodology that will be developed during this project will overcome known hurdles and limitations of currently available NGS, TGS, and SMS technologies, resulting in technology that is cost-efficient, highly accurate, easy to setup and utilize, capable of de novo sequencing and modified base calling, and yields highly simplistic data for easy analysis and post possessing. Through significant advancements made during this program, this resulting technology will revolutionize the use of the genome and epigenome, radically change standard R&D and clinical practices, and greatly advance clinical diagnostics, prognostics, and therapeutic decision making. Project Narrative The novel single-molecule sequencing (SMS) technology developed during this project will enable high- accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats via a cost-efficient and easy-to-use methodology. The impact of these advances in SMS will eventually enable wide-scale, routine clinical care and diagnostics toward advanced precision medicine, not just R&D. The performance and accessibility of such technology will transform the understanding and application of genomics and epigenomics, the associated clinical practices, that ability to provide precision clinical diagnostics, prognostics, and therapeutic decision making for improved public healthcare and wellbeing.",The Enzymatic Reader,9677956,R43HG010427,"['Biological', 'Biological Sciences', 'Caliber', 'Chemicals', 'Chemistry', 'Church', 'Complex', 'DNA Primers', 'DNA Sequence', 'DNA polymerase A', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disadvantaged', 'Drops', 'Electrodes', 'Enzymes', 'Evaluation', 'Future', 'Genome', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Ions', 'Label', 'Length', 'Lipid Bilayers', 'Logistics', 'Methodology', 'Methods', 'Motor', 'Movement', 'Noise', 'Nucleotides', 'Performance', 'Personal Satisfaction', 'Phase', 'Polymerase', 'Polymers', 'Preparation', 'Process', 'Proteins', 'RNA', 'Reader', 'Reading Frames', 'Reproducibility', 'Risk', 'Sampling', 'Side', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Software Tools', 'Speed', 'Stretching', 'System', 'Technology', 'Therapeutic', 'Third Generation Sequencing', 'Time', 'base', 'clinical care', 'clinical diagnostics', 'clinical practice', 'cost', 'cost efficient', 'electric field', 'epigenome', 'epigenomics', 'epitranscriptomics', 'improved', 'instrumentation', 'machine learning algorithm', 'nanopore', 'next generation sequencing', 'novel', 'precision medicine', 'prevent', 'prognostic', 'programs', 'research and development', 'single molecule', 'solid state', 'transcriptomics']",NHGRI,"ELECTRONIC BIOSCIENCES, INC.",R43,2019,247611,0.019979378932605665
"Novel Statistical methods for DNA Sequencing Data, and applications to Autism. Summary One of the major problems in human genetics is understanding the genetic causes underlying complex phenotypes, including neuropsychiatric traits such as autism spectrum disorders and schizophrenia. Despite tremendous work over the past few decades, the underlying biological mechanisms are poorly understood in most cases. Recent advances in high-throughput, massively parallel genomic technologies have revolutionized the field of human genetics and promise to lead to important scientific advances. Despite this progress in data generation, it remains very challenging to analyze and interpret these data. The main focus of this proposal is the development of powerful statistical methods for the integration of whole-genome sequencing data with rich functional genomics data with the goal to improve the discovery of genes involved in autism spectrum disorders. We propose to integrate data from many different sources, including epigenetic data from projects such as ENCODE, Roadmap, and PsychENCODE, eQTL data from the GTEx, PsychENCODE and CommonMind consortia, data from large scale databases of genetic variation such as ExAC and gnomAD, in order to predict functional effects of genetic variants in non-coding genetic regions in a tissue and cell type specific manner, and generate functional maps across large number of tissues and cell types in the human body that we can then use to identify novel associations with autism in whole-genome sequencing studies. The proposed functional predictions and functional maps will be broadly available in the popular ANNOVAR database. We further propose to use these functional predictions in the analysis of almost 20,000 whole genomes from three large whole genome sequencing studies for autism. We believe that the proposed research is very timely and has the potential to substantially improve the analysis of non-coding genetic variation, and hence provide new insights into the biological mechanisms underlying risk to autism, and more broadly to other neuropsychiatric diseases. Narrative Autism Spectrum Disorders are common diseases with major impact on public health. Although coding variation has been extensively studied for its role in affecting risk to autism, the analysis of non-coding variation poses tremendous challenges. The proposed statistical methods and their applications to nearly 20,000 whole genomes from three large autism whole genome sequencing studies will improve our understanding of the biological mechanisms involved in autism with important implications for disease treatment strategies.","Novel Statistical methods for DNA Sequencing Data, and applications to Autism.",9735436,R01MH095797,"['Affect', 'Anterior', 'Biochemical', 'Biological', 'Biological Assay', 'Brain region', 'Chromatin', 'Code', 'Collection', 'Complex', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Encyclopedia of DNA Elements', 'Epigenetic Process', 'Generations', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Human Genetics', 'Human body', 'Individual', 'International', 'Lead', 'Maps', 'Measures', 'Methods', 'Molecular', 'Phenotype', 'Prefrontal Cortex', 'Public Health', 'Research', 'Risk', 'Role', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Source', 'Statistical Methods', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Variant', 'Work', 'autism spectrum disorder', 'cell type', 'cingulate cortex', 'data integration', 'design', 'epigenomics', 'exome', 'frontal lobe', 'functional genomics', 'gene discovery', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic data', 'histone modification', 'improved', 'insight', 'large-scale database', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'software development', 'supervised learning', 'tool', 'trait', 'treatment strategy', 'whole genome']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,454366,-0.0015462172904811688
"Mapping RNA polymerase in tissue samples with ChRO-seq. PROJECT ABSTRACT Deciphering how complex programs of gene expression and regulation contribute to human disease is one of the major challenges facing the field of genomics. Over the past decade, a wealth of new high-throughput genomics tools have revolutionized how we identify active genomic regions and appear poised to make great strides in understanding the mechanisms of disease. Yet the application of most of these technologies has been limited to established cell lines. Currently, approaches being developed to comprehensively map functional elements across the genome involve combining data from several different genome-wide experimental assays, making them expensive and impractical to use in clinical isolates of limited quantity or even to analyze new cell lines. Compounding these technical difficulties, gene expression is a complex and highly tissue dependent biological process, and many important applications will require the direct interrogation of clinical isolates or other similarly limited sources of sample. Thus, efficient new tools that map the repertoire of functional elements across the genome are likely to transform the biomedical and clinical sciences.  We propose to develop Chromatin Run-On and Sequencing (ChRO-seq) and a suite of computational tools for mapping transcription directly in limited tissue samples. Our approach uses a single genome-wide molecular assay to efficiently identify the location of promoters and enhancers, transcription factor binding sites, gene and lincRNA boundaries, transcription levels, and impute certain histone modifications. Preliminary ChRO-seq data reveals patterns of transcription that are virtually identical to those using Precision Run on and Sequencing in cultured cells, but can easily be applied in solid tissue samples. We applied our preliminary ChRO-seq technology to several primary tumors, revealing new insights into how transcriptional regulation underlies cancer development and progression, and providing a key proof-of-concept motivating further technology development. We anticipate that ChRO-seq and the computational methods proposed will enable the efficient discovery of functional elements in virtually any cell sample. In addition, ChRO-seq has the unique advantage that it can be applied in limited tissue samples and clinical isolates even after the degradation of mRNA. PROJECT NARRATIVE We propose to develop a suite of molecular and computational technologies that allow researchers to directly measure transcriptional regulation of genes, enhancers, and lincRNAs in limited clinical isolates. These technologies are anticipated to have a major impact on the biomedical sciences, enabling the genome-wide interrogation of transcription during virtually any disease process for the first time.",Mapping RNA polymerase in tissue samples with ChRO-seq.,9637410,R01HG009309,"['Archives', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cells', 'ChIP-seq', 'Chromatin', 'Clinical', 'Clinical Sciences', 'Code', 'Complex', 'Computing Methodologies', 'Consumption', 'Cultured Cells', 'DNA Sequence', 'DNA-Directed RNA Polymerase', 'Data', 'Deoxyribonuclease I', 'Detection', 'Development', 'Disease', 'Elements', 'Enhancers', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Gold', 'Hypersensitivity', 'Location', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Computations', 'Nuclear', 'Outcome', 'Pattern', 'Performance', 'Population', 'Primary Neoplasm', 'Process', 'Protocols documentation', 'RNA', 'RNA Polymerase I', 'Regulatory Element', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Solid', 'Source', 'Specimen', 'Speed', 'Technology', 'Time', 'Tissue Sample', 'Tissues', 'Transcriptional Regulation', 'Untranslated RNA', 'computational suite', 'computerized tools', 'deep learning', 'established cell line', 'experimental study', 'genome-wide', 'genomic tools', 'histone modification', 'human disease', 'improved', 'innovation', 'insight', 'mRNA Transcript Degradation', 'next generation', 'novel', 'predictive tools', 'programs', 'promoter', 'scale up', 'technology development', 'tool', 'transcription factor', 'virtual']",NHGRI,CORNELL UNIVERSITY,R01,2019,387500,-0.018885429089746514
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9552183,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Algorithms', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA sequencing', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Link', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'STEM research', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Time Series Analysis', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'deep learning', 'disorder risk', 'fitness', 'flexibility', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2018,297725,0.018383981387173627
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9406205,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'predictive modeling', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2018,372603,0.04038422301295893
"Population genomics of adaptation Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",Population genomics of adaptation,9554999,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependence', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Link', 'Location', 'Machine Learning', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Supervision', 'Techniques', 'Time', 'Variant', 'Work', 'deep learning', 'deep neural network', 'fight against', 'genomic data', 'global health', 'human disease', 'learning strategy', 'malaria infection', 'malaria transmission', 'markov model', 'novel', 'recurrent neural network', 'resistance allele', 'response', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",R01,2018,18944,0.043894417079007104
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell- based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off- target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,9678132,U01HL145793,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'learning strategy', 'next generation', 'novel', 'novel therapeutics', 'response', 'safety testing', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NHLBI,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2018,651251,0.01654998677460554
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9617314,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Supervision', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2018,249000,0.021390296776236297
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9547454,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2018,332952,0.012763286213803083
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9490340,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2018,1536744,0.028213666708367405
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9764035,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2018,74880,0.028213666708367405
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9764029,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2018,114880,0.028213666708367405
"Selective Whole Genome Amplification - Enabling Microbial Population Genomics Microbial population genetic research has been crucial for understanding pathogen dynamics, virulence, host specificity, and many other topics; in many cases uncovering unexpected and transformative biological processes. However, conventional population genetic analyses are limited by the quantity of sequence data from each sample. The temporal, spatial, and evolutionary resolution of techniques that rely on single gene sequences or multi-locus sequence typing are often insufficient to study biological processes on fine scales, precisely the scales at which many evolutionary and mechanistic process occur. Population genomics offers a vast quantity of sequence information for inferring evolutionary and ecological processes on very fine spatial and temporal scales, inferences that are critical to understanding and eventually controlling many infectious diseases. The promise of population genomics is tempered, however, by difficulties in isolating and preparing microbes for next-generation sequencing. We have developed the selective whole genome amplification (SWGA) technology to sequence microbial genomes from complex biological specimens without relying on labor-intensive laboratory culture, even if the focal microbial genome constitutes only a miniscule fraction of the natural sample. The primary hindrance to popular adoption of SWGA for microbial genomic studies is not its effectiveness in producing samples suitable for next-generation sequencing but in the upfront investment needed to develop an effective protocol to amplify the genome of a specific microbial species. Identifying an SWGA protocol that consistently results in selective and even amplification across the target genome is currently hindered by computationally-inefficient software that can evaluate a very limited set of the potentially effective solutions. Further, this software uses marginally-effective optimality criteria as there is currently only a limited understanding of the true criteria that result in highly-selective and even amplification of a target genome. As a result, SWGA protocol development is currently costly in both time and resources. A primary goal of the proposed research is to identify the criteria that result in optimal SWGA by analyzing next- generation sequencing data with advanced machine learning techniques. These optimality criteria will be integrated into a freely-available, computationally-efficient swga development program that will reduce the upfront investment in SWGA protocol development, thus allowing researchers to address medically- and biologically-important questions in any microbial species. In the near term, this project will also generate effective SWGA protocols for four microbial species which can be used immediately to address fundamental questions in evolutionary biology, disease progression, and emerging infectious disease dynamics. From a global disease perspective, this work is imperative as the majority of microbial species cannot easily be cultured and are in danger of becoming bystanders in the genomics revolution that is currently elucidating evolutionary processes and molecular mechanisms in cultivable microbial species. Addressing many of the major outstanding questions about pathogen evolution will require analyses of populations of microbial genomes. Although population genomic studies would provide the analytical resolution to investigate evolutionary and mechanistic processes on fine spatial and temporal scales – precisely the scales at which these processes occur – microbial population genomic research is currently hindered by the practicalities of obtaining sufficient quantities of genomes to analyze. We propose to develop an innovative, cost-effective, practical, and publically-available technology to collect sufficient quantities of microbial genomic DNA necessary for next-generation microbial genome sequencing.",Selective Whole Genome Amplification - Enabling Microbial Population Genomics,9507167,R21AI137433,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Biological', 'Biological Process', 'Biology', 'Characteristics', 'Communicable Diseases', 'Complex', 'Computer software', 'Coupling', 'DNA', 'Data', 'Development', 'Disease', 'Disease Progression', 'Effectiveness', 'Emerging Communicable Diseases', 'Evolution', 'Foundations', 'Genes', 'Genetic Research', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Health', 'Human', 'Investigation', 'Investments', 'Laboratory culture', 'Machine Learning', 'Medical', 'Metaphor', 'Methods', 'Microbe', 'Microbial Genome Sequencing', 'Microsatellite Repeats', 'Molecular', 'Organism', 'Population', 'Population Analysis', 'Population Genetics', 'Process', 'Program Development', 'Protocols documentation', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Shapes', 'Specificity', 'Specimen', 'System', 'Techniques', 'Technology', 'Time', 'Virulence', 'Work', 'cost', 'cost effective', 'design', 'genetic analysis', 'genetic approach', 'host-microbe interactions', 'improved', 'innovation', 'microbial', 'microbial genome', 'next generation', 'next generation sequencing', 'novel', 'pathogen', 'prevent', 'protocol development', 'vector', 'whole genome']",NIAID,UNIVERSITY OF PENNSYLVANIA,R21,2018,242837,0.04435520270226526
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades — such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases — have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9486266,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2018,479215,-0.004093649279005045
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9593406,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2018,1420000,0.0024301540439596358
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9531422,R01HG009188,"['Affect', 'Base Pairing', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'Custom', 'DNA', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modernization', 'Modification', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Reproducibility', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'collaborative environment', 'cost', 'enzyme activity', 'experimental study', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,468098,-0.015452775391489853
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9562959,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2018,1186500,0.03759634353387125
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9420662,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2018,2000000,0.029534531573562322
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9545035,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2018,67704,0.0007431195754188314
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9498252,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'cancer therapy', 'computerized data processing', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",R01,2018,158992,0.032845051240561446
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9483341,U01HG009086,"['Accounting', 'Affect', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Validation', 'Variant', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic architecture', 'genetic variant', 'genome sciences', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2018,864186,0.016791894022335684
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9425925,R01HG009906,"['Address', 'Biological Neural Networks', 'CRISPR/Cas technology', 'Cell Count', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Dimensions', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Gene Targeting', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Imagery', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'base', 'cell type', 'chromosome conformation capture', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'forest', 'genome annotation', 'genome browser', 'genome-wide', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,PENNSYLVANIA STATE UNIV HERSHEY MED CTR,R01,2018,383250,0.004683795753653977
"Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen The impact of fungal pathogens on human health is devastating. They infect billions of people worldwide, and kill more than 1.5 million each year. The most vulnerable are people with reduced immune function, such as those with HIV or those undergoing immune suppressing treatments for cancer or organ transplants. One of the most pervasive fungal pathogens is Candida albicans, which kills almost 40% of people suffering from bloodstream infections. Treating these infections is extremely difficult, as fungi are closely related to humans and there are very few drugs that kill the fungus without host toxicity. With the emergence of drug resistance, the development of new therapeutic strategies is now crucial. To address this important clinical need and identify new antifungal drug targets, it is critical to uncover mechanisms that enable C. albicans to cause life-threatening human disease.  We are one of the first academic labs to obtain a powerful resource that will allow us to test the function of almost every gene in the C. albicans genome. This resource includes a collection of double barcoded heterozygous mutants covering ~90% of the genome, and a collection of strains covering ~40% of the genome where the expression of the remaining wild-type allele of a gene is governed by the tetracycline-repressible promoter. This resource provides an unprecedented opportunity to identify genes that control key virulence traits such as morphogenesis. It also enables the identification of determinants of commensalism and virulence, and to further elucidate the molecular mechanisms involved. We have optimized a functional genomics platform for massively parallel analysis of fungal virulence traits using next generation sequencing to quantify the relative proportion of each barcoded strain in pooled assays. We have also optimized high- resolution image analysis of cellular morphology and structures, and assays for identifying genes important for commensalism, virulence, and interaction with host immune cells. Our studies will provide the first global analysis of C. albicans morphogenesis, commensalism, and virulence.  Our studies will: 1) develop a computational platform to predict C. albicans essential genes, and expand the collection of tetracycline-repressible conditional expression strains to cover most non-essential genes, since genes required for pathogen viability in vitro provide little insight into mechanisms of host adaptation or virulence; 2) identify novel regulators of key virulence traits such as morphogenesis; and 3) identify determinants of C. albicans host adaptation and virulence on a genome scale. This work will provide an expanded functional genomics resource to advance the field, and will leverage this resource to elucidate the genes and genetic networks governing morphogenesis and virulence. This will reveal new strategies to cripple fungal pathogens, and drug targets to improve clinical outcome. The focus of this proposal is to elucidate mechanisms governing morphogenesis, commensalism, and virulence of the fungal pathogen Candida albicans. This eukaryotic pathogen has a profound impact on human health, with a global incidence of Candida bloodstream infections of ~400,000 cases, associated with crude mortality rates of 42% and attributable mortality rates of 27%; C. albicans is the most prevalent Candida species in almost all patient populations. The central hypotheses that govern this proposal are that functional genomic analysis of C. albicans using genome-scale libraries of double barcoded heterozygous gene deletion mutants and tetracycline-repressible conditional expression strains will reveal novel circuitry governing morphogenesis, commensalism, and virulence, and that targeting this circuitry has therapeutic potential for combating life-threatening fungal infections.","Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen",9536694,R01AI127375,"['Address', 'Alleles', 'Aneuploidy', 'Antifungal Agents', 'Antifungal Therapy', 'Ascomycota', 'Biological Assay', 'Biology', 'Candida', 'Candida albicans', 'Cells', 'Cellular Morphology', 'Cellular Structures', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Cues', 'Cytolysis', 'Data Set', 'Development', 'Diploidy', 'Drug Targeting', 'Drug resistance', 'Environment', 'Essential Genes', 'Filament', 'Foundations', 'Gastrointestinal tract structure', 'Gene Deletion', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Library', 'Genomic approach', 'Genomics', 'Goals', 'Growth', 'HIV', 'Health', 'Human', 'Image Analysis', 'Immune', 'Immune system', 'Immunocompromised Host', 'In Vitro', 'Incidence', 'Individual', 'Infection', 'Investments', 'Laboratories', 'Libraries', 'Life', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Morphogenesis', 'Mucous Membrane', 'Mus', 'Mutation', 'Mycoses', 'Nosocomial Infections', 'Organ Transplantation', 'Organism', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Property', 'Publishing', 'Resistance development', 'Resolution', 'Resources', 'Sepsis', 'Solid', 'Symbiosis', 'Systemic infection', 'Testing', 'Tetracyclines', 'Therapeutic', 'Toxic effect', 'Virulence', 'Work', 'Yeast Model System', 'Yeasts', 'attributable mortality', 'cancer therapy', 'combat', 'fitness', 'functional genomics', 'fungus', 'gene replacement', 'genome-wide', 'genomic platform', 'high resolution imaging', 'human disease', 'immune function', 'improved', 'insight', 'member', 'microbiota', 'mortality', 'mutant', 'new therapeutic target', 'next generation sequencing', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'pathogen', 'patient population', 'programs', 'promoter', 'quantitative imaging', 'response', 'trait', 'unpublished works']",NIAID,UNIVERSITY OF TORONTO,R01,2018,570554,-0.00420108711667114
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9416160,R00HG008171,"['Advisory Committees', 'Architecture', 'Area', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Code', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Individual', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'RNA library', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'cancer drug resistance', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'experimental study', 'functional genomics', 'genetic element', 'genome analysis', 'genome editing', 'genome-wide', 'genomic predictors', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'scaffold', 'screening', 'small hairpin RNA', 'targeted nucleases', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2018,242325,0.03496454362272129
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9548692,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2018,1002519,0.00029604730343296703
"POPULATION GENOMICS OF ADAPTATION Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",POPULATION GENOMICS OF ADAPTATION,9815897,R01GM117241,[' '],NIGMS,UNIVERSITY OF OREGON,R01,2018,276973,0.043894417079007104
"Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID) DESCRIPTION (provided by applicant): As a result of the accelerated pace of development of technologies for characterizing the human genome, the rate-limiting step for large scale genomic investigation in clinical populations is now phenotyping. This is particularly the case for neuropsychiatric (NP) illness, where phenotypes are complex, biomarkers are lacking, and the primary cell types of interest are difficult to access directly. It has become apparent that both rare and common genetic variation contributes to disease risk and that this risk crosses traditional diagnostic boundaries in psychiatry. Taking advantage of a large, already-established NP biobank could dramatically accelerate progress toward understanding the cross-disorder mechanism of action of disease liability genes. This study proposes novel applications of emerging technologies in informatics and cellular neurobiology to eliminate this phenotyping bottleneck. In doing so, it will accelerate investigation of clinical and cellular phenotypes for understanding single and multilocus/polygenic associations. Aim 1: Adapt and expand one of the largest NP cellular biobanks by parsing electronic health records with gold-standard assessment of cognition and other RDoC phenotypes. Aim 2: Define the genome-wide multidimensional functional genomics (MFG) landscape in NP disease into which the transcriptomic signature (RNA-seq) of each induced neuron (IN) representing a clinically characterized individual is projected. The projection provides the mapping from molecular to phenotypic characterization and a directionality towards healthful/neurotypical states used in Aim 3. Aim 3: Develop a probabilistic model of gene expression dependencies that will predict which small molecular perturbations are likely to shift the IN transcriptomic signature in a healthful direction in the MFG and to then update the model based on measured perturbations in the MFG. Aim 4: Select patient samples to study in greater detail for epigenetic (DNA methylation, histone marks and RNA editing) and transcriptional control particularly with regard to activity dependent changes that have been implicated in many NP diseases. Aim 5: Here we assess just how well the clinical phenotypes are informed by the genome-wide characterizations and assess which is more robust. PUBLIC HEALTH RELEVANCE: This study is designed to answer the question: can we use the fruits of the first phases of the human genome project to create a new and more robust scheme of classifying neuropsychiatric disease, one that is more reliable with regard to prognosis of these diseases, more insightful as to the biological aberration in each category and, therefore, more effective in treating the patient.",Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID),9536132,P50MH106933,"['Agonist', 'Alcohol or Other Drugs use', 'Anxiety Disorders', 'Biological', 'Biological Markers', 'Categories', 'Cells', 'Cellular Neurobiology', 'Clinical', 'Code', 'Cognition', 'Cognitive', 'Comorbidity', 'Complex', 'Computer Simulation', 'Consent Forms', 'DNA Methylation', 'DSM-IV', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Dimensions', 'Disease', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Electrophysiology (science)', 'Emerging Technologies', 'Enrollment', 'Epigenetic Process', 'Equipment and supply inventories', 'Fibroblasts', 'Fruit', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genetic Variation', 'Genomics', 'Goals', 'Gold', 'Growth', 'Healthcare Systems', 'Histones', 'Human Genome', 'Human Genome Project', 'Individual', 'Informatics', 'Investigation', 'Laboratories', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Modeling', 'Molecular', 'Mood Disorders', 'National Human Genome Research Institute', 'National Institute of Mental Health', 'Natural Language Processing', 'Neurons', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacology', 'Phase', 'Phenotype', 'Population', 'Prosencephalon', 'Psychiatry', 'Psychotic Disorders', 'Public Domains', 'RNA', 'RNA Editing', 'RNA Splicing', 'Research Domain Criteria', 'Resolution', 'Resources', 'Risk', 'Running', 'Sampling', 'Scheme', 'Small RNA', 'Standardization', 'Statistical Models', 'Stress', 'Transcript', 'Transcriptional Regulation', 'Translations', 'Untranslated RNA', 'Update', 'Visit', 'Vocabulary', 'base', 'biobank', 'cell type', 'clinical investigation', 'clinical phenotype', 'clinically relevant', 'design', 'disorder risk', 'experimental study', 'functional genomics', 'genome sequencing', 'genome-wide', 'high risk', 'insight', 'interest', 'molecular phenotype', 'neuropsychiatric disorder', 'neuropsychiatric symptom', 'neuropsychiatry', 'neuroregulation', 'novel', 'outcome forecast', 'patient population', 'phenomenological models', 'predicting response', 'prognostic', 'public health relevance', 'relating to nervous system', 'response', 'technology development', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIMH,HARVARD MEDICAL SCHOOL,P50,2018,3291017,0.00353013033336667
"Mapping RNA polymerase in tissue samples with ChRO-seq. PROJECT ABSTRACT Deciphering how complex programs of gene expression and regulation contribute to human disease is one of the major challenges facing the field of genomics. Over the past decade, a wealth of new high-throughput genomics tools have revolutionized how we identify active genomic regions and appear poised to make great strides in understanding the mechanisms of disease. Yet the application of most of these technologies has been limited to established cell lines. Currently, approaches being developed to comprehensively map functional elements across the genome involve combining data from several different genome-wide experimental assays, making them expensive and impractical to use in clinical isolates of limited quantity or even to analyze new cell lines. Compounding these technical difficulties, gene expression is a complex and highly tissue dependent biological process, and many important applications will require the direct interrogation of clinical isolates or other similarly limited sources of sample. Thus, efficient new tools that map the repertoire of functional elements across the genome are likely to transform the biomedical and clinical sciences.  We propose to develop Chromatin Run-On and Sequencing (ChRO-seq) and a suite of computational tools for mapping transcription directly in limited tissue samples. Our approach uses a single genome-wide molecular assay to efficiently identify the location of promoters and enhancers, transcription factor binding sites, gene and lincRNA boundaries, transcription levels, and impute certain histone modifications. Preliminary ChRO-seq data reveals patterns of transcription that are virtually identical to those using Precision Run on and Sequencing in cultured cells, but can easily be applied in solid tissue samples. We applied our preliminary ChRO-seq technology to several primary tumors, revealing new insights into how transcriptional regulation underlies cancer development and progression, and providing a key proof-of-concept motivating further technology development. We anticipate that ChRO-seq and the computational methods proposed will enable the efficient discovery of functional elements in virtually any cell sample. In addition, ChRO-seq has the unique advantage that it can be applied in limited tissue samples and clinical isolates even after the degradation of mRNA. PROJECT NARRATIVE We propose to develop a suite of molecular and computational technologies that allow researchers to directly measure transcriptional regulation of genes, enhancers, and lincRNAs in limited clinical isolates. These technologies are anticipated to have a major impact on the biomedical sciences, enabling the genome-wide interrogation of transcription during virtually any disease process for the first time.",Mapping RNA polymerase in tissue samples with ChRO-seq.,9420630,R01HG009309,"['Archives', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cells', 'ChIP-seq', 'Chromatin', 'Clinical', 'Clinical Sciences', 'Code', 'Complex', 'Computing Methodologies', 'Cultured Cells', 'DNA Sequence', 'DNA-Directed RNA Polymerase', 'Data', 'Deoxyribonuclease I', 'Detection', 'Development', 'Disease', 'Elements', 'Enhancers', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Gold', 'Hypersensitivity', 'Location', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Computations', 'Nuclear', 'Outcome', 'Pattern', 'Performance', 'Population', 'Primary Neoplasm', 'Process', 'Protocols documentation', 'RNA', 'RNA Polymerase I', 'Regulatory Element', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Solid', 'Source', 'Specimen', 'Speed', 'Technology', 'Time', 'Tissue Sample', 'Tissues', 'Transcriptional Regulation', 'Untranslated RNA', 'computerized tools', 'deep learning', 'established cell line', 'experimental study', 'genome-wide', 'genomic tools', 'histone modification', 'human disease', 'improved', 'innovation', 'insight', 'mRNA Transcript Degradation', 'next generation', 'novel', 'predictive tools', 'programs', 'promoter', 'scale up', 'technology development', 'tool', 'transcription factor', 'virtual']",NHGRI,CORNELL UNIVERSITY,R01,2018,387500,-0.018885429089746514
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9205487,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2017,372603,0.04038422301295893
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9328097,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Algorithms', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA sequencing', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'STEM research', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Time Series Analysis', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'disorder risk', 'fitness', 'flexibility', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2017,298655,0.018383981387173627
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9333402,K99HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Supervision', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",K99,2017,37785,0.021390296776236297
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9382936,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Architecture', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2017,225087,0.012763286213803083
"Population genomics of adaptation Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",Population genomics of adaptation,9383198,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Biological Neural Networks', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependency', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Learning', 'Link', 'Location', 'Machine Learning', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Recurrence', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Supervision', 'Techniques', 'Time', 'Variant', 'Work', 'fight against', 'genomic data', 'global health', 'human disease', 'learning strategy', 'malaria infection', 'malaria transmission', 'markov model', 'novel', 'resistance allele', 'response', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",R01,2017,295480,0.043894417079007104
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9270925,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2017,1573499,0.028213666708367405
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9565097,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2017,93567,0.028213666708367405
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9344966,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Molecular Profiling', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2017,1186500,0.03759634353387125
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9353854,R01HG009188,"['Affect', 'Base Pairing', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'Custom', 'DNA', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modernization', 'Modification', 'Molecular Models', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Reproducibility', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'collaborative environment', 'cost', 'enzyme activity', 'experimental study', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2017,438098,-0.015452775391489853
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9248178,U24HG009446,"['ATAC-seq', 'Alleles', 'Alpha Cell', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2017,2000000,0.029534531573562322
"Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.",Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons,9275674,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Docking', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,119777,0.028816085776734293
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9301573,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,216422,0.035046167275585995
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9357752,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2017,62304,0.0007431195754188314
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9275537,U01HG009086,"['Accounting', 'Affect', 'Architecture', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Validation', 'Variant', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2017,864186,0.016791894022335684
"Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen The impact of fungal pathogens on human health is devastating. They infect billions of people worldwide, and kill more than 1.5 million each year. The most vulnerable are people with reduced immune function, such as those with HIV or those undergoing immune suppressing treatments for cancer or organ transplants. One of the most pervasive fungal pathogens is Candida albicans, which kills almost 40% of people suffering from bloodstream infections. Treating these infections is extremely difficult, as fungi are closely related to humans and there are very few drugs that kill the fungus without host toxicity. With the emergence of drug resistance, the development of new therapeutic strategies is now crucial. To address this important clinical need and identify new antifungal drug targets, it is critical to uncover mechanisms that enable C. albicans to cause life-threatening human disease.  We are one of the first academic labs to obtain a powerful resource that will allow us to test the function of almost every gene in the C. albicans genome. This resource includes a collection of double barcoded heterozygous mutants covering ~90% of the genome, and a collection of strains covering ~40% of the genome where the expression of the remaining wild-type allele of a gene is governed by the tetracycline-repressible promoter. This resource provides an unprecedented opportunity to identify genes that control key virulence traits such as morphogenesis. It also enables the identification of determinants of commensalism and virulence, and to further elucidate the molecular mechanisms involved. We have optimized a functional genomics platform for massively parallel analysis of fungal virulence traits using next generation sequencing to quantify the relative proportion of each barcoded strain in pooled assays. We have also optimized high- resolution image analysis of cellular morphology and structures, and assays for identifying genes important for commensalism, virulence, and interaction with host immune cells. Our studies will provide the first global analysis of C. albicans morphogenesis, commensalism, and virulence.  Our studies will: 1) develop a computational platform to predict C. albicans essential genes, and expand the collection of tetracycline-repressible conditional expression strains to cover most non-essential genes, since genes required for pathogen viability in vitro provide little insight into mechanisms of host adaptation or virulence; 2) identify novel regulators of key virulence traits such as morphogenesis; and 3) identify determinants of C. albicans host adaptation and virulence on a genome scale. This work will provide an expanded functional genomics resource to advance the field, and will leverage this resource to elucidate the genes and genetic networks governing morphogenesis and virulence. This will reveal new strategies to cripple fungal pathogens, and drug targets to improve clinical outcome. The focus of this proposal is to elucidate mechanisms governing morphogenesis, commensalism, and virulence of the fungal pathogen Candida albicans. This eukaryotic pathogen has a profound impact on human health, with a global incidence of Candida bloodstream infections of ~400,000 cases, associated with crude mortality rates of 42% and attributable mortality rates of 27%; C. albicans is the most prevalent Candida species in almost all patient populations. The central hypotheses that govern this proposal are that functional genomic analysis of C. albicans using genome-scale libraries of double barcoded heterozygous gene deletion mutants and tetracycline-repressible conditional expression strains will reveal novel circuitry governing morphogenesis, commensalism, and virulence, and that targeting this circuitry has therapeutic potential for combating life-threatening fungal infections.","Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen",9213066,R01AI127375,"['Address', 'Alleles', 'Aneuploidy', 'Antifungal Agents', 'Antifungal Therapy', 'Ascomycota', 'Biological Assay', 'Biology', 'Candida', 'Candida albicans', 'Cells', 'Cellular Morphology', 'Cellular Structures', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Cues', 'Cytolysis', 'Data Set', 'Development', 'Diploidy', 'Drug Targeting', 'Drug resistance', 'Environment', 'Essential Genes', 'Filament', 'Foundations', 'Gastrointestinal tract structure', 'Gene Deletion', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Library', 'Genomic approach', 'Genomics', 'Goals', 'Growth', 'HIV', 'Health', 'Human', 'Image Analysis', 'Immune', 'Immune system', 'Immunocompromised Host', 'In Vitro', 'Incidence', 'Individual', 'Infection', 'Investments', 'Laboratories', 'Libraries', 'Life', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Morphogenesis', 'Mus', 'Mutation', 'Mycoses', 'Nosocomial Infections', 'Organ Transplantation', 'Organism', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Property', 'Publishing', 'Resistance development', 'Resolution', 'Resources', 'Sepsis', 'Solid', 'Symbiosis', 'Systemic infection', 'Testing', 'Tetracyclines', 'Therapeutic', 'Toxic effect', 'Virulence', 'Work', 'Yeast Model System', 'Yeasts', 'attributable mortality', 'cancer therapy', 'combat', 'fitness', 'functional genomics', 'fungus', 'gene replacement', 'genome-wide', 'genomic platform', 'high resolution imaging', 'human disease', 'immune function', 'improved', 'insight', 'killings', 'member', 'microbiota', 'mortality', 'mutant', 'new therapeutic target', 'next generation sequencing', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'pathogen', 'patient population', 'programs', 'promoter', 'quantitative imaging', 'response', 'trait', 'unpublished works']",NIAID,UNIVERSITY OF TORONTO,R01,2017,591721,-0.00420108711667114
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9258454,R00HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Individual', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'RNA library', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'experimental study', 'functional genomics', 'genetic element', 'genome analysis', 'genome editing', 'genome-wide', 'genomic predictors', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2017,244439,0.03496454362272129
"Systematic, Genome-Scale Functional Characterization of Conserved smORFs PROJECT SUMMARY Short peptides (10-100aa) are important regulators of physiology, development and metabolism, however their detection is difficult due to size and abundance. A stunning 30% of annotated human smORF genes include disease-associated variants mapped within exons, compared to 15% of human genes in general. Further, many smORFs are conserved across the entire metazoan phylogeny from invertebrates to vertebrates including man. These ultra-conserved functional smORF genes we call the Conserved smORF Catalog or CSC. These genes have been conserved across more than 500myr of evolution, and yet we know almost nothing at all about their functions. Due to a century of genetic analysis, the genome of the model organism Drosophila melanogaster has the most complete functional annotation among metazoans. Functional annotations derived from Drosophila have been instrumental in hypothesis-based drug development for more than thirty years, and more recently have made possible the biological interpretation of hundreds of SNPs detected in genome-wide association studies (GWAS). Hence, functional annotations derived in fly for conserved genes are transferable to human and are of direct clinical relevance. Remarkably, less than 10% of smORFs in Drosophila have been studied functionally, or experimentally verified as generating peptides. A combination of genome engineering, computational, molecular, and functional studies will be used to systematically and comprehensively characterize the CSC, representing the first genome-scale characterization of smORFs in any organism providing a wealth of information on the biological functions of this poorly studied class of proteins. In total, we will characterize and functionally annotate ~400 conserved smORFs using CRISPR knockout followed by phenotyping and rescue assays. We will assess the phenotypes of the mutants, measuring viability, morphology, fecundity and fertility, lifespan, metabolism (sugar and lipid levels), and a number of behavioral phenotypes. For smORFs with robust phenotypes, we will then attempt to rescue a subset of these mutants in three ways: first, by inserting the whole deleted RNA; second, with a version of the RNA with the smORF(s) removed by the addition a stop codon; and lastly, using a micro- construct containing only the smORF and the endogenous promoter. We will generate direct evidence for translation using tagged expression analysis and targeted MS/MS to scan for predicted polypeptides in the whole embryo and tissue dissection samples. In addition to validating the existence of the predicted molecules, this dataset will provide a foundational gold standard for further development of tools for the computational prediction of functional micropeptides. These studies are directed toward the understanding of basic life processes and lay the foundation for promoting better human health. PROJECT NARRATIVE As a public resource, our studies will combine genome-scale phenotyping with detailed functional characterization that will assess the effects of evolutionary conserved small open reading frames (smORFs) on animal viability, development, fecundity, metabolism, longevity and behavior. We will apply state-of-the art methods in Ribosomal profiling, CRISPR genome engineering and targeted mass spectrometry together with the development of new computational tools and analyses to generate a foundational gold standard dataset for the study of smORFs and the prediction of functional smORFs in genome annotation. Many of the genes encoding these molecules have been found to play important roles in human diseases such as neurodegeneration, developmental disorders and cancer.","Systematic, Genome-Scale Functional Characterization of Conserved smORFs",9228843,R01HG009352,"['Adipose tissue', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Arthropods', 'Autoimmune Diseases', 'Behavior', 'Behavioral', 'Biological', 'Biological Assay', 'Biological Process', 'CRISPR/Cas technology', 'Catalogs', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'Codon Nucleotides', 'Collection', 'Computer Analysis', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Dissection', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Targeting', 'Evolution', 'Exons', 'Fertility', 'Foundations', 'Frameshift Mutation', 'Gene Transfer', 'Genes', 'Genetic Transcription', 'Genome', 'Genome engineering', 'Gold', 'Health', 'Human', 'Human Genome', 'Image', 'In Situ', 'Invertebrates', 'Knock-out', 'Life', 'Lipids', 'Literature', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Messenger RNA', 'Metabolism', 'Methods', 'Molecular', 'Morphology', 'Muscle', 'National Human Genome Research Institute', 'Nerve Degeneration', 'Nervous system structure', 'Neurodegenerative Disorders', 'Neurotransmitters', 'Ontology', 'Open Reading Frames', 'Organism', 'Peptides', 'Phenotype', 'Phylogeny', 'Physiology', 'Play', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Reproducibility', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Scanning', 'System', 'Technology', 'Terminator Codon', 'Time', 'Tissues', 'Translating', 'Translations', 'Variant', 'Vertebrates', 'adipokines', 'base', 'clinically relevant', 'computerized tools', 'developmental disease', 'drug development', 'drug resource', 'embryo tissue', 'fly', 'gene function', 'genetic analysis', 'genome annotation', 'genome wide association study', 'genome-wide', 'human disease', 'in situ imaging', 'insight', 'knock-down', 'man', 'mutant', 'novel', 'overexpression', 'polypeptide', 'promoter', 'ribosome profiling', 'sugar', 'tool', 'tool development', 'translational genomics', 'virtual']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,R01,2017,1002725,0.00029604730343296703
"Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID) DESCRIPTION (provided by applicant): As a result of the accelerated pace of development of technologies for characterizing the human genome, the rate-limiting step for large scale genomic investigation in clinical populations is now phenotyping. This is particularly the case for neuropsychiatric (NP) illness, where phenotypes are complex, biomarkers are lacking, and the primary cell types of interest are difficult to access directly. It has become apparent that both rare and common genetic variation contributes to disease risk and that this risk crosses traditional diagnostic boundaries in psychiatry. Taking advantage of a large, already-established NP biobank could dramatically accelerate progress toward understanding the cross-disorder mechanism of action of disease liability genes. This study proposes novel applications of emerging technologies in informatics and cellular neurobiology to eliminate this phenotyping bottleneck. In doing so, it will accelerate investigation of clinical and cellular phenotypes for understanding single and multilocus/polygenic associations. Aim 1: Adapt and expand one of the largest NP cellular biobanks by parsing electronic health records with gold-standard assessment of cognition and other RDoC phenotypes. Aim 2: Define the genome-wide multidimensional functional genomics (MFG) landscape in NP disease into which the transcriptomic signature (RNA-seq) of each induced neuron (IN) representing a clinically characterized individual is projected. The projection provides the mapping from molecular to phenotypic characterization and a directionality towards healthful/neurotypical states used in Aim 3. Aim 3: Develop a probabilistic model of gene expression dependencies that will predict which small molecular perturbations are likely to shift the IN transcriptomic signature in a healthful direction in the MFG and to then update the model based on measured perturbations in the MFG. Aim 4: Select patient samples to study in greater detail for epigenetic (DNA methylation, histone marks and RNA editing) and transcriptional control particularly with regard to activity dependent changes that have been implicated in many NP diseases. Aim 5: Here we assess just how well the clinical phenotypes are informed by the genome-wide characterizations and assess which is more robust. PUBLIC HEALTH RELEVANCE: This study is designed to answer the question: can we use the fruits of the first phases of the human genome project to create a new and more robust scheme of classifying neuropsychiatric disease, one that is more reliable with regard to prognosis of these diseases, more insightful as to the biological aberration in each category and, therefore, more effective in treating the patient.",Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID),9320861,P50MH106933,"['Agonist', 'Alcohol or Other Drugs use', 'Anxiety Disorders', 'Biological', 'Biological Markers', 'Categories', 'Cells', 'Cellular Neurobiology', 'Clinical', 'Code', 'Cognition', 'Cognitive', 'Comorbidity', 'Complex', 'Computer Simulation', 'Consent Forms', 'DNA Methylation', 'DSM-IV', 'Dependency', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Dimensions', 'Disease', 'Electronic Health Record', 'Electrophysiology (science)', 'Emerging Technologies', 'Enrollment', 'Epigenetic Process', 'Equipment and supply inventories', 'Fibroblasts', 'Fruit', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genetic Variation', 'Genomics', 'Goals', 'Gold', 'Growth', 'Healthcare Systems', 'Histones', 'Human Genome', 'Human Genome Project', 'Individual', 'Informatics', 'Investigation', 'Laboratories', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Modeling', 'Molecular', 'Moods', 'National Human Genome Research Institute', 'National Institute of Mental Health', 'Natural Language Processing', 'Neurons', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Pharmacology', 'Phase', 'Phenotype', 'Population', 'Prediction of Response to Therapy', 'Prosencephalon', 'Psychiatry', 'Psychotic Disorders', 'Public Domains', 'RNA', 'RNA Editing', 'RNA Splicing', 'Research Domain Criteria', 'Resolution', 'Resources', 'Risk', 'Running', 'Sampling', 'Scheme', 'Small RNA', 'Standardization', 'Statistical Models', 'Stress', 'Transcript', 'Transcriptional Regulation', 'Translations', 'Untranslated RNA', 'Update', 'Visit', 'Vocabulary', 'base', 'biobank', 'cell type', 'clinical investigation', 'clinical phenotype', 'clinically relevant', 'design', 'disorder risk', 'experimental study', 'functional genomics', 'genome sequencing', 'genome-wide', 'high risk', 'insight', 'interest', 'molecular phenotype', 'neuropsychiatric disorder', 'neuropsychiatric symptom', 'neuropsychiatry', 'neuroregulation', 'novel', 'outcome forecast', 'patient population', 'phenomenological models', 'predicting response', 'prognostic', 'public health relevance', 'relating to nervous system', 'response', 'technology development', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIMH,HARVARD MEDICAL SCHOOL,P50,2017,2548493,0.00353013033336667
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,8994718,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Health', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Peptide Sequence Determination', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'flu', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'receptor binding', 'research study', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2016,370329,0.04038422301295893
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9145232,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA Resequencing', 'DNA Sequence', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Health', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'coping', 'disorder risk', 'fitness', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'stem', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2016,303092,0.018383981387173627
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,9015770,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'genomic data', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2016,71329,-0.002095624754547384
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9180486,K99HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'abstracting', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",K99,2016,83411,0.021390296776236297
"DNA Sequencing Using Single Molecule Electronics PROJECT SUMMARY / ABSTRACT  Progress in DNA sequencing has occurred through multiple stages of disruptive new technologies being introduced to the field, each of which has increased sequencing capabilities by lowering costs, improving throughput, and reducing errors. The goal of this research project is to investigate a new, all-electronic sequencing method that has the potential to become the next transformative step for DNA sequencing. This new method is based on single DNA polymerase molecules bound to nanoscale electronic transistors, a hybrid device that transduces the activity of a single polymerase molecule into an electronic signal.  The goal of this research project is to determine whether these hybrid polymerase-transistors are truly applicable to DNA sequencing and the competitive environment of advanced sequencing technologies. To answer this question, the project teams the scientists who have developed the devices with Illumina, Inc., a worldwide leader in the DNA sequencing market. The experiments proposed here build on encouraging preliminary results, first to demonstrate accurate DNA sequencing and second to evaluate whether the new technique could become a competitive challenge to other sequencing methods. The interdisciplinary team will combine state-of-the-art techniques from protein engineering, nanoscale fabrication, and machine learning to customize polymerase's activity and its interactions with the electronic transistors. If successful, nanoscale solid-state devices like transistors provide one of the best opportunities for increasing sequencing capabilities while decreasing sequencing costs, so that DNA sequencing can become a standard technique in health care and disease treatment. PROJECT NARRATIVE  Over the past two decades, DNA sequencing has transformed from a heroic, nearly impossible task to a routine component of modern laboratory research. The field of DNA sequencing has improved tremendously through a strategy of modifying and monitoring polymerases, a key enzyme at the heart of many DNA sequencing technologies. This proposal is motivated by developments in the field of single-molecule electronics, which provide an entirely new mode for listening to the activity of single polymerase molecules. This electronic method is very different from the biochemical, optical, or nanopore-based techniques currently in use, and it has inherent advantages that could provide exciting possibilities for DNA sequencing. The project will tailor single-molecule electronics for the specific purpose of DNA sequencing and determine whether this strategy could lead to a new generation of sequencing technology.",DNA Sequencing Using Single Molecule Electronics,9172062,R01HG009188,"['Affect', 'Base Pairing', 'Binding', 'Biochemical', 'Carbon', 'Charge', 'Collaborations', 'DNA', 'DNA Sequence', 'DNA-Directed DNA Polymerase', 'Data', 'Development', 'Devices', 'Discrimination', 'Disease', 'Electronics', 'Enzyme Kinetics', 'Enzymes', 'Event', 'Foundations', 'Generations', 'Goals', 'Health Care Research', 'Healthcare', 'Heart', 'Hybrids', 'Individual', 'Laboratory Research', 'Lead', 'Machine Learning', 'Marketing', 'Massive Parallel Sequencing', 'Methods', 'Modality', 'Modification', 'Molecular Models', 'Monitor', 'Motion', 'Mutation', 'Nanotechnology', 'Noise', 'Nucleotides', 'Optics', 'Performance', 'Polymerase', 'Process', 'Protein Engineering', 'Proteins', 'Publishing', 'Reading', 'Research', 'Research Project Grants', 'Resolution', 'Route', 'Scientist', 'Signal Transduction', 'Single-Stranded DNA', 'Site', 'Staging', 'Surface', 'System', 'Techniques', 'Technology', 'Temperature', 'Transistors', 'Variant', 'Work', 'base', 'collaborative environment', 'cost', 'enzyme activity', 'improved', 'molecular modeling', 'nanoelectronics', 'nanopore', 'nanoscale', 'new technology', 'novel', 'research study', 'response', 'scale up', 'single molecule', 'single walled carbon nanotube', 'solid state']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2016,584552,-0.015452775391489853
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,9268117,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost effectiveness', 'cost efficient', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2016,1378926,0.062471276735212924
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9096856,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,2201640,0.03605116433779756
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9288931,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,224176,0.03605116433779756
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,9103177,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'learning strategy', 'meetings', 'population based', 'programs', 'whole genome']",NHGRI,STANFORD UNIVERSITY,R01,2016,300000,0.028264982145971798
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9097763,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2016,52816,0.0015849050856312275
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community.         PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.            ","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9132585,U01HG009086,"['Accounting', 'Affect', 'Architecture', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Learning', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Validation', 'Variant', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'functional genomics', 'genetic variant', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'research study', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2016,854333,0.016791894022335684
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,8974432,K99HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Health', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",K99,2016,25020,0.03496454362272129
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9242250,R00HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Health', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2016,246031,0.03496454362272129
"Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID) DESCRIPTION (provided by applicant): As a result of the accelerated pace of development of technologies for characterizing the human genome, the rate-limiting step for large scale genomic investigation in clinical populations is now phenotyping. This is particularly the case for neuropsychiatric (NP) illness, where phenotypes are complex, biomarkers are lacking, and the primary cell types of interest are difficult to access directly. It has become apparent that both rare and common genetic variation contributes to disease risk and that this risk crosses traditional diagnostic boundaries in psychiatry. Taking advantage of a large, already-established NP biobank could dramatically accelerate progress toward understanding the cross-disorder mechanism of action of disease liability genes. This study proposes novel applications of emerging technologies in informatics and cellular neurobiology to eliminate this phenotyping bottleneck. In doing so, it will accelerate investigation of clinical and cellular phenotypes for understanding single and multilocus/polygenic associations. Aim 1: Adapt and expand one of the largest NP cellular biobanks by parsing electronic health records with gold-standard assessment of cognition and other RDoC phenotypes. Aim 2: Define the genome-wide multidimensional functional genomics (MFG) landscape in NP disease into which the transcriptomic signature (RNA-seq) of each induced neuron (IN) representing a clinically characterized individual is projected. The projection provides the mapping from molecular to phenotypic characterization and a directionality towards healthful/neurotypical states used in Aim 3. Aim 3: Develop a probabilistic model of gene expression dependencies that will predict which small molecular perturbations are likely to shift the IN transcriptomic signature in a healthful direction in the MFG and to then update the model based on measured perturbations in the MFG. Aim 4: Select patient samples to study in greater detail for epigenetic (DNA methylation, histone marks and RNA editing) and transcriptional control particularly with regard to activity dependent changes that have been implicated in many NP diseases. Aim 5: Here we assess just how well the clinical phenotypes are informed by the genome-wide characterizations and assess which is more robust. PUBLIC HEALTH RELEVANCE: This study is designed to answer the question: can we use the fruits of the first phases of the human genome project to create a new and more robust scheme of classifying neuropsychiatric disease, one that is more reliable with regard to prognosis of these diseases, more insightful as to the biological aberration in each category and, therefore, more effective in treating the patient.",Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID),9114170,P50MH106933,"['Agonist', 'Alcohol or Other Drugs use', 'Anxiety Disorders', 'Biological', 'Biological Markers', 'Categories', 'Cells', 'Cellular Neurobiology', 'Clinical', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Consent Forms', 'DNA Methylation', 'DSM-IV', 'Dependency', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Dimensions', 'Disease', 'Electronic Health Record', 'Emerging Technologies', 'Enrollment', 'Epigenetic Process', 'Equipment and supply inventories', 'Fibroblasts', 'Fruit', 'Gene Expression', 'Genes', 'Genetic Variation', 'Genomics', 'Goals', 'Gold', 'Growth', 'Health', 'Healthcare Systems', 'Histones', 'Human Genome', 'Human Genome Project', 'Individual', 'Informatics', 'Investigation', 'Laboratories', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Modeling', 'Molecular', 'National Institute of Mental Health', 'Natural Language Processing', 'Neurons', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Prediction of Response to Therapy', 'Prosencephalon', 'Psychiatry', 'Psychotic Mood Disorders', 'Public Domains', 'RNA', 'RNA Editing', 'RNA Splicing', 'Research Domain Criteria', 'Resolution', 'Resources', 'Risk', 'Running', 'Sampling', 'Scheme', 'Small RNA', 'Statistical Models', 'Stress', 'Transcript', 'Transcriptional Regulation', 'Translations', 'Untranslated RNA', 'Update', 'Visit', 'Vocabulary', 'base', 'biobank', 'cell type', 'clinical investigation', 'clinical phenotype', 'clinically relevant', 'design', 'disorder risk', 'functional genomics', 'genome sequencing', 'genome-wide', 'high risk', 'induced pluripotent stem cell', 'insight', 'interest', 'neuropsychiatric disorder', 'neuropsychiatric symptom', 'neuropsychiatry', 'neuroregulation', 'novel', 'outcome forecast', 'patient population', 'phenomenological models', 'predicting response', 'prognostic', 'relating to nervous system', 'research study', 'response', 'technology development', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIMH,HARVARD MEDICAL SCHOOL,P50,2016,2560433,0.00353013033336667
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control.         PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.                ",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,8859887,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Peptide Sequence Determination', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'flu', 'genome sequencing', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'research study', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2015,381704,0.04038422301295893
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale.         PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.                ",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,8887722,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA Resequencing', 'DNA Sequence', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'coping', 'disorder risk', 'fitness', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'stem', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2015,303504,0.018383981387173627
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8825472,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis', 'transcriptome sequencing']",NCI,BROWN UNIVERSITY,R01,2015,71329,-0.002095624754547384
"Informatics Tools for High-Throughput Sequences Data Analysis DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK. The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.",Informatics Tools for High-Throughput Sequences Data Analysis,8788050,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2015,967608,0.0444247507929872
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8889700,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2015,2005492,0.062471276735212924
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9147033,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Health', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,489338,0.03605116433779756
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8935854,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,2116462,0.03605116433779756
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,8930750,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs']",NHGRI,STANFORD UNIVERSITY,R01,2015,292499,0.028264982145971798
"Human-Specific Gain and Loss of Function DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university. PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.",Human-Specific Gain and Loss of Function,8796200,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomic Segment', 'Genomic approach', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2015,54194,0.029205604298795443
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,8898177,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2015,52816,0.0015849050856312275
"Genome engineering tools for functional screening of non-coding elements     DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root).         PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.                ",Genome engineering tools for functional screening of non-coding elements,8804084,K99HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Laboratories', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Relative (related person)', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",K99,2015,99937,0.03496454362272129
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8840551,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'genetic variant', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2015,219004,-0.017186069627295523
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,0.06263454597252392
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,0.06263454597252392
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,0.06263454597252392
"Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID)     DESCRIPTION (provided by applicant): As a result of the accelerated pace of development of technologies for characterizing the human genome, the rate-limiting step for large scale genomic investigation in clinical populations is now phenotyping. This is particularly the case for neuropsychiatric (NP) illness, where phenotypes are complex, biomarkers are lacking, and the primary cell types of interest are difficult to access directly. It has become apparent that both rare and common genetic variation contributes to disease risk and that this risk crosses traditional diagnostic boundaries in psychiatry. Taking advantage of a large, already-established NP biobank could dramatically accelerate progress toward understanding the cross-disorder mechanism of action of disease liability genes. This study proposes novel applications of emerging technologies in informatics and cellular neurobiology to eliminate this phenotyping bottleneck. In doing so, it will accelerate investigation of clinical and cellular phenotypes for understanding single and multilocus/polygenic associations. Aim 1: Adapt and expand one of the largest NP cellular biobanks by parsing electronic health records with gold-standard assessment of cognition and other RDoC phenotypes. Aim 2: Define the genome-wide multidimensional functional genomics (MFG) landscape in NP disease into which the transcriptomic signature (RNA-seq) of each induced neuron (IN) representing a clinically characterized individual is projected. The projection provides the mapping from molecular to phenotypic characterization and a directionality towards healthful/neurotypical states used in Aim 3. Aim 3: Develop a probabilistic model of gene expression dependencies that will predict which small molecular perturbations are likely to shift the IN transcriptomic signature in a healthful direction in the MFG and to then update the model based on measured perturbations in the MFG. Aim 4: Select patient samples to study in greater detail for epigenetic (DNA methylation, histone marks and RNA editing) and transcriptional control particularly with regard to activity dependent changes that have been implicated in many NP diseases. Aim 5: Here we assess just how well the clinical phenotypes are informed by the genome-wide characterizations and assess which is more robust.          PUBLIC HEALTH RELEVANCE: This study is designed to answer the question: can we use the fruits of the first phases of the human genome project to create a new and more robust scheme of classifying neuropsychiatric disease, one that is more reliable with regard to prognosis of these diseases, more insightful as to the biological aberration in each category and, therefore, more effective in treating the patient.                ",Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID),9117098,P50MH106933,"['Agonist', 'Alcohol or Other Drugs use', 'Anxiety Disorders', 'Biological', 'Biological Markers', 'Categories', 'Cells', 'Cellular Neurobiology', 'Clinical', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Consent Forms', 'DNA Methylation', 'DSM-IV', 'Dependency', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electronic Health Record', 'Emerging Technologies', 'Enrollment', 'Epigenetic Process', 'Equipment and supply inventories', 'Fibroblasts', 'Fruit', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genetic Transcription', 'Genetic Variation', 'Genomics', 'Goals', 'Gold', 'Growth', 'Healthcare Systems', 'Histones', 'Human Genome', 'Human Genome Project', 'Individual', 'Informatics', 'Investigation', 'Laboratories', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Modeling', 'Molecular', 'Moods', 'National Institute of Mental Health', 'Natural Language Processing', 'Neurons', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Principal Investigator', 'Prosencephalon', 'Psychiatry', 'Public Domains', 'RNA', 'RNA Editing', 'RNA Splicing', 'Research Domain Criteria', 'Resources', 'Risk', 'Running', 'Sampling', 'Scheme', 'Small RNA', 'Statistical Models', 'Stress', 'Symptoms', 'Testing', 'Transcript', 'Transcriptional Regulation', 'Untranslated RNA', 'Update', 'Visit', 'Vocabulary', 'base', 'biobank', 'cell type', 'clinical investigation', 'clinical phenotype', 'clinically relevant', 'design', 'disorder risk', 'functional genomics', 'genome sequencing', 'genome-wide', 'high risk', 'induced pluripotent stem cell', 'insight', 'interest', 'neuropsychiatry', 'novel', 'outcome forecast', 'phenomenological models', 'prognostic', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'technology development', 'transcriptome sequencing', 'transcriptomics']",NIMH,HARVARD MEDICAL SCHOOL,P50,2015,194377,0.00353013033336667
"Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID)     DESCRIPTION (provided by applicant): As a result of the accelerated pace of development of technologies for characterizing the human genome, the rate-limiting step for large scale genomic investigation in clinical populations is now phenotyping. This is particularly the case for neuropsychiatric (NP) illness, where phenotypes are complex, biomarkers are lacking, and the primary cell types of interest are difficult to access directly. It has become apparent that both rare and common genetic variation contributes to disease risk and that this risk crosses traditional diagnostic boundaries in psychiatry. Taking advantage of a large, already-established NP biobank could dramatically accelerate progress toward understanding the cross-disorder mechanism of action of disease liability genes. This study proposes novel applications of emerging technologies in informatics and cellular neurobiology to eliminate this phenotyping bottleneck. In doing so, it will accelerate investigation of clinical and cellular phenotypes for understanding single and multilocus/polygenic associations. Aim 1: Adapt and expand one of the largest NP cellular biobanks by parsing electronic health records with gold-standard assessment of cognition and other RDoC phenotypes. Aim 2: Define the genome-wide multidimensional functional genomics (MFG) landscape in NP disease into which the transcriptomic signature (RNA-seq) of each induced neuron (IN) representing a clinically characterized individual is projected. The projection provides the mapping from molecular to phenotypic characterization and a directionality towards healthful/neurotypical states used in Aim 3. Aim 3: Develop a probabilistic model of gene expression dependencies that will predict which small molecular perturbations are likely to shift the IN transcriptomic signature in a healthful direction in the MFG and to then update the model based on measured perturbations in the MFG. Aim 4: Select patient samples to study in greater detail for epigenetic (DNA methylation, histone marks and RNA editing) and transcriptional control particularly with regard to activity dependent changes that have been implicated in many NP diseases. Aim 5: Here we assess just how well the clinical phenotypes are informed by the genome-wide characterizations and assess which is more robust.          PUBLIC HEALTH RELEVANCE: This study is designed to answer the question: can we use the fruits of the first phases of the human genome project to create a new and more robust scheme of classifying neuropsychiatric disease, one that is more reliable with regard to prognosis of these diseases, more insightful as to the biological aberration in each category and, therefore, more effective in treating the patient.                ",Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID),8929310,P50MH106933,"['Agonist', 'Alcohol or Other Drugs use', 'Anxiety Disorders', 'Biological', 'Biological Markers', 'Categories', 'Cells', 'Cellular Neurobiology', 'Clinical', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Consent Forms', 'DNA Methylation', 'DSM-IV', 'Dependency', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electronic Health Record', 'Emerging Technologies', 'Enrollment', 'Epigenetic Process', 'Equipment and supply inventories', 'Fibroblasts', 'Fruit', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genetic Transcription', 'Genetic Variation', 'Genomics', 'Goals', 'Gold', 'Growth', 'Healthcare Systems', 'Histones', 'Human Genome', 'Human Genome Project', 'Individual', 'Informatics', 'Investigation', 'Laboratories', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Modeling', 'Molecular', 'Moods', 'National Institute of Mental Health', 'Natural Language Processing', 'Neurons', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Principal Investigator', 'Prosencephalon', 'Psychiatry', 'Public Domains', 'RNA', 'RNA Editing', 'RNA Splicing', 'Research Domain Criteria', 'Resources', 'Risk', 'Running', 'Sampling', 'Scheme', 'Small RNA', 'Statistical Models', 'Stress', 'Symptoms', 'Testing', 'Transcript', 'Transcriptional Regulation', 'Untranslated RNA', 'Update', 'Visit', 'Vocabulary', 'base', 'biobank', 'cell type', 'clinical investigation', 'clinical phenotype', 'clinically relevant', 'design', 'disorder risk', 'functional genomics', 'genome sequencing', 'genome-wide', 'high risk', 'induced pluripotent stem cell', 'insight', 'interest', 'neuropsychiatry', 'novel', 'outcome forecast', 'phenomenological models', 'prognostic', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'technology development', 'transcriptome sequencing', 'transcriptomics']",NIMH,HARVARD MEDICAL SCHOOL,P50,2015,2563635,0.00353013033336667
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8683213,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2014,221252,0.039411521676402966
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8685211,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2014,69189,0.007525108446497065
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8601147,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2014,989800,0.0444247507929872
"Gene Prediction by Markov Models and Complementary Methods DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad. NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8909702,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'pyrosequencing', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2014,100000,0.028321720635079945
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8725717,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2014,2015775,0.062471276735212924
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8774407,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2014,1623265,0.03605116433779756
"Statistical and computational analysis in whole genome sequencing studies.     DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges.         PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.                ",Statistical and computational analysis in whole genome sequencing studies.,8750827,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs', 'public health relevance']",NHGRI,STANFORD UNIVERSITY,R01,2014,300000,0.028264982145971798
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8635216,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2014,51530,0.029205604298795443
"CSHL Computational and Comparative Genomics Course     DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions.         PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.                 ",CSHL Computational and Comparative Genomics Course,8737540,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'public health relevance', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2014,52816,0.0015849050856312275
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8642168,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2014,207764,-0.017186069627295523
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,0.06263454597252392
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.          Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8699810,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic DNA', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,641093,0.07131705804439663
"Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID)     DESCRIPTION (provided by applicant): As a result of the accelerated pace of development of technologies for characterizing the human genome, the rate-limiting step for large scale genomic investigation in clinical populations is now phenotyping. This is particularly the case for neuropsychiatric (NP) illness, where phenotypes are complex, biomarkers are lacking, and the primary cell types of interest are difficult to access directly. It has become apparent that both rare and common genetic variation contributes to disease risk and that this risk crosses traditional diagnostic boundaries in psychiatry. Taking advantage of a large, already-established NP biobank could dramatically accelerate progress toward understanding the cross-disorder mechanism of action of disease liability genes. This study proposes novel applications of emerging technologies in informatics and cellular neurobiology to eliminate this phenotyping bottleneck. In doing so, it will accelerate investigation of clinical and cellular phenotypes for understanding single and multilocus/polygenic associations. Aim 1: Adapt and expand one of the largest NP cellular biobanks by parsing electronic health records with gold-standard assessment of cognition and other RDoC phenotypes. Aim 2: Define the genome-wide multidimensional functional genomics (MFG) landscape in NP disease into which the transcriptomic signature (RNA-seq) of each induced neuron (IN) representing a clinically characterized individual is projected. The projection provides the mapping from molecular to phenotypic characterization and a directionality towards healthful/neurotypical states used in Aim 3. Aim 3: Develop a probabilistic model of gene expression dependencies that will predict which small molecular perturbations are likely to shift the IN transcriptomic signature in a healthful direction in the MFG and to then update the model based on measured perturbations in the MFG. Aim 4: Select patient samples to study in greater detail for epigenetic (DNA methylation, histone marks and RNA editing) and transcriptional control particularly with regard to activity dependent changes that have been implicated in many NP diseases. Aim 5: Here we assess just how well the clinical phenotypes are informed by the genome-wide characterizations and assess which is more robust.          PUBLIC HEALTH RELEVANCE: This study is designed to answer the question: can we use the fruits of the first phases of the human genome project to create a new and more robust scheme of classifying neuropsychiatric disease, one that is more reliable with regard to prognosis of these diseases, more insightful as to the biological aberration in each category and, therefore, more effective in treating the patient.                ",Neuropsychiatric Genome-Scale and RDOC Individualized Domains (N-GRID),8698507,P50MH106933,"['Agonist', 'Alcohol or Other Drugs use', 'Anxiety Disorders', 'Biological', 'Biological Markers', 'Categories', 'Cells', 'Cellular Neurobiology', 'Clinical', 'Clinical Trials', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Consent Forms', 'DNA Methylation', 'DSM-IV', 'Dependency', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Electronic Health Record', 'Emerging Technologies', 'Enrollment', 'Epigenetic Process', 'Equipment and supply inventories', 'Fibroblasts', 'Fruit', 'Functional RNA', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Gold', 'Growth', 'Healthcare Systems', 'Histones', 'Human Genome', 'Human Genome Project', 'Individual', 'Informatics', 'Investigation', 'Laboratories', 'Maps', 'Measurement', 'Measures', 'Methodology', 'Modeling', 'Molecular', 'Moods', 'National Institute of Mental Health', 'Natural Language Processing', 'Neurons', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Population', 'Principal Investigator', 'Prosencephalon', 'Psychiatry', 'Public Domains', 'RNA', 'RNA Editing', 'RNA Splicing', 'Resources', 'Risk', 'Running', 'Sampling', 'Scheme', 'Small RNA', 'Statistical Models', 'Stress', 'Symptoms', 'Testing', 'Transcript', 'Transcriptional Regulation', 'Update', 'Visit', 'Vocabulary', 'base', 'biobank', 'cell type', 'clinical phenotype', 'clinically relevant', 'design', 'disorder risk', 'functional genomics', 'genome sequencing', 'genome-wide', 'high risk', 'induced pluripotent stem cell', 'insight', 'interest', 'neuropsychiatry', 'novel', 'outcome forecast', 'prognostic', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'technology development', 'transcriptome sequencing', 'transcriptomics']",NIMH,HARVARD MEDICAL SCHOOL,P50,2014,2625284,0.00353013033336667
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8518436,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2013,220626,0.039411521676402966
"Analytical Approaches to Massive Data Computation with Applications to Genomics     DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.              n/a",Analytical Approaches to Massive Data Computation with Applications to Genomics,8599823,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics']",NCI,BROWN UNIVERSITY,R01,2013,71329,0.007617205187262019
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8416349,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2013,964551,0.0444247507929872
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8722983,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,115680,0.062471276735212924
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8548395,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,1871839,0.062471276735212924
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8457179,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2013,47114,0.029205604298795443
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8599826,R01CA180777,"['Bees', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2013,214832,-0.017186069627295523
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,0.06263454597252392
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.          Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8537965,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2013,624741,0.07131705804439663
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.        PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.              DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8350385,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Base Sequence', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'RNA', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'tool', 'transcription factor', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2012,220000,0.03574128844574812
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.      PUBLIC HEALTH RELEVANCE: The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.              The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8237596,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Delivery Systems', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2012,1010000,0.04171430313000159
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8494858,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2012,371054,0.06453082764334268
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.           Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8320160,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'metagenome', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2012,204974,0.011982394121923515
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8521766,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2012,75000,0.028321720635079945
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8266525,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2012,577121,0.028321720635079945
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8402447,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Set', 'Development', 'Disease', 'Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2012,2460045,0.062471276735212924
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.         The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8331495,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2012,103535,0.006946726404062074
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,8228154,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'ChIP-seq', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'amplisome', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,323572,0.004978285324467374
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8310258,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2012,367520,0.014576785659032004
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8537085,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2012,139853,0.014576785659032004
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,0.06263454597252392
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.        PUBLIC HEALTH RELEVANCE: Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.              Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8373752,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2012,654177,0.06858464356240544
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8107695,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2011,108418,0.06453082764334268
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,8134360,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative genomics', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait', 'treatment strategy']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2011,342569,0.018489423580518333
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8053866,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2011,577264,0.028321720635079945
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.      PUBLIC HEALTH RELEVANCE:    Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.                 Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8192895,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2011,180669,0.015947947084346485
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,8035949,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'amplisome', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,321670,0.004978285324467374
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.       PUBLIC HEALTH RELEVANCE: The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.              The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8164533,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2011,102709,0.01732929181196483
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8150462,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2011,407746,0.014576785659032004
"A Comprehensive catalog of human DNasel hypersensitive sites The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNasel hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNasel hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5). n/a",A Comprehensive catalog of human DNasel hypersensitive sites,8321717,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Insulator Elements', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Molecular', 'Noise', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Predictive Value', 'Preparation', 'Production', 'Public Domains', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'cost effective', 'density', 'design', 'digital', 'experience', 'functional genomics', 'genome-wide', 'high standard', 'high throughput screening', 'histone modification', 'human tissue', 'in vivo', 'insight', 'meetings', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2011,342218,0.007261568422688994
"A Comprehensive catalog of human DNasel hypersensitive sites The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNasel hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNasel hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5). n/a",A Comprehensive catalog of human DNasel hypersensitive sites,8320051,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Insulator Elements', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Molecular', 'Noise', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Predictive Value', 'Preparation', 'Production', 'Public Domains', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'cost effective', 'density', 'design', 'digital', 'experience', 'functional genomics', 'genome-wide', 'high standard', 'high throughput screening', 'histone modification', 'human tissue', 'in vivo', 'insight', 'meetings', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2011,2615448,0.007261568422688994
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,8071964,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,BOISE STATE UNIVERSITY,R01,2011,431250,0.0025244897595220607
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,8321269,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,BOISE STATE UNIVERSITY,R01,2011,797678,0.0025244897595220607
"Statistical Methods for Correlated and High-Dimensional Biomedical Data We propose in the renewal of this MERIT award application to continue developing advanced statistical and computational methods for analysis of correlated and high-dimensional data, which arise frequently in health science research, especially in cancer research. Correlated data are often observed in observational studies and clinical trials, such as longitudinal studies and familial studies. High-dimensional data have emerged rapidly in recent years due to the advance of high-throughput 'omics technologies, e.g, in Genome- Wide Association Studies (GWAS), and genome-wide epigenetic (DNA methylation) studies. Massive next generation sequencing data are soon available. There is an urgent need to develop advanced stati stical and computational methods for analyzing such high throughput 'omics data in observational studies and clinical trials. We propose to develop statistical and computational methods for analysis of (1) genome-wide association studies; (2) sequencing data for studying rare variant effects; (3) genome-wide DNA methylation studies; (4) gene-gene and gene-environment interactions. We will develop methods for both case-control studies and cohort studies, such as longitudinal studies and survival studies. We will study the theoretical properties of the proposed methods and evaluate the finite s ample performance using simulation studies. We will develop efficient numerical algorithms and user-friendly statistical software, and disseminate these tools to health sciences researchers. In collaboration with biomedical investigators, we will apply the proposed models methods to data from several genome-wide epidemiological studies in cancer and other chronic diseases. RELEVANCE (See instructions):  Development of new statistical methods for analysis of correlated and high-dimensional data will provide  powerful analytic tools to advance 'omics research in observational studies and clinical trials and to help  understand the roles of genes, gene products, and the environment in causing human diseases.",Statistical Methods for Correlated and High-Dimensional Biomedical Data,8117858,R37CA076404,"['Advanced Development', 'Aging', 'Algorithms', 'Award', 'Biomedical Research', 'Case-Control Studies', 'Childhood', 'Chronic Disease', 'Clinical Treatment', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Computer software', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Development', 'Dimensions', 'Disease', 'Environment', 'Epidemiologic Studies', 'Epigenetic Process', 'Etiology', 'Explosion', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health Sciences', 'Human Genome', 'Instruction', 'Intervention', 'Investigation', 'Knowledge', 'Longitudinal Studies', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Measures', 'Messenger RNA', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Prevention strategy', 'Property', 'Proteins', 'Public Health Schools', 'Research', 'Research Personnel', 'Role', 'Scientist', 'Single Nucleotide Polymorphism', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Technology', 'Testing', 'Theoretical Studies', 'Variant', 'anticancer research', 'cancer risk', 'case control', 'computer science', 'computerized data processing', 'epigenomics', 'gene environment interaction', 'gene interaction', 'genome wide association study', 'genome-wide', 'health science research', 'high throughput analysis', 'human disease', 'malignant breast neoplasm', 'new technology', 'next generation', 'novel', 'programs', 'simulation', 'tool', 'user-friendly']",NCI,HARVARD SCHOOL OF PUBLIC HEALTH,R37,2011,308344,-0.03841690299104806
"A Data Analysis Center for integration of fly and worm modENCODE datasets    DESCRIPTION (provided by applicant):  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human. We foresee four central roles for the DAC, and have organized our aims around them. Aim 1: We will provide common computational guidelines for data processing in fly and worm, a common computational infrastructure and pipeline for common analysis and statistical tasks. Aim 2: We will facilitate and carry out element-specific integrative analyses to identify diverse classes of functional elements based on combinations of relevant datasets coming from multiple groups. This includes (a) enhancers, promoters, insulators, and other regions of regulatory importance, (b) protein-coding and non-coding genes, (c) regulatory networks of transcription factor and microRNA targeting, and (d) sequence features predictive of diverse classes of functional elements. Aim 3: We will carry out exploratory data analyses across different data types to discover potentially novel correlations and insights relating diverse classes of elements. In particular we will apply dimensionality reduction techniques to coordinate-based genome-wide genomic and epigenomic datasets, we will apply clustering and bi-clustering methods to identify functionally related sets of genes and modules, and we will analyze structural and dynamic properties of discovered networks. Aim 4: We will carry out comparative analyses across the two model organisms, and also with yeast and human. We will provide an ortholog resource between the species, compare regulatory relationships and dynamics for orthologous cell lines and developmental points, and carry over biological knowledge across model organisms and human. To achieve these four aims, we will work closely with members of the consortium, the modENCODE Analysis Working Group (AWG), consisting of all Principal Investigators and analysis groups, and the Data Coordination Center (DCC), responsible for all data sharing within the consortium and with the larger worm and fly communities.      PUBLIC HEALTH RELEVANCE:  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.           Narrative The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.",A Data Analysis Center for integration of fly and worm modENCODE datasets,8327885,RC2HG005639,"['Animal Model', 'Animals', 'Beryllium', 'Binding', 'Biological', 'Biology', 'Biomedical Research', 'Boundary Elements', 'Caenorhabditis elegans', 'Cataloging', 'Catalogs', 'Cell Line', 'Chromatin', 'Code', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Development', 'Disease', 'Elements', 'Embryonic Development', 'Enhancers', 'Functional RNA', 'Galaxy', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Guidelines', 'Health', 'Histones', 'Human', 'Human Genome', 'Hypersensitivity', 'Indium', 'Knowledge', 'Logic', 'Machine Learning', 'Measures', 'Messenger RNA', 'Methodology', 'Methods', 'MicroRNAs', 'Nucleic Acid Regulatory Sequences', 'Orthologous Gene', 'Polymerase', 'Post-Transcriptional Regulation', 'Principal Component Analysis', 'Principal Investigator', 'Property', 'Proteins', 'Reading', 'Recurrence', 'Regulatory Element', 'Replication Initiation', 'Research Infrastructure', 'Resolution', 'Resources', 'Role', 'Site', 'System', 'Techniques', 'Testing', 'Tissues', 'Transcriptional Regulation', 'Ursidae Family', 'Variant', 'Work', 'Yeasts', 'base', 'chromatin immunoprecipitation', 'combinatorial', 'comparative', 'computer infrastructure', 'computerized data processing', 'computerized tools', 'cost', 'data exchange', 'data integration', 'data modeling', 'data sharing', 'epigenomics', 'file format', 'fly', 'gene function', 'genome-wide', 'insight', 'markov model', 'member', 'next generation', 'novel', 'promoter', 'public health relevance', 'research study', 'sequence learning', 'task analysis', 'transcription factor', 'working group']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RC2,2011,1296550,-0.014135023959142644
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8061704,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'information model', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2011,2923298,0.031482221402185255
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,0.059803817898030576
"Human Variation Detection and Visualization on the DNAnexus Web 2.0 platform    DESCRIPTION (provided by applicant): DNAnexus proposes to develop a complete solution for the identification and stratification of personal genetic variation from ultra-high-throughput sequencing projects. The solution will be implemented as a Web 2.0 service and online browsing tool that will integrate public data sources such as the 1000 genomes project, comparative information, and the ENCODE II project data. Users will be able to browse and stratify the identified variation in the context of these genomic annotations, and according to the likely functional impact. In Phase I of our project, we will develop a basic browser for displaying sequence reads that are mapped to a reference genome with our state-of-the-art read mapper. The browser will support viewing mate paired reads as well as display of the variation between these reads and the reference genome. It will facilitate the algorithmic development that we will perform during Phase II, and it will be the foundation for the more sophisticated variation browser also proposed in Phase II. In Phase II, we will develop algorithms for detecting genomic variation, and a state-of-the-art browser for viewing variation in the context of existing genome annotations, functional genomic and comparative genomic data. Our algorithms for detecting variation will support all major types of genomic variation, including SNPs, microindels, larger insertions and deletions, duplications, copy number variations, inversions, and translocations. Our algorithms will be based on state-of-the-art statistical and machine learning methodology for human genome resequencing. The DNAnexus browser will have two components: a list browser that displays variation as a list filtered and stratified by criteria that a user chooses, and a powerful GUI whose navigation capabilities are inspired by modern online tools such as Google Maps.      PUBLIC HEALTH RELEVANCE: DNAnexus proposes to develop a complete solution for identifying and analyzing personal genetic variation for individuals whose genomes are sequenced using new sequencing technologies. Users will be able to browse an individual genome in the context of public data sources such as the 1000 genomes project, comparative information to other mammalian species, and functional data from the ENCODE II project.           Project Narrative DNAnexus proposes to develop a complete solution for identifying and analyzing personal genetic variation for individuals whose genomes are sequenced using new sequencing technologies. Users will be able to browse an individual genome in the context of public data sources such as the 1000 genomes project, comparative information to other mammalian species, and functional data from the ENCODE II project.",Human Variation Detection and Visualization on the DNAnexus Web 2.0 platform,7909096,R43HG005794,"['Algorithms', 'Architecture', 'Arts', 'Code', 'Copy Number Polymorphism', 'DNA Resequencing', 'Data', 'Data Display', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Environment', 'Foundations', 'Genes', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Human Genetics', 'Human Genome', 'Imagery', 'Individual', 'Internet', 'Machine Learning', 'Maps', 'Methodology', 'Partner in relationship', 'Phase', 'Point Mutation', 'Reading', 'Services', 'Solutions', 'Statistical Methods', 'Stratification', 'Technology', 'Variant', 'base', 'comparative', 'comparative genomics', 'cost', 'flexibility', 'functional genomics', 'genome sequencing', 'graphical user interface', 'insertion/deletion mutation', 'public health relevance', 'tool']",NHGRI,"DNANEXUS, INC.",R43,2010,74477,0.02434949000188031
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7913074,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,1248287,0.06453082764334268
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8121894,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,300000,0.06453082764334268
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8144973,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,113520,0.06453082764334268
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8147585,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,151816,0.06453082764334268
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7902231,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative genomics', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2010,346884,0.018489423580518333
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,7809669,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Arts', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2010,573248,0.028321720635079945
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,7795846,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Solid', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,326175,0.004978285324467374
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8020799,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2010,408559,0.014576785659032004
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,7802061,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'public health relevance', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2010,435435,0.0025244897595220607
"A Data Analysis Center for integration of fly and worm modENCODE datasets    DESCRIPTION (provided by applicant):  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human. We foresee four central roles for the DAC, and have organized our aims around them. Aim 1: We will provide common computational guidelines for data processing in fly and worm, a common computational infrastructure and pipeline for common analysis and statistical tasks. Aim 2: We will facilitate and carry out element-specific integrative analyses to identify diverse classes of functional elements based on combinations of relevant datasets coming from multiple groups. This includes (a) enhancers, promoters, insulators, and other regions of regulatory importance, (b) protein-coding and non-coding genes, (c) regulatory networks of transcription factor and microRNA targeting, and (d) sequence features predictive of diverse classes of functional elements. Aim 3: We will carry out exploratory data analyses across different data types to discover potentially novel correlations and insights relating diverse classes of elements. In particular we will apply dimensionality reduction techniques to coordinate-based genome-wide genomic and epigenomic datasets, we will apply clustering and bi-clustering methods to identify functionally related sets of genes and modules, and we will analyze structural and dynamic properties of discovered networks. Aim 4: We will carry out comparative analyses across the two model organisms, and also with yeast and human. We will provide an ortholog resource between the species, compare regulatory relationships and dynamics for orthologous cell lines and developmental points, and carry over biological knowledge across model organisms and human. To achieve these four aims, we will work closely with members of the consortium, the modENCODE Analysis Working Group (AWG), consisting of all Principal Investigators and analysis groups, and the Data Coordination Center (DCC), responsible for all data sharing within the consortium and with the larger worm and fly communities.      PUBLIC HEALTH RELEVANCE:  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.           Narrative The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.",A Data Analysis Center for integration of fly and worm modENCODE datasets,7943875,RC2HG005639,"['Animal Model', 'Animals', 'Binding', 'Biological', 'Biology', 'Biomedical Research', 'Boundary Elements', 'Caenorhabditis elegans', 'Cataloging', 'Catalogs', 'Cell Line', 'Chromatin', 'Code', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Development', 'Disease', 'Elements', 'Embryonic Development', 'Enhancers', 'Functional RNA', 'Galaxy', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Guidelines', 'Health', 'Histones', 'Human', 'Human Genome', 'Hypersensitivity', 'Indium', 'Knowledge', 'Logic', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Nucleic Acid Regulatory Sequences', 'Orthologous Gene', 'Polymerase', 'Post-Transcriptional Regulation', 'Principal Component Analysis', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Reading', 'Recurrence', 'Regulation', 'Regulatory Element', 'Replication Initiation', 'Research Infrastructure', 'Resolution', 'Resources', 'Role', 'Site', 'System', 'Techniques', 'Testing', 'Tissues', 'Transcriptional Regulation', 'Ursidae Family', 'Variant', 'Work', 'Yeasts', 'base', 'chromatin immunoprecipitation', 'combinatorial', 'comparative', 'computer infrastructure', 'computerized data processing', 'computerized tools', 'cost', 'data exchange', 'data integration', 'data modeling', 'data sharing', 'epigenomics', 'file format', 'fly', 'gene function', 'genome-wide', 'insight', 'markov model', 'member', 'next generation', 'novel', 'promoter', 'public health relevance', 'research study', 'sequence learning', 'task analysis', 'transcription factor', 'working group']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RC2,2010,1316360,-0.014135023959142644
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7780085,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,3513343,0.031482221402185255
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8138946,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,745063,0.031482221402185255
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7622614,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2009,1224323,0.06453082764334268
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7681225,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2009,351164,0.018489423580518333
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,7656528,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Arts', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2009,560000,0.028321720635079945
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,7635337,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Solid', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2009,330660,0.004978285324467374
"Developing Proteogenomic Mapping for Human Genome Annotation    DESCRIPTION (provided by applicant): Genome sequencing efforts are producing ever greater quantities of raw DNA sequence, but the annotation process for locating and determining the function of genetic elements has not kept up. While many aspects of annotation are difficult, it is particularly challenging to determine which parts of a genome sequence encode proteins, and therefore how the processes leading to protein translation are regulated. Not only are technologies for examining proteins more limited than those for studying RNA transcription, in an extensive study of transcription by the Encyclopedia of DNA elements consortium, a picture of great complexity emerged. The project uncovered many novel exons, alternative splice forms, and novel regulatory elements. These results indicate that nearly 9/10ths of human genes undergo alternative splicing, and the average gene produces approximately 6 splice variants. Rather than solidify knowledge regarding the location and function of genes, these results question whether we accurately know what constitutes a gene, and how the products encoded by genes determine the function of cells. The results particularly obfuscate determination of which transcripts are selected for translation to protein, further complicating annotation efforts. To address that gap, our project will determine which transcripts encode proteins, and how these are affected in several tissue types and disease conditions. We will use large tandem mass spectrometry-based proteomic data sets, mapping the analyzed protein data directly to several available human genome sequences, along with sets of predicted transcripts produced by the N-SCAN and CONTRAST gene finders, to reveal which parts of transcripts are translated into proteins, and in which types of cells this translation occurs. To accomplish this, our project has three specific aims: 1) to develop high-accuracy methods and software for mapping proteomic data from mass spec analyzed proteins directly to the genome locus encoding them; 2) to develop an analysis pipeline software system using a novel rule-based information management approach; and 3) to apply these developments for the high-throughput analysis of large proteomic data sets, identifying the transcripts that encode proteins in distinct tissue types and disease conditions, and placing the results in a publicly accessible track in the UCSC genome browser. We believe this project will yield significant knowledge about the location and timing of protein translation in cells, which will potentiate further investigation of how misregulation of the path from transcription to translation leads to human disease conditions.   PUBLIC HEALTH RELEVANCE:  Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.              NARRATIVE Sequencing of the human genome is complete, but figuring out where genes are located, how they function, and how they cause or prevent human diseases like cancer has only just begun. Genes act as blueprints for RNA and proteins, the workhorses of the cell. We are developing technologies to address the key challenges of determining which genes specify the building of which proteins and how this process is orchestrated to ultimately unravel how disease processes occur.",Developing Proteogenomic Mapping for Human Genome Annotation,7583730,R01HG003700,"['Address', 'Affect', 'Algorithms', 'Alternative Splicing', 'Biochemical', 'Cell physiology', 'Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Custom', 'DNA', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Elements', 'Exons', 'Foundations', 'Funding', 'Gene Targeting', 'Genes', 'Genetic Transcription', 'Genome', 'Goals', 'Grant', 'Histocompatibility Testing', 'Human', 'Human Genome', 'Imagery', 'Information Management', 'Investigation', 'Isotope Labeling', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mass Spectrum Analysis', 'Measures', 'Methods', 'Mining', 'Modeling', 'Nature', 'Paint', 'Peptides', 'Play', 'Procedures', 'Process', 'Protein Analysis', 'Proteins', 'Proteomics', 'Quality Control', 'RNA', 'RNA Splicing', 'Regulation', 'Regulatory Element', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Structure', 'System', 'Technology', 'Time', 'Tissues', 'Transcript', 'Translating', 'Translations', 'Variant', 'base', 'cell type', 'design', 'experience', 'flexibility', 'gene function', 'genetic element', 'genome sequencing', 'high throughput analysis', 'human disease', 'improved', 'new technology', 'novel', 'prevent', 'public health relevance', 'software systems', 'tandem mass spectrometry', 'web interface']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2009,450000,0.0025244897595220607
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7788875,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Ontology', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'empowered', 'genome wide association study', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'histone modification', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'public health relevance', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2009,142123,-0.0015458336682288062
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7595813,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'biological systems', 'comparative', 'computer based statistical methods', 'data integration', 'design', 'flexibility', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface', 'web site']",NIGMS,PRINCETON UNIVERSITY,R01,2009,243004,0.010611804448795548
"A Data Analysis Center for integration of fly and worm modENCODE datasets    DESCRIPTION (provided by applicant):  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human. We foresee four central roles for the DAC, and have organized our aims around them. Aim 1: We will provide common computational guidelines for data processing in fly and worm, a common computational infrastructure and pipeline for common analysis and statistical tasks. Aim 2: We will facilitate and carry out element-specific integrative analyses to identify diverse classes of functional elements based on combinations of relevant datasets coming from multiple groups. This includes (a) enhancers, promoters, insulators, and other regions of regulatory importance, (b) protein-coding and non-coding genes, (c) regulatory networks of transcription factor and microRNA targeting, and (d) sequence features predictive of diverse classes of functional elements. Aim 3: We will carry out exploratory data analyses across different data types to discover potentially novel correlations and insights relating diverse classes of elements. In particular we will apply dimensionality reduction techniques to coordinate-based genome-wide genomic and epigenomic datasets, we will apply clustering and bi-clustering methods to identify functionally related sets of genes and modules, and we will analyze structural and dynamic properties of discovered networks. Aim 4: We will carry out comparative analyses across the two model organisms, and also with yeast and human. We will provide an ortholog resource between the species, compare regulatory relationships and dynamics for orthologous cell lines and developmental points, and carry over biological knowledge across model organisms and human. To achieve these four aims, we will work closely with members of the consortium, the modENCODE Analysis Working Group (AWG), consisting of all Principal Investigators and analysis groups, and the Data Coordination Center (DCC), responsible for all data sharing within the consortium and with the larger worm and fly communities.      PUBLIC HEALTH RELEVANCE:  The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.           Narrative The aims of the ENCODE (Encyclopedia of DNA Elements) and modENCODE (model organism ENCODE) projects are to apply high-throughput, cost-efficient approaches to generate a catalog of functional elements in the human, worm, and fly genomes, which will serve as the basis for biomedical research advances. By their smaller genome size, powerful genetics, and ease of experimentation, D. melanogaster and C. elegans can help guide the study of functional elements in the human genome, reveal new insights into global gene regulation and embryo development, and enable experimental studies of gene function and regulation which are not accessible in mammalian systems. This proposal aims to enhance the value of these datasets by creating a Data Analysis Center (DAC) to support, facilitate, and enhance integrative analyses of the modENCODE consortium in fly and worm, to achieve a high-resolution annotation of all their functional elements, and to reveal new insights into the biology and gene regulation of animal genomes including the human.",A Data Analysis Center for integration of fly and worm modENCODE datasets,7854989,RC2HG005639,"['Animal Model', 'Animals', 'Binding', 'Biological', 'Biology', 'Biomedical Research', 'Boundary Elements', 'Caenorhabditis elegans', 'Cataloging', 'Catalogs', 'Cell Line', 'Chromatin', 'Classification', 'Code', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Coordinating Center', 'Data Set', 'Development', 'Disease', 'Elements', 'Embryonic Development', 'Enhancers', 'Functional RNA', 'Galaxy', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Guidelines', 'Health', 'Histones', 'Human', 'Human Genome', 'Hypersensitivity', 'Indium', 'Knowledge', 'Logic', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Nucleic Acid Regulatory Sequences', 'Orthologous Gene', 'Polymerase', 'Post-Transcriptional Regulation', 'Principal Component Analysis', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Reading', 'Recurrence', 'Regulation', 'Regulatory Element', 'Replication Initiation', 'Research Infrastructure', 'Resolution', 'Resources', 'Role', 'Site', 'System', 'Techniques', 'Testing', 'Tissues', 'Transcriptional Regulation', 'Ursidae Family', 'Variant', 'Work', 'Yeasts', 'base', 'chromatin immunoprecipitation', 'combinatorial', 'comparative', 'computer infrastructure', 'computerized data processing', 'computerized tools', 'cost', 'data exchange', 'data integration', 'data modeling', 'data sharing', 'epigenomics', 'file format', 'fly', 'gene function', 'genome-wide', 'insight', 'markov model', 'member', 'next generation', 'novel', 'promoter', 'public health relevance', 'research study', 'sequence learning', 'task analysis', 'transcription factor', 'working group']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RC2,2009,1473507,-0.014135023959142644
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7941562,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,1038804,0.031482221402185255
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7581087,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,3437506,0.031482221402185255
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7499147,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Depth', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Numbers', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'foot', 'insight', 'member', 'novel', 'quality assurance', 'scale up', 'size', 'symposium', 'theories', 'tool']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2008,1200000,0.06453082764334268
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7522602,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Class', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pan troglodytes', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Rate', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2008,369424,0.018489423580518333
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7404447,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2008,243004,0.010611804448795548
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7538527,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'GDF15 gene', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Histones', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Modification', 'Numbers', 'Ontology', 'PLAB Protein', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'RNA', 'Range', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2008,157474,-0.0015458336682288062
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,0.019521316314702483
"A Comprehensive catalog of human DNasel hypersensitive sites    DESCRIPTION (provided by applicant):   The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNaseI hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNaseI hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5).           n/a",A Comprehensive catalog of human DNasel hypersensitive sites,7410206,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Class', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonuclease I', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Histones', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Modification', 'Molecular', 'Noise', 'Numbers', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Preparation', 'Production', 'Public Domains', 'Range', 'Rate', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Score', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Standards of Weights and Measures', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'density', 'design', 'digital', 'experience', 'functional genomics', 'high throughput screening', 'human tissue', 'in vivo', 'insight', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2007,3114596,0.007692856990086793
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7214148,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2007,240927,0.010611804448795548
"Genetical Genomics Analysis Software    DESCRIPTION (provided by applicant): Response to drug treatment is thought dependent upon genotype for many modern therapies. Knowledge of how each genotype responds to a particular therapy is bene?cial only in that one can identify portions of the population which cannot reap the benefits of said treatment. A better course of action is to identify not only which genotype responds, or not, to a particular therapy, but to identify which region of the genome is responding, or not, and how. We believe that this information will lead to new drug targets and better therapies that benefit a larger portion of the population. The goal of this proposal is to provide a suite of software tools for genetic and genomic scientists performing gene mapping experiments with genomic data as the response variable. These tools will ideally provide functionality for 1) detecting polymorphic regions of the genome that con- fer transcript expression differences, 2) identify polymorphic regions of the genome that impart expression differences in genes located elsewhere in the genome, and 3) detecting interactions between loci that may correspond to epistatic effects on transcription. Some software already exists to perform each of these tasks as distinct independent solutions. This proposal intends to produce an integrated solution, S+EQTL (S-PLUS for expression quantitative trait loci mapping), that utilizes the power of S-PLUS and both incorporates and extends the functionality of an exist- ing genetics suite. By providing scientists with an integrated set of tools for genomics experiments with a genetic component, more productive time can be spent interpreting the results rather than transforming data into different formats to be processed by multiple software analysis packages. This software should also address one of the most dif?cult aspects of genetical genomics exper- iments, the so called curse of dimensionality. As the genomics community continues gathering knowledge of transcripts in various organisms, the arrays that interrogate transcript abundance only grow larger in the number of transcript species included. In the absence of tools designed for this purpose, the research scientist is left with the option of either focusing on a narrow set of previously known genes or performing a grid-wise search on all genes in the array. The former is not interesting as these genes are likely well studied and may provide little novel insight. The latter is computationally demanding and may not be possible on the new, larger arrays. A recent publication presents a novel solution that may be enhanced to gain both power and scale using Bayesian methodology. Knowledge of how each genotype responds to a particular drug therapy is beneficial only in that one can identify portions of the population which cannot reap the benefits of said treatment. A better course of action is to identify not only which genotype responds, or not, to a particular therapy, but to identify which region of the genome is responding, or not, and how. We believe that the development of analytic tools for gene mapping experiments to identify this information will lead to new drug targets and better therapies that benefit a larger portion of the population.          n/a",Genetical Genomics Analysis Software,7216142,R43GM079852,"['Address', 'Air', 'Algorithms', 'Animal Genetics', 'Anus', 'Arizona', 'Bioconductor', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Biotechnology', 'Bovine Spongiform Encephalopathy', 'Cations', 'Cattle', 'Chromosome Mapping', 'Code', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Con-fer', 'Data', 'Data Analyses', 'Data Set', 'Department of Defense', 'Depth', 'Development', 'Diagnostic', 'Disease', 'Disease regression', 'Doctor of Philosophy', 'Drug Delivery Systems', 'Educational process of instructing', 'Educational workshop', 'Employment', 'Ensure', 'Exons', 'Family suidae', 'Fatty acid glycerol esters', 'Foundations', 'Gene Combinations', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Research', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Government', 'Government Agencies', 'Graph', 'Imagery', 'Individual', 'Industry', 'Institution', 'International', 'Investments', 'Iowa', 'Knowledge', 'Lead', 'Left', 'Libraries', 'Literature', 'Liver', 'Location', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular Genetics', 'Mus', 'Nebraska', 'North Carolina', 'Numbers', 'Obese Mice', 'Obesity', 'Organism', 'Output', 'Pathway interactions', 'Performance', 'Pharmaceutical Preparations', 'Pharmaceutical Services', 'Pharmacotherapy', 'Phase', 'Phenotype', 'Population', 'Principal Investigator', 'Process', 'Publications', 'Purpose', 'Quantitative Genetics', 'Quantitative Trait Loci', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Single Nucleotide Polymorphism', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Software Tools', 'Solutions', 'Standards of Weights and Measures', 'Sus scrofa', 'Techniques', 'Telecommunications', 'Testing', 'Therapeutic', 'Thermogenesis', 'Thinking', 'Time', 'Time Series Analysis', 'Tissues', 'Training', 'Transcript', 'Treatment Protocols', 'United States National Aeronautics and Space Administration', 'United States National Institutes of Health', 'Universities', 'Washington', 'Work', 'animal breeding', 'base', 'design', 'experience', 'expression vector', 'genetic pedigree', 'hazard', 'improved', 'insight', 'interest', 'lecturer', 'novel', 'professor', 'programs', 'prototype', 'research and development', 'research study', 'response', 'skills', 'software development', 'success', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R43,2007,101707,-0.00022728252861053556
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7185305,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Biology, Other', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Computer information processing', 'Data', 'Databases', 'Depth', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'functional genomics', 'genetic element', 'genome database', 'human disease', 'interest', 'model organisms databases', 'repository', 'size', 'tool']",NHGRI,JACKSON LABORATORY,P41,2007,3146180,0.031482221402185255
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,7120160,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2006,414036,0.022593391869251016
"Shifting Conceptions of Human Identity DESCRIPTION (provided by applicant):  . One of the most important questions raised by the ongoing achievements of the Human Genome Project is how this new biological knowledge - and the powers it confers - will affect our identity and self-understanding as human beings. This book project focuses on one key aspect of this complex issue: exploring the extent to which human identity can be reconciled with deliberate design or partial redesign. The author proposes to shed new light on this question by comparing the debates surrounding two areas of scientific innovation that are not normally associated with each other, but that are in fact deeply related: the enterprise of human genetic intervention and the enterprise of building intelligent machines. Both these enterprises entail ""pushing the limits"" of traditional concepts of what it means to be human; and both ultimately confront their makers with the same core ""family"" of questions: What are the defining features of human personhood? To what extent can those features be modified or extended, before human personhood begins to break down? Can some (or all) of those features find embodiment in an entity other than a human being? These kinds of questions are no longer the sole province of science fiction writers, but have been taken up with increasing seriousness by mainstream scientists and technologists, as well as by a wide array of ""science watchers"" in academia, legislative circles, and the news media.   . Through documentary research and interviews, this project aims to deepen our understanding of the history and sociology of the debates surrounding these powerful new technologies, electro-mechanical and biological, that are perceived as destabilizing human identity. The intended audience for the book is a broad one: scientists and technological practitioners interested in the social and cultural reception of their research; legislators and other policymakers with a stake in the governance of science; general educated readers who are concerned about the role of science and technology in shaping our collective future. n/a",Shifting Conceptions of Human Identity,6915830,R03HG003298,"['adult human (21+)', 'artificial intelligence', 'behavioral /social science research tag', 'biotechnology', 'books', 'clinical research', 'ethics', 'genetic manipulation', 'history of life science', 'human subject', 'identity', 'interview', 'robotics', 'self concept', 'sociology /anthropology']",NHGRI,VANDERBILT UNIVERSITY,R03,2005,75833,0.02708566559840517
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6908174,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2005,375000,0.012130415241466035
"BioHDF - Open Binary File Standards for Bioinformatics DESCRIPTION (provided by applicant):  Geospiza Inc. and the National Center for Supercomputing Applications (NCSA) are creating a standards based software framework around NCSA's Heirarchical Data Format (HDF5). The envisioned framework will integrate algorithms important in DNA and protein sequence analysis to create scalable high throughput software systems which will be accessed using new graphical user interfaces (GUIs) to provide researchers with new views of their data to finish sequencing projects in large-scale genome sequencing, microbial genome sequencing, viral epidemiology, polymorphism detection, phylogenetic analysis, multi-locus sequence typing, confirmatory sequencing, and EST analysis.    In our vision, algorithms will be either integrated into the system to directly read and write from HDF5 project files, or they will communicate with project files via filter programs that produce standardized XML formatted data. Through this model, a scalable solution will support different applications of DNA sequencing, fulfilling the many needs and requirements expressed by the medical research community now and into the future. As the first step in this process we will, define requirements for editing and versioning data in DNA sequencing, research and propose data models for the computational phases of DNA sequencing and annotating DNA sequence data using existing standards, create a prototype application for DNA sequencing based SNP discovery, and engage the bioinformatics community for BioHDF adoption.       In the past ten years the cost of sequencing DNA has dropped over 1000 fold and the amount of raw sequence data, entering our national repositories is doubling every 12 months. DNA sequencing is fundamental to biological research activities such as genomics, systems biology, and clinical medicine. Proposals are being sought to decrease sequencing costs by two orders of magnitude through technology refinements with an ultimate vision of developing technology to sequence human genome equivalents for $1000 each. The amount of data that will be produced through these endeavors is unimaginable. However, the $1,000 genome will not advance medical research unless we integrate all phases of the DNA sequencing process and treat the creation, management, finishing, analysis, and sharing of the data as common goals. n/a",BioHDF - Open Binary File Standards for Bioinformatics,6992995,R41HG003792,"['DNA', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'functional /structural genomics', 'genetic mapping', 'genetic polymorphism', 'mathematics', 'molecular biology information system', 'nucleic acid sequence', 'single nucleotide polymorphism', 'virus genetics']",NHGRI,"GEOSPIZA, INC.",R41,2005,142775,0.013005735400479073
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,6952028,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2005,412000,0.022593391869251016
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6912979,R44HG002244,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data management', 'genotype', 'informatics', 'mathematical model', 'nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2004,191986,0.03158813065739967
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6788945,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2004,375000,0.012130415241466035
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,6737944,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2004,400000,0.022593391869251016
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6622259,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2003,560392,0.03158813065739967
"Development of Bioinformatic Tools for Virtual Cloning    DESCRIPTION (provided by applicant): The ability to delineate (at least in theory) all the proteins encoded in the human genome and all of those encoded by the genomes of major human parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease.  However, the vast increase in biological knowledge that has resulted from the last decade of genomic DNA sequencing has led us to a a crisis in bioinformatics.  This crisis is two-fold: analysis of data and planning of experiments.  Most of the scientific resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology.  All modern experimental molecular biology (and, increasingly, structural biology) depends upon the availability of plasmid clones to address specific scientific questions.  Although software facilitating DNA manipulations exists, few programs advise users of optimal strategies and none automate the process of clone generation. Genomics initiatives identify proteins at the genome level and demand the generation of hundreds of expression clones for recombinant protein production in exogenous hosts such as E. coli.  Establishment of libraries of expression clones requires automation and optimization as well as effective means of data storage, archiving, annotation and query.  To address these needs, as well as to facilitate routine DNA manipulations in virtually any molecular biology laboratory, we propose (1) to test and build a task centered virtual cloning expert system that serves as a knowledge base for DNA manipulations, and (2) to test and build an information automaton for the construction of expression clone libraries in support of structural genomics initiatives and other high throughput experiments.         n/a",Development of Bioinformatic Tools for Virtual Cloning,6583437,R43GM067279,"['artificial intelligence', ' biomedical automation', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' expression cloning', ' gene expression', ' genetic library', ' genetic manipulation', ' informatics', ' molecular biology information system', ' transfection /expression vector']",NIGMS,"VIRMATICS, LLC",R43,2003,100000,0.004341666913743912
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6444292,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2002,531259,0.03158813065739967
"Functional Genomics Software   DESCRIPTION (Applicant's abstract): A substantial commercial potential exists        for software tools that allow a biomedical research scientist to use genomic         data to form experimentally testable hypotheses. These will be used to exploit       genomic sequence data to understand the aetiology of disease, to improve             diagnostic tools, and to develop more effective therapies. The Master Catalog,       a commercial product developed jointly by EraGen Biosciences and the Benner          laboratory at the University of Florida, provides a convenient framework for         implementing heuristics that do this. The Master Catalog is a naturally              organized database that contains evolutionary trees, multiple sequence               alignments, and reconstructed evolutionary intermediates for all of the              proteins in the GenBank database. The Benner laboratory has developed and            anecdotally tested heuristics that date events in the molecular history,             provide evidence for and against functional recruitment within a protein             family, detect distant homologs, associate individual residues important for         functional changes with a crystal structure, find metabolic and regulatory           pathways, and correlate events in the molecular record with the history of life      on Earth. This Phase I proposal seeks to validate a set of these heuristics          more broadly to determine their suitability for database-wide application. In        Phase II, we will implement these within the Master Catalog, and launch a            commercial bioinformatics product to support functional analysis of genomic          databases.                                                                           PROPOSED COMMERCIAL APPLICATION:  In its present version, the Master Catalog is a successful commercial product within a  niche: ""best in class"" of bioinformatics databases.  Adding a validated set of heuristics  for extracting functional information from genome databases will make it the software  of choice for most functional genomics work, and be a central tool in the pharmaceutical/  biotechnology industries.  Academic versions and student versions will find markets in most  universities.                                                                                      n/a",Functional Genomics Software,6337786,R41HG002331,"['artificial intelligence', ' biochemical evolution', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' functional /structural genomics', ' informatics', ' molecular biology information system', ' nucleic acid sequence']",NHGRI,"ERAGEN BIOSCIENCES, INC.",R41,2001,96855,0.0014550250150918856
"Development of a joint machine learning/de novo assembly system for resolving viral quasispecies PROJECT SUMMARY Viral hepatitis from hepatitis B (HBV) establishes chronic infections in >250M people worldwide; chronicity is on the rise, and approximately one-third of the world’s population (2 billion) has serologic evidence of exposure. HBV coinfection with HCV and HIV is a hidden consequence of the substance use disorder epidemic. Viral populations have extremely high sequence diversity and rapidly evolve, which explains the vaccine failure rates and viral resistance to existing therapies and makes discovering lasting therapies extremely challenging. Next Generation Sequencing (NGS) is the method of choice to assess the intra-host virus population, termed a “quasispecies”. While a large set of short DNA sequencing reads are acquired that represent the virions in the quasispecies, computational technologies are limited in their analysis capabilities, resulting in particularly low resolution of complex HBV genomic structures. Another challenge is assembling NGS reads representing short fragment of the host genome into full strains (haplotypes) without knowledge of their true occurrence in the samples. To meet these challenges, GATACA is developing pathogen-specific bioinformatics software, GAT-ML (GATACA Assembly Tool – machine learning [ML]) to support treatment discovery and improve infection control. Its specifically designed algorithm utilizes novel ML methodologies adapted and modified for assisting genome assembly that will allow GAT-ML to reconstruct complete viral haplotypes and populations by learning the ‘language’ of the sequences. Tailored initially for HBV samples, GAT and its new ML system will be integrated for feasibility testing in this Phase I with the following Specific Aims: 1. Specific Aim 1. Build a joint learning system. Train and test natural language processing (NLP) methods on HBV genetic variation. 2. Specific Aim 2. Implement and test the machine learning methods in GAT (GAT-ML). We anticipate a working tool for characterizing HBV haplotypes, validated with multi-sourced datasets, and extensive testing and benchmarking of offline and integrated methods. The proposed project will develop and increase the capabilities of our novel computational tool, GAT, to help researchers identify the full spectrum of genetic features of a viral population—such as emergence and persistence of resistance or baseline polymorphisms regardless of their frequencies—and translate these findings to the development of new or improved antiviral drugs and other applications requiring high analytic sensitivity. GAT will particularly benefit researchers working in preclinical stages of drug development who require rapid, sensitive, and reliable results to inform decisions about which targets to advance to clinical trial testing.",Development of a joint machine learning/de novo assembly system for resolving viral quasispecies,10011686,R43AI152894,"['Adoption', 'Algorithm Design', 'Algorithms', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Chronic', 'Chronic Hepatitis', 'Classification', 'Clinical Trials', 'Complex', 'Computer software', 'DNA Structure', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Epidemic', 'Failure', 'Frequencies', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'HIV', 'HIV/HCV', 'Haplotypes', 'Healthcare', 'Hepatitis B', 'Hepatitis B Virus', 'Infection Control', 'Joints', 'Knowledge', 'Language', 'Language Development', 'Learning', 'Link', 'Liver diseases', 'Machine Learning', 'Metagenomics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Mutation', 'Natural Language Processing', 'Outcome', 'Pattern', 'Performance', 'Phase', 'Population', 'Population Analysis', 'Privatization', 'Research Personnel', 'Resistance', 'Resolution', 'Sampling', 'Semantics', 'Serological', 'Serotyping', 'Source', 'Speed', 'Substance Use Disorder', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translating', 'Trust', 'Vaccines', 'Validation', 'Variant', 'Viral', 'Viral hepatitis', 'Virion', 'Virus', 'base', 'chronic infection', 'co-infection', 'commercialization', 'computerized tools', 'contig', 'design', 'drug development', 'improved', 'insertion/deletion mutation', 'machine learning algorithm', 'machine learning method', 'multiple data sources', 'neural network', 'next generation sequencing', 'novel', 'pathogen', 'pre-clinical', 'structural genomics', 'syntax', 'tool', 'viral resistance']",NIAID,"GATACA, LLC",R43,2020,267225,0.012054975667952357
"POPULATION GENOMICS OF ADAPTATION Project Summary Malaria that results from Plasmodium falciparum is among the most globally devastating human diseases. The principle vector of malaria, mosquitoes of the Anopheles gambiae species complex, are thus central targets for controlling the human health burden of Plasmodium. For nearly two decades, there have been large-scale, coordinated efforts to diminish mosquito populations, generally through spraying and insecticide treated bed nets. Indeed such control efforts have now led to a nearly 50% decrease in the rates of malaria infection in many parts of sub-Saharan Africa. At present, however, control efforts of A. gambiae are being threatened by evolutionary responses within mosquitos: A. gambiae populations have shown increases in insecticide resistance as well as behavioral adaptations that allow mosquitos to avoid spraying all together. Thus adaptation of mosquitos to the control efforts themselves is currently a risk to maintain the gains made in the fight against malaria. In this proposal we lay out an integrated population genomic approach for systematically identifying regions of the A. gambiae genome that are evolving adaptively in response to ongoing control efforts. Our approach centers upon state-of-the-art supervised machine learning techniques that we have recently introduced for finding the signatures of selective sweeps in genomes (Schrider and Kern, 2016), coupled with the large-scale population genomic datasets currently in production by the Ag1000G consortium. Project Narrative Malaria is a mosquito-borne infectious disease that has enormous impacts on human health globally. For the past 16 years, large gains have been made in decreasing the rate of malaria transmission through control of its mosquito vector Anopheles gambiae; unfortunately at present these control efforts are in danger of collapse due to the evolution of insecticide resistance in the mosquitos. We aim to discover the genomic targets of such resistance through the development of sophisticated population genomic approaches and their application to state-of- the-art genome sequence datasets from Anopheles gambiae.",POPULATION GENOMICS OF ADAPTATION,9957109,R01GM117241,"['Affect', 'Africa South of the Sahara', 'Anopheles Genus', 'Anopheles gambiae', 'Awareness', 'Back', 'Beds', 'Behavioral', 'Catalogs', 'Cessation of life', 'Chromosomes', 'Classification', 'Complex', 'Coupled', 'Culicidae', 'Data', 'Data Set', 'Dependence', 'Detection', 'Development', 'Distant', 'Equipment and supply inventories', 'Evolution', 'Frequencies', 'Funding', 'Genome', 'Genomic approach', 'Genomics', 'Geography', 'Goals', 'Health', 'Human', 'Individual', 'Insecticide Resistance', 'Insecticides', 'Link', 'Location', 'Malaria', 'Methodology', 'Methods', 'Mosquito-borne infectious disease', 'Mutation', 'Pattern', 'Phase', 'Plasmodium', 'Plasmodium falciparum', 'Population', 'Prevalence', 'Production', 'Recording of previous events', 'Research', 'Residual state', 'Resistance', 'Risk', 'Sampling', 'Techniques', 'Time', 'Variant', 'Work', 'deep neural network', 'fight against', 'genomic data', 'global health', 'human disease', 'machine learning method', 'malaria infection', 'malaria mosquito', 'malaria transmission', 'markov model', 'novel', 'recurrent neural network', 'resistance allele', 'response', 'supervised learning', 'support vector machine', 'tool', 'vector', 'vector control', 'vector mosquito']",NIGMS,UNIVERSITY OF OREGON,R01,2020,295000,0.043894417079007104
"Genetic determinants of 4D genome folding in human cardiac development PROJECT SUMMARY A major unanswered question is how chromatin topology coordinates human development and cellular differentiation, and how genome folding is differentially regulated in human disease. It is thought that three- dimensional (3D) chromatin organization is driven by transcriptional regulators, but fundamental mechanisms of this regulation as it relates to disease-relevant human cells have not been well explored. We propose to elucidate the temporally dynamic 3D nucleome (4DN) that underlies human cardiac differentiation, its molecular underpinnings, and the impact of mutations that underly defective 4DN organization in human congenital heart disease (CHD). CHDs are the most common birth defect and arise from abnormal heart development. The genetic basis of CHD is largely mutations in genes encoding chromatin modifiers (e.g. WDR5, KMT2D) and transcription factors (TFs, e.g. TBX5, GATA4), many of which also cause adult-onset arrhythmias. The impact of CHD mutations on the 4DN has not been explored. We hypothesize that 3D genome folding is highly regulated during cardiac differentiation and is impacted by disease-causing mutations in transcriptional regulators and non-coding elements. We will use iPS cell models and machine learning to elucidate dynamic 3D chromatin organization in human cardiomyocytes and endothelial cells during normal and diseased cardiac differentiation. We propose 3 specific aims: Aim 1: Establish a kilobase-scale 4D map of genome folding in human cardiomyocytes (CM) and endothelial cell (EC) differentiation. We will use directed differentiation of human iPS cells towards the two major cell types of the developing heart: CMs and ECs, and using microC across a fine time course of differentiation we will define at kilobase scale the 3D organization of the genome, capturing the states of developmental intermediates and the final differentiated cells. This aim will generate an essential integrated 4DN template for discovery in cardiac differentiation. In Aim 2: we will Determine the regulatory and disease-related basis for cardiac 3D chromatin organization. We will perform microC in iPS cell lines with CHD-associated mutations in transcriptional regulators, differentiated into CMs and ECs. These findings will establish the degree to which CHD is caused by abnormal genome folding and chromatin states, with important relevance to other human cardiovascular diseases. Finally, Aim 3 will address High-throughput screening of millions of CHD and synthetic non- coding mutations with a deep-learning model of dynamic genome folding. We will build a deep-learning model predicting 3D chromatin contact frequencies across cardiac differentiation at kilobase-resolution. By introducing thousands of CHD patient deletions and other non-coding mutations in silico, we will prioritize variants likely to interact with transcriptional regulators to cause disease through disrupted genome folding. Several candidates will be validated in engineered iPS cells differentiated into CMs and ECs. These results will provide a novel platform for computational discovery of disease variant impact across diverse human diseases PROJECT NARRATIVE Congenital heart defects are present in 1-2 out of 100 births, and are caused by mutations in genes that may control the 3-dimensional organization of chromosomes. We will use human cellular models and Artificial Intelligence to understand how chromosome organization is controlled during heart development and altered by disease processes. This will reveal fundamental concepts of gene regulation and may lead to a better understanding congenital heart defects towards improving diagnosis and treatment.",Genetic determinants of 4D genome folding in human cardiac development,10118056,U01HL157989,"['3-Dimensional', 'ATAC-seq', 'Address', 'Adult', 'Architecture', 'Arrhythmia', 'Artificial Intelligence', 'Birth', 'CCCTC-binding factor', 'Cardiac', 'Cardiac Myocytes', 'Cardiac development', 'Cardiovascular Diseases', 'Cell Differentiation process', 'Cell Line', 'Cell model', 'Cells', 'ChIP-seq', 'Child', 'Chromatin', 'Chromatin Remodeling Factor', 'Chromosome Structures', 'Code', 'Complement', 'Complex', 'Congenital Abnormality', 'Congenital Heart Defects', 'Data', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Endothelial Cells', 'Engineering', 'Enhancers', 'Family', 'Frequencies', 'GATA4 gene', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Transcription', 'Genome', 'Genomics', 'Heart', 'Heart Abnormalities', 'Heart Diseases', 'Histones', 'Human', 'Human Biology', 'Human Development', 'Lead', 'Machine Learning', 'Maps', 'Measures', 'Modeling', 'Molecular', 'Mutate', 'Mutation', 'Patients', 'Phenotype', 'Process', 'Proteins', 'Regulation', 'Regulator Genes', 'Regulatory Element', 'Resolution', 'SMARCA2 gene', 'Structure', 'Time', 'Untranslated RNA', 'Variant', 'Work', 'cardiogenesis', 'cell type', 'cohesin', 'computational platform', 'congenital heart disorder', 'deep learning', 'disease-causing mutation', 'high throughput screening', 'histone modification', 'human disease', 'human model', 'human pluripotent stem cell', 'improved', 'in silico', 'machine learning method', 'novel', 'predictive modeling', 'promoter', 'stem cell differentiation', 'stem cell model', 'transcription factor']",NHLBI,J. DAVID GLADSTONE INSTITUTES,U01,2020,742240,-0.02664718262574175
"Center for Critical Assessment of Genome Interpretation Genomic data hold the promise of revolutionizing our understanding and treatment of human disease. Multiple barriers stand between the acquisition of the data and realizing these and other benefits. Rapid accumulation of genomic data far exceeds our capacity to reliably interpret genomic variation. New developments in artificial intelligence and machine learning, combined with increased computing power and domain knowledge, provide hope for the deployment of enhanced computational tools in both basic research and clinical practice. Use of these methods critically depends upon reliable characterization of their performance.  The Center for Critical Assessment of Genome Interpretation (C-CAGI) will address these needs, through objective evaluation of the state of the art in relating human genetic variation and health. CAGI has had five editions since 2010 with 50 challenges posed to the community taken on by hundreds of predictors, leading to scores of publications about prediction methods and their assessment. We propose for C-CAGI to continue to advance the field of variant interpretation through the following Specific Aims: 1. Develop community experiments to evaluate the quality of computational methods for interpreting genomic variation data. C-CAGI will conduct community experiments in which participants make bona fide blinded predictions of disease related phenotypes on the basis of genomic data. We will engage a diverse predictor community to spur innovation. The CAGI Ethics Forum will vet studies to ensure that privacy and sharing maintain the highest standards and will educate the community. 2. Assess the quality of current computational methods for interpreting genomic variation data; highlight innovations and progress at interactive conferences. Predictions will be evaluated by independent assessors, who will be supported by new assessment approaches from C-CAGI. Results will be presented at CAGI experiment conferences with deep technical engagement, which will be interleaved with reflective CAGIâ meetings that create an environment for a comprehensive evaluation of the field, facilitating identification of major bottlenecks and problems faced by the current genome interpretation approaches. 3. Broadly disseminate the results and conclusions from the CAGI experiments and analysis. C-CAGI will outreach to the broader scientific and clinical community through its publications, and the creation of a calibrated reference integrated into the most common workflows for ready adoption. CAGI will also be represented at international meetings with presentations and workshops. 4. Operate effectively and responsively. C-CAGI will operate efficiently as it closely interacts with hundreds of participants. CAGI will build upon a robust information infrastructure that securely facilitates data dissemination, prediction submission, and assessment. Genomic variation is responsible for numerous rare diseases, for propensity for many common traits and diseases, for drug response, and is a key characteristic of cancer evolution. At present, our ability to characterize genetic differences far exceeds our capacity to interpret them either for basic research understanding or for clinical application. The Center for Critical Assessment of Genome Interpretation, operating on robust ethical foundations, will provide an evaluation of the current state of the art and help promote progress in understanding the impact of genomic variation.",Center for Critical Assessment of Genome Interpretation,9937546,U24HG007346,"['Address', 'Adoption', 'Affect', 'Amino Acid Sequence', 'Artificial Intelligence', 'Basic Science', 'Blinded', 'Characteristics', 'Clinical', 'Communities', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Ensure', 'Environment', 'Ethics', 'Evaluation', 'Evolution', 'Foundations', 'Genetic', 'Genetic Variation', 'Genome', 'Health', 'Human Genetics', 'Infrastructure', 'International', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Nucleotides', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Privacy', 'Provider', 'Publications', 'RNA Splicing', 'Rare Diseases', 'Secure', 'Structure', 'Trust', 'Variant', 'Work', 'base', 'clinical application', 'clinical practice', 'computerized tools', 'data acquisition', 'data dissemination', 'exome', 'experimental study', 'genetic information', 'genetic variant', 'genomic data', 'genomic variation', 'high standard', 'human disease', 'innovation', 'meetings', 'multiple omics', 'operation', 'outreach', 'response', 'symposium', 'trait', 'whole genome']",NHGRI,UNIVERSITY OF CALIFORNIA BERKELEY,U24,2020,314933,0.026874462783135638
"Deep learning approaches to decipher the impact of mobile element insertion on alternative splicing in neurological disorders The purpose of this training and research application is to study the functional impact of mobile element insertions (MEIs) in neurological disorders (NDs) using new developments in deep learning techniques. MEIs are transposable DNA fragments that are able to insert throughout the human genome. There are at least 124 independent MEIs associated with human diseases. Approximately 20% of these diseases represent a spectrum of NDs, yet the overall contribute of MEIs to the etiology of NDs has not been systematically estimated. To address this, we will (1) characterize functional MEIs in GTEx cohorts in healthy individuals; (2) build a comprehensive functional map of MEIs to determine tissue-specific and brain-specific impact; and (3) impute transcriptional changes on various NDs where whole-genome sequencing (WGS) data will be generated. The proposed application will also develop an extensive research program for Dr. Dadi Gao, a computational biologist and statistical geneticist who has trained in functional genomic studies of alternative splicing in neurodegenerative disorders and therapeutic targeting of a splicing defect that causes a severe neurodevelopmental disorder. He has developed novel methods to investigate regulation of the transcriptome and to facilitate analyses in drug development. He now seeks to expand his expertise by applying statistical and deep learning models on large cohorts of sequencing data from controls and cases with NDs from post-mortem tissues, then impute functional consequences of MEIs from WGS in large-scale disease cohorts. The training plan consists of two years of mentored research to learn new skills in genome analysis, MEI characterization, and advanced deep learning techniques, followed by three years of shaping an independent laboratory. The research plan is developed to comprehensively explore functional variation in the genome by decomposing transcriptomic changes against MEIs. Dr. Michael Talkowski at Massachusetts General Hospital, Harvard, and the Broad Institute will serve as the primary mentor, while Dr. Manolis Kellis at MIT and the MIT Computational Biology Group, and the Broad Institute will serve as a co-mentor and close collaborator. These mentors are recognized experts in genomic structural variants, functional genomics, the genetics of neurological disorders, and computational modeling to establish functional elements in the human genome. In addition, a team of independent investigators from basic and translational research will provide Dr. Gao with comprehensive feedback to keep both his science and career development on track. The highly collaborative environment in CGM, MGH, Harvard Medical School, the Broad Institute and the University of Michigan Medical School will prepare Dr. Gao for his transition to an independent investigator. This outstanding mentorship team and training program will facilitate the career development of Dr. Gao as he seeks to redefine the functional maps of MEIs in the human genome and to impute their impact in large-scale neurological disorders. Mobile element insertions (MEIs) represent a largely undefined component of the genetic architecture of neurological disorders, as a number of MEIs have been associated with alternative splicing in these disorders but large-scale genome-wide functional characterization has not been systematically performed across tissues. This program study will functionally characterize the impact of MEIs on alternative splicing from whole-genome sequencing and transcriptome sequencing in large cohorts using new developments in deep learning models. These results will enhance our understanding of the etiological role and pathogenic mechanisms associated with MEIs in neuronal development and human neurological disorders.",Deep learning approaches to decipher the impact of mobile element insertion on alternative splicing in neurological disorders,10041366,K99NS118109,"['Address', 'Algorithms', 'Alternative Splicing', 'Alzheimer&apos', 's Disease', 'Autopsy', 'Basic Science', 'Biology', 'Blood', 'Brain', 'Brain Diseases', 'CRISPR/Cas technology', 'Cells', 'Chromosome Pairing', 'Cohort Studies', 'Computational Biology', 'Computer Analysis', 'Computer Models', 'DNA', 'DNA Insertion Elements', 'Data', 'Data Set', 'Defect', 'Detection', 'Development', 'Disease', 'Disease model', 'Dorsal', 'Dystonia', 'Elements', 'Etiology', 'Event', 'Evolution', 'Excision Repair', 'Familial Dysautonomia', 'Feedback', 'Fellowship', 'Filipino', 'General Hospitals', 'Generations', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Haplotypes', 'Human', 'Human Genome', 'Individual', 'Institutes', 'International', 'Introns', 'Laboratories', 'Lateral', 'Lead', 'Learning', 'Linear Regressions', 'Link', 'Machine Learning', 'Maps', 'Massachusetts', 'Measures', 'Mentors', 'Mentorship', 'Methods', 'Michigan', 'Mind', 'Minisatellite Repeats', 'Modeling', 'Molecular', 'Mosaicism', 'Neurodegenerative Disorders', 'Neurodevelopmental Disorder', 'Neuromuscular Diseases', 'Neurons', 'Outcome', 'Parkinsonian Disorders', 'Pathogenicity', 'Pattern', 'Peripheral', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Prefrontal Cortex', 'Process', 'Property', 'RNA Splicing', 'Regulation', 'Research', 'Research Personnel', 'Retroelements', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Shapes', 'Short Interspersed Nucleotide Elements', 'Source', 'Specificity', 'Structure', 'TAF1 gene', 'Techniques', 'Therapeutic Trials', 'Tissue-Specific Splicing', 'Tissues', 'Training', 'Training Programs', 'Transcription Alteration', 'Translational Research', 'Universities', 'Untranslated RNA', 'Variant', 'Work', 'brain tissue', 'career development', 'cohort', 'collaborative environment', 'convolutional neural network', 'deep learning', 'drug development', 'functional genomics', 'functional outcomes', 'gene function', 'genetic architecture', 'genome analysis', 'genome editing', 'genome sequencing', 'genome-wide', 'human disease', 'in silico', 'insight', 'medical schools', 'mind control', 'nervous system disorder', 'neuron development', 'novel', 'programs', 'response', 'skills', 'statistical learning', 'structural genomics', 'therapeutic target', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,K99,2020,91226,-0.002817184212865639
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell-based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off-target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,10016298,U01AI157189,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Bar Codes', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Proliferation', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'machine learning method', 'next generation', 'novel', 'novel therapeutics', 'off-target mutation', 'off-target site', 'response', 'safety assessment', 'safety testing', 'side effect', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NIAID,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2020,610710,0.01654998677460554
"A priori adaptive evolution predictions for antibiotic resistance through genome-wide network analyses and machine learning SUMMARY Adaptive evolution (AE) is both a “force of good” as it can help to optimize biological processes in industry, but it is also a “force of frustration” when infectious diseases exploit AE to escape the host immune system or become resistant to drugs. It has long been assumed close to impossible to make predictions on AE due to the presumed predominating influences of random forces and events. However, the observation that evolutionary repeatability across traits and species is far more common than previously thought, suggests that AE, with the right data and approach, may become (partially) predictable. Indeed, we found through experiments with the bacterial pathogen Streptococcus pneumoniae on its response to antibiotics and the emergence of antimicrobial resistance, that in order to make AE predictable a detailed understanding of at least two aspects of the bacterial system are required: 1.) the genetic constraints of the system (i.e. the architecture of the organismal network); and 2.) where and how in the system stress is experienced and processed. We showed that by mapping out ~25% of the bacterium's network, determining phenotypic and transcriptional antibiotic responses, applying network analyses to capture and quantify the responses in a network context, and exploiting experimental evolution to pin-point adaptive mutations in the genome it becomes possible, by means of machine learning, to uncover hidden patterns in the data that make AE predictions feasible. This means that the network in interaction with the environment shapes the adaptive landscape, it limits available solutions and makes some solutions more likely than others, thereby driving repeatability and enabling predictability. In this proposal we build on these exciting developments with the goal to map out the constraints of S. pneumoniae's entire network and develop a machine learning model that can forecast adaptive evolution a priori, and on a genome-wide scale. To accomplish this, we combine in aim 1 parts of Tn-Seq, dTn-Seq and Drop-Seq to finalize a new tool Tn-Seq^2 (Tn-Seq squared) that is able to map genetic-interactions in high-throughput and genome-wide. We use Tn-Seq^2 to reconstruct the first genome-wide genetic interaction network for S. pneumoniae in the presence of 20 antibiotics. In aim 2 we create 85 HA-tagged Transcription factor induction (TFI) strains and: a) Determine with ChIP-Seq the DNA-binding sites for all 85 TFs in S. pneumoniae; b) By overexpressing each TFI strain followed by RNA- Seq we determine each TFs regulatory signature; c) Use a Transcriptional Regulator Induced Phenotype screen in the presence of 20 antibiotics to untangle environment specific links between genetic and transcriptional perturbations and their phenotypic outcomes. Lastly, in aim 3, we train and test a variety of machine learning approaches to design an optimal model that predicts which genes in the genome are most likely to adapt in the presence of a specific antibiotic. The development of this predictive AE model, will not only be useful in predicting the emergence of antibiotic resistance, but the strategy should be valuable for most any biological field for which adaptive changes are important, ranging from biological engineering to cancer. NARRATIVE Adaptive evolution (AE) is the driving force behind the emergence of antibiotic resistance and if it were possible to predict AE before it happens, it could help in preventing resistance. Here we use cutting-edge existing and newly designed genomics tools and analytical approaches to develop a machine learning model that can predict AE a priori, and on a genome-wide scale.",A priori adaptive evolution predictions for antibiotic resistance through genome-wide network analyses and machine learning,10049219,R01AI148470,"['Achievement', 'Affect', 'Antibiotic Resistance', 'Antibiotics', 'Architecture', 'Automobile Driving', 'Bacteria', 'Binding Sites', 'Biological', 'Biological Process', 'Biomass', 'ChIP-seq', 'Communicable Diseases', 'Complex', 'DNA Binding', 'Data', 'Development', 'Drug resistance', 'Engineering', 'Ensure', 'Environment', 'Escherichia coli', 'Event', 'Evolution', 'Exposure to', 'Fermentation', 'Frustration', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Immune system', 'Immunotherapeutic agent', 'Industry', 'Life', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Microfluidics', 'Modeling', 'Mutation', 'Organism', 'Outcome', 'Pathway Analysis', 'Pattern', 'Phenotype', 'Photosynthesis', 'Planet Earth', 'Process', 'Resistance', 'Shapes', 'Streptococcus pneumoniae', 'Stress', 'System', 'Testing', 'Time', 'Training', 'Yeasts', 'design', 'driving force', 'droplet sequencing', 'emerging antibiotic resistance', 'emerging antimicrobial resistance', 'experience', 'experimental study', 'genetic architecture', 'genome-wide', 'genomic tools', 'network architecture', 'novel', 'overexpression', 'pathogenic bacteria', 'predictive modeling', 'prevent', 'process optimization', 'programs', 'response', 'tool', 'trait', 'transcription factor', 'transcriptome', 'transcriptome sequencing', 'transposon sequencing']",NIAID,BOSTON COLLEGE,R01,2020,391250,-0.01326689011777916
"Scalable detection and interpretation of structural variation in human genomes PROJECT SUMMARY Structural variation (SV), is a diverse class of genome variation that includes copy number variants (CNVs) such as deletions and duplications, as well as balanced rearrangements, such as inversions and reciprocal translocations. A typical human genome harbors >4,000 SVs larger than 300bp and their large size increases the potential to delete or duplicate genes, disrupt chromatin structure, and alter expression. Despite their prevalence and potential for phenotypic consequence, SVs remain notoriously difficult to detect and genotype with high accuracy. Much of this difficulty is driven by the fact DNA sequence alignment “signals” indicating SVs are far more complex than for single-nucleotide and insertion deletion variants. Unlike SNP alignments that vary only in allele state, alignments supporting SVs vary in state (supports an alternate structure or not) alignment location, and type. Consequently, the accuracy of SV discovery is much lower than that of SNPs and INDELs. Furthermore, SV pipelines scale poorly and are difficult to run. These challenges are a barrier for single genome analysis and studies of families must invest substantial effort into eliminating a sea of false positives. These problems become exponentially more acute for large-scale sequencing efforts such as TOPmed, the Centers for Common Disease Genetics, and the All of Us program. Software efficiency is key to scalability for such projects. However, of equal importance is comprehensive, accurate discovery.  Building upon more than a decade of software development experience and analyzing SV in diverse disease contexts, we have invested significant effort into understanding the causes of the insufficient accuracy for SV discovery. These efforts, together with our research and development experience in this area, give us unique insight into improving the accuracy and scalability of SV discovery. Our goal is to narrow the accuracy gap between SNP/INDEL variation and structural variation discovery. These developments will empower studies of human genomes in diverse contexts and will therefore have broad impact. Our goals are to: 1. Develop a deep learning model to correct systematic variation in sequence depth. This new machine  learning model will correct systematic biases in DNA sequence depth and dramatically improve the  discovery of deletions and duplications. 2. Improve the speed, scalability, and accuracy of SV detection and genotyping. Using new algorithms,  we will bring the accuracy of SV detection much closer to that of SNP and INDEL discovery and allow  accurate SV discovery to be deployed at scale. 3. Create a map of genomic constraint for SV from population-scale genome analysis. We will deploy  our new methods to detect and genotype structural variation among tens of thousands of human genomes.  The resulting SV map will empower the creation of a model of genomic constraint for SV and enable new  software to predict deleterious SVs, especially in the noncoding genome. PROJECT NARRATIVE Single-nucleotide DNA changes paint an incomplete picture of a human’s genome. A more complete picture must include a genome's structural variation (SV), an important class of genome variation that includes copy number variants (CNVs) such as deletions and duplications. However, existing methods have poor accuracy. As the genetics community transitions to large-scale genome sequencing studies, there is an acute need for improved SV discovery methods. This proposal introduces a series of algorithmic and software innovations that will empower SV discovery, genotyping, and interpretation in large-scale human disease studies.",Scalable detection and interpretation of structural variation in human genomes,9973582,R01HG010757,"['Acute', 'Affect', 'Algorithmic Software', 'Algorithms', 'All of Us Research Program', 'Alleles', 'Area', 'Automobile Driving', 'Biological Assay', 'Chromatin Structure', 'Chromosome Structures', 'Clip', 'Cloud Computing', 'Code', 'Communities', 'Complex', 'Computer software', 'Copy Number Polymorphism', 'DNA', 'DNA Sequence', 'Data', 'Data Reporting', 'Detection', 'Development', 'Disease', 'Environment', 'Error Sources', 'Exhibits', 'Family Study', 'Funding', 'Future', 'Gene Duplication', 'Gene Expression', 'Gene Fusion', 'Gene Structure', 'Genetic', 'Genetic Diseases', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Human Genome', 'Individual', 'Laboratories', 'Large-Scale Sequencing', 'Location', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Noise', 'Nucleotides', 'Paint', 'Pathogenicity', 'Performance', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Process', 'Reciprocal Translocation', 'Research', 'Running', 'Sampling', 'Sea', 'Sensitivity and Specificity', 'Sequence Alignment', 'Series', 'Signal Transduction', 'Software Tools', 'Source', 'Speed', 'Structure', 'Systematic Bias', 'Techniques', 'Technology', 'Training', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'algorithm development', 'base', 'convolutional neural network', 'deep learning', 'developmental disease', 'dosage', 'exome', 'experience', 'genome analysis', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'large datasets', 'method development', 'nanopore', 'novel', 'prevent', 'research and development', 'software development', 'success', 'tool', 'variant detection', 'whole genome']",NHGRI,UNIVERSITY OF UTAH,R01,2020,692048,0.0211194574267875
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9868315,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'de novo mutation', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'machine learning method', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'supervised learning', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2020,237600,0.021390296776236297
"Identification of Transposable Element Insertions in the Kids First Data Project Summary Insertion of transposable elements (TEs, sometimes referred to as “jumping genes”) into the human genome can be pathogenic. Our aim in this project is to use sophisticated computational approaches to characterize TE insertions in the whole-genome sequencing data generated in the Gabriella Miller Kids First Pediatric Research Program and identify any insertional mutations that may disrupt gene function. The large scale of the Kids First program provides an unprecedented opportunity to investigate the role of TE insertions in childhood cancers and structural birth defects, as well as to create a resource of reference TE maps that will be important for all other TE studies. We will first modify our existing algorithm called xTEA for the trio design of the Kids First studies and increase the accuracy and efficiency of the algorithm. Then, we will apply it to the thousands of trios that have been profiled in the Kids First program, using a pipeline optimized for the cloud environment. The resulting set of TE insertions (especially L1, Alu, SVA, and HERV insertions) will be curated with all relevant features and be made into a database for the community. We will also apply machine learning methods to improve the calls once a sufficient amount of training data have been obtained. To investigate the potential pathogenicity of the mutation, we will first focus on insertions within genes, but we will also explore those in regulatory elements inferred from epigenetic profiling data. PROJECT NARRATIVE Transposable elements, or “jumping genes”, are genetic elements that can alter the DNA of an individual. We aim to utilize a computational method to identify such elements in the genome sequencing data generated in the Gabriella Miller Kids First Pediatric Research Program. Our analysis will identify transposable elements that may be causal for a disease phenotype.",Identification of Transposable Element Insertions in the Kids First Data,9957262,R03CA249364,"['Algorithms', 'Communities', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Insertion Elements', 'DNA Transposable Elements', 'Data', 'Data Set', 'Databases', 'Disease', 'Elements', 'Endogenous Retroviruses', 'Environment', 'Gene Frequency', 'Genes', 'Genetic Diseases', 'Genome', 'Genomics', 'Genotype', 'Human Genome', 'Individual', 'Inherited', 'Insertion Mutation', 'Jumping Genes', 'Length', 'Location', 'Machine Learning', 'Malignant Childhood Neoplasm', 'Malignant Neoplasms', 'Maps', 'Mendelian disorder', 'Methods', 'Modeling', 'Mutation', 'Neurons', 'Output', 'Parents', 'Paste substance', 'Pathogenicity', 'Pediatric Research', 'Play', 'Population', 'Regulatory Element', 'Reporting', 'Resources', 'Retrotransposon', 'Role', 'Sampling', 'Sensitivity and Specificity', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Speed', 'Structural Congenital Anomalies', 'Training', 'base', 'cloud based', 'cohort', 'design', 'disease phenotype', 'epigenetic profiling', 'gene function', 'genetic element', 'genome sequencing', 'improved', 'machine learning method', 'proband', 'programs', 'transcriptome sequencing', 'whole genome']",NCI,HARVARD MEDICAL SCHOOL,R03,2020,169041,-0.009586706225699907
"Advancing evolutionary genetic inference in humans and other taxa Project Summary/Abstract Background: A major challenge in evolutionary genomics is to characterize the forces shaping present-day patterns of genetic variation. For instance, the extent and manner in which natural selection affects genetic diversity remains highly controversial. Researchers have largely addressed this problem by developing statistical tests or summaries of genome sequence variation that provide insights into the evolutionary forces at play. However, because such approaches typically rely on a single univariate summary of the data, valuable discriminatory information present in the original dataset is lost. A more fruitful strategy would thus be to use multidimensional summaries of genomic data (e.g. a large vector of summary statistics) or even the totality of the input data (e.g. a matrix-representation of a sequence alignment) to make more accurate inferences. An even more powerful approach is to utilize data sets in which the same population is sampled at multiple time points, allowing one to observe evolutionary dynamics in action. Although such genomic time-series data are becoming more prevalent, the development of appropriate computational methodologies has lagged behind the proliferation of such data. Proposal: The Schrider Lab seeks to develop and apply powerful machine learning methods for evolutionary inference. Our work over the next five years will yield powerful software tools leveraging novel representations of genomic datasets, including time-series data. These efforts will dramatically improve researchers' ability to make accurate evolutionary inferences from both population genomic and phylogenetic data. Indeed, preliminary results demonstrate that our methods vastly outperform current approaches in evolutionary genetics. More importantly, we will use these tools to answer pressing evolutionary questions. In particular, our use of time-series data will reveal loci responsible for recent adaptation with much greater confidence than currently possible. Our efforts will help to resolve the controversy over the role of adaptation in shaping patterns of diversity across the human genome. This research has important implications for public health as well, as genes underlying recent adaptations are enriched for disease-associations. Moreover, we are constructing a time-series dataset in the mosquito vector species Aedes aegypti and Aedes albopictus. We will interrogate these data for evidence of recent and ongoing adaptation—this work will reveal loci responsible for the evolution of resistance to insecticides and other control efforts. Encouraging preliminary data also suggest that our work in phylogenetics will substantially improve inferential power in this important research area. More broadly, the success of the novel approaches described in this proposal has the potential to transform the methodological landscape of evolutionary genomic data analysis. Project Narrative The work proposed here seeks to develop and apply powerful machine-learning based software tools for evolutionary genetic inference in humans, mosquito vectors, and other species. Such efforts have important health implications, as they can identify genes involved in adaptation, which in humans are often associated with disease and in mosquitos are often associated with resistance to insecticides and other control efforts.",Advancing evolutionary genetic inference in humans and other taxa,10028474,R35GM138286,"['Address', 'Aedes', 'Affect', 'Area', 'Computing Methodologies', 'Culicidae', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Health', 'Human', 'Human Genome', 'Insecticides', 'Machine Learning', 'Methodology', 'Methods', 'Natural Selections', 'Pattern', 'Phylogenetic Analysis', 'Play', 'Population', 'Public Health', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Sampling', 'Sequence Alignment', 'Series', 'Shapes', 'Software Tools', 'Testing', 'Time', 'Variant', 'Work', 'base', 'genomic data', 'improved', 'insight', 'machine learning method', 'novel', 'novel strategies', 'statistics', 'success', 'time use', 'tool', 'vector', 'vector mosquito']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R35,2020,382894,0.009326475238596746
"Functional annotation of new genes aided by deep learning New genes (NGs) are generated by multiple mechanisms and their end-piece sequences are identified as the chimeric transcript sequence from multiple human sources including healthy and disease tissues. Therefore, NGs have been recognized as important biomarkers and therapeutic targets for precision medicine. Many efforts have been made to study individual NG function and to identify relevant drug targets. However, the current in-depth research and achievements are mainly concentrated on several driver NGs, and classical cancer drugs have been directly used to target the NG domains, such as the kinase domain of BCR-ABL1 fusion protein in leukemia. Some of the fusion proteins with retaining DNA-binding domains such as transcription factors can directly bind their target genes, such as the EWSR1-FLI fusion actively recruiting BAF complex. Recently, the downstream effectors of driver FGs have emerged as therapeutic targets. For example, targeting the downstream CCND2 inhibited RUNX1/ETO-driven leukemic expansion in vitro and in vivo and inhibition of STAT5, the downstream factor of NUP214-ABL1 led to the induction of leukemia cell death. However, the functions of most identified FGs have not been systematically investigated. This is mainly due to the limitations of traditional tools and the high cost of experimental procedures. Therefore, there is an urgent need to develop new tools for analyzing NG breakpoint-specific features systemically in the human genome and predict their originating and regulatory mechanisms, such as upstream and downstream effectors. In-depth annotation based on NG structure is important for understanding the cellular mechanisms of NGs. Effective use of systematic bioinformatics tools for functional annotation can provide a deeper insight into the role of NGs in the development and progression of diseases such as cancers to find direct and indirect therapeutic targets. In this study, we will develop five bioinformatics tools for the functional annotation and feature analysis of NGs, a predictive pipeline for automatic analysis of downstream effects of NGs, and a predictive method for tracing the origin of NGs. This project will be a substantial contribution to public health by systematicallydeep annotating thefunction of new genes (NGs) in cancer and neurodegenerative diseases such as Alzheimer's disease. These systematic annotation results will be performed through ChimerAnno (a tool for functional annotation of human chimeric genes), FGviewer (a tool for visualizing multi-tiered functional features of fusion genes), NGeffector with DeepChIP (a tool for predicting NG downstream effectors with enhanced transcription factor binding site prediction with deep learning), DeepCLIP (a tool for providing evidence of origin of NGs for trans-splicing mechanism) and hBPAI (a tool for feature extraction of human new genes' breakpoint (BP)). The application of these approaches on all kinds of human BPs of new genes will be integrated into NewGeneDB, New Gene annotation Database. This study will advance our knowledge in NG regulatory mechanisms in diseases and open up the possibility of novel treatments of cancer tools and platforms for analyzing NGs. and other diseases in the future by providing powerful",Functional annotation of new genes aided by deep learning,10029297,R35GM138184,"['ABL1 gene', 'Achievement', 'Alzheimer&apos', 's Disease', 'Antineoplastic Agents', 'Binding', 'Binding Sites', 'Biological Markers', 'CCND2 gene', 'Cell Death', 'Chimeric Proteins', 'Complex', 'DNA Binding Domain', 'Databases', 'Development', 'Disease', 'Disease Progression', 'Drug Targeting', 'EWSR1 gene', 'Future', 'Gene Structure', 'Genes', 'Human', 'Human Genome', 'In Vitro', 'Individual', 'Knowledge', 'Leukemic Cell', 'Malignant Neoplasms', 'Methods', 'NUP214 gene', 'Neurodegenerative Disorders', 'Phosphotransferases', 'Procedures', 'Public Health', 'RUNX1 gene', 'Recruitment Activity', 'Regulator Genes', 'Research', 'Role', 'Source', 'Stat5 protein', 'Tissues', 'Trans-Splicing', 'Transcript', 'base', 'bioinformatics tool', 'cancer therapy', 'chimeric gene', 'cost', 'deep learning', 'feature extraction', 'fusion gene', 'gene function', 'in vivo', 'insight', 'leukemia', 'novel', 'precision medicine', 'predictive tools', 'therapeutic target', 'tool', 'transcription factor']",NIGMS,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,R35,2020,334778,-0.03446347735715134
"Clinical Research Education in Genome Science (CREiGS) Project Summary/Abstract  The sensitivity and availability of omic technologies have enabled the genomic, transcriptomic and proteomic characterization of disease phenotypes, at the tissue and even the single cell level. This has allowed development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. Patients of providers who have participated in these educational initiatives also benefit as it allows for more rapid integration of genomic study findings into the clinical care setting. Thus, in response to PAR-19-185, we propose to develop and implement the Clinical Research Education in Genome Science (CREiGS) program that will not only focus on the analysis of genomic data, but also on gene-expression data, the integration of these two data types, as well as introductory theory and application of statistical and machine learning methods. Specifically we propose to accomplish the following specific aims: 1. Develop and successfully implement the online and in-person phases of CREiGS to increase the methodologic ingenuity by which researchers tackle important genomics-related clinical problems. 2. Establish a Diversity Recruitment External Advisory Board to ensure that the most effective strategies are employed to recruit URM doctoral students, postdoctoral fellows, and faculty from academic institutions nationwide into CREiGS. 3. Enhance the dissemination phase of CREiGS by packaging and uploading the asynchronous lectures and the online critical thinking/problem solving assessments with solutions for publicly available, online teaching resources. 4. Implement effective methods to evaluate the efficacy of CREiGS by examining:1) the participants' grasp of the CREiGS core competencies, 2) the clarity and quality of the curriculum, 3) program logistics and operation, and 4) the participants' short-term and long-term success attributed to participation in CREiGS. In summary, we posit that CREiGS will provide participants with a solid foundation in genomics science to answer complex, clinical questions. We believe that CREiGS supports the mission of the NHGRI by providing researchers with rigorous training to “accelerate medical breakthroughs that improve human health.” Project Narrative The sensitivity and availability of omic technologies have allowed for the development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. The overall goal of the Clinical Research Education in Genome Science program is to increase the methodologic ingenuity of students, postdoctoral fellows, and faculty from academic institutions nationwide through a solid foundation in genomics science to answer complex, clinical research questions and improve patient care.",Clinical Research Education in Genome Science (CREiGS),9934567,R25HG011021,"['Area', 'Biomedical Research', 'Cells', 'Clinical', 'Clinical Data', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Critical Thinking', 'Data', 'Data Analyses', 'Development', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Hour', 'Human', 'Hybrids', 'Institution', 'Knowledge', 'Logistics', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Human Genome Research Institute', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Persons', 'Phase', 'Phenotype', 'Play', 'Postdoctoral Fellow', 'Problem Solving', 'Proteomics', 'Provider', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Role', 'Single Nucleotide Polymorphism', 'Solid', 'Statistical Methods', 'Students', 'Technology', 'Tissues', 'Training', 'Translational Research', 'Treatment outcome', 'Underrepresented Minority', 'Underserved Population', 'Update', 'cancer therapy', 'clinical care', 'clinical efficacy', 'computerized tools', 'data integration', 'data management', 'disease phenotype', 'disorder subtype', 'doctoral student', 'education research', 'genetic analysis', 'genome sciences', 'genomic data', 'grasp', 'health disparity', 'improved', 'individual patient', 'innovation', 'lectures', 'machine learning method', 'operation', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'response', 'statistical and machine learning', 'success', 'theories', 'therapy development', 'tool', 'transcriptomics', 'treatment optimization', 'virtual']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R25,2020,161662,-0.01927367628296489
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,10124880,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,50000,0.0024301540439596358
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9980967,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,1100000,0.0024301540439596358
"What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives,10162151,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Participant', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,100000,0.004389619662409906
"Visualization, modeling and validation of chromatin interaction data The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Several recent high- throughput technologies based on Chromatin Conformation Capture (3C) have emerged (such as 4C, 5C, Hi-C and ChIA-PET) and given us an unprecedented opportunity to study the higher-order genome organization. Among them, Hi-C technology is of particular interest due to its unbiased genome-wide coverage that can measure chromatin interaction intensities between any two given genomic loci. However, Hi-C data analysis and interpretation are still in the early stages. One of the main challenges is how to efficiently visualize chromatin interaction data, so that the scientific community to visualize and use it for their own research. In addition, due to the complex experimental procedure and high sequencing cost, Hi-C has only been performed in a limited number of cell/tissue types. Finally, the underlying mechanism of chromatin interactions remains largely unclear. Therefore, the PI will propose the following aims: Aim 1. Build an interactive and customizable 3D genome browser. We will build an interactive and customizable 3D browser, which allows users to navigate Hi-C data and other high-throughput chromatin organization data, including ChIA-PET and Capture Hi-C. We have built a prototype of the 3D genome browser (www.3dgenome.org). Our browser will allow users to conveniently browse chromatin interaction data with other data types (such as ChIP-Seq and RNA-Seq) from the genomic region in the same window simultaneously. Our system will also empower the users to create their own session and query their own Hi-C and other epigenomic data. Aim 2. Impute chromatin interaction using other genomic/epigenomic information. We will predict Hi-C interaction frequencies using other available genomic and epigenomic data in the same cell type, such as ChIP-Seq data for histone modifications and transcription factors. We will build our prediction model and then systematically impute Hi-C interaction matrices for all 127 cell types whose epigenomes are available thanks to recent effort by the ENCODE and Roadmap Epigenome projects. Aim 3. Perform validation experiments for computational method in aim 1 and 2. We will perform 20 3C experiments in hESC and GM cell lines, coupled with genome engineering by CRISPR/Cas9, to evaluate Hi-C prediction method in aim 2. The three dimensional (3D) organization of mammalian genomes is tightly linked to gene regulation, as it can reveal the physical interactions between distal regulatory elements and their target genes. Although several recent high-throughput technologies including Hi-C have emerged and given us an unprecedented opportunity to study 3D chromatin interaction in high resolution, its analysis and interpretation are still in the early stages. Here we propose to develop a suite of statistical modeling and computational methods to model and validate chromatin interaction using other genomic/epigenomics data, and build an interactive and customizable 3D genome browser.","Visualization, modeling and validation of chromatin interaction data",9840492,R01HG009906,"['3-Dimensional', 'Address', 'CRISPR/Cas technology', 'Cell Line', 'Cell physiology', 'Cells', 'ChIP-seq', 'Chromatin', 'Chromatin Interaction Analysis by Paired-End Tag Sequencing', 'Chromatin Remodeling Factor', 'Chromosome Territory', 'Communities', 'Complex', 'Computer Models', 'Computing Methodologies', 'Country', 'Coupled', 'Data', 'Data Analyses', 'Distal', 'Elements', 'Environment', 'Event', 'Frequencies', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome engineering', 'Genomic Segment', 'Genomics', 'Intuition', 'Knock-out', 'Learning', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Molecular', 'Procedures', 'Regulator Genes', 'Regulatory Element', 'Research', 'Resolution', 'Statistical Models', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Validation', 'Visit', 'Visualization', 'base', 'cell type', 'chromosome conformation capture', 'convolutional neural network', 'cost', 'epigenome', 'epigenomics', 'experimental study', 'genome annotation', 'genome browser', 'genome-wide', 'genomic locus', 'high throughput technology', 'histone modification', 'human embryonic stem cell', 'interest', 'mammalian genome', 'performance tests', 'predictive modeling', 'prototype', 'random forest', 'repository', 'transcription factor', 'transcriptome sequencing', 'web site']",NHGRI,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,395000,0.004683795753653977
"Computational modeling of spatial genome organization and gene regulation PROJECT SUMMARY/ABSTRACT The three-dimensional (3D) organization of the genome plays an essential role in genome stability, gene regulation, and many diseases, including cancer. The recent development of high-throughput chromatin conformation capture (Hi-C) and its variants provide an unprecedented opportunity to investigate higher-order chromatin organization. Despite the rapidly accumulating resources for investigating 3D genome organization, our understanding of the regulatory mechanisms and functions of the genome organization remain largely incomplete. Hi-C analyses and 3D genome research are still in their early stage and face several challenges. First, high-resolution chromatin contact maps require extremely deep sequencing and hence have been achieved only for a few cell lines. Second, it is computationally challenging to complement 3D genome structure with one-dimensional (1D) genomic and epigenomic features. Third, recent studies have just begun to infer associations between chromatin interactions and genetic variants and to identify potential target genes of those variants at the genome-wide scale. Given these challenges and my unique multi-disciplinary training, my long-term research goal is to develop innovative computational and statistical methods to uncover the interplay between 3D genome structure and function. Speciﬁcally, in the next ﬁve years, I will i) develop computational approaches to enhance the resolution of existing Hi-C data and investigate ﬁne-scale 3D genome architecture as well as its spatiotemporal dynamics and ii) build scalable and interpretable machine learning models that leverage 1D epigenomic data to predict cell type-speciﬁc 3D chromatin interactions and gene expression and elucidate the function of 3D genome organization in gene regulation and human diseases. The completion of the proposed work will deepen our knowledge of 3D genome architecture as well as its functions in gene regulation and disease. PROJECT NARRATIVE The overarching mission of my research is to understand the interplay between genome architecture and gene regulation. Recent development of high-throughput chromatin conformation capture techniques has allowed us to look beyond the nucleotide sequence of DNA and investigate the principles of higher-order chromatin organization. I will develop innovative computational and statistical strategies to investigate 3D genome organization at an unprecedented scale, thereby elucidating the impacts of genome organization on gene regulation and disease.",Computational modeling of spatial genome organization and gene regulation,9999005,R35GM133678,"['3-Dimensional', 'Architecture', 'Base Sequence', 'Cell Line', 'Chromatin', 'Complement 3d', 'Computer Models', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Development', 'Dimensions', 'Disease', 'Face', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genome Stability', 'Genomics', 'Goals', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mission', 'Modeling', 'Play', 'Research', 'Resolution', 'Resources', 'Role', 'Statistical Methods', 'Structure', 'Techniques', 'Training', 'Variant', 'Work', 'cell type', 'chromosome conformation capture', 'deep sequencing', 'epigenomics', 'genetic variant', 'genome-wide', 'human disease', 'innovation', 'multidisciplinary', 'spatiotemporal']",NIGMS,UNIVERSITY OF CALIFORNIA RIVERSIDE,R35,2020,378309,-0.005153911472360548
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9990809,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'statistical and machine learning', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2020,332952,0.011304429795329859
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9870944,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2020,397125,0.032845051240561446
"HERCULES: Exposome Research Center PROJECT SUMMARY: HERCULES The vision of the HERCULES P30 is to demonstrably advance the role of environmental health sciences in clinical and public health settings using the platform of the exposome. Healthcare and biomedical research have become increasingly genome-centric. While much of this is due to the impressive achievements in genomics, which have consistently outpaced gains in environmental health, it is our contention that a more persuasive case needs to be made for environmental factors. Science and intuition support the idea that the environment plays just as large of a role as genetics for the majority of diseases. The exposome, which embraces a strategy and scale similar to genomic research, is poised to elevate the environment in discussions of health and disease. We will continue to grow and enhance the environmental health science research portfolio at Emory through cutting-edge technologies and innovative data solutions. We will build upon the superb relationships we have built with the local community and continue to push the mission of NIEHS on campus and across the scientific landscape. Based on the extraordinary progress over our first three years, we propose to retain our theme to use exposome-related concepts and approaches to improve human health. This simple and unifying vision will continue to stimulate discovery, promote collaboration, and enhance communication through the following Specific Aims: Specific Aim 1. To marshal physical and intellectual resources to support exposome-related approaches (high-resolution metabolomics, analytical chemistry, systems biology, machine learning, bioinformatics, high-throughput toxicology, and spatial and temporal statistical models) through cores, pilot funding, mentoring, and research forums. Specific Aim 2. To make major contributions towards exposome and environmental health science research. Specific Aim 3. To provide career development activities around innovative and emerging concepts and approaches related to the exposome. Specific Aim 4. To enhance and expand existing relationships with community partners to resolve environmental health issues in the community using exposome principles. Specific Aim 5. To provide infrastructure and resources to facilitate rapid translation of novel scientific findings into the development of prevention and treatment strategies in humans. Pursuit of HERCULES' aims will advance environmental health sciences within our institutions and in the scientific community. PROJECT NARRATIVE: HERCULES Human health and disease is dictated by a combination of genetic and environmental factors. The HERCULES Center is focused on providing a more comprehensive assessment of these environmental influences by utilizing exposome-based concepts and approaches.",HERCULES: Exposome Research Center,9902428,P30ES019776,"['Achievement', 'Analytical Chemistry', 'Award', 'Bioinformatics', 'Biomedical Research', 'Climate', 'Clinical', 'Collaborations', 'Communication', 'Communities', 'Community Outreach', 'Core Facility', 'Data', 'Data Science Core', 'Development', 'Discipline', 'Disease', 'Environment', 'Environmental Health', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Care Research', 'Health Sciences', 'Human', 'Individual', 'Infrastructure', 'Institution', 'Intuition', 'Leadership', 'Letters', 'Machine Learning', 'Marshal', 'Mentors', 'Mission', 'National Institute of Environmental Health Sciences', 'Phase', 'Play', 'Prevention strategy', 'Productivity', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Statistical Models', 'Strategic Planning', 'Systems Biology', 'Technology', 'Toxicology', 'Translations', 'Update', 'Vision', 'base', 'career development', 'catalyst', 'health science research', 'improved', 'innovation', 'metabolomics', 'novel', 'operation', 'ranpirnase', 'treatment strategy']",NIEHS,EMORY UNIVERSITY,P30,2020,1492946,0.028213666708367405
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9928344,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2020,1186500,0.03759634353387125
"Advanced algorithms to infer and analyze 3D genome structures Project Summary For the past decade, the population-cell Hi-C technique has significantly improved our ability to discover genome-wide DNA proximities. However, because population Hi-C is based on a pool of cells, it will not help us reveal each single cell's 3D genome structure or understand cell-to-cell variability in terms of 3D genome structure and gene regulation. It is also difficult to achieve a high resolution, such as 1 Kbp, with population Hi- C; therefore, when finding and analyzing the spatial interactions for the promoter or enhancer regions typically associated with biologically-important regulatory elements, population Hi-C data's resolution is too low to be useful. Moreover, while we know that the CTCF-cohesin complex plays a key role in the formation of genome 3D structures, the question is whether long non-coding RNAs (lncRNAs) are involved in the process since lncRNAs have been found to recruit proteins needed for chromatin remodeling, and our preliminary research has found that lncRNA LINC00346 directly interacts with CTCF. Finally, while members of the bioinformatics community, including the PI, have developed many algorithms to reconstruct 3D genome structures based on population Hi-C data, important questions still must be answered regarding how 3D genome structures are involved in gene regulation and whether there are relationships between 3D genome structures and genetic and epigenetic features. The PI proposes to conduct leading research to overcome these challenges and address these questions. During the next five years, the PI will develop algorithms to reconstruct the 3D whole- genome structures for single cells and analyze cell-to-cell variabilities in terms of 3D genome structure and gene regulation. The PI will develop a deep learning algorithm to enhance the resolution of population Hi-C data to that of Capture Hi-C data (1 Kbp) so that we can make good use of the large amount of Hi-C data accumulated in the past decade. An online database will be built to allow the community to access both population and single-cell 3D genome structures in an integrated way. The PI will work with a cancer biologist to discover any lncRNAs that function as a scaffold to fine-tune the CTCF-cohesin protein complex, as well as two neuron scientists to develop a more complete understanding of gene regulation while considering 3D genome and other genetic and epigenetic features. Given the PI's track record and productivity, having three computational goals and two collaborative goals is not only feasible but computationally and biologically rewarding. In five years, once the proposed studies are accomplished, the PI should have established a uniquely independent place in the field of 3D genome, maintaining leading positions in inferring single-cell 3D genome structures, enhancing Hi-C data resolution, and building 3D genome databases, while establishing similar positions in reconstructing high-resolution 3D genome structures, finding lncRNAs' roles in the formation of genome structures, and understanding how 3D genome structures are involved in gene regulation. Project Narrative The three-dimensional (3D) structure of genome is critically important for gene regulation; and the abnormal 3D genome structures are often associated with diseases such as cancer. This proposal aims to reconstruct and analyze the 3D genome structures of thousands of individual cells, study how 3D genome structures participate in gene regulation, determine whether long noncoding RNAs (lncRNAs) are involved in the formation of genome structures, and build an online knowledge base integrating 3D genome structures with other biological information.",Advanced algorithms to infer and analyze 3D genome structures,10027542,R35GM137974,"['3-Dimensional', 'Address', 'Algorithms', 'Bioinformatics', 'Biological', 'Cells', 'Communities', 'Complex', 'DNA', 'Data', 'Databases', 'Disease', 'Enhancers', 'Epigenetic Process', 'Gene Expression Regulation', 'Genetic', 'Genome', 'Goals', 'Individual', 'Malignant Neoplasms', 'Neurons', 'Other Genetics', 'Play', 'Population', 'Positioning Attribute', 'Process', 'Productivity', 'Proteins', 'Regulatory Element', 'Research', 'Resolution', 'Rewards', 'Role', 'Scientist', 'Structure', 'Techniques', 'Untranslated RNA', 'Work', 'base', 'chromatin remodeling', 'cohesin', 'deep learning algorithm', 'genome database', 'genome-wide', 'improved', 'knowledge base', 'member', 'promoter', 'protein complex', 'recruit', 'scaffold', 'three dimensional structure', 'whole genome']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R35,2020,354694,0.004264380381270743
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades — such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases — have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9876220,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2020,479215,-0.004093649279005045
"A database for high-resolution chromatin contact maps and human genetic variants Abstract After the completion of the Human Genome Project, several landmarking consortia have accumulated large amounts of genomic data towards understanding the functions of human genome. The ENCODE project has annotated genome-wide regulatory elements. The Roadmap Epigenomic project has characterized tissue-speciﬁc variation in epigenetic state. The NIH Common Fund GTEx project has delineated tissue-speciﬁc gene expression and transcription regulation. The NIH Common Fund 4D Nucleome (4DN) project has revealed dynamic 3D chromatin organization in many cell and tissue types. Each of the aforementioned consortia has generated thousands or even tens of thousands of datasets, and provided different insights regarding human genome at an unprecedent scale and depth. However, the datasets generated from these consortia are isolated in terms of cell types and tissue types covered, how the data are stored, and the resolution of the genomic data. These gaps bring realistic data analysis challenges to biomedical researchers when they use these public datasets jointly in their research — they need to go through different data portals with heterogeneous processing pipelines, different data formats, and unmatched resolutions. We aim to develop the most cutting-edge deep learning approaches to impute high-resolution chromatin contact maps, and integrate the high-resolution chromatin contact maps with transcriptional data available from GTEx project and epigenomic data from ENCODE/Roadmap. We plan to share the integrated data on a public web server with a multi-panel interactive visualization genome browser. The integrated data will provide an important resource for understanding of tissue-speciﬁc genetic variation in the light of the spatial organization of these genomic and epigenomic elements and their functional implications. Project Narrative The goal of this project is to develop novel computational methods to integrate 4DN datasets with GTEx datasets and ENCODE/Roadmap datasets. The integrated datasets will be critical resource to unveil the mechanisms of the genetic variants identiﬁed in genome-wide association studies. The new knowledge gained here could help us understand the genetic basis of many human diseases.",A database for high-resolution chromatin contact maps and human genetic variants,10109293,R03OD030599,"['3-Dimensional', 'Address', 'Area', 'Base Pairing', 'Cells', 'Chromatin', 'Chromatin Structure', 'Communities', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Elements', 'Epigenetic Process', 'Funding', 'Gene Expression Regulation', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Internet', 'Knowledge', 'Maps', 'Molecular', 'Nucleosomes', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Technology', 'Tissue-Specific Gene Expression', 'Tissues', 'Transcriptional Regulation', 'United States National Institutes of Health', 'Variant', 'Visualization', 'Work', 'cell type', 'data format', 'data integration', 'data portal', 'data resource', 'data visualization', 'deep learning', 'epigenomics', 'expectation', 'genetic variant', 'genome annotation', 'genome browser', 'genome sciences', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'insight', 'interest', 'next generation sequencing', 'novel', 'open data', 'web server']",OD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R03,2020,265495,-0.018339928189208488
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9858390,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data infrastructure', 'data integration', 'data standards', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'large scale data', 'member', 'mouse genome', 'multiple data types', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2020,2000000,0.029534531573562322
"Computational approaches for identifying epigenomic contexts of somatic mutations ABSTRACT During normal development, aging, and diseases such as cancer, DNA damage due to endogenous and external factors, and repair defects result in accumulation of different types of somatic mutations including single nucleotide substitutions, small InDels, copy number alterations, translocations, and ploidy changes. While a vast majority of somatic mutations in the genome are not disease drivers, their patterns of genetic changes and associated context can provide insights into past exposure to mutagens, mechanisms of DNA damage and repair defects, and extent of genomic instability, which are important for understanding disease etiology, minimizing hazardous environmental exposure, and also predicting efficacy of emerging treatment strategies such as immunotherapy. A number of mutation signatures have been identified based on local sequence contexts to address this need. But, mechanisms of DNA damage and repair preferences depend on both local sequence and epigenomic contexts, and it remains to be understood whether epigenomic contexts of emerging mutation signatures can provide critical, complementary etiological insights at a genome-wide scale, which are not apparent from sequence contexts alone. This is of fundamental importance, because (i) etiology of many of the emerging mutation signatures is currently unknown, (ii) DNA damage response and repair depends on tissue contexts, and defects in core DNA repair genes often result in cancer development in tissue-specific manner, and (iii) differences in the extent of DNA damage and repair between stem and differentiated cells within the same tissues have consequences for aging and disease incidence rates. Built logically on our previous works, we propose to develop computational approaches to determine the impact of epigenomic contexts on the patterns of somatic mutations within and across tissue types, and validate computational predictions using targeted experiments. In Aim-1, we will develop an epigenomic context preference map for emerging mutation signatures. In Aim-2, we will determine the basis of tissue-dependent differences in mutation profiles attributed to DNA repair defects. In Aim-3, we will predict the extent of cell lineage-dependent patterns of mutation accumulation from the mutational landscape of terminal cells. I am currently an early stage investigator, and the proposal is aligned with my long-term goal to identify fundamental principles of mutability and evolvability of somatic genomes. Our project will deliver novel resources and knowledge for addressing questions regarding genomic integrity during development and aging, and diseases such as cancer. ! PUBLIC HEALTH RELEVANCE: The proposed project will use computational biology approaches to determine epigenomic context preference for somatic mutations, and use that to infer tissue-dependent changes in mutation patterns. Our results will provide fundamental insights into aspects of genome maintenance, which is important for advancing our understanding of cancer etiology, reducing exposure to mutagenic factors, and also predicting efficacy of emerging treatment strategies. !",Computational approaches for identifying epigenomic contexts of somatic mutations,9902467,R01GM129066,"['Address', 'Affect', 'Aging', 'Biometry', 'Blood', 'Cancer Etiology', 'Cancer Relapse', 'Cell Differentiation process', 'Cell Line', 'Cell Lineage', 'Cells', 'Chromatin', 'Clinical', 'Computational Biology', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'Data', 'Defect', 'Development', 'Disease', 'Doctor of Philosophy', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Evolution', 'Exposure to', 'Genome', 'Genomic DNA', 'Genomic Instability', 'Genomics', 'Goals', 'Immunotherapy', 'Incidence', 'Knowledge', 'Least-Squares Analysis', 'Location', 'Maintenance', 'Malignant Neoplasms', 'Maps', 'Modeling', 'Mutagenesis', 'Mutagens', 'Mutation', 'Nuclear', 'Nucleotides', 'Pathway interactions', 'Pattern', 'Ploidies', 'Point Mutation', 'Process', 'Publishing', 'Radiation Tolerance', 'Research Personnel', 'Resources', 'Role', 'Somatic Mutation', 'Source', 'Tissues', 'Work', 'base', 'cancer genomics', 'computer framework', 'epigenomics', 'experimental study', 'genome integrity', 'genome-wide', 'human tissue', 'improved', 'insertion/deletion mutation', 'insight', 'markov model', 'medical schools', 'novel', 'preference', 'public health relevance', 'random forest', 'repaired', 'response', 'stem', 'stem cells', 'tissue stem cells', 'transcriptomics', 'treatment strategy']",NIGMS,RBHS -CANCER INSTITUTE OF NEW JERSEY,R01,2020,324350,-0.037561906610610124
"A Proteogenomic Search Engine for Direct Mass Spectrometric Identification of Variant Proteins Using Genomic Data Project Summary / Abstract Mass spectrometry-based proteomics, used in conjunction with genomics, has been called proteogenomics. Recent exponential increases in variant identification by next-generation sequencing (NGS) is redefining the concept of the human genome/proteome. Our project is the commercialization of a first-to-market proteomic database search engine for mass spectrometry capable of directly reading NGS data for the identification of mutilations from individual samples or from curated resources. Such an offering has the potential to bring together these two fields, enabling validation of mutations at the protein-level. Mutated proteins have been shown to make ideal targets for drug therapies and diagnostics in cancer. Our software will provide an intuitive user experience, approachable by scientists who may not be expert both proteomic and genomic data analysis. Since the search engine is guided by prior knowledge, performance exceeds current practice. The software will come complete with a full array of post-processing validation, and visualization tools. Project Narrative The detection of protein variants, which differ from those predicted from the reference human genome sequence, can make ideal candidates for the development of targeted treatments and diagnostics for many clinical conditions such as cancer. This project proposes the development a first-of-its-kind “proteogenomics” search engine for the identification of protein variants by mass spectrometry, making direct use of genomic data and ever-growing public and private databases of genetic variation.",A Proteogenomic Search Engine for Direct Mass Spectrometric Identification of Variant Proteins Using Genomic Data,10082114,R44CA217432,"['Agreement', 'Cancer Vaccines', 'Clinical', 'Communities', 'Complement', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Databases', 'Deposition', 'Detection', 'Development', 'Diagnostic', 'Environment', 'Future', 'Gefitinib', 'Genetic Databases', 'Genetic Diseases', 'Genetic Variation', 'Genomics', 'Goals', 'Guidelines', 'Heterozygote', 'Human Genome', 'Individual', 'Informatics', 'Intuition', 'Ions', 'Isomerism', 'Knowledge', 'Licensing', 'Malignant Neoplasms', 'Marketing', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Modification', 'Monitor', 'Mutate', 'Mutation', 'Mutation Detection', 'Pathway interactions', 'Peptides', 'Performance', 'Phase', 'Post Translational Modification Analysis', 'Privatization', 'Probability', 'Protein Databases', 'Proteins', 'Proteome', 'Proteomics', 'Publications', 'Readability', 'Reading', 'Research', 'Resolution', 'Resources', 'Risk', 'Running', 'Sales', 'Sampling', 'Scientist', 'Services', 'Small Business Innovation Research Grant', 'Testing', 'Transcript', 'Validation', 'Variant', 'Visualization', 'Visualization software', 'Work', 'base', 'commercialization', 'cost', 'experience', 'genomic data', 'graphical user interface', 'human reference genome', 'improved', 'next generation sequencing', 'open source', 'protein expression', 'proteogenomics', 'prototype', 'repository', 'scaffold', 'search engine', 'support vector machine', 'targeted treatment', 'tool', 'transcriptome sequencing']",NCI,"SPECTRAGEN INFORMATICS, LLC",R44,2020,529294,-0.030414648363699958
"Computational Methods for Next-Generation Comparative Genomics PROJECT SUMMARY Recent advances in regulatory genomics, especially 3D genome organization in cell nucleus, suggest that existing methods for cross-species comparisons are limited in their ability to fully understand the evolution of non-coding genome function. In particular, it is known that genomes are compartmentalized to distinct compartments in the nucleus such as nuclear lamina and nuclear speckles. Such nuclear compartmentalization is an essential feature of higher-order genome organization and is linked to various important genome functions such as DNA replication timing and transcription. Unfortunately, to date no study exists that directly compares nuclear compartmentalization between human and other mammals. In addition, there are no computational models available that consider the continuous nature of multiple features of nuclear compartmentalization and function, which is critical to integrate genome-wide functional genomic data and datasets that measure cytological distance to multiple compartments across species. In this project, we will develop novel algorithms and generate new datasets to directly address two key questions: (1) How to identify the evolutionary patterns of nuclear compartmentalization? (2) What types of sequence evolution may drive spatial localization changes across species? The proposed project represents the first endeavor in comparative genomics for nuclear compartmentalization. Our Specific Aims are: (1) Developing new probabilistic models for identifying evolutionary patterns of nuclear compartmentalization. (2) Identifying genome-wide evolutionary patterns of nuclear compartmentalization in primate species based on TSA-seq and Repli-seq. (3) Developing new algorithms to connect sequence features to nuclear compartmentalization through cross-species comparisons. Successful completion of these aims will result in novel computational tools and new datasets that will be highly valuable for the comparative genomics community. Integrating the new computational tools and unique datasets will provide invaluable insights into the relationship between sequence evolution and changes in nuclear genome organization in mammalian species. Therefore, the proposed research is expected to advance comparative genomics to a new frontier and provide new perspectives for studying human genome function PROJECT NARRATIVE The proposed research is relevant to public health because the outcome of the project is expected to enhance the analyses of nuclear genome organizations across primate species to better understand genome function and human biology. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Computational Methods for Next-Generation Comparative Genomics,9959498,R01HG007352,"['3-Dimensional', 'Address', 'Algorithms', 'CRISPR/Cas technology', 'Cell Nucleus', 'Cells', 'Communities', 'Complement', 'Computer Models', 'Computing Methodologies', 'Crete', 'Cytology', 'DNA Insertion Elements', 'DNA Replication Timing', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genetic Transcription', 'Genome', 'Genomics', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Knowledge', 'Lamin Type B', 'Link', 'Machine Learning', 'Mammals', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular Profiling', 'Nature', 'Nuclear', 'Nuclear Lamina', 'Outcome', 'Pattern', 'Phenotype', 'Primates', 'Psyche structure', 'Public Health', 'Research', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Time', 'Translating', 'United States National Institutes of Health', 'Untranslated RNA', 'Visualization', 'base', 'comparative genomics', 'computerized tools', 'frontier', 'functional genomics', 'genetic variant', 'genome-wide', 'genomic data', 'genomic locus', 'improved', 'insight', 'mental function', 'next generation', 'novel', 'predictive modeling']",NHGRI,CARNEGIE-MELLON UNIVERSITY,R01,2020,360268,0.025060484570913973
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,-0.01134077503123524
"Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation Project Summary/Abstract: With the surge of large genomics data, there is an immense increase in the breadth and depth of different omics datasets and an increasing importance in the topic of privacy of individuals in genomic data science. Detailed genetic and environmental characterization of diseases and conditions relies on the large-scale mining of functional genomics data; hence, there is great desire to share data as broadly as possible. However, there is a scarcity of privacy studies focused on such data. A key first step in reducing private information leakage is to measure the amount of information leakage in functional genomics data, particularly in different data file types. To this end, we propose to to derive information-theoretic measures for private information leakage in different data types from functional genomics data. We will also develop various file formats to reduce this leakage during sharing. We will approach the privacy analysis under three aims. First, we will develop statistical metrics that can be used to quantify the sensitive information leakage from raw reads. We will systematically analyze how linking attacks can be instantiated using various genotyping methods such as single nucleotide variant and structural variant calling from raw reads, signal profiles, Hi-C interaction matrices, and gene expression matrices. Second, we will study different algorithms to implement privacy-preserving transformations to the functional genomics data in various forms. Particularly, we will create privacy-preserving file formats for raw sequence alignment maps, signal track files, three-dimensional interaction matrices, and gene expression quantification matrices that contain information from multiple individuals. This will allow us to study the sources of sensitive information leakages other than raw reads, for example signal profiles, splicing and isoform transcription, and abnormal three-dimensional genomic interactions. Third, we will investigate the reads that can be mapped to the microbiome in the raw human functional genomics datasets. We will use inferred microbial information to characterize private information about individuals, and then combine the microbial information with the information from human mapped reads to increase the re-identification accuracy in the linking attacks described in the second aim. We will use the tools to quantify the sensitive information and privacy-preserving file formats in the available datasets from large sequencing projects, such as the ENCODE, The Cancer Genome Atlas, 1,000 Genomes, gEUVADIS, and Genotype-Tissue Expression projects. Project Narrative: Sharing large-scale functional genomics data is critical for scientific discovery, but comes with important privacy concerns related to the possible misuse of such data. This proposal will quantify and manage the rieslkasted to releasing functional genomics datasets, based on integrating inferred genotypes from the raw sequence files, signal tracks, and microbiome mapped sequences. Finally, we will develop file formats, statistical methodologies, and related software for anonymization of functional genomics data that enable open sharing.",Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation,9970939,R01HG010749,"['3-Dimensional', 'Address', 'Algorithms', 'Assessment tool', 'Biology', 'ChIP-seq', 'Code', 'Computer software', 'Consent', 'DNA sequencing', 'Data', 'Data Files', 'Data Science', 'Data Set', 'Databases', 'Diet', 'Disease', 'Environment', 'Equilibrium', 'Extravasation', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype', 'Genotype-Tissue Expression Project', 'Glean', 'Human', 'Individual', 'Institutes', 'Laws', 'Learning', 'Letters', 'Life Style', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Mining', 'Motivation', 'Participant', 'Patients', 'Phenotype', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Protein Isoforms', 'Protocols documentation', 'Provider', 'Pythons', 'Quantitative Trait Loci', 'RNA Splicing', 'Research Personnel', 'Risk', 'Risk Assessment', 'Sampling', 'Sequence Alignment', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Smoker', 'Source', 'Structure', 'Techniques', 'The Cancer Genome Atlas', 'Tissues', 'Variant', 'base', 'clinically relevant', 'computerized data processing', 'data mining', 'data sharing', 'experimental study', 'file format', 'functional genomics', 'genome sequencing', 'genomic data', 'human tissue', 'interest', 'large datasets', 'microbial', 'microbiome', 'open data', 'privacy preservation', 'social', 'tool', 'transcriptome sequencing']",NHGRI,YALE UNIVERSITY,R01,2020,523409,0.0017512902559945144
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",10116927,U01HG009086,"['Accounting', 'Affect', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Validation', 'Variant', 'analysis pipeline', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic architecture', 'genetic testing', 'genetic variant', 'genome sciences', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2020,864183,0.016791894022335684
"Novel Statistical methods for DNA Sequencing Data, and applications to Autism. Summary One of the major problems in human genetics is understanding the genetic causes underlying complex phenotypes, including neuropsychiatric traits such as autism spectrum disorders and schizophrenia. Despite tremendous work over the past few decades, the underlying biological mechanisms are poorly understood in most cases. Recent advances in high-throughput, massively parallel genomic technologies have revolutionized the field of human genetics and promise to lead to important scientific advances. Despite this progress in data generation, it remains very challenging to analyze and interpret these data. The main focus of this proposal is the development of powerful statistical methods for the integration of whole-genome sequencing data with rich functional genomics data with the goal to improve the discovery of genes involved in autism spectrum disorders. We propose to integrate data from many different sources, including epigenetic data from projects such as ENCODE, Roadmap, and PsychENCODE, eQTL data from the GTEx, PsychENCODE and CommonMind consortia, data from large scale databases of genetic variation such as ExAC and gnomAD, in order to predict functional effects of genetic variants in non-coding genetic regions in a tissue and cell type specific manner, and generate functional maps across large number of tissues and cell types in the human body that we can then use to identify novel associations with autism in whole-genome sequencing studies. The proposed functional predictions and functional maps will be broadly available in the popular ANNOVAR database. We further propose to use these functional predictions in the analysis of almost 20,000 whole genomes from three large whole genome sequencing studies for autism. We believe that the proposed research is very timely and has the potential to substantially improve the analysis of non-coding genetic variation, and hence provide new insights into the biological mechanisms underlying risk to autism, and more broadly to other neuropsychiatric diseases. Narrative Autism Spectrum Disorders are common diseases with major impact on public health. Although coding variation has been extensively studied for its role in affecting risk to autism, the analysis of non-coding variation poses tremendous challenges. The proposed statistical methods and their applications to nearly 20,000 whole genomes from three large autism whole genome sequencing studies will improve our understanding of the biological mechanisms involved in autism with important implications for disease treatment strategies.","Novel Statistical methods for DNA Sequencing Data, and applications to Autism.",9923466,R01MH095797,"['Affect', 'Anterior', 'Biochemical', 'Biological', 'Biological Assay', 'Brain region', 'Chromatin', 'Code', 'Collection', 'Complex', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Encyclopedia of DNA Elements', 'Epigenetic Process', 'Generations', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Human Genetics', 'Human body', 'Individual', 'International', 'Lead', 'Maps', 'Measures', 'Methods', 'Molecular', 'Phenotype', 'Prefrontal Cortex', 'Public Health', 'Research', 'Risk', 'Role', 'Schizophrenia', 'Scientific Advances and Accomplishments', 'Source', 'Statistical Methods', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Variant', 'Work', 'autism spectrum disorder', 'cell type', 'cingulate cortex', 'data integration', 'design', 'epigenomics', 'exome', 'frontal lobe', 'functional genomics', 'gene discovery', 'genetic variant', 'genome sequencing', 'genome-wide', 'genomic data', 'histone modification', 'improved', 'insight', 'large scale data', 'large-scale database', 'neuropsychiatric disorder', 'neuropsychiatry', 'novel', 'software development', 'supervised learning', 'tool', 'trait', 'treatment strategy', 'whole genome']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,446831,-0.0015462172904811688
"Mapping RNA polymerase in tissue samples with ChRO-seq. PROJECT ABSTRACT Deciphering how complex programs of gene expression and regulation contribute to human disease is one of the major challenges facing the field of genomics. Over the past decade, a wealth of new high-throughput genomics tools have revolutionized how we identify active genomic regions and appear poised to make great strides in understanding the mechanisms of disease. Yet the application of most of these technologies has been limited to established cell lines. Currently, approaches being developed to comprehensively map functional elements across the genome involve combining data from several different genome-wide experimental assays, making them expensive and impractical to use in clinical isolates of limited quantity or even to analyze new cell lines. Compounding these technical difficulties, gene expression is a complex and highly tissue dependent biological process, and many important applications will require the direct interrogation of clinical isolates or other similarly limited sources of sample. Thus, efficient new tools that map the repertoire of functional elements across the genome are likely to transform the biomedical and clinical sciences.  We propose to develop Chromatin Run-On and Sequencing (ChRO-seq) and a suite of computational tools for mapping transcription directly in limited tissue samples. Our approach uses a single genome-wide molecular assay to efficiently identify the location of promoters and enhancers, transcription factor binding sites, gene and lincRNA boundaries, transcription levels, and impute certain histone modifications. Preliminary ChRO-seq data reveals patterns of transcription that are virtually identical to those using Precision Run on and Sequencing in cultured cells, but can easily be applied in solid tissue samples. We applied our preliminary ChRO-seq technology to several primary tumors, revealing new insights into how transcriptional regulation underlies cancer development and progression, and providing a key proof-of-concept motivating further technology development. We anticipate that ChRO-seq and the computational methods proposed will enable the efficient discovery of functional elements in virtually any cell sample. In addition, ChRO-seq has the unique advantage that it can be applied in limited tissue samples and clinical isolates even after the degradation of mRNA. PROJECT NARRATIVE We propose to develop a suite of molecular and computational technologies that allow researchers to directly measure transcriptional regulation of genes, enhancers, and lincRNAs in limited clinical isolates. These technologies are anticipated to have a major impact on the biomedical sciences, enabling the genome-wide interrogation of transcription during virtually any disease process for the first time.",Mapping RNA polymerase in tissue samples with ChRO-seq.,9850271,R01HG009309,"['Archives', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biological Process', 'Cell Line', 'Cells', 'ChIP-seq', 'Chromatin', 'Clinical', 'Clinical Sciences', 'Code', 'Complex', 'Computing Methodologies', 'Consumption', 'Cultured Cells', 'DNA Sequence', 'DNA-Directed RNA Polymerase', 'Data', 'Deoxyribonuclease I', 'Detection', 'Development', 'Disease', 'Elements', 'Enhancers', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Gold', 'Hypersensitivity', 'Location', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Molecular', 'Molecular Computations', 'Nuclear', 'Outcome', 'Pattern', 'Performance', 'Population', 'Primary Neoplasm', 'Process', 'Protocols documentation', 'RNA', 'RNA Polymerase I', 'Regulatory Element', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Solid', 'Source', 'Specimen', 'Speed', 'Technology', 'Time', 'Tissue Sample', 'Tissues', 'Transcriptional Regulation', 'Untranslated RNA', 'computational suite', 'computerized tools', 'deep learning', 'established cell line', 'experimental study', 'genome-wide', 'genomic tools', 'histone modification', 'human disease', 'improved', 'innovation', 'insight', 'mRNA Transcript Degradation', 'next generation', 'novel', 'predictive tools', 'programs', 'promoter', 'scale up', 'technology development', 'tool', 'transcription factor', 'virtual']",NHGRI,CORNELL UNIVERSITY,R01,2020,387500,-0.018885429089746514
"Privacy-preserving genomic medicine at scale 1 Project Summary  2  3 High-throughput sequencing, biomedical imaging, and electronic health record technologies are 4 generating health-related datasets of unprecedented scale. Integrative analysis of these  5 resources promises to reveal new biology and drive personal and precision medicine. Yet, the  6 sensitive nature of these data often requires that they be kept in isolated silos, limiting their 7 usefulness to science. The goal of this project is to develop innovative privacy-preserving  8 algorithms to enable data sharing and drive genomic medicine. Crucially, we will draw upon our  9 past success in secure genome analysis and algorithmic expertise in computational biology to 10 address the imminent need to perform complex integrative analyses securely and at scale. 11 Current privacy-preserving tools are prohibitively too costly to perform the complex 12 calculations required in genomic analysis. We previously leveraged the highly structured nature 13 of biological data and novel optimization strategies to implement efficient pipelines for secure 14 genome-wide association studies (GWAS) and drug interaction predictions which scaled to 15 millions of samples. In this project, we will further exploit the unique properties of biomedical data 16 to: (i) develop secure integrative analysis methods for genomic medicine; (ii) develop an easy-to- 17 use programming environment with advanced automated optimizations to facilitate the adoption 18 of privacy-preserving analyses; and (iii) promote the use of our privacy techniques to gain novel 19 biological insights through large-scale collaborative genetic studies of multi-ethnic cohorts. 20 With co-I’s Amarasinghe (MIT) and Cho (Broad Institute), we aim to apply these tools to 21 realize the first multi-institution, multi-national secure genetic studies with our partners at the 22 Swiss Personalized Health Network, UK Biobank, Finnish FinnGen, All of Us, NIH NCBI, Broad 23 and Barcelona Supercomputing Center (Letters of Support). We will also use our privacy- 24 preserving approaches to study genomic origins of polygenic traits for disease as well as 25 neuroimaging and other clinical phenotypes. We will continue to actively integrate our methods 26 into community standards (MPEG-G, GA4GH). 27 Successful completion of these aims will result in computational methods and open-source, 28 easy-to-use, production-grade implementations that open the door to secure integration and 29 analysis of massive sets of sensitive genomic and clinical data. With input from our collaborations, 30 we will build these tools and apply them to better understand the molecular causes of human 31 health and its translation to the clinic. Project Narrative Combining genomic and health-related data from millions of patients will empower the development of clinically relevant measures of human health and disease risks. However, this task requires securely sharing sensitive data at an immense scale beyond what existing cryptographic platforms can achieve. Here we develop novel computational methods to enable biomedical data integration, analysis, and interpretation in a privacy-preserving and highly scalable manner.",Privacy-preserving genomic medicine at scale,9998648,R01HG010959,"['Address', 'Adoption', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biological', 'Biology', 'Clinic', 'Clinical Data', 'Collaborations', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Data Analyses', 'Data Pooling', 'Data Security', 'Data Set', 'Disease', 'Drug Interactions', 'Electronic Health Record', 'Engineering', 'Environment', 'Genetic', 'Genetic study', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Institutes', 'Institution', 'Knowledge', 'Letters', 'Machine Learning', 'Mainstreaming', 'Measures', 'Medical Imaging', 'Medical Records', 'Medicine', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Patients', 'Performance', 'Pharmacology', 'Polygenic Traits', 'Privacy', 'Process', 'Production', 'Property', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Science', 'Secure', 'Security', 'Software Engineering', 'Software Tools', 'Standardization', 'Stream', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Time', 'Translations', 'United States National Institutes of Health', 'Work', 'analysis pipeline', 'base', 'biobank', 'bioimaging', 'clinical development', 'clinical phenotype', 'clinically relevant', 'cohort', 'computer framework', 'cost', 'cryptography', 'data analysis pipeline', 'data integration', 'data sharing', 'data warehouse', 'disorder risk', 'epidemiology study', 'experimental study', 'genome analysis', 'genome wide association study', 'genomic data', 'health data', 'innovation', 'insight', 'monomethoxypolyethylene glycol', 'neuroimaging', 'novel', 'open source', 'polygenic risk score', 'precision medicine', 'preservation', 'privacy preservation', 'statistics', 'success', 'task analysis', 'theories', 'tool']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,636185,0.010552382026383141
"Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration PROJECT SUMMARY The cell nucleus is a heterogeneous organelle that consists of nuclear bodies such as nuclear lamina, speckles, nucleoli and PML bodies. These structures continuously tether and tug chromatin at the small and large scales to synergistically orchestrate dynamic functions in distinct spatio-temporal compartments. A major obstacle to the production of navigable 4D reference maps and relating structure to function in the nucleus remains understanding how these different scales of organization influence each other. In particular, we have a poor understanding of the large-scale genome organization. Growing evidence suggests that such nuclear compartmentalization is causally connected with vital genome functions in human health and disease. However, the principles of this nuclear compartmentalization, its dynamics during changes in cell conditions, and its functional relevance are poorly understood. One lesson from Phase 1 4DN was the huge gap in throughput between imaging methods, that directly measure large-scale multi-landmark relationships, and genomic methods, that aim for whole genome high-resolution maps but are indirect measurements and provide limited information about large-scale compartments. For this 4DN UM1 Center application, we propose to meet these needs through the following Aims: (1) Generate multi-modal imaging and genomic datasets to reveal the structure, dynamics, and function of nuclear compartmentalization; (2) Develop and apply computational tools for data-driven genome structure modeling and integrative analysis of nuclear compartmentalization; (3) Develop an integrative analysis and visualization platform with navigable 4D reference maps of nuclear organization. The combined datasets and results of our proposed approaches will advance our understanding of nuclear compartmentalization, the interwoven connections among different nuclear components, and their functional significance. Our new integrative analysis tools and data-driven predictive models will produce more complete nuclear organization reference maps that integrate large-scale chromosome structure data from live and super-resolution microscopy with multi-modal genomic data including smaller scale chromatin interaction maps and predict functional relationships and dynamic responses. Our navigable reference maps will be publicly accessible through an analysis platform that provides interactive visualization of multiple data types, thus enabling investigators with diverse expertise to simultaneously explore their own data and related datasets/tools and promoting collaborations that will open new horizons into the role of the 4D nucleome in human health and disease. PROJECT NARRATIVE The proposed research is relevant to public health because it will enhance our understanding of nuclear genome organization and functions that are increasingly being linked to health and disease. Because we develop tools to disseminate this information and enable others to work with our data and their own data, we will also bring nuclear architecture to bear on a broad range of ongoing health related research. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration,10156141,UM1HG011593,"['Address', 'Architecture', 'Atlases', 'Binding', 'Biochemical', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Chromatin', 'Chromatin Loop', 'Chromatin Structure', 'Chromosome Structures', 'Chromosomes', 'Collaborations', 'Communities', 'Complement', 'Computing Methodologies', 'Cytology', 'DNA Replication Timing', 'Data', 'Data Set', 'Development', 'Disease', 'Formulation', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Interphase Chromosome', 'Intuition', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Methods', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nuclear', 'Nuclear Lamina', 'Nuclear Structure', 'Organelles', 'Outcome', 'Output', 'Phase', 'Population', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Structural Models', 'Structure', 'Technology', 'Three-Dimensional Imaging', 'United States National Institutes of Health', 'Ursidae Family', 'Validation', 'Variant', 'Visualization', 'Work', 'base', 'cell cycle genetics', 'cell type', 'computer framework', 'computerized tools', 'data exploration', 'data integration', 'data tools', 'experimental study', 'genome-wide', 'genomic data', 'histone modification', 'imaging modality', 'improved', 'insight', 'machine learning algorithm', 'mental function', 'multimodal data', 'multimodality', 'multiple data types', 'multiscale data', 'predictive modeling', 'response', 'spatiotemporal', 'structured data', 'tool', 'transcription factor', 'transcriptome sequencing', 'user-friendly', 'whole genome']",NHGRI,CARNEGIE-MELLON UNIVERSITY,UM1,2020,2075409,0.019628401800288825
"Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen The impact of fungal pathogens on human health is devastating. They infect billions of people worldwide, and kill more than 1.5 million each year. The most vulnerable are people with reduced immune function, such as those with HIV or those undergoing immune suppressing treatments for cancer or organ transplants. One of the most pervasive fungal pathogens is Candida albicans, which kills almost 40% of people suffering from bloodstream infections. Treating these infections is extremely difficult, as fungi are closely related to humans and there are very few drugs that kill the fungus without host toxicity. With the emergence of drug resistance, the development of new therapeutic strategies is now crucial. To address this important clinical need and identify new antifungal drug targets, it is critical to uncover mechanisms that enable C. albicans to cause life-threatening human disease.  We are one of the first academic labs to obtain a powerful resource that will allow us to test the function of almost every gene in the C. albicans genome. This resource includes a collection of double barcoded heterozygous mutants covering ~90% of the genome, and a collection of strains covering ~40% of the genome where the expression of the remaining wild-type allele of a gene is governed by the tetracycline-repressible promoter. This resource provides an unprecedented opportunity to identify genes that control key virulence traits such as morphogenesis. It also enables the identification of determinants of commensalism and virulence, and to further elucidate the molecular mechanisms involved. We have optimized a functional genomics platform for massively parallel analysis of fungal virulence traits using next generation sequencing to quantify the relative proportion of each barcoded strain in pooled assays. We have also optimized high- resolution image analysis of cellular morphology and structures, and assays for identifying genes important for commensalism, virulence, and interaction with host immune cells. Our studies will provide the first global analysis of C. albicans morphogenesis, commensalism, and virulence.  Our studies will: 1) develop a computational platform to predict C. albicans essential genes, and expand the collection of tetracycline-repressible conditional expression strains to cover most non-essential genes, since genes required for pathogen viability in vitro provide little insight into mechanisms of host adaptation or virulence; 2) identify novel regulators of key virulence traits such as morphogenesis; and 3) identify determinants of C. albicans host adaptation and virulence on a genome scale. This work will provide an expanded functional genomics resource to advance the field, and will leverage this resource to elucidate the genes and genetic networks governing morphogenesis and virulence. This will reveal new strategies to cripple fungal pathogens, and drug targets to improve clinical outcome. The focus of this proposal is to elucidate mechanisms governing morphogenesis, commensalism, and virulence of the fungal pathogen Candida albicans. This eukaryotic pathogen has a profound impact on human health, with a global incidence of Candida bloodstream infections of ~400,000 cases, associated with crude mortality rates of 42% and attributable mortality rates of 27%; C. albicans is the most prevalent Candida species in almost all patient populations. The central hypotheses that govern this proposal are that functional genomic analysis of C. albicans using genome-scale libraries of double barcoded heterozygous gene deletion mutants and tetracycline-repressible conditional expression strains will reveal novel circuitry governing morphogenesis, commensalism, and virulence, and that targeting this circuitry has therapeutic potential for combating life-threatening fungal infections.","Systematic Analysis of Morphogenesis, Commensalism, and Virulence in a Leading Human Fungal Pathogen",9985737,R01AI127375,"['Address', 'Alleles', 'Aneuploidy', 'Antifungal Agents', 'Antifungal Therapy', 'Ascomycota', 'Bar Codes', 'Biological Assay', 'Biology', 'Candida', 'Candida albicans', 'Cells', 'Cellular Morphology', 'Cellular Structures', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Cues', 'Cytolysis', 'Data Set', 'Development', 'Diploidy', 'Drug Targeting', 'Drug resistance', 'Environment', 'Essential Genes', 'Filament', 'Foundations', 'Gastrointestinal tract structure', 'Gene Deletion', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Library', 'Genomic approach', 'Genomics', 'Goals', 'Growth', 'HIV', 'Health', 'Human', 'Image Analysis', 'Immune', 'Immune system', 'Immunocompromised Host', 'In Vitro', 'Incidence', 'Individual', 'Infection', 'Investments', 'Laboratories', 'Libraries', 'Life', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Morphogenesis', 'Mus', 'Mutation', 'Mycoses', 'Nosocomial Infections', 'Organ Transplantation', 'Organism', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Property', 'Publishing', 'Resistance development', 'Resolution', 'Resources', 'Sepsis', 'Solid', 'Symbiosis', 'Systemic infection', 'Testing', 'Tetracyclines', 'Therapeutic', 'Toxic effect', 'Virulence', 'Work', 'Yeast Model System', 'Yeasts', 'attributable mortality', 'cancer therapy', 'cancer transplantation', 'combat', 'computational platform', 'fitness', 'functional genomics', 'fungus', 'gene replacement', 'genome-wide', 'genomic platform', 'high resolution imaging', 'human disease', 'immune function', 'improved', 'insight', 'member', 'mortality', 'mucosal microbiota', 'mutant', 'new therapeutic target', 'next generation sequencing', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'pathogen', 'pathogenic fungus', 'patient population', 'programs', 'promoter', 'quantitative imaging', 'response', 'trait', 'unpublished works']",NIAID,UNIVERSITY OF TORONTO,R01,2020,545004,-0.00420108711667114

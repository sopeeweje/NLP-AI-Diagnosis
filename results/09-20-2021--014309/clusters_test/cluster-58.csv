text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"SCH: A Computer Vision and Lens-Free Imaging System for Automatic Monitoring of Infections Automated monitoring and screening of various physiological signals is an indispensable tool in modern medicine. However, despite the  preponderance of long-term monitoring and screening modalities for certain vital signals, there are a significant number of applications for  which no automated monitoring or screening is available. For example, patients in need of urinary catheterization are at significant risk of  urinary tract infections, but long-term monitoring for a developing infection while a urinary catheter is in place typically requires a caregiver to  frequently collect urine samples which then must be transported to a laboratory facility to be tested for a developing infection. Disruptive  technologies at the intersection of lens-free imaging, fluidics, image processing, computer vision and machine learning offer a tremendous  opportunity to develop new devices that can be connected to a urinary catheter to automatically monitor urinary tract infections. However, novel  image reconstruction, object detection and classification, and deep learning algorithms are needed to deal with challenges such as low image  resolution, limited labeled data, and heterogeneity of the abnormalities to be detected in urine samples. This project brings together a multidisciplinary team of computer scientists, engineers and clinicians to design, develop and test a system that integrates lens-free imaging, fluidics, image processing, computer vision and machine learning to automatically monitor urinary tract infections. The system will take a urine sample as an input, image the sample with a lens-free microscope as it flows through a fluidic channel, reconstruct the images using advanced holographic reconstruction algorithms, and detect and classify abnormalities, e.g., white blood cells, using advanced computer vision and machine learning algorithms. Specifically, this project will: (1) design fluidic and optical hardware to appropriately sample urine from patient lines, flow the sample through the lens-free imager, and capture holograms of the sample; (2) develop holographic image reconstruction algorithms based on deep network architectures constrained by the physics of light diffraction to produce high quality images of the specimen from the lens-free holograms; (3) develop deep learning algorithms requiring a minimal level of manual supervision to detect various abnormalities in the fluid sample that might be indicative of a developing infection (e.g., the presence of white bloods cells or bacteria); and (4) integrate the above hardware and software developments into a system to be validated on urine samples obtained from patient discards against standard urine monitoring and screening methods. RELEVANCE (See instructions):  This project could lead to the development of a low-cost device for automated screening and monitoring of urinary tract infections (the most  common hospital and nursing home acquired infection), and such a device could eliminate the need for patients or caregivers to manually collect  urine samples and transport them to a laboratory facility for testing and enable automated long-term monitoring and screening for UTIs. Early  detection of developing UTIs could allow caregivers to preemptively remove the catheter before the UTI progressed to the point of requiring  antibiotic treatment, thus reducing overall antibiotic usage. The technology to be developed in this project could also be used for screening  abnormalities in other fluids, such as central spinal fluid, and the methods to detect and classify large numbers of cells in an image could lead to  advances in large scale multi-object detection and tracking for other computer vision applications. n/a",SCH: A Computer Vision and Lens-Free Imaging System for Automatic Monitoring of Infections,10162472,R01AG067396,"['Algorithms', 'Antibiotic Therapy', 'Antibiotics', 'Bacteria', 'Bacteriuria', 'Caregivers', 'Catheters', 'Cations', 'Cells', 'Cerebrospinal Fluid', 'Classification', 'Clinical', 'Computer Vision Systems', 'Computers', 'Data', 'Detection', 'Development', 'Devices', 'Diagnostic', 'Diffusion', 'Early Diagnosis', 'Engineering', 'Erythrocytes', 'Evaluation', 'Goals', 'Hospital Nursing', 'Image', 'Infection', 'Instruction', 'Knowledge', 'Label', 'Laboratories', 'Lead', 'Leukocytes', 'Light', 'Lighting', 'Liquid substance', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Microscope', 'Modality', 'Modern Medicine', 'Monitor', 'Nursing Homes', 'Optics', 'Patients', 'Performance', 'Physics', 'Physiological', 'Prevalence', 'Principal Investigator', 'Procedures', 'Process', 'Resistance', 'Resolution', 'Risk', 'Sampling', 'Scientist', 'Signal Transduction', 'Specimen', 'Supervision', 'Surface', 'System', 'Technology', 'Testing', 'Training', 'Urinalysis', 'Urinary Catheterization', 'Urinary tract infection', 'Urine', 'base', 'biological heterogeneity', 'classification algorithm', 'cost', 'deep learning algorithm', 'design', 'diffraction of light', 'heterogenous data', 'hologram', 'image processing', 'image reconstruction', 'imager', 'imaging system', 'laboratory facility', 'lens', 'machine learning algorithm', 'multidisciplinary', 'network architecture', 'novel', 'particle', 'reconstruction', 'screening', 'software development', 'tool', 'urinary']",NIA,JOHNS HOPKINS UNIVERSITY,R01,2021,283097
"A Machine Learning Alternative to Beamforming to Improve Ultrasound Image Quality for Interventional Access to the Kidney Project Summary  Despite the widespread prevalence of ultrasound imaging in hospitals today, the clinical utility of ultrasound guidance is severely hampered by clutter and reverberation artifacts that obscure structures of interest and com- plicate anatomical measurements. Clutter is particularly problematic in overweight and obese individuals, who account for 78.6 million adults and 12.8 million children in North America. Similarly, interventional procedures of- ten require insertion of one or more metal tools, which generate reverberation artifacts that obfuscate instrument location, orientation, and geometry, while obscuring nearby tissues, thus additionally hampering ultrasound im- age quality. Although artifacts are problematic, ultrasound continues to persist primarily because of its greatest strengths (i.e., mobility, cost, non-ionizing radiation, real-time visualization, and multiplanar views) in comparison to existing image-guidance options, but it would be signiﬁcantly more useful without problematic artifacts.  Our long-term project goal is to use state-of-the-art machine learning techniques to provide interventional radiologists with artifact-free ultrasound-based images. We will initially develop a new framework alternative to the ultrasound beamforming process that removes needle tip reverberations and acoustic clutter caused by multipath scattering in near-ﬁeld tissues when guiding needles to the kidney to enable removal of painful kidney stones. Our ﬁrst aim will test convolutional neural networks (CNNs) that input raw channel data and output human readable images with no artifacts caused by multipath scattering and reverberations. A secondary goal of the CNNs is to learn the minimum number of parameters required to create these new CNN-based images. Our second aim will validate the trained algorithms with ultrasound data from experimental phantom and ex vivo tissue. Our third aim will extend our evaluation to ultrasound images of in vivo porcine kidneys. This work is the ﬁrst to propose bypassing the entire beamforming process and replacing it with machine learning and computer vision techniques to remove traditionally problematic noise artifacts and create a fundamentally new type of artifact-free, high-contrast, high-resolution, ultrasound-based image for guiding interventional procedures.  This work combines the expertise of an imaging scientist, a computer scientist, and an interventional ra- diologist to explore an untapped, understudied area that is only recently made feasible through improvements in computing power, advances in computer vision capabilities, and new knowledge about dominant sources of image degradation. Translation to in vivo cases is enabled by our clinical collaboration with the Department of Radiology at the Johns Hopkins Hospital. With support from the NIH Trailblazer Award, our team will be the ﬁrst to develop these tools and capabilities to eliminate noise artifacts in interventional ultrasound, opening the door to a new paradigm in ultrasound image formation, which will directly beneﬁt millions of patients with clearer, easier-to-interpret ultrasound images. Subsequent R01 funding will customize our innovation to addi- tional application-speciﬁc ultrasound procedures (e.g., breast biopsies, cancer detection, autonomous surgery). Project Narrative Artifacts in ultrasound images, speciﬁcally artifacts caused by multipath scattering and acoustic reverberations (which occur when imaging through the abdominal tissue of overweight and obese patients or visualizing metallic surgical tools), remain as a major clinical challenge. There are no existing solutions to eliminate these artifacts based on today's signal processing techniques. The goal of this project is to step away from conventional signal processing models and instead learn from raw data examples with state-of-the-art machine learning techniques that differentiate artifacts from true signals, and thereby deliver clearer, easier-to-interpret images.",A Machine Learning Alternative to Beamforming to Improve Ultrasound Image Quality for Interventional Access to the Kidney,10170765,R21EB025621,"['Abdomen', 'Acoustics', 'Adult', 'Age', 'Anatomy', 'Area', 'Award', 'Breast biopsy', 'Bypass', 'Cancer Detection', 'Child', 'Clinical', 'Collaborations', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Evaluation', 'Excision', 'Family suidae', 'Funding', 'Geometry', 'Goals', 'Hospitals', 'Human', 'Image', 'Intervention', 'Interventional Ultrasonography', 'Kidney', 'Kidney Calculi', 'Knowledge', 'Learning', 'Location', 'Machine Learning', 'Measurement', 'Metals', 'Modeling', 'Morphologic artifacts', 'Needles', 'Network-based', 'Noise', 'Nonionizing Radiation', 'North America', 'Operative Surgical Procedures', 'Output', 'Overweight', 'Pain', 'Patients', 'Prevalence', 'Procedures', 'Process', 'Radiology Specialty', 'Readability', 'Resolution', 'Scientist', 'Signal Transduction', 'Source', 'Structure', 'Techniques', 'Testing', 'Time', 'Tissues', 'Translations', 'Ultrasonography', 'United States National Institutes of Health', 'Visualization', 'Work', 'algorithm training', 'base', 'convolutional neural network', 'cost', 'image guided', 'image guided intervention', 'imaging scientist', 'improved', 'in vivo', 'innovation', 'instrument', 'interest', 'metallicity', 'obese patients', 'obese person', 'radiologist', 'signal processing', 'tool']",NIBIB,JOHNS HOPKINS UNIVERSITY,R21,2021,235027
"Optical design and the development of high accuracy automated tick classification using computer vision Abstract. The incidence of US tick-borne diseases has more than doubled in the last two decades. Due to lack of effective vaccines for tick-borne diseases, prevention of tick bites remains the primary focus of disease mitigation. Tick vector surveillance—monitoring an area to understand tick species composition, abundance, and spatial distribution—is key to providing the public with accurate and up-to-date information when they are in areas of high risk, and enabling precision vector control when necessary. Despite the importance of vector surveillance, current practices are highly resource intensive and require significant labor and time to collect and identify vector specimens. Acarologist or field taxonomist expertise is a limited resource required for tick identification, creating a significant capability barrier for national tick surveillance practice. While mobile applications to facilitate passive surveillance and reporting of human-tick encounters have grown in popularity, variable image quality, limited engagement, and scientist misidentification of rare, invasive, or morphologically similar tick species hinder the scalability of this approach. No automated solutions exist to build tick identification capacity. We seek to develop the first imaging and automated identification system capable of instantaneously and accurately identifying the top nine tick vectors in the US. This proposal will first characterize the optical requirements necessary to image diagnostic morphological features associated with adult ticks and develop a standardized imaging platform for tick identification. This will enable the development of a high-quality tick image dataset in partnership with the Walter Reed Biosystems Unit (WRBU) which will be used to train high-accuracy computer vision models for tick species and sex identification. Ultimately the approaches developed here will enable new tick identification tools for both the lab and citizen scientists; allowing vector surveillance managers to leverage image recognition in a practical system that will increase capacity and capability for biosurveillance, and equipping citizen scientists with improved tools to identify tick species during a human-tick encounter. Project Narrative. Despite the importance of tick vector surveillance for disease prevention, current practices to collect and identify specimens are resource intensive, limiting the quality and quantity of the data informing control efforts. Here we propose the determination of optical requirements for visualization of diagnostic features of the top nine US tick vectors, and the development of high-accuracy computer vision algorithms for the identification of tick species and sex for use in a standardized optical configuration. The high-accuracy tick classification system developed through this proposal promises to expand capacity and capability for tick vector surveillance.",Optical design and the development of high accuracy automated tick classification using computer vision,10325667,R43AI162425,"['Adult', 'Agreement', 'Algorithm Design', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Car Phone', 'Cellular Phone', 'Classification', 'Collaborations', 'Computer Vision Systems', 'Culicidae', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnostic', 'Diagnostic Imaging', 'Disease', 'Disease Surveillance', 'Disease Vectors', 'Future', 'Goals', 'Grain', 'Human', 'Image', 'Incidence', 'Insecta', 'Larva', 'Learning', 'Leg', 'Life', 'Lighting', 'Methods', 'Modeling', 'Molecular', 'Monitor', 'Morphology', 'Nymph', 'Optics', 'Phase', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Resolution', 'Resources', 'Scientist', 'Spatial Distribution', 'Specimen', 'Standardization', 'Surveillance Methods', 'System', 'Telephone', 'Testing', 'Tick-Borne Diseases', 'Ticks', 'Time', 'Training', 'Vaccines', 'Validation', 'Visual', 'Visualization', 'Work', 'base', 'citizen science', 'convolutional neural network', 'design', 'detection method', 'disorder prevention', 'field study', 'flexibility', 'high resolution imaging', 'high risk', 'human disease', 'imaging platform', 'imaging system', 'improved', 'insight', 'intelligent algorithm', 'interest', 'mobile application', 'novel', 'sample collection', 'sex', 'tick bite', 'tool', 'validation studies', 'vector', 'vector control', 'vector tick']",NIAID,"VECTECH, LLC",R43,2021,295705
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,10129965,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2021,403882
"Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit SUMMARY  Many of the estimated four million adults in the U.S. with severe speech and physical impairments (SSPI) resulting from neurodevelopmental or neurodegenerative diseases cannot rely on current assistive technologies (AT) for communication. During a single day, or as their disease progresses, they may transition from one access technology to another due to fatigue, medications, changing physical status, or progressive motor dysfunction. There are currently no clinical or AT solutions that adapt to the multiple, dynamic access needs of these individuals, leaving many people poorly served. This competitive renewal, called BCI-FIT (Brain Computer Interface-Functional Implementation Toolkit) adds to our innovative multidisciplinary translational research conducted over the past 11 years for the advancement of science related to non-invasive BCIs for communication for these clinical populations. BCI-FIT relies on active inference and transfer learning to customize a completely adaptive intent estimation classifier to each user's multiple modality signals in real-time. The BCI-FIT acronym has many implications: our BCI fits to each user's brain signals; to the environment, offering relevant personal language; to the user's internal states, adjusting signals based on drowsiness, medications, physical and cognitive abilities; and to users' learning patterns from BCI introduction to expert use.  Three specific aims are proposed: (1) Develop and evaluate methods for optimizing system and user performance with on-line, robust adaptation of multi-modal signal models. (2) Develop and evaluate methods for efficient user intent inference through active querying. (3) Integrate language interaction and letter/word supplementation as input modalities in real-time BCI use. Four single case experimental research designs will evaluate both user performance and technology performance for functional communication with 35 participants with SSPI in the community, and 30 healthy controls for preliminary testing. The same dependent variables will be tested in all experiments: typing accuracy (correct character selections divided by total character selections), information transfer rate (ITR), typing speed (correct characters/minute), and user experience (UX) questionnaire responses about comfort, workload, and satisfaction. Our goal is to establish individualized recommendations for each user based on a combination of clinical and machine expertise. The clinical expertise plus user feedback added to active sensor fusion and reinforcement learning for intent inference will produce optimized multi-modal BCIs for each end-user that can adjust to short- and long-term fluctuating function. Our research is conducted by four sub-teams who have collaborated successfully to implement translational science: Electrical/computer engineering; Neurophysiology and systems science; Natural language processing; and Clinical rehabilitation. The project is grounded in solid machine learning approaches with models of participatory action research and AAC participation. This project will improve technologies and BCI technical capabilities, demonstrate BCI implementation paradigms and clinical guidelines for people with severe disabilities. PROJECT NARRATIVE The populations of US citizens with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies, as proposed in BCI-FIT. This project implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit,10213005,R01DC009834,"['Adult', 'Attention', 'Behavioral', 'Brain', 'Calibration', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Clinical assessments', 'Cognition', 'Cognitive', 'Communication', 'Communities', 'Computers', 'Custom', 'Data', 'Decision Making', 'Disease', 'Drowsiness', 'Electroencephalography', 'Engineering', 'Environment', 'Eye Movements', 'Fatigue', 'Feedback', 'Goals', 'Guidelines', 'Head Movements', 'Impairment', 'Individual', 'Informed Consent', 'Knowledge', 'Language', 'Learning', 'Letters', 'Life', 'Locked-In Syndrome', 'Machine Learning', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Motor Skills', 'Movement', 'Muscle', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Participant', 'Partner Communications', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Policies', 'Population', 'Protocols documentation', 'Psychological Transfer', 'Psychological reinforcement', 'Public Health', 'Questionnaires', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Design', 'Role', 'Science', 'Secondary to', 'Self-Help Devices', 'Sensory', 'Signal Transduction', 'Solid', 'Source', 'Speech', 'Speed', 'Supplementation', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Vocabulary', 'Workload', 'acronyms', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinical implementation', 'cognitive ability', 'community based participatory research', 'computer science', 'disability', 'experience', 'experimental study', 'improved', 'innovation', 'learning strategy', 'motor disorder', 'multidisciplinary', 'multimodality', 'neurophysiology', 'phrases', 'residence', 'response', 'satisfaction', 'sensor', 'signal processing', 'simulation', 'spelling', 'theories', 'visual tracking']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2021,915264
"SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env PROJECT SUMMARY (See instructions): Chronic wounds affect 6.5 million patients in the U.S., with an estimated treatment cost of $25 billion. Our team proposes research to advance our existing NSF-funded smartphone wound analysis system, which helps patients monitor their diabetic foot ulcers, providing them with instant feedback on healing progress. Our wound system analyzes a smartphone image of the patients' wound, detects the wound area and tissue composition, and generates a proprietary healing score by comparing the current image with a past image. Our envisioned chronic wound assessment system will support evidence-based decisions by the care team while visiting patients, and move wound care toward digital objectivity. We define digital objectivity as the synthesis of wound assessment metrics that are extracted autonomously from images in order to generate objective actionable feedback, enabling clinicians not trained as wound specialists to deliver ""standardized wound care"". Digital objectivity contrasts with the current practice of subjective, visual inspection of wounds based on physician experience. The first aim will develop image processing algorithms to mitigate wound analysis errors caused by non-ideal lighting in some clinical or home settings, and when the wound is photographed from arbitrary camera angles and distance. While our previous wound system worked well in ideal conditions, non-ideal lighting caused large errors and healthy skin was detected as the wound area in extreme cases. The second aim extends our existing wound analysis system that targets only diabetic wounds to handle arterial, venous and pressure ulcers, expanding the potential user. The third aim will synthesize algorithms that autonomously generate actionable wound decision rules that are learned from decisions taken by actual wound clinicians. This research is joint work of Worcester Polytechnic Institute (WPI) (technical expertise in image processing, machine learning and smartphone programming) and University of Massachusetts Medical School (UMMS) (clinical expertise on wounds, and wound patient recruitment to validate our work) RELEVANCE (See instructions): We propose research to advance our existing smartphone wound analysis system, which detects the wound area and tissue composition, and generates a proprietary healing score from a wound image. Our wound assessment system will give patients instant, actionable feedback and enable clinicians not trained as wound specialists to make objective, evidence-based wound care decisions and deliver standardized care.",SCH:Smartphone Wound Image Parameter Analysis and Decision Support in Mobile Env,10066353,R01EB025801,"['Affect', 'Algorithms', 'Area', 'Caring', 'Cellular Phone', 'Clinical', 'Diabetic Foot Ulcer', 'Feedback', 'Funding', 'Home environment', 'Image', 'Institutes', 'Instruction', 'Joints', 'Lighting', 'Machine Learning', 'Massachusetts', 'Patient Monitoring', 'Patient Recruitments', 'Patient imaging', 'Patients', 'Physicians', 'Research', 'Skin', 'Specialist', 'Standardization', 'System', 'Systems Analysis', 'Technical Expertise', 'Tissues', 'Treatment Cost', 'Universities', 'Varicose Ulcer', 'Visit', 'Visual', 'Work', 'base', 'chronic wound', 'decubitus ulcer', 'diabetic ulcer', 'digital', 'evidence base', 'experience', 'healing', 'image processing', 'medical schools', 'standardized care', 'wound', 'wound care']",NIBIB,WORCESTER POLYTECHNIC INSTITUTE,R01,2021,373738

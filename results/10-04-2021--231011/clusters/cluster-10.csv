text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Temporal relation discovery for clinical text Project Summary / Abstract The current proposal continues the investigation on the topic of temporal relation extraction from the Electronic Medical Records (EMR) clinical narrative funded by the NLM since 2010 (Temporal Histories of Your Medical Events, or THYME; thyme.healthnlp.org). Through our efforts so far, we have defined the topic as an active area of research attracting attention across the world. Since its inception, the project has pushed the boundaries of this highly challenging task by investigating new computational methods within the context of the latest developments in the fields of natural language processing (NLP), machine learning (ML), artificial intelligence (AI) and biomedical informatics (BMI) resulting in 60+ publications/presentations. We have made our best performing methods available to the community open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES; ctakes.apache.org). In 2015, 2016, 2017 and 2018, we organized an international shared task (Clinical TempEval) on the topic under the umbrella of the highly prestigious SemEval, thus inviting the international community to work with our THYME data and improve on our results. Clinical TempEval has been highly successful with many participants each year, resulting in new discoveries and many publications. We have made all our data along with our gold standard annotations available to the community through the hNLP Center (center.healthnlp.org).  The underlying theme of this renewal is novel methods for combining explicit domain knowledge (linguistic, semantic, biomedical ontological, clinical), readily available unlabeled data (health-related social media, EMRs), and modern machine learning techniques (e.g. neural networks) for temporal relation extraction from the EMR clinical narrative. Therefore, our renewal proposes a novel and much needed exploration of this line of research:  Specific Aim 1: Develop computational models for novel rich semantic representations such as the Abstract Meaning Representations to encapsulate a single, coherent, full-document graphical representation of meaning for temporal relation extraction  Specific Aim 2: Develop computational methods to infuse domain knowledge (linguistic, semantic, biomedical ontological, clinical) into modern machine learning techniques such as NNs for temporal relation extraction – through input representations, pre-trained vectors, or architectures  Specific Aim 3: Develop novel methods for combining labeled and unlabeled data from various sources (EMR, health-related social media, newswire) for temporal relation extraction from the clinical narrative  Specific Aim 4: Apply the best performing methods for temporal relation extraction developed in SA1-3 to temporally sensitive phenotypes for direct translational sciences studies. Dissemination efforts through publications and open source releases into Apache cTAKES. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EMR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9735964,R01LM010090,"['Address', 'Apache', 'Architecture', 'Area', 'Artificial Intelligence', 'Attention', 'Clinical', 'Cognitive', 'Communities', 'Complex', 'Computer Simulation', 'Computerized Medical Record', 'Computing Methodologies', 'Coupled', 'Data', 'Development', 'Disease', 'Encapsulated', 'Engineering', 'Event', 'Fostering', 'Foundations', 'Funding', 'Goals', 'Gold', 'Health', 'Image', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Label', 'Linguistics', 'Link', 'Machine Learning', 'Medical', 'Methods', 'Modernization', 'Natural Language Processing', 'Nature', 'Participant', 'Patient Care', 'Patients', 'Phenotype', 'Publications', 'Recording of previous events', 'Research', 'Semantics', 'Signs and Symptoms', 'Solid', 'Source', 'Speed', 'Structure', 'System', 'Techniques', 'Text', 'Thyme', 'Time', 'TimeLine', 'Training', 'Translational Research', 'Vision', 'Work', 'advanced disease', 'base', 'biomedical informatics', 'biomedical ontology', 'clinically relevant', 'cohesion', 'electronic data', 'electronic structure', 'epidemiology study', 'improved', 'individualized medicine', 'learning community', 'neural network', 'next generation', 'novel', 'open source', 'programs', 'relating to nervous system', 'social media', 'symptom treatment', 'vector']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2019,626851,0.001766230567720322
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9771473,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Hazard Models', 'Health system', 'Individual', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning algorithm', 'learning strategy', 'multimodality', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'prognostic', 'repository', 'response', 'risk prediction model', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2019,251983,0.015029294017985475
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9818711,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'learning strategy', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2019,320194,0.01605499985278066
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9743225,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2019,528283,0.006427916634859874
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,0.033919004170786665
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9731665,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'informatics\xa0tool', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2019,599962,0.03367087079289189
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9857305,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'machine learning algorithm', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2019,249000,0.018036650318522238
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9855180,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2019,702655,0.005706184536502936
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9676043,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,308000,0.023731959185028784
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. n/a",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9882672,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,149810,0.0220453897837359
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. • ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. • UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types • U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database • St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9853317,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Infrastructure', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'multidimensional data', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2019,855741,0.02705756126834778
"Finding emergent structure in multi-sample biological data with the dual geometry of cells and features    A fundamental question in biomedical data analysis is how to capture biological heterogeneity and characterize the complex spectrum of health states (or disease conditions) in patient cohorts. Indeed, much effort has been invested in developing new technologies that provide groundbreaking collections of genomic information at a single cell resolution, unlocking numerous potential advances in understanding the progression and driving forces of biological states. However, these new biomedical technologies produce large volumes of data, quantified by numerous measurements, and often collected in many batches or samples (e.g., from different patients, locations, or times). Exploration and understanding of such data are challenging tasks, but the potential for new discoveries at a level previously not possible justifies the considerable effort required to overcome these difficulties.  In this project we focus on multi-sample single-cell data, e.g., from a multi-patient cohort, where data points represent cells, data features represent gene expressions or protein abundances, and samples (e.g., considered as separate batches or datasets) represent patients. We consider a duality or interaction between constructing an intrinsic geometry of cells (e.g., with manifold learning techniques) and processing data features as signals over it (e.g., with graph signal processing techniques). We propose the utilization of this duality for several data exploration tasks, including data denoising, identifying noise-invariant phenomena, cluster characterization, and aligning cellular features over multiple datasets. Furthermore, we expect the dual multiresolution organization of data points and features to allow us to compute aggregated signatures that represent patients, and then provide a novel data embedding that reveals multiscale structure from the cellular level to the patient level.  The proposed research combines recent advances in several fields at the forefront of data science, including geometric deep learning, manifold learning, and harmonic analysis. The methods developed in this project will provide novel advances in each of these fields, while also establishing new relations between them. Furthermore, the challenges addressed by these methods are a foundational prerequisite for new advances in genomic research, and more generally in empirical data analysis where data is collected in varying experimental environments. The developed algorithms and methods in this project will be validated in several biomedical settings, including characterizing Zika immunity in Dengue patients, tracking progress of Lyme disease, and predicting the effectiveness of immunotherapy. In this project we will develop new algorithms for biomedical data analysis that will characterize the complex spectrum of health states across various patient populations. These algorithms will leverage the large volumes of data collected by new biomedical technologies, focusing on single-cell data. Specific analysis will be carried out for characterizing Zika immunity in Dengue patients, tracking progress of Lyme disease, and predicting the effectiveness of immunotherapy.",Finding emergent structure in multi-sample biological data with the dual geometry of cells and features   ,9903563,R01GM135929,"['Address', 'Algorithms', 'Biological', 'Biomedical Technology', 'Cells', 'Cellular Structures', 'Collection', 'Complex', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Dengue', 'Disease', 'Effectiveness', 'Environment', 'Foundations', 'Gene Expression', 'Gene Proteins', 'Genomics', 'Geometry', 'Graph', 'Health', 'Immunity', 'Immunotherapy', 'Instruction', 'Learning', 'Location', 'Lyme Disease', 'Measurement', 'Methods', 'Noise', 'Patients', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Structure', 'Techniques', 'Time', 'Zika Virus', 'algorithmic methodologies', 'biological heterogeneity', 'cohort', 'computerized data processing', 'deep learning', 'denoising', 'driving force', 'new technology', 'novel', 'patient population', 'signal processing']",NIGMS,MICHIGAN STATE UNIVERSITY,R01,2019,380650,-0.01111402375881278
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9765194,R44CA206782,"['Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'data warehouse', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'off-patent', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2019,238768,0.0007428684410760132
"Research and development of an open, extensible, web-based information extraction workbench for systematic review Project Summary  1 More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence-based  2 medicine, with each review requiring, on average, between six months to one year of effort to complete. One of the most  3 time consuming and repetitive aspects of this endeavor involves extraction of detailed information from a large number  4 of scientific documents. The specific data items extracted differ among disciplines, but within a given scientific domain,  5 certain data points are extracted repeatedly for each review conducted. Research on use of natural language processing  6 (NLP) for extracting individual data elements has shown that it has the potential to greatly reduce the laborious, time  7 intensive, and repetitive nature of this step. However, there is currently no integrated, automatic data extraction platform  8 that meets the needs of the systematic review community. We propose a web-based data extraction software platform  9 specifically designed for usage in the domain of systematic review. By combining multiple state-of-the-art data extraction 10 methods utilizing NLP, text mining and machine learning, into a single, unified user interface, we will thereby empower 11 the end-user with a powerful and novel tool for automating an otherwise arduous task. 12 The research we propose encompasses three specific aims: (1) develop new data extraction models using deep learning 13 and a new technique called “data programming”; (2) develop a web-based platform to semi-automate the process; (3) 14 design protocols and standards for packaging extraction models as software components and integrating work done by 15 other research groups and vendors. In the first aim, we will contribute novel data extraction modules designed and 16 trained specifically to extract data elements of interest to those conducting systematic reviews in the domain of 17 environmental health. For this research, we will employ state-of-the-art machine learning, NLP and text mining 18 methodologies to train and evaluate several novel extraction components. In our second aim, we will develop a web- 19 based workbench which will allow users to upload scientific documents for automated data extraction. Our system will 20 also be designed to allow for integration of data extraction approaches (components) from other research groups, thus 21 enabling end users to choose from a wide variety of advanced data extraction methodologies within one unified and 22 intuitive software environment. In our third aim, we will develop new protocols to standardize the inputs and outputs 23 for data extraction components. The resulting interface, which will enable seamless integration of third party extraction 24 components into the workbench, will also facilitate the incorporation of feedback from users such that extraction 25 components can be continuously improved based on real-time data. 26 Our overarching goal is to translate emerging semi-automated extraction technologies out of the lab and into practical 27 software and to bring to market both the software itself as well as several premium data extraction components. The 28 results of the research conducted for Aims 1-3 represent the first step in this direction and will provide the foundation for 29 future developments. These result will take us one step closer to the dream of creating “living systematic reviews,” which 30 are maintained using automated or semi-automated methods and updated regularly as new evidence becomes available. Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a flexible, extensible software system that automates the crucial and resource-intensive process of extracting key data elements from scientific documents, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.","Research and development of an open, extensible, web-based information extraction workbench for systematic review",9623091,R43ES029901,"['Communities', 'Computer software', 'Consensus', 'Data', 'Data Element', 'Development', 'Discipline', 'Dreams', 'Environment', 'Environmental Health', 'Evidence Based Medicine', 'Feedback', 'Foundations', 'Future', 'Goals', 'Health', 'Individual', 'Internet', 'Intuition', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Online Systems', 'Output', 'Process', 'Protocols documentation', 'Research', 'Resources', 'Source', 'Standardization', 'Supervision', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Translating', 'Update', 'Vendor', 'Work', 'artificial neural network', 'base', 'data integration', 'deep learning', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'interest', 'learning strategy', 'model design', 'novel', 'research and development', 'software systems', 'systematic review', 'text searching', 'tool']",NIEHS,"SCIOME, LLC",R43,2018,225000,0.012492255807342307
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9532186,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2018,259358,0.015029294017985475
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9614770,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genetic Diseases', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,538700,0.006427916634859874
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,0.043457338852838166
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,0.033919004170786665
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9525704,R01LM010817,"['Automation', 'Case-Control Studies', 'Clinical Trials', 'Cohort Studies', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'clinical care', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,599947,0.03367087079289189
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty. PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,9521525,R01LM011975,"['Address', 'Advocate', 'Affect', 'Affordable Care Act', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Modernization', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'community based participatory research', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'individual patient', 'lexical', 'programs', 'public health relevance', 'recruit', 'support tools', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2018,356845,0.012372922697442012
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9545836,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'deep learning', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2018,569784,-0.00023185377682632187
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9477110,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,548298,0.02934920783136225
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9549126,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2018,81977,0.018036650318522238
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,0.02180805317221066
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9635840,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Awareness', 'Basic Science', 'Biological', 'Biomedical Research', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathogenicity', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2018,854309,0.005706184536502936
NIST Assistance with NTP SR Automation NIEHS seeks advice from NIST in the areas of human language technology and natural language processing component evaluations that support the measurement of systems that automatically extract toxicology information from publications to support the complex human task of systematic review of literature. NIST is positioned to assist NIEHS building upon existing test and evaluation infrastructure through its Text Analysis Conference (TAC) program. NIST is coordinating the 2019 Systematic Review Information Extraction evaluation (SRIE 2019) task for NIEHS as part of the Retrieval Group’s Text Analysis Conference (TAC) program. This coordination includes advising NIEHS on developing annotation guidelines; advising NIEHS on dataset construction and distribution; writing guidelines for the evaluation task; developing scoring methods and supporting software; including the evaluation task as part of the TAC program and call for participation; accepting participant submissions in the evaluation; evaluating those submissions; and reporting results of the evaluation. NIST and NIH will design an evaluation task in this domain. n/a,NIST Assistance with NTP SR Automation,9794240,ES18001002,"['Advertisements', 'Area', 'Automation', 'Complex', 'Computer software', 'Data Set', 'Development', 'Evaluation', 'Guidelines', 'Human', 'Language', 'Measurement', 'National Institute of Environmental Health Sciences', 'Natural Language Processing', 'Participant', 'Positioning Attribute', 'Preparation', 'Publications', 'Reporting', 'Research', 'Research Infrastructure', 'Retrieval', 'Review Literature', 'Scoring Method', 'System', 'Technology', 'Testing', 'Text', 'Toxicology', 'United States National Institutes of Health', 'Writing', 'biomedical informatics', 'design', 'programs', 'symposium', 'systematic review']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,200000,0.0036916457002782598
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9453640,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2018,305500,0.023731959185028784
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9532909,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Exposure to', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'bioinformatics resource', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'individualized medicine', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2018,365327,0.022407099040755076
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9407024,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2018,395628,-0.0028185099461648985
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9392642,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2017,262150,0.015029294017985475
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM. PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9275458,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Evidence based practice', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Medicine', 'Methods', 'Modeling', 'Modernization', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2017,210198,0.015378650166824404
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,0.043457338852838166
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,0.033919004170786665
"From genomics to natural language processing: A protected environment for research computing in the health science NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Health sciences researchers are often required to manage, mine, and analyze restricted patient data (Protected Health Information, PHI) to facilitate and advance their research aims. They are often required to do this without access to central information technology expertise or resources to facilitate their research aims. These researchers are often left to their own devices to “solve” their research compute and data needs and are challenged due to lack of available resources, barriers from central IT, and/or lack of knowledge of available resources. A further challenge is that “small” data sets— data that researchers could formerly handle on office resources—have morphed and grown into the big data domain through the explosion of technical advances and significant expansion in various research directions. Examples include: genomics research, image analysis, simulation, natural language processing, and mining of EMRs. Therefore, the need exists to develop a framework for managing and processing this data securely and reliably. This S10 equipment proposal is to replace the “protected environment” (PE) prototype the University of Utah’s Center for High Performance Computing (CHPC) and Department of Biomedical Informatics built six years ago and has operated since. The PE consists of both high performance computing and virtual machine (VM) components and associated storage sufficient to manage, protect and analyze HIPAA protected health information. This environment has been very successful and has grown significantly in scope. CHPC isolated this protected environment in the secured University of Utah Downtown Data Center and setup a network protected logical partition that provided research groups specific access to individual data sets. As the environment and technology developed, CHPC added additional security features such as two-factor authentication for entry and audit/monitoring. Unfortunately, the prototype has reached the point where demand is surpassing capability and all the hardware is aged and off-warranty. To give an idea of users of the virtual machine farm component, the Biomedical Informatics Core (BMIC) REDCap (Research Electronic Data Capture) environment for data collection has over 2,500 users in 1,500 projects supporting over $25M in NIH funding at the University of Utah, including support for more than 25 active NIH R-01 grants. Moreover, the HIPAA compliant protected environment was a key factor that aided passing the recent University of Utah HIPAA audit. The “protected environment” also helped the University of Utah Health Sciences Center and the BMIC justify the NCATS Center Clinical and Translational Science award (1ULTR001067). NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Project Narrative: The proposed “Protected Environment” instrument will provide research computing and data management capabilities for health sciences researchers to properly manage, secure, and analyze HIPAA regulated protected health information. The technology will not only support a large number of clinical trials, but also enable research in Human Genetics and Natural Language Processing of electronic health records.",From genomics to natural language processing: A protected environment for research computing in the health science,9274445,S10OD021644,"['Award', 'Big Data', 'Clinical Sciences', 'Data', 'Data Collection', 'Data Set', 'Devices', 'Environment', 'Equipment', 'Explosion', 'Farming environment', 'Funding', 'Genomics', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'High Performance Computing', 'Image Analysis', 'Individual', 'Information Technology', 'Knowledge', 'Left', 'Mining', 'Monitor', 'Natural Language Processing', 'Patients', 'Research', 'Research Personnel', 'Resources', 'Secure', 'Security', 'Technology', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Utah', 'aged', 'biomedical informatics', 'computerized data processing', 'electronic data', 'prototype', 'simulation', 'virtual']",OD,UNIVERSITY OF UTAH,S10,2017,493595,-0.019774365299977663
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9282279,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Custom', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Source', 'Standardization', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2017,455519,0.006931062142326403
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9310440,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Custom', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2017,599995,0.03367087079289189
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty. PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,9306183,R01LM011975,"['Address', 'Advocate', 'Affect', 'Affordable Care Act', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Modernization', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Recruitment Activity', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'community based participatory research', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'individual patient', 'lexical', 'programs', 'public health relevance', 'support tools', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2017,368201,0.012372922697442012
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9442241,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,57870,0.02934920783136225
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9266490,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'analytical method', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,552544,0.02934920783136225
"SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system Project Summary More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence- based medicine, with each review requiring, on average, between six months to one year of effort to complete. In order to remain accurate, systematic reviews require regular updates after their initial publication, with most reviews out of date within five years. In the screening phase of systematic review, researchers use detailed inclusion/exclusion criteria to decide whether each article in a set of candidate citations is relevant to the research question under consideration. For each article considered, a researcher reads the title and abstract and evaluates its content with respect to the prespecified criteria. A typical review may require screening thousands or tens of thousands of articles in this manner. Under the assumption that it takes a skilled reviewer 30-90 seconds, on average, to screen a single abstract, dual-screening a set of 10,000 abstracts may require between 150 to 500 hours of labor. We have shown in previous work that automated machine learning methods for article prioritization can reduce by more than 50% the human effort required to screen articles for inclusion in a systematic review. Recently, we have further extended these methods and packaged them into a web-based, collaborative systematic review software application called SWIFT-Active Screener. Active Screener has been used successfully to reduce the effort required to screen articles for systematic reviews conducted at a variety of organizations including the National Institute of Environmental Health Science (NIEHS), the United States Environmental Protection Agency (EPA), the United States Department of Agriculture (USDA), The Endocrine Disruption Exchange (TEDX), and the Evidence Based Toxicology Collaboration (EBTC). These early adopters have provided us with an abundance of useful data and user feedback, and we have identified several areas where we can continue to improve our methods and software. Our goal for the current proposal is to conduct additional research and development to make significant improvements to SWIFT-Active Screener, including several innovations that will be necessary for commercial success. The research we propose encompasses three specific aims: (1) Investigate several improvements to statistical algorithms used for article prioritization and recall estimation. We will explore promising avenues for further improving the performance of our existing algorithms and address critical technical issues that limit the applicability of our current methods (Aim 1 – Improved Statistical Models). (2) Explore ways in which we can improve our models and methods to handle the scenario in which an existing systematic review is updated with new data several years after its initial publication (Aim 2 – New Methods for Systematic Review Updates). (3) Investigate several questions related to scaling the system to support hundreds to thousands of simultaneous screeners (Aim 3 - Software Engineering for Scalability, Usability and Full Text Extraction). Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a web-based, collaborative systematic review software application that uses machine learning to prioritize documents for screening, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.",SWIFT-ActiveScreener: research and development of an intelligent web-based document screening system,9467160,R43ES029001,"['Address', 'Algorithms', 'Area', 'Collaborations', 'Computer software', 'Consensus', 'Data', 'Discipline', 'Endocrine disruption', 'Environmental Health', 'Evidence Based Medicine', 'Exclusion Criteria', 'Feedback', 'Goals', 'Health', 'Hour', 'Human', 'Literature', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'National Institute of Environmental Health Sciences', 'Online Systems', 'Performance', 'Phase', 'Process', 'Publications', 'Research', 'Research Personnel', 'Software Engineering', 'Statistical Algorithm', 'Statistical Models', 'System', 'Text', 'Toxicology', 'United States Department of Agriculture', 'United States Environmental Protection Agency', 'Update', 'Work', 'evidence base', 'improved', 'innovation', 'learning strategy', 'research and development', 'screening', 'success', 'systematic review', 'usability']",NIEHS,"SCIOME, LLC",R43,2017,211900,0.03422640611626623
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9371707,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Databases', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'permissiveness', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2017,93824,0.018036650318522238
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,229691,0.02180805317221066
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9251828,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Modernization', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Standardization', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'interoperability', 'knowledge translation', 'learning progression', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool', 'translational impact']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,428512,0.023826436302358197
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9285168,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2017,267313,0.023731959185028784
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9486059,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Dimensions', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'high dimensionality', 'improved', 'innovation', 'inter-individual variation', 'interoperability', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2017,1408263,0.02705756126834778
"Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.",Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons,9275674,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Docking', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,119777,0.018048344603439632
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9301573,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,216422,0.021882526657894145
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9306202,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Molecular Analysis', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,591990,0.02459470382186923
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9357656,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2017,572778,-0.00023185377682632187
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9337497,R01LM010090,"['Apache', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'Intuition', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translational Research', 'Trees', 'Vision', 'Visualization software', 'Work', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'electronic structure', 'experience', 'individual patient', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'symptom treatment', 'syntax', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2017,643621,-0.003672966185055609
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9326315,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Custom', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'FAIR principles', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'MicroRNAs', 'Mining', 'Modification', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Post-Translational Regulation', 'Process', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Site', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'interoperability', 'kinase inhibitor', 'knowledge base', 'knowledge integration', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2017,370434,0.022407099040755076
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9443736,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,50000,0.0007428684410760132
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9536289,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,589741,0.0007428684410760132
An Integrated Biomedical Dataset Discovery System for Immunological Research Databases  The The Contractor will develop a prototype of an integrated biomedical dataset discovery system for Immunological Research Databases (BIRD) to facilitate integrated search for data/knowledge/tools of interest from DAIT bioinformatics resources. n/a,An Integrated Biomedical Dataset Discovery System for Immunological Research Databases ,9574416,72201700019C,"['Bioinformatics', 'Contractor', 'Data', 'Data Discovery', 'Data Reporting', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Immune system', 'Immunology', 'Ingestion', 'Knowledge', 'Maps', 'Metadata', 'Modeling', 'Natural Language Processing', 'Ontology', 'Research', 'Resources', 'System', 'Techniques', 'Technology', 'Terminology', 'base', 'data integration', 'indexing', 'information organization', 'interest', 'prototype', 'tool', 'user-friendly']",NIAID,"TECHWAVE INTERNATIONAL, INC.",N43,2017,224960,0.004273599833258295
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,9193091,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'International', 'Joints', 'Knowledge', 'Letters', 'Linguistics', 'Literature', 'Manuals', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Planet Earth', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2017,396248,-0.0028185099461648985
"Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) promises to transform the way that physicians treat their patients, resulting in better quality and more consistent care informed directly by the totality of relevant evidence. However, clinicians do not have the time to keep up to date with the vast medical literature. Systematic reviews, which provide rigorous, comprehensive and transparent assessments of the evidence pertaining to specific clinical questions, promise to mitigate this problem by concisely summarizing all pertinent evidence. But producing such reviews has become increasingly burdensome (and hence expensive) due in part to the exponential expansion of the biomedical literature base, hampering our ability to provide evidence-based care.  If we are to scale EBM to meet the demands imposed by the rapidly growing volume of published evidence, then we must modernize EBM tools and methods. More specifically, if we are to continue generating up-to-date evidence syntheses, then we must optimize the systematic review process. Toward this end, we propose developing new methods that combine crowdsourcing and machine learning to facilitate efficient annotation of the full-texts of articles describing clinical trials. These annotations will comprise mark-up of sections of text that discuss clinically relevant fields of importance in EBM, such as discussion of patient characteristics, interventions studied and potential sources of bias. Such annotations would make literature search and data extraction much easier for systematic reviewers, thus reducing their workload and freeing more time for them to conduct thoughtful evidence synthesis.  This will be the first in-depth exploration of crowdsourcing for EBM. We will collect annotations from workers with varying levels of expertise and cost, ranging from medical students to workers recruited via Amazon Mechanical Turk. We will develop and evaluate novel methods of aggregating annotations from such heterogeneous sources. And we will use the acquired manual annotations to train machine learning models that automate this markup process. Models capable of automatically identifying clinically salient text snippets in full-text articles describing clinical trials would be broadly useful for biomedical literature retrieval tasks and would have impact beyond our immediate application of EBM.         PUBLIC HEALTH RELEVANCE: We propose to develop crowdsourcing and machine learning methods to annotate clinically important sentences in full-text articles describing clinical trials Ultimately, we aim to automate such annotation, thereby enabling more efficient practice of evidence- based medicine (EBM).         ",Crowdsourcing Mark-up of the Medical Literature to Support Evidence-Based Medicine and Develop Automated Annotation Capabilities,9076888,UH2CA203711,"['Artificial Intelligence', 'Automated Annotation', 'Caring', 'Characteristics', 'Clinical', 'Clinical Trials', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Development', 'Effectiveness', 'Eligibility Determination', 'Environment', 'Evidence Based Medicine', 'Health', 'Human', 'Hybrids', 'Individual', 'Information Retrieval', 'Intervention', 'Intervention Studies', 'Literature', 'Machine Learning', 'Manuals', 'Mechanics', 'Medical', 'Medical Students', 'Methods', 'Modeling', 'Outcome', 'Paper', 'Participant', 'Patients', 'Physicians', 'Population', 'Population Characteristics', 'Preparation', 'Process', 'Publishing', 'Quality of Care', 'Recruitment Activity', 'Reporting', 'Retrieval', 'Rewards', 'Source', 'System', 'Text', 'Time', 'Training', 'Work', 'Workload', 'abstracting', 'base', 'clinically relevant', 'collaborative environment', 'cost', 'crowdsourcing', 'demographics', 'detector', 'dosage', 'evidence base', 'learning strategy', 'meetings', 'novel', 'phrases', 'public health relevance', 'scale up', 'skills', 'systematic review', 'text searching', 'tool']",NCI,NORTHEASTERN UNIVERSITY,UH2,2016,240650,0.015378650166824404
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9146381,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,457075,0.006931062142326403
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9243496,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,150865,0.006931062142326403
"Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine We hypothesize that a flexible, configurable suite of automated informatics tools can reduce significantly the effort needed to generate systematic reviews while maintaining or even improving their quality. To test this hypothesis, we propose: Aim 1. To extend our research on automated RCT tagging to include additional study types and provide public resources. A) Machine learning models will be created that automatically assign probability estimates to three types of observational studies that are widely examined by systematic reviewers. B) The RCT and other taggers will be evaluated prospectively for newly published PubMed articles. C) All PubMed articles will be automatically tagged for RCT, cohort, case-control and cross-sectional studies and annotated in a public dataset linked to a public query interface. Users will also receive tags for articles from non-PubMed data sources on demand. Aim 2. To evaluate the performance and usability of our tools when used by systematic reviewers under field conditions. A) The tools will be customized and integrated to facilitate field evaluation. B) A three-stage evaluation: 1. Retrospective evaluation of Metta and RCT Tagger performance. 2. Real-time “shadowing”. 3. Prospective controlled study. Aim 3. To identify additional clinical trial articles, appearing after a published systematic review was completed, that are relevant to the review topic. Aim 4. To identify publications related to specific ClinicalTrials.gov registered trials. Aim 5. To develop and evaluate new machine learning methods and tools that will facilitate rapid evidence scoping for new systematic review topics. A) Methods will be developed for ranking articles with respect to their relevance to a proposed new systematic review topic. B) A scoping tool will be created that displays articles ranked by predicted relevance, tagged with study design attributes, sample sizes, and Cochrane risk of bias estimates. The proposed studies will advance the automation of early steps in the process of writing systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care. Project Narrative Systematic reviews are essential for determining which treatments and interventions are safe and effective. At present, systematic reviews are written largely by laborious manual methods. The proposed studies will reduce the time and effort needed to write systematic reviews, and thereby enhance evidence-based medicine and the incorporation of best practices into clinical care.",Text Mining Pipeline to Accelerate Systematic Reviews in Evidence-Based Medicine,9176354,R01LM010817,"['Automation', 'Clinical Trials', 'Controlled Study', 'Cross-Sectional Studies', 'Data Set', 'Data Sources', 'Evaluation', 'Evidence Based Medicine', 'Informatics', 'Intervention', 'Link', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Observational Study', 'Performance', 'Probability', 'Process', 'PubMed', 'Publications', 'Publishing', 'Research', 'Research Design', 'Resources', 'Risk', 'Sample Size', 'Staging', 'Testing', 'Time', 'Writing', 'case control', 'clinical care', 'cohort', 'flexibility', 'improved', 'learning strategy', 'prospective', 'systematic review', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2016,599995,0.03367087079289189
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,0.043457338852838166
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty. PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,9127807,R01LM011975,"['Address', 'Advocate', 'Affect', 'Affordable Care Act', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Lead', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Mullerian duct inhibiting substance', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Recruitment Activity', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'community based participatory research', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'individual patient', 'lexical', 'programs', 'support tools', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2016,362613,0.012372922697442012
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows. Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,9065611,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Process', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'abstracting', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'meetings', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,548438,0.02934920783136225
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,0.02180805317221066
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,0.02180805317221066
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design Our Vision: We propose DeepLink, a versatile data translator that integrate multi-scale, heterogeneous, and multi-source biomedical and clinical data. The primary goal of DeepLink is to enable meaningful bidirectional translation between clinical and molecular science by closing the interoperability gap between models and knowledge at different scales. The translator will enhance clinical science with molecular insights from basic and translational research (e.g. genetic variants, protein interactions, pathway functions, and cellular organization), and enable the molecular sciences by connecting biological discoveries with their pathophysiological consequences (e.g. diseases, signs and symptoms, pharmacological effects, physiological systems). Fundamental differences in the language and semantics used to describe the models and knowledge between the clinical and molecular domains results in an interoperability gap. DeepLink will systematically and comprehensively close this gap. We will begin with the latest technology in semantic knowledge graphs to support an extensible architecture for dynamic data federation and knowledge harmonization. We will design a system for multi-scale model integration that is ontology-based and will combine model execution with prior, curated biomedical knowledge. Our design strategy will be iterative and participatory and anchored by 10 major milestones. In a series of demonstrations of DeepLink’s functions, we will address one of the major challenges facing translational science: reproducibility of biomedical research findings that are based on evolving molecular datasets. Reproducibility of analyses and replication of results are central to scientific advancement. Many landmark studies have used data that are constantly being updated, curated, and pared down over time. Our series of demonstrations projects are designed to prototype the technology required for a scalable and robust translator as well as the techniques we will use to close the interoperability gap for a specific use case. The demonstration project will, itself, will be a significant and novel contribution to science. DeepLink will be able to answer questions that are currently enigmatic. Examples include: - From clinicians: What is the comparative effectiveness of all the treatments for disease Y given a patient's genetic/metabolic/proteomic profile? What are the functional variants in cell type X that are associated with differential treatment outcomes? What metabolite perturbations in cell type Y are associated with different subtypes of disease X? - From basic science researchers: What is known about disease Y across all model organisms (even those not designed to model Y)? What are all the clinical phenotypes that result from a change in function in protein X? Which biological pathways are affected by a pathogenic variant of disease Y? What patient data are available to evaluate a molecularlyderived clinical hypothesis? Challenges and Our Approaches: DeepLink will close the interoperability gap that currently prohibits molecular discoveries from leading to clinical innovations. DeepLink will be technologically driven, addressing the challenges associated with large, heterogeneous, semantically ambiguous, continuously changing, partially overlapping, and contextually dependent data by using (1) scalable, distributed, and versioned graph stores; (2) semantic technologies such as ontologies and Linked Data; (3) network analysis quality control methods; (4) machine-learning focused data fusion methods; (5) context-aware text mining, entity recognition and relation extraction; (6) multi-scale knowledge discovery using patient and molecular data; and (7) presentation of actionable knowledge to clinicians and basic scientists via user-friendly interfaces. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338982,OT3TR002027,"['Address', 'Affect', 'Animal Model', 'Architecture', 'Basic Science', 'Biological', 'Biomedical Research', 'Cell physiology', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Data', 'Data Set', 'Disease', 'Genetic', 'Goals', 'Graph', 'Knowledge', 'Knowledge Discovery', 'Language', 'Link', 'Machine Learning', 'Metabolic', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Physiological', 'Proteins', 'Proteomics', 'Quality Control', 'Reproducibility', 'Research Personnel', 'Science', 'Scientist', 'Semantics', 'Series', 'Signs and Symptoms', 'Source', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'Treatment outcome', 'Update', 'Variant', 'Vision', 'base', 'cell type', 'clinical phenotype', 'comparative effectiveness', 'design', 'disorder subtype', 'genetic variant', 'innovation', 'insight', 'interoperability', 'molecular domain', 'multi-scale modeling', 'novel', 'prototype', 'text searching', 'user-friendly']",NCATS,COLUMBIA UNIVERSITY HEALTH SCIENCES,OT3,2016,1183132,0.005706184536502936
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9049511,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Health', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'knowledge translation', 'learning progression', 'learning strategy', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428512,0.023826436302358197
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design New technologies afford the acquisition of dense “data clouds” of individual humans. However, heterogeneity, dimensionality and multi-scale nature of such data (genomes, transcriptomes, clinical variables, etc.) pose a new challenge: How can one query such dense data clouds of mixed data as an integrated set (as opposed to variable by variable) against multiple knowledge bases, and translate the joint molecular information into the clinical realm? Current lexical mapping and brute-force data mining seek to make heterogeneous data interoperable and accessible but their output is fragmented and requires expertise to assemble into coherent actionable information. We propose DeepTranslate, an innovative approach that incorporates the known actual physical organization of biological entities that are the substrate of pathogenesis into (i) networks (data graphs) and (ii) hierarchies of concepts that span the multiscale space from molecule to clinic. Organizing data sources along such natural structures will allow translation of burgeoning high-dimensional data sets into concepts familiar to clinicians, while capturing mechanistic relationships. DeepTranslate will take a hybrid approach to learn and organize its content from both (i) existing generic comprehensive knowledge sources (GO, KEGG, IDC, etc.) and (ii) newly measured instances of individual data clouds from two demonstration projects: (1) ISB’s Pioneer 100 and (2) St. Jude Lifetime cancer survivors. We will focus on diabetes as test case. These two studies cover a deep biological scale-space and thus can test the full extent of the multiscale capacity of DeepTranslate in a focused application. 1. TYPES OF RESEARCH QUESTION ENABLED. How can a clinician find out that the dozens of “out of range” variables observed in a patient’s data cloud, form a connected set with respect to pathophysiology pathways, from gene to clinical variable? How can the high-dimensional data of studies that measure for each individual 100+ data points of various types (“personal data clouds”) be analyzed as one set in an integrated fashion (as opposed to variable by variable) against existing knowledge bases and also be used to improve the databases? DeepTranslate addresses these two types of questions and thereby will accelerate translation of future personal data clouds into (A) care decisions and (B) hypotheses on new disease mechanisms / treatments, thereby benefiting providers as well as researchers. 2. USE OF EXPERTISE AND RESOURCES. ■ ISB: pioneer in personalized, big-data driven medicine (Demo Project 1); biomedical content expertise; multiscale omics and molecular pathogenesis, big data analysis, housing of databases for public access; query engine designs, GUI. ■ UCSD: leader in biomedical data integration; automated assembly of molecular and clinical data into hierarchical structures; translation between data types ■ U Montreal: biomedical database curation from literature and construction of gene/protein/drug interaction networks; machine learning, open resource database ■ St Jude CRH: Cancer monitoring Demo Project 2, cancer patient data analytics. 3. POTENTIAL DATA AND INFRASTRUCTURE CHALLENGES. (a) Existing comprehensive clinical data sources are not uniform and not explicitly based on biological networks; cross-mapping is being performed at NLM based on lexical relationships: HPO (phenotypes) vs. SNOMED CT (for EMR) vs. IDC or Merck Manual (for diseases). Careful selection of these sources in close collaboration with NLM is needed. (b) Existing molecular pathway databases are static, based on averages of heterogeneous non-stratified populations, while the newly measured high-dimensional data clouds are varied due to intra-individual temporal fluctuation and inter-individual variation. How this will affect building of ontotypes in our hybrid approach, and how large cohorts of data clouds must be to offer statistical power is yet to be determined. Our two Demonstration Projects with their uniquely deep (high-dimensional and multiscale) data in cohorts of limited but growing size are thus crucial first steps in a long journey of collective learning in the TRANSLATOR community. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9338977,OT3TR002026,"['Address', 'Affect', 'Architecture', 'Big Data', 'Biological', 'CRH gene', 'Cancer Patient', 'Cancer Survivor', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Collaborations', 'Communities', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Diabetes Mellitus', 'Disease', 'Drug Interactions', 'Functional disorder', 'Future', 'Gene Proteins', 'Generic Drugs', 'Genes', 'Genome', 'Graph', 'Heterogeneity', 'Housing', 'Human', 'Hybrids', 'Individual', 'Joints', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Molecular', 'Monitor', 'Nature', 'Output', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Phenotype', 'Population', 'Provider', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'SNOMED Clinical Terms', 'Saint Jude Children&apos', 's Research Hospital', 'Source', 'Structure', 'Testing', 'Translating', 'Translations', 'base', 'cohort', 'data integration', 'data mining', 'design', 'improved', 'innovation', 'inter-individual variation', 'knowledge base', 'lexical', 'molecular assembly/self assembly', 'new technology', 'transcriptome']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT3,2016,1071976,0.02705756126834778
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9096856,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,2201640,0.022787082520970697
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9288931,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,224176,0.022787082520970697
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9161233,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'Work', 'base', 'dashboard', 'empowered', 'genome sequencing', 'improved', 'insight', 'meetings', 'microbial community', 'peer', 'repository', 'social', 'tool', 'transcriptomics', 'web portal']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2016,575532,-0.00023185377682632187
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9113614,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,601709,0.02459470382186923
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,0.036820417026028715
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9123422,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Natural Products', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'novel therapeutics', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2016,419732,0.028160794682889803
"Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery PROJECT SUMMARY Protein post-translational modification (PTM) plays a critical role in many diseases; however, critical gaps remain in research infrastructure for global analysis of PTMs. Key PTM information concerning enzyme- substrate relationships, regulation of PTM enzymes, PTM cross-talk, and functional consequences of PTM remains buried in the scientific literature. Meanwhile, while high-throughput panomics (genomic, transcriptomic, proteomic, PTM proteomic) data offer an unprecedented opportunity for the discovery of PTM-disease relationships, the data must be analyzed in an integrated and easily accessible knowledge framework in order for researchers and clinicians to gain a molecular understanding of disease. The goal of this application is to develop a collaborative knowledge environment for semantic annotation of scientific literature and integrative panomics analysis for PTM-disease knowledge discovery in precision medicine. We propose to connect PTM information from literature mining and curated databases in a knowledge resource on an ontological framework that supports analysis of panomics data in the context of PTM networks. To broaden impact and foster collaborative development, our resource will be FAIR (Findable, Accessible, Interoperable, Reusable) and interoperable with community standards.  The specific aims are: (i) develop a novel NLP (natural language processing) system for full-scale literature mining and PTM-disease knowledge extraction; (ii) develop a PTM knowledge resource for integrative panomics analysis and network discovery; and (iii) provide a FAIR collaborative environment for scalable semantic annotation and knowledge integration. The proposed system will build upon the NLP technologies and text mining tools already developed by our team and the bioinformatics infrastructure at the Protein Information Resource (PIR). The iPTMnet web portal will allow searching, browsing, visualization and analysis of PTM networks and PTM-related mutations in conjunction with user-supplied omics data, including panomics data from major national initiatives. Use scenarios will include identification of disease-driving genetic variants and analysis of cellular responses to kinase inhibitors. Our PTM knowledgebase will be disseminated with an RDF triple-store and a SPARQL endpoint for semantic queries, while our text mining tools and full-scale literature mining results will be disseminated in the BioC community standard for seamless integration to other text mining pipelines. To engage the community semantic annotation of scientific literature, we will host a hackathon to develop tools to expose BioC-annotated literature corpora to the semantic web, as well as an annotation jamboree to explore tagging of scientific text with precise ontological terms. This project will thus offer a unique research resource for PTM-disease network discovery as well as an integrable collaborative knowledge framework to support Big Data to Knowledge in precision medicine. PROJECT NARRATIVE Precision medicine requires a detailed understanding of the molecular events that are disrupted in disease, including changes in protein post-translational modifications (PTM) that are hallmarks of many diseases. The proposed resource will support analysis of genomic-scale data for exploring PTM-disease networks and PTM-related mutations, as well as knowledge dissemination on the semantic web. These combined efforts will accelerate basic understanding of disease processes and discovery of diagnostic targets and more effective individualized therapies.",Semantic Literature Annotation and Integrative Panomics Analysis for PTM-Disease Knowledge Network Discovery,9195864,U01GM120953,"['Address', 'Adopted', 'Automobile Driving', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Cells', 'Clinical', 'Communities', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Educational workshop', 'Environment', 'Enzymes', 'Europe', 'Event', 'Fostering', 'Gene Proteins', 'Genomics', 'Goals', 'Graph', 'Hybrids', 'Imagery', 'Information Resources', 'Knowledge', 'Knowledge Discovery', 'Knowledge Extraction', 'Length', 'Link', 'Literature', 'Malignant Neoplasms', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Mutation', 'Names', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Phosphorylation', 'Phosphorylation Site', 'Phosphotransferases', 'Play', 'Post Translational Modification Analysis', 'Post-Translational Modification Site', 'Post-Translational Protein Processing', 'Process', 'Protein Databases', 'Protein Family', 'Proteins', 'Proteomics', 'PubMed', 'Publications', 'Regulation', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Sequence Alignment', 'Staging', 'System', 'Technology', 'Text', 'Tissues', 'Translational Research', 'Variant', 'abstracting', 'cell type', 'collaborative environment', 'computer based Semantic Analysis', 'enzyme substrate', 'genetic analysis', 'genetic variant', 'hackathon', 'indexing', 'information organization', 'innovation', 'kinase inhibitor', 'knowledge base', 'novel', 'precision medicine', 'protein complex', 'protein protein interaction', 'response', 'system architecture', 'text searching', 'therapeutic target', 'tool', 'transcriptomics', 'web portal', 'web services', 'web site']",NIGMS,UNIVERSITY OF DELAWARE,U01,2016,374400,0.022407099040755076
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9146765,R01LM010090,"['Apache', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Imagery', 'Informatics', 'Information Retrieval', 'International', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Outcomes Research', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Process', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translating', 'Translational Research', 'Trees', 'Vision', 'Work', 'abstracting', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'experience', 'individual patient', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'symptom treatment', 'syntax', 'tool', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2016,643863,-0.003672966185055609
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9200905,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Decision Modeling', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Immigration', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Legal', 'Legal patent', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2016,221175,0.0007428684410760132
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans. PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8985684,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'electronic book', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2016,405708,-0.0028185099461648985
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8840267,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2015,490160,0.04433813248146277
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims.         PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.            ",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,8874546,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Knowledge', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2015,478724,0.006931062142326403
"Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers ﻿    DESCRIPTION (provided by applicant): Lifespans continue to increase, chronic disease survival rates are drastically improved, and treatments are being discovered for a variety of illnesses. This rapidly changing scenario requires patients to participate in their recovery, understand written information and directions thereby calling upon patients to have increasingly more complex health literacy. However, time availability of practitioners or other resources to explain the required information has not increased to match. As a result, finding efficient means to improving patient health literacy is an increasingly important topic in healthcare. Increased health literacy may promote healthy lifestyle behaviors and increase access to health services by the population. It has been argued that for the Patient Protection and Affordable Care Act to be successful, more effort is needed to increase the health literacy of millions of Americans. Similarly, the Healthy People 2020 statement by the Department of Health and Human Services identified improving health literacy (HC/HIT-1) as an important national goal. The broad- long term objectives of this project are to contribute to increasing the health literacy of patients and health information consumers and provide caregivers an evidence-based tool for simplifying text. The most commonly used tool for estimating the difficulty of text is the readability formula. They are not sufficient, however, because there is no evidence to support a connection between their use and decreases in difficulty. This problem is addressed by using modern resources and techniques for discovering traits that make health-related text difficult and developing a tool to guide the simplification of text. . There are four specific aims of this project: 1) Identify differentiating features of easy versus difficult texts, 2) Design a simplification strategy using computer algorithms, 3) Measure the impact of simplification on perceived and actual text difficulty with online participants and a representative community sample, 4) Create free, online software that incorporates proven features algorithmically. Corpus analysis will be conducted to compare easy and difficult texts with each other and discover lexical, grammatical, semantic, and composition and discourse features typical for each. Then, simplification algorithms will be designed and developed relying on rule-based techniques to leverage available resources, e.g., vocabularies, or on machine learning approaches for discovering the best combinations of features for simplification. A representative writer will simplify text by relying on the suggestios provided by an online that tool that uses simplification algorithms. The effect of simplification wll be tested in comprehensive user studies to evaluate the effect on both actual and perceived difficulty. Features successfully shown to decrease text difficulty will be incorporated in an onlie software program designed to reduce text difficulty.         PUBLIC HEALTH RELEVANCE: Improving health literacy is an important national goal and necessary trait for a healthy population. Providing understandable information is critical but few tools exist to help write understandable text. We aim to discover features indicative of difficult text, design translation algorithms and create a free, online software tool for rewriting health-related text with demonstrated impact on perceived and actual text difficulty            ",Evidence-based Strategy and Tool to Simplify Text for Patients and Consumers,8884888,R01LM011975,"['Address', 'Advocate', 'Affect', 'Algorithms', 'American', 'Arizona', 'Behavior', 'Caregivers', 'Caring', 'Chronic Disease', 'Communities', 'Complement', 'Complex', 'Comprehension', 'Computational Linguistics', 'Computational algorithm', 'Computer software', 'Conflict (Psychology)', 'Data Set', 'Education', 'Education Projects', 'Ensure', 'Faculty', 'Feedback', 'Funding', 'Goals', 'Health', 'Health Promotion', 'Health Services Accessibility', 'Health behavior', 'Health education', 'Healthcare', 'Healthy People 2020', 'Individual', 'Lead', 'Longevity', 'Machine Learning', 'Measures', 'Medical', 'Medical Informatics', 'Medicine', 'Methods', 'Minority', 'Mullerian duct inhibiting substance', 'Natural Language Processing', 'Outcome', 'Participant', 'Patients', 'Pilot Projects', 'Population', 'Process', 'Public Health', 'Readability', 'Reader', 'Recovery', 'Recruitment Activity', 'Research', 'Research Design', 'Resources', 'Sampling', 'Semantics', 'Software Tools', 'Survival Rate', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Translations', 'Underserved Population', 'United States Dept. of Health and Human Services', 'Universities', 'Vocabulary', 'Work', 'Writing', 'base', 'college', 'combat', 'cost effective', 'design', 'evidence base', 'health literacy', 'healthy lifestyle', 'improved', 'lexical', 'programs', 'public health relevance', 'tool', 'trait', 'user-friendly', 'volunteer']",NLM,UNIVERSITY OF ARIZONA,R01,2015,376917,0.012372922697442012
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,0.043457338852838166
"Bio Text NLP ﻿    DESCRIPTION (provided by applicant):         Since our last renewal, the challenges for biomedical researchers of keeping up with the scientific literature have become even more acute. Last year marked the first time that Medline indexed more than a million journal articles; more than 210,000 of these had full text deposited in PubMedCentral, bringing the total number of full texts archived in PMC to over 3 million. The stunning pleiotropy of genes and their products, combined with the adoption of genome-scale technologies throughout biomedical research, has made obsolete the notion that reading within one's own specialty plus a few ""top"" journals is enough to keep track of all of the results relevan to one's research. Fortunately, advances in biomedical natural language processing and increasing access to digital full text journal publications offer the potential for innovative new approaches to delivering relevant information to working bench scientists.         We hypothesize that realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing Biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.             Project Narrative This project will affect public health by increasing the ability of biologists to investigate hypotheses using the biomedical literature. Realizing the potential of biomedical natural language processing applied to full text journal articles to make a sustained and powerful contribution to biomedical research requires contextualizing biomedical natural language processing in the daily life of bench scientists, focusing on their unmet information gathering needs, and providing interfaces that fit well into existing research workflows.",Bio Text NLP,8819017,R01LM009254,"['Acute', 'Address', 'Adopted', 'Adoption', 'Affect', 'Archives', 'Area', 'Biomedical Research', 'Characteristics', 'Collaborations', 'Complex', 'Data Set', 'Deposition', 'Discipline', 'Ensure', 'Environment', 'Funding', 'Genes', 'Genomics', 'Goals', 'Heart Diseases', 'Histone Code', 'Information Retrieval', 'Journals', 'Life', 'Literature', 'Malignant Neoplasms', 'Measures', 'Methods', 'Molecular Biology', 'Names', 'Natural Language Processing', 'Performance', 'Process', 'Public Health', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Semantics', 'Techniques', 'Technology', 'Text', 'Time', 'To specify', 'United States National Institutes of Health', 'Visual', 'Work', 'abstracting', 'base', 'digital', 'genome-wide', 'improved', 'indexing', 'information gathering', 'innovation', 'interest', 'journal article', 'medical specialties', 'meetings', 'novel', 'novel strategies', 'pleiotropism', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,548439,0.02934920783136225
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,0.02180805317221066
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"".         PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.                 ",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,8864679,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Solutions', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'design', 'innovation', 'interest', 'knowledge translation', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2015,445887,0.023826436302358197
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9147033,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Health', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,489338,0.022787082520970697
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8935854,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,2116462,0.022787082520970697
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8866232,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,602189,0.02459470382186923
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9126755,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,40625,0.028160794682889803
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9117880,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,379655,0.028160794682889803
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences. Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8906938,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2015,217378,0.03244792291432475
"Temporal relation discovery for clinical text ﻿    DESCRIPTION (provided by applicant):         The overarching long-term vision of our research is to create novel technologies for processing clinical free text. We will build upon the previous work of our ongoing project ""Temporal relation discovery for clinical text"" (R01LM010090) dubbed Temporal Histories of Your Medical Events (THYME; thyme.healthnlp.org) which has been focusing on methodology for event, temporal expressions and temporal relations discovery from the clinical text residing in the Electronic Health Records (EHR). We developed a comprehensive approach to temporality in the clinical text and innovated in computable temporal representations, methods for temporal relation discovery and their evaluation, rendering temporality to end users - resulting in over 35+ papers and presentations. Our dissemination is international and far-reaching as the best performing methods are released open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (ctakes.apache.org). The methods we developed are now being used in such nation-wide initiatives as the Electronic Medical Records and Genomics (eMERGE), Pharmacogenomics Network (PGRN), Informatics for Integrating the Biology and the Bedside (i2b2), Patient Centered Outcomes Research Institute and National Cancer Institute's Informatics Technology for Cancer Research (ITCR). Through our participation in organizing major international bakeoffs - CLEF/ShARe 2014, SemEval 2014 Analysis of Clinical Text Task 7, SemEval 2015 Analysis of Clinical Text Task 14, SemEval 2015 Clinical TempEval Task 6 - we further disseminated the THYME resources and challenged the international research community to explore new solutions to the unsolved temporality task. Through all these activities it became clear that computational approaches to temporality still present great challenges and usability of the output is still limited. Therefore, we propose to further innovate on methodologies and end user experience.             Specific Aim 1: Extract enhanced representations and novel features to support deriving timeline information.     Specific Aim 2: Develop methods to amalgamate individual patient episode timelines into an aggregate patient-level timeline.     Specific Aim 3: Mine the EHR - the unstructured clinical text and the structured codified information - for full patient-level temporality.     Specific Aim 4: Develop a comprehensive temporal visualization tool     Specific Aim 5: Develop methodology for and perform extrinsic evaluation on specific use case.     Specific Aim 6: (1) Evaluate state-of-the-art of temporal relations through organizing international challenges under the auspices of SemEval, (2) Disseminate the results through publications, presentations, and open source code in Apache cTAKES. Functional testing.             Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EHR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,8927274,R01LM010090,"['Apache Indians', 'Automobile Driving', 'Biology', 'Chronology', 'Clinical', 'Collection', 'Colon Carcinoma', 'Communication', 'Communities', 'Complex', 'Computerized Medical Record', 'Data', 'Data Set', 'Disease', 'Electronic Health Record', 'Ensure', 'Epidemiology', 'Evaluation', 'Event', 'Genomics', 'Goals', 'Human', 'Imagery', 'Individual', 'Informatics', 'Information Retrieval', 'International', 'Joints', 'Knowledge Extraction', 'Language', 'Life', 'Link', 'Machine Learning', 'Malignant neoplasm of brain', 'Medical', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Multiple Sclerosis', 'National Cancer Institute', 'Outcomes Research', 'Output', 'Paper', 'Patient-Focused Outcomes', 'Patients', 'Pharmacogenomics', 'Phenotype', 'Process', 'Publications', 'Recording of previous events', 'Records', 'Research', 'Research Institute', 'Resolution', 'Resources', 'Science', 'Semantics', 'Signs and Symptoms', 'Solutions', 'Source Code', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Thyme', 'Time', 'TimeLine', 'Translating', 'Translational Research', 'Trees', 'Vision', 'Work', 'abstracting', 'anticancer research', 'autism spectrum disorder', 'clinically relevant', 'data mining', 'experience', 'innovation', 'new technology', 'next generation', 'novel', 'open source', 'syntax', 'tool', 'usability']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2015,720481,-0.003672966185055609
"HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS     DESCRIPTION (provided by applicant): The aim of this proposal is to implement a novel way of processing and accessing the vast detailed knowledge contained within collections of scientific publications on the regulation of transcription initiation in bacterial models. In princple, this model for processing and reading information and new knowledge is applicable to other biological domains, potentially benefiting any area of biomedical knowledge. It is certainly criticl to generate new strategies to cope with the ever-increasing amount of knowledge generated in genomics and in biomedical research at large. Improving the efficiency of the traditional high-quality manual curation of scientific publications will enable us also to expand the type of biological knowledge, beyond mechanisms and their elements in the genome, to start including their connections with larger regulated processes and eventually physiological properties of the cell. We will first implement the necessary technology to improve our curation by means of a computational system that has text mining capabilities for preprocessing the papers before a human expert curator identifies which sentences contain the information that is to be added to the database. Premarked options selected by the curators will accelerate their decisions. The accumulative precise mapping between sentences and curated knowledge will provide training sets for text mining technologies to improve their automatic extraction. The curator practices will become more efficient, enabling us to curate selected high-impact published reviews to place mechanisms into a rich context of their physiological processes and general biology. Another relevant component of our proposal is the improved modeling of regulated processes by means of new concepts in biology that capture larger collections of coregulated genes and their concatenated reactions. Starting from all interactions of a local regulator, coregulated regulators and their domain of action will be incorporated to construct the biobricks of complex decisions, as they are encoded in the genome. These are conceptual containers that capture the organization of knowledge to describe the genetic programming of cellular capabilities. These proposals will be formalized and proposed within an international consortium focused in enriching standard models or ontologies of gene regulation for use by the scientific community. Finally, a portal to navigate across all the sentences of a given corpus of a large number (more than 5,000) of related papers will be implemented. The different avenues of navigation will essentially use two technologies, one dealing with automatically generating simpler sentences from original sentences as input, and the other one with the classification of papers based on their theme or ontology. Their combination will enable a novel navigation reading system. If we achieve our aims, this project will give a proof-of-principle prototype with clearly innovative higher levels of large amounts of integrated knowledge. Future directions may adapt these concepts and methods to the biology of higher organisms, including humans.         PUBLIC HEALTH RELEVANCE: Scientific knowledge reported within publications provides a wealth of knowledge that we barely capture in databases for genomics. Enhancing the effectiveness of the processing and representation of all this knowledge will change the way we encode our understanding of concatenated interactions that are organized into networks and processes governing cell behavior. Given the conservation in evolution of the nature of biological complexity, a better encoding of our understanding of a bacterial cell shall influence that of any other living organism.            ",HIGH THROUGHPUT LITERATURE CURATION OF GENETIC REGULATION IN BACTERIAL MODELS,8817212,R01GM110597,"['Area', 'Bacteria', 'Bacterial Model', 'Binding Sites', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Books', 'Cells', 'Classification', 'Collection', 'Communities', 'Complex', 'Data Set', 'Databases', 'Effectiveness', 'Elements', 'Escherichia coli', 'Evolution', 'Foundations', 'Future', 'Gene Expression Regulation', 'Genes', 'Genetic', 'Genetic Programming', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Human', 'Indium', 'International', 'Joints', 'Knowledge', 'Letters', 'Life', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Natural Language Processing', 'Nature', 'Ontology', 'Operon', 'Organism', 'Paper', 'Physiological', 'Physiological Processes', 'Process', 'Property', 'Publications', 'Publishing', 'Reaction', 'Reading', 'Regulation', 'Regulon', 'Reporting', 'Research Infrastructure', 'Series', 'Signal Transduction', 'Site', 'Solid', 'Source', 'System', 'Technology', 'Text', 'Training', 'Transcription Initiation', 'Transcriptional Regulation', 'base', 'cell behavior', 'coping', 'digital', 'experience', 'feeding', 'functional genomics', 'improved', 'innovation', 'member', 'microbial community', 'model organisms databases', 'novel', 'novel strategies', 'promoter', 'prototype', 'public health relevance', 'response', 'software development', 'text searching', 'tool', 'transcription factor', 'usability']",NIGMS,CENTER FOR GENOMIC SCIENCES,R01,2015,406247,-0.0028185099461648985
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8734439,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2014,489755,0.04433813248146277
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8669161,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2014,212994,0.04274374498677738
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8722030,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2014,260727,-0.010940226425208149
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,0.02180805317221066
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8774407,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2014,1623265,0.022787082520970697
"Developing and applying information extraction resources and technology to create     DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another.              Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8694375,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2014,746561,0.02459470382186923
"In silico identification of phyto-therapies     DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits.                RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.  ",In silico identification of phyto-therapies,8749705,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R01,2014,389869,0.028160794682889803
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.             Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8727093,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2014,223898,0.03244792291432475
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8714053,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,307471,0.020888120618467485
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453          PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8722026,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2014,342375,-0.00959664531721098
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8474789,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2013,486484,0.04433813248146277
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8484438,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2013,202118,0.04274374498677738
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8532984,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2013,247288,-0.010940226425208149
"Temporal relation discovery for clinical text No abstract available  Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,8535820,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2013,663984,0.016595446049537835
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8733018,UH3HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH3,2013,500000,-0.01636160077289722
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8426190,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'stress disorder', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,315672,0.004061430561060075
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8535824,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,291730,0.020888120618467485
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral No abstract available  Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8528719,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Chips', 'Genes', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'genome analysis', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2013,526738,0.02657575486249491
"Improving access to multi-lingual health information through machine translation No abstract available  PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8517814,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2013,323176,-0.039089674089472004
"Temporal relation discovery for clinical text    DESCRIPTION (provided by applicant): The overarching long-term vision of our research is to create novel technologies for processing clinical free text. Such technologies will enable sophisticated and efficient indexing, retrieval and data mining over the ever increasing amounts of electronic clinical data. Processing free text poses a number of challenges to which the fields of Artificial intelligence, natural language processing and computer science in general have made advances. Methods for processing free text are informed by linguistic theory combined with the power of statistical inferencing. A key component to the next step, natural language understanding, is discovering events and their relations on a timeline. Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles.        The goal of our current proposal is to discover temporal relations from clinical free text through achieving four specific aims:        Specific Aim 1: Develop (1) a temporal relation annotation schema and guidelines for clinical free text based on TimeML, which will require extensions to Treebank, PropBank and VerbNet annotation guidelines to the clinical domain, (2) an annotated corpus following the temporal relations schema with additions to Treebank, PropBank and VerbNet, (3) a descriptive study comparing temporal relations in the clinical and general domains.        Specific Aim 2: Extend and evaluate existing methods and/or develop new algorithms for temporal relation discovery in the clinical domain. Component-level evaluation        Specific Aim 3: Integrate best method and/or a variety of methods for temporal relation discovery into the open source Mayo Clinic IE pipeline and release as open source annotators in the pipeline. Functional testing. Dissemination activities.        Specific Aim 4: System-level evaluation. Test the functionality of the enhanced Mayo Clinic IE pipeline on translational research use cases, e.g. the progression of colon cancer as documented in clinical notes and pathology reports, the progression of brain tumor as documented in radiology reports.        The methods we will use for the temporal relation discovery are based on machine learning, e.g., Support Vector Machine technology. Such methods require the annotation of a reference standard from which the computations are derived. The best methods will be released as part of the Mayo Clinic Information Extraction System for the larger community to use and contribute to. We will test the methods against biomedical queries.           Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,8324662,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2012,745770,0.00744810111054288
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.       This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8309015,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2012,179306,0.04433813248146277
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8471822,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2012,224100,0.04274374498677738
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8701603,R01GM095476,[' '],NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2012,332000,0.04433813248146277
"Scalable and Robust Clinical Text De-Identification Tools     DESCRIPTION (provided by applicant): Exploiting the full potential of information rich and rapidly growing repositories of patient clinical text is hampered by the absence of scalable and robust de-identification tools. Clinical text contains protected health information (PHI), and the Health Insurance Portability and Accountability Act (HIPAA) restricts research use of patient information containing PHI to specific, limited, IRB-approved projects. As a result, vast repositories of clinical text remain under-used by internal researchers, and are even less available for external transmission to outside collaborators or for centralized processing by state-of-the-art natural language processing (NLP) technologies. De-identification, which is the removal of PHI from clinical text, is challenging. Despite their availability for over a decade, commercially available automated systems are expensive, require local tailoring, and have not gained widespread market penetration. Manual methods are costly and do not scale, yet continue to be used despite the small amount of residual PHI they leave behind. Open source de-identification tools based on state-of-the-art machine learning technologies can perform at or above the level of manual approaches but also suffer from the residual PHI problem. Current de-identification approaches, then, also severely limit the use and mobility of clinical text while exposing patients to privacy risks. These approaches redact PHI, blacking it out or replacing it with symbols (e.g., ""Here for cardiac eval is Mr. **PT_NAME<AA>, a **AGE<60s> yo male with his son Doug ...""). Traditional approaches leave residual PHI (""Doug"" in this example) to be easily noticed by readers of the text, as it remains plainly visible among the prominent redactions. We developed and pilot tested an alternative approach we believe addresses the residual PHI problem. Our approach uses the strategy of concealing, rather than trying to eliminate, residual PHI. We call it the ""Hiding In Plain Sight"" (HIPS) approach. HIPS replaces all known PHI with ""surrogate"" PHI- fictional names, ages, etc.-that look real but do not refer to any actual patient. A HIPS version of the above text is: ""Here for cardiac eval is Mr. Jones, a 64 yo male with his son Doug ..."" where the name ""Jones"" and age ""64"" are fictional surrogates, but the name ""Doug"" is residual PHI. To a reader, the surrogates and the residual PHI are indistinguishable. This prevents the reader from detecting the latter, avoiding disclosure. Our preliminary studies suggest that HIPS can reduce the risk of disclosure of residual PHI by a factor of 10. This yields overall performance that far surpasses the performance attainable by manual methods, and is unlikely to be matched, we believe, by additional incremental improvements in PHI tagging models (i.e., efforts to reduce residual PHI). Our pilot studies indicate IRBs would welcome the HIPS approach if it were shown to be effective through rigorous evaluation. To expand usage of clinical text and enhance patient privacy, we propose to formalize rules of effective surrogate generation (Aim 1), extend related de-identification confidence scoring methods (Aim 2), and conduct rigorous efficacy testing of HIPS in diverse institutional settings (Aim 3).                  All known automated de-identification methods leave behind a small amount of residual protected health information (PHI), which presents a risk of disclosing patient privacy and creates barriers to more widespread internal use and external sharing of information-rich clinical text for broad research purpose. This project advances and evaluates the efficacy of a novel method, called the Hiding In Plain Sight (HIPS) approach, which conceals residual PHI by replacing all other instance of PHI found in a document with realistic appearing but fictitious surrogates. Rigorous efficacy testing is needed to confirm that HIPS surrogates effectively reduce risk of exposing patient privacy by concealing the small amount of residual PHI all known de-identification leave behind.",Scalable and Robust Clinical Text De-Identification Tools,8345041,R01LM011366,"['Address', 'Age', 'Applied Research', 'Cardiac', 'Clinical', 'Detection', 'Disclosure', 'Evaluation', 'Excision', 'Foundations', 'Generations', 'Health', 'Health Insurance Portability and Accountability Act', 'Healthcare', 'Human', 'Information Theory', 'Institutional Review Boards', 'Left', 'Machine Learning', 'Manuals', 'Marketing', 'Methods', 'Modeling', 'Monitor', 'Names', 'Natural Language Processing', 'Patients', 'Penetration', 'Performance', 'Pilot Projects', 'Plant Roots', 'Privacy', 'Process', 'Publishing', 'Reader', 'Reporting', 'Research', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Risk', 'Scoring Method', 'Simulate', 'Son', 'Source', 'System', 'Technology', 'Testing', 'Text', 'Validation', 'Vision', 'Work', 'base', 'efficacy testing', 'male', 'novel', 'open source', 'patient privacy', 'prevent', 'programs', 'repository', 'software systems', 'tool', 'transmission process']",NLM,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,R01,2012,318849,-0.010940226425208149
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8515555,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2012,290837,-0.034612539684131145
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,8309419,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,596909,0.0008503873827542827
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8318224,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,572436,0.021988627250890597
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.        Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8303361,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2012,516448,-0.01636160077289722
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8238371,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,326844,0.004061430561060075
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.              Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8442618,K99LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,MAYO CLINIC ROCHESTER,K99,2012,96552,0.03244792291432475
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8326650,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,317212,0.020888120618467485
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8319670,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2012,352514,-0.00959664531721098
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.      PUBLIC HEALTH RELEVANCE: This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.          This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8106768,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2011,500273,0.04169841305342808
"Temporal relation discovery for clinical text    DESCRIPTION (provided by applicant): The overarching long-term vision of our research is to create novel technologies for processing clinical free text. Such technologies will enable sophisticated and efficient indexing, retrieval and data mining over the ever increasing amounts of electronic clinical data. Processing free text poses a number of challenges to which the fields of Artificial intelligence, natural language processing and computer science in general have made advances. Methods for processing free text are informed by linguistic theory combined with the power of statistical inferencing. A key component to the next step, natural language understanding, is discovering events and their relations on a timeline. Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles.        The goal of our current proposal is to discover temporal relations from clinical free text through achieving four specific aims:        Specific Aim 1: Develop (1) a temporal relation annotation schema and guidelines for clinical free text based on TimeML, which will require extensions to Treebank, PropBank and VerbNet annotation guidelines to the clinical domain, (2) an annotated corpus following the temporal relations schema with additions to Treebank, PropBank and VerbNet, (3) a descriptive study comparing temporal relations in the clinical and general domains.        Specific Aim 2: Extend and evaluate existing methods and/or develop new algorithms for temporal relation discovery in the clinical domain. Component-level evaluation        Specific Aim 3: Integrate best method and/or a variety of methods for temporal relation discovery into the open source Mayo Clinic IE pipeline and release as open source annotators in the pipeline. Functional testing. Dissemination activities.        Specific Aim 4: System-level evaluation. Test the functionality of the enhanced Mayo Clinic IE pipeline on translational research use cases, e.g. the progression of colon cancer as documented in clinical notes and pathology reports, the progression of brain tumor as documented in radiology reports.        The methods we will use for the temporal relation discovery are based on machine learning, e.g., Support Vector Machine technology. Such methods require the annotation of a reference standard from which the computations are derived. The best methods will be released as part of the Mayo Clinic Information Extraction System for the larger community to use and contribute to. We will test the methods against biomedical queries.           Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,8138604,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2011,695125,0.00744810111054288
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence    DESCRIPTION (provided by applicant): Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.           Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8190163,K99LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2011,89802,0.04148332345813026
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,8075593,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2011,274386,0.0446420052540029
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8034342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2011,332732,-0.034612539684131145
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,8117587,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,597135,0.0008503873827542827
"Phenotype Discovery in NHLBI Genomic Studies (PhD)    DESCRIPTION (provided by applicant): Abstract Researchers continually upload data into public repositories at a rapid pace, yet utilize few common standards for annotation, making it close to impossible to compare or associate data across studies. To address this problem, we will develop a defined meta- data model and build an integrated system called Phenotype Discovery (PhD) that enables researchers to query and find genomic studies of interest in public repositories as well as upload new data into our database (sdGaP), in a standardized manner. A Query Interpreter (QI) will utilize text mining and natural language processing techniques to map free text into concepts in biomedical ontologies, allowing non-structured queries to be answered efficiently. In Phase I of the project, we will develop a proof-of-concept system that can retrospectively structure phenotypic descriptions in dbGaP, and will work with domain experts in pneumology to build use cases and evaluate the automated mappings. In Phase II of the project, we will extend the domain expertise to cardiology, hematology, and sleep disorders to build a more comprehensive system, expanding the phenotype annotation to transcriptome databases, and integrating a flexible automated genotype annotation tool for sdGaP. We will develop a user-friendly interface to prospectively assist researchers in uploading their data with standardized phenotypic annotations. We will provide the tool for free from our website and continuously improve its quality, based on user feedback and usage data.      PUBLIC HEALTH RELEVANCE: Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.           Relevance Phenotype Discovery (PhD) represents a novel, automated system to describe the characteristics of patients whose genetic information is available in public data repositories, without compromising their privacy. This initiative is greatly needed so that more researchers can make use of data collected from projects funded by public agencies. PhD uses new methodology for natural language processing and semantic integration to interpret the narrative text as well as variables and their values from studies in genomic databases. Standardized terminologies will be utilized to ensure that data can be analyzed across different studies.         ",Phenotype Discovery in NHLBI Genomic Studies (PhD),8145134,UH2HL108785,"['Address', 'Bioinformatics', 'Cardiology', 'Characteristics', 'Collaborations', 'Computer software', 'Data', 'Databases', 'Deposition', 'Dictionary', 'Ensure', 'Environment', 'Feedback', 'Funding', 'Gene Expression Profile', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Hematology', 'Informatics', 'Learning', 'Lung diseases', 'Maps', 'Methodology', 'Methods', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patients', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Privacy', 'Protocols documentation', 'Pulmonology', 'Research', 'Research Personnel', 'Scientist', 'Semantics', 'Sleep Disorders', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Training', 'Work', 'abstracting', 'base', 'biomedical informatics', 'biomedical ontology', 'data modeling', 'database of Genotypes and Phenotypes', 'flexibility', 'improved', 'interest', 'novel', 'programs', 'prototype', 'repository', 'study characteristics', 'text searching', 'tool', 'user-friendly', 'web site']",NHLBI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",UH2,2011,540294,-0.0032131377196095534
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8139258,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,513952,0.021988627250890597
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,8055527,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Traumatic Stress Disorders', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,248610,-0.027345686340504334
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,8115129,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2011,136978,0.03635618973574179
"Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery    DESCRIPTION (provided by applicant):       A great challenge in the biomedical informatics domain is to develop computational methods that combine existing knowledge and experimental data to derive new knowledge regarding biological systems and disease mechanisms. Most knowledge regarding genes and proteins in biomedical literature is stored in the form of free text that is not suitable for computation, and the manual processes of encoding this body of knowledge into computable form cannot keep up with the rate of knowledge accumulation. The main thrust of the proposed research is to design novel statistical text-mining algorithms to acquire and represent knowledge regarding genes and proteins from free-text literature, and further to combine this acquired knowledge with experimental data to derive new knowledge. We will organize the proposed research to the following specific aims. Specific Aim 1. Develop ontology-guided semantic modeling algorithms for extracting biological concepts from free text, in which we will design hierarchical probabilistic topic models that are capable of representing biological concepts as a hierarchy and develop novel learning algorithms to infer biological concepts from free-text documents. Specific Aim 2. Integrate semantic modeling with BioNLP to extract textual evidence supporting protein-function annotations. We will develop information extraction algorithms that will combine the results of hierarchical semantic analysis and BioNLP to identify the text regions that will most likely provide evidence regarding the function of genes/proteins and map the extracted information to a controlled vocabulary. Specific Aim 3. Develop a framework to unify the procedures of knowledge reasoning and data mining for knowledge discovery. In this aim, we will reason using existing knowledge (represented in the form of an ontology) to reveal functional modules among the genes from the experimental data. We will then further develop algorithms that will reveal relationships between these gene modules by mining system-scaled experimental data. The overall framework will integrate functional reasoning and data mining in an iterative manner to refine the knowledge progressively and to derive rules such as: when genes involved in biological process X are perturbed, genes involved in biological process Y will respond. We will test the framework on the data from yeast-system biology studies and the Cancer Genome Atlas (TCGA) project to gain insights into the cellular systems and disease mechanisms of cancer cells.           In recent decades, biomedical sciences have achieved significant advances; most of the knowledge resulting from research is stored in the form of biomedical literature in the form free-text. This project develop computational approaches to extract knowledge from biomedical literature, represent the knowledge in computable form, and combined the knowledge with experiment data to gain insights into biological systems and disease mechanisms",Ontology-Driven Methods for Knowledge Acquisition and Knowledge Discovery,8202896,R01LM011155,"['Accounting', 'Achievement', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Computing Methodologies', 'Controlled Vocabulary', 'Data', 'Disease', 'Gene Proteins', 'Genes', 'Goals', 'Knowledge', 'Knowledge Discovery', 'Knowledge acquisition', 'Learning', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Names', 'Natural Language Processing', 'Ontology', 'Procedures', 'Process', 'Proteins', 'Psyche structure', 'Research', 'Science', 'Semantics', 'Structure', 'System', 'Systems Biology', 'Testing', 'Text', 'The Cancer Genome Atlas', 'Training', 'Tweens', 'Yeasts', 'biological systems', 'biomedical informatics', 'cancer cell', 'data mining', 'design', 'insight', 'interest', 'knowledge of results', 'novel', 'protein function', 'protein protein interaction', 'research study', 'text searching']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2011,312599,0.020888120618467485
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,8138590,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2011,356340,-0.00959664531721098
"Temporal relation discovery for clinical text    DESCRIPTION (provided by applicant): The overarching long-term vision of our research is to create novel technologies for processing clinical free text. Such technologies will enable sophisticated and efficient indexing, retrieval and data mining over the ever increasing amounts of electronic clinical data. Processing free text poses a number of challenges to which the fields of Artificial intelligence, natural language processing and computer science in general have made advances. Methods for processing free text are informed by linguistic theory combined with the power of statistical inferencing. A key component to the next step, natural language understanding, is discovering events and their relations on a timeline. Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles.        The goal of our current proposal is to discover temporal relations from clinical free text through achieving four specific aims:        Specific Aim 1: Develop (1) a temporal relation annotation schema and guidelines for clinical free text based on TimeML, which will require extensions to Treebank, PropBank and VerbNet annotation guidelines to the clinical domain, (2) an annotated corpus following the temporal relations schema with additions to Treebank, PropBank and VerbNet, (3) a descriptive study comparing temporal relations in the clinical and general domains.        Specific Aim 2: Extend and evaluate existing methods and/or develop new algorithms for temporal relation discovery in the clinical domain. Component-level evaluation        Specific Aim 3: Integrate best method and/or a variety of methods for temporal relation discovery into the open source Mayo Clinic IE pipeline and release as open source annotators in the pipeline. Functional testing. Dissemination activities.        Specific Aim 4: System-level evaluation. Test the functionality of the enhanced Mayo Clinic IE pipeline on translational research use cases, e.g. the progression of colon cancer as documented in clinical notes and pathology reports, the progression of brain tumor as documented in radiology reports.        The methods we will use for the temporal relation discovery are based on machine learning, e.g., Support Vector Machine technology. Such methods require the annotation of a reference standard from which the computations are derived. The best methods will be released as part of the Mayo Clinic Information Extraction System for the larger community to use and contribute to. We will test the methods against biomedical queries.           Relevance (max 2-3 sentences) Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and create a timeline.",Temporal relation discovery for clinical text,7983243,R01LM010090,"['Algorithms', 'Artificial Intelligence', 'Automated Annotation', 'Brain Neoplasms', 'Clinic', 'Clinical', 'Clinical Data', 'Colon Carcinoma', 'Communities', 'Data', 'Development', 'Disease', 'Electronics', 'Evaluation', 'Event', 'Goals', 'Guidelines', 'Linguistics', 'Link', 'Machine Learning', 'Medical Records', 'Methods', 'Modeling', 'Natural Language Processing', 'Pathology Report', 'Performance', 'Process', 'Radiology Specialty', 'Reference Standards', 'Reporting', 'Research', 'Retrieval', 'Signs and Symptoms', 'System', 'Technology', 'Testing', 'Text', 'TimeLine', 'Translational Research', 'Vision', 'base', 'clinically relevant', 'computer science', 'data mining', 'indexing', 'new technology', 'next generation', 'open source', 'theories']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2010,775112,0.00744810111054288
"COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems.  To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati,8173654,76201000029C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Health Care Costs', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data mining', 'flexibility', 'innovation', 'novel', 'novel strategies', 'rapid growth']",NLM,NORTHEASTERN UNIVERSITY,N03,2010,377968,0.033666054317353984
"COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible,8170612,76201000023C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Medical Informatics', 'Patients', 'Population', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'prototype', 'rapid growth']",NLM,"CYCORP, INC.",N03,2010,377967,0.043942674193530505
"COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat  In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat ,8172635,76201000024C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Disease', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Human', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patients', 'Population', 'Research', 'Research Personnel', 'Research Project Grants', 'Statistical Methods', 'Structure', 'System', 'Text', 'Thinking', 'biomedical scientist', 'clinical care', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,N03,2010,373073,0.044700791369085696
"COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical,8173644,76201000025C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computer Simulation', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patient Care', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'clinical application', 'evidence based guidelines', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,STANFORD UNIVERSITY,N03,2010,378000,0.04419961477252165
"COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que,8173663,76201000032C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Communities', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'rapid growth', 'social']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,N03,2010,301251,0.035114977929454574
"COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica,8173959,76201000034C,"['Caring', 'Clinical', 'Clinical Data', 'Clinical Decision Support Systems', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'clinical decision-making', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SAMSUNG SDS AMERICA, INC.",N03,2010,377925,0.034789372708724405
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173956,76201000033C,"['Biology', 'Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computer software', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Molecular Biology', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data modeling', 'flexibility', 'innovation', 'novel strategies', 'open source', 'prototype', 'rapid growth']",NLM,UNIVERSITY OF COLORADO DENVER,N03,2010,377982,0.037206148053624856
"COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a","COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec",8173645,76201000026C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Population', 'Prevention', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'cost effectiveness', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SIMULCONSULT, INC.",N03,2010,377991,0.032939525563821966
"COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization,8173653,76201000028C,"['Age', 'Caring', 'Chronic Disease', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Complex', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Disease Management', 'Face', 'Family', 'Funding', 'Human', 'Imagery', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Methods', 'Patients', 'Population', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'Visual', 'biomedical scientist', 'design', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"RUTGERS, THE STATE UNIV OF N.J.",N03,2010,300216,0.03410994138127813
"COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De,8173961,76201000035C,"['Caring', 'Childhood', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Disease', 'Expert Systems', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Lead', 'Machine Learning', 'Medical', 'Memory', 'Mental Depression', 'Outcome', 'Patients', 'Probability', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'evidence base', 'flexibility', 'health application', 'innovation', 'novel strategies', 'rapid growth', 'tool', 'treatment planning']",NLM,SRI INTERNATIONAL,N03,2010,377997,0.03337997465028171
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7936991,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,BOSTON CHILDREN'S HOSPITAL,RC1,2010,491408,0.01321445367401136
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7851323,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2010,278345,0.0446420052540029
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7881671,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,264640,0.014716139106556246
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,7772342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2010,326303,-0.034612539684131145
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7908806,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,618469,0.0008503873827542827
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),8110238,U54CA121852,"['Address', 'Algorithms', 'Area', 'Binding', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Databases', 'Development', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Genes', 'Genetic Engineering', 'Genomics', 'Internet', 'Investigation', 'Literature', 'Machine Learning', 'Maps', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Research Personnel', 'Resources', 'Signal Pathway', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'base', 'biomedical ontology', 'computer framework', 'data mining', 'design', 'genome-wide', 'graphical user interface', 'knowledge base', 'multidisciplinary', 'natural language', 'novel', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2010,29814,-0.018591692803929485
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7935408,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,515594,0.021988627250890597
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173647,76201000027C,[' '],NLM,STANFORD UNIVERSITY,N03,2010,378000,0.037206148053624856
"COMPUTATIONAL THINKING-Text Mining of clinical narratives: In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Text Mining of clinical narratives:,8173660,76201000031C,[' '],NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,N03,2010,336943,0.03597561504015584
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7799875,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,250650,-0.027345686340504334
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7933715,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2010,388462,0.03635618973574179
"Improving access to multi-lingual health information through machine translation    DESCRIPTION (provided by applicant): The results of our proposed research will extend the usability of MT in healthcare and serve as a foundation for further research into improving the availability of health materials for individuals with Limited English Proficiency. Our description of public health translation work from Aim 1 will provide new understanding of existing barriers to translation. The error analysis from Aim 2 will identify specific focus areas for improving MT. Aim 3 will provide fundamentally new MT technology designed to adapt generic systems to the health domain, as well as a prototype implementation of a domain-adapted post-processing module. The evaluation studies in Aim 4 will provide a model for evaluation of machine translation technologies and provide benchmarks from which to evaluate advances in the machine translations for health materials in the future. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for those with limited English proficiency.    Review CriteriaSignificanceInvestigator(s)InnovationApproachEnvironmentReviewer 121321Reviewer 212121Reviewer 333453           PROJECT NARRATIVE The ability to access health information in the U.S. depends greatly on the ability to speak English. Yet a growing number of people in this country speak a language other than English. We propose to develop novel domain-specific natural language processing and machine translation technology and evaluate its impact on the process of producing multilingual health materials. Ultimately, this work will advance us towards the long term goal of eliminating health disparities caused by language barriers and improve access to pertinent multilingual health information for individuals with limited English proficiency.",Improving access to multi-lingual health information through machine translation,7946175,R01LM010811,"['Achievement', 'Address', 'Affect', 'Area', 'Arts', 'Benchmarking', 'Child', 'Cognitive', 'Communities', 'Computational algorithm', 'Country', 'Decision Making', 'Disasters', 'Disease Outbreaks', 'Evaluation', 'Evaluation Studies', 'Foundations', 'Funding', 'Future', 'Generic Drugs', 'Goals', 'Health', 'Health Communication', 'Health Professional', 'Healthcare', 'Home environment', 'Improve Access', 'Individual', 'Information Services', 'Language', 'Life', 'Measures', 'Modeling', 'Natural Language Processing', 'Outcome', 'Population', 'Process', 'Production', 'Public Health', 'Public Health Practice', 'Readiness', 'Regulation', 'Research', 'Safe Sex', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Time', 'Translating', 'Translation Process', 'Translations', 'Trust', 'United States', 'United States National Library of Medicine', 'Update', 'Vaccinated', 'Work', 'Writing', 'base', 'cost', 'design', 'flu', 'health disparity', 'health literacy', 'improved', 'insight', 'meetings', 'member', 'novel', 'portability', 'prototype', 'usability']",NLM,UNIVERSITY OF WASHINGTON,R01,2010,370693,-0.00959664531721098
"National Center: Multiscale Analysis of Genomic and Cellular Networks (MAGNet) A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.   n/a",National Center: Multiscale Analysis of Genomic and Cellular Networks (MAGNet),8012947,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2010,3757192,-0.019499698152719534
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7872692,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,66015,-0.020195186748873127
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7673720,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2009,142851,-0.020195186748873127
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7918614,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'clinical practice', 'forgetting', 'innovation', 'natural language', 'preference', 'speech processing', 'symposium']",NLM,UNIVERSITY OF CHICAGO,R01,2009,185745,0.010682445297184755
"Multi-source clinical Question Answering system    DESCRIPTION (provided by applicant):   / Abstract (Limit: 1 page) Our proposal addresses the following challenge area: 06-LM-101* Intelligent Search Tool for Answering Clinical Questions. Develop new computational approaches to information retrieval that would allow a clinician or clinical researcher to pose a single query that would result in search of multiple data sources to produce a coherent response that highlights key relevant information which may signal new insights for clinical research or patient care. Information that could help a clinician diagnose or manage a health condition, or help a clinical researcher explore the significance of issues that arise during a clinical trial, is scattered across many different types of resources, such as paper or electronic charts, trial protocols, published biomedical articles, or best-practice guidelines for care. Develop artificial intelligence and information retrieval approaches that allow a clinician or researcher confronting complex patient problems to pose a single query that will result in a search that appears to ""understand"" the question, a search that inspects multiple databases and brings findings together into a useful answer. Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. QA systems enhance the results of search engines by providing a concise summary of relevant information along with source hits. PubMed (http://www.ncbi.nlm.nih.gov/pubmed/) is the most ubiquitous biomedical search engine, however because it is a search engine the information retrieved is based on keyword searches and is not presented in a form for immediate consumption; the user has to drill down into the content of the webpages to find the facts/statements of interest. Moreover, the information that the clinician needs is likely to be of different types, for example a definition of a syndrome in combination with specific actions triggered by a particular diagnosis for a particular patient. Such information resides in different sources - encyclopedic and the EMR - and has to be dynamically accessed and presented to the user in an easily digestible format. We propose to develop a unified platform for clinical QA from multiple sources of clinical and biomedical narrative that implements semantic processing of the questions by fusing two existing technologies - the Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. The specific research questions we are aiming to answer are: ""How much effort is required to port a general semantic QA system to the clinical domain? How much additional domain-specific training is required? ""What is the accuracy of such a system? Question Answering in the clinical domain is an emerging area of research. The challenges in the field are mainly attributed to the number of components that require domain specific training along with strict system requirements in terms of high precision and recall complemented by an accessible and user-friendly presentation. Our approach to overcome them is to re-use components already in place as part of Mayo clinical Text Analysis and Knowledge Extraction System and the University of Colorado's Question Answering System. Our approach is innovative in bringing together information from encyclopedic sources and the EMR to present it into a unified form to the clinician at the point of care or the investigator in the lab. The technology for that is based on semantic language processing which aims at ""understanding"" the meaning of the question and the narrative. Our proposed system holds the potential to impact quality of healthcare and translational research. Our approach is feasible because it uses content already in the EMR at the Mayo Clinic along with general medical knowledge from multiple readily-available resources. The proposed system will be built off mature and tested components allowing a fast and robust delivery cycle. Our unique integration of technologies together with sophisticated statistical machine learning algorithms applied to rich linguistic knowledge about events, contradictions, semantic structure, and question-types, will allow us to build a system which significantly extends the range of possible question types and responses available to clinicians, and seamlessly fuses these to generate a response. Our proposed work represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied (Ely et al., 2005). We aim to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab. As such, the proposed cQA has the potential to play a vital and important decision- support role for the physician or the biomedical investigator. (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.               Relevance (max 2-3 sentences) Clinical question answering (cQA) systems focus on the physician needs usually at the point of care, or the investigator in the lab. The questions usually asked either require information highly specific to their patient, e.g. the patient's lab results or previous history, answered by the patient's health record, or a more general type of information usually answered through generally available information sources. Our proposed work to provide a unified multi-source solution for semantic retrieval, access and summarization of relevant information at the point of care or the lab, represents a high impact area that has the potential to improve healthcare delivery because it addresses needs that have been well-documented and studied.",Multi-source clinical Question Answering system,7842799,RC1LM010608,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Colorado', 'Complement', 'Complex', 'Consumption', 'Data Sources', 'Databases', 'Diagnosis', 'Electronics', 'Environment', 'Event', 'Health', 'Information Retrieval', 'Knowledge', 'Knowledge Extraction', 'Linguistics', 'Machine Learning', 'Medical', 'Paper', 'Patient Care', 'Patients', 'Physician&apos', 's Role', 'Physicians', 'Play', 'Practice Guidelines', 'Protocols documentation', 'PubMed', 'Publishing', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Semantics', 'Signal Transduction', 'Solutions', 'Source', 'Structure', 'Syndrome', 'System', 'Technology', 'Testing', 'Text', 'Training', 'Translational Research', 'Universities', 'Work', 'abstracting', 'base', 'clinically relevant', 'health care delivery', 'health care quality', 'health record', 'improved', 'innovation', 'insight', 'interest', 'language processing', 'point of care', 'response', 'semantic processing', 'tool', 'user-friendly']",NLM,MAYO CLINIC ROCHESTER,RC1,2009,497477,0.01321445367401136
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7664538,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'systematic review', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,318898,0.03268975865356211
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7656692,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,282304,0.0446420052540029
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7651469,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'genome database', 'improved', 'interest', 'natural language', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,273993,0.0008530605877998769
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7651387,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,267801,0.014716139106556246
"Textpresso: information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso: information retrieval and extraction system for biological literature,7583249,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Body of uterus', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2009,320000,-0.034612539684131145
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7906366,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,123630,-0.0044623663608457384
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8151670,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,14785,-0.0044623663608457384
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7662449,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,167303,-0.0044623663608457384
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7840891,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'drug discovery', 'flexibility', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'natural language', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2009,39294,-0.0044623663608457384
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7668609,R01LM008111,"['Address', 'Biomedical Research', 'Body of uterus', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,613451,0.0008503873827542827
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7914681,U54CA121852,"['Address', 'Algorithms', 'Area', 'Binding', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Databases', 'Development', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Genes', 'Genetic Engineering', 'Genomics', 'Internet', 'Investigation', 'Literature', 'Machine Learning', 'Maps', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Research Personnel', 'Resources', 'Signal Pathway', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'base', 'biomedical ontology', 'computer framework', 'data mining', 'design', 'genome-wide', 'graphical user interface', 'knowledge base', 'multidisciplinary', 'natural language', 'novel', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2009,116802,-0.018591692803929485
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7672256,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Readability', 'Reader', 'Reading', 'Self Care', 'Self Management', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'fourth grade', 'improved', 'instrument', 'literacy', 'ninth grade', 'patient oriented', 'prevent', 'programs', 'tenth grade', 'tool', 'web site']",NIDDK,UNIVERSITY OF UTAH,R01,2009,398216,0.0019789171043795703
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7781934,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,505564,0.021988627250890597
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7908946,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'meetings', 'natural language', 'population based', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2009,71200,0.005483716679816321
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7591237,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,272818,-0.027345686340504334
"Semi-Automated Abstract Screening for Comparative Effectiveness Reviews    DESCRIPTION (provided by applicant): In this three-year project, we aim to apply state-of-the-art information analysis technologies to assist the production of systematic reviews and meta-analyses that are increasingly being used as a foundation for evidence-based medicine (EBM) and comparative effectiveness reviews. We plan to develop a human guided computerized abstract screening tool to greatly reduce the need to perform a tedious but crucial step of manually screening many thousands of abstracts generated by literature searches in order to retrieve a small fraction potentially relevant for further analysis. This tool will combine proven machine learning techniques with a new open source tool that enables management of the screening process. This new technology will enable investigators to screen abstracts in a small fraction of the time compared to the current manual process. It will reduce the time and cost of producing systematic reviews, provide clear documentation of the process and potentially perform the task more accurately. With the acceptance of EBM and increasing demands for systematic reviews, there is a great need for tools to assist in generating new systematic reviews and in updating them. This need cannot be more pressing. The recent passage of the American Recovery and Reinvestment Act and the $1.1 billion allocated for comparative effectiveness research have created an unprecedented need for systematic reviews and opportunities to improve the methodologies and efficiency of their conduct.   We herein propose the development of novel, open-source software to help systematic reviewers better   cope with these torrents of data. The research and development of this tool will be carried out by a highly experienced team of systematic review investigators with computer scientists at Tufts University who began to collaborate last year as a result of Tufts being awarded one of the NIH Clinical Translational Science Awards (CTSA). We will pursue dissemination of the new technology through numerous channels including, but not limited to publication, presentation at conferences, exploring interest in its adoption by the Agency for Healthcare Research and Quality (AHRQ) Evidence-based Practice Center (EPC) Program, Cochrane Collaboration, CTSA network, and other groups conducting systematic reviews, and production of tutorial material. Our aims are:   1. Conduct research to design and implement a semi-automated system using machine learning and   information retrieval methods to identify relevant abstracts in order to improve the accuracy and efficiency of systematic reviews.   2. Develop Abstrackr, an open-source system with a Graphical User Interface (GUI) for screening abstracts, that applies the methods developed in Aim 1 to automatically exclude irrelevant abstracts/articles.   3. Evaluate the performance of the active learning model developed in Aim 1 and the functionality of   Abstrackr developed in Aim 2 through application to a collection of manually screened datasets of   biomedical abstracts that will subsequently be made publicly available for use as a repository to spur   research in the machine learning and information retrieval communities.           Systematic reviewing is a scientific approach to objectively summarizing the effectiveness and safety of existing treatments for diseases, a prerequisite for informed healthcare decision-making. Systematic reviewers must read many thousands of medical study abstracts, the vast majority of which are completely irrelevant to the review at hand. This is hugely laborious and time consuming. We propose to build a computerized system that automatically excludes a large number of the irrelevant abstracts, thereby accelerating the process and expediting the application of the systematic review findings to patient care.",Semi-Automated Abstract Screening for Comparative Effectiveness Reviews,7786337,R01HS018494,[' '],AHRQ,TUFTS MEDICAL CENTER,R01,2009,362692,0.03635618973574179
"Onto-BioThesaurus: ontological representation of gene/protein names for biomedica    DESCRIPTION (provided by applicant):       The long-term goal of our research is to develop resources and tools for knowledge retrieval management in the biomedical domain. As the pace of biomedical research accelerates, researchers become more and more dependent on computers to manage the explosive amount of biomedical information being published. The high quality of many databases is guaranteed by database curators who extract and synthesize information stored in literature or other databases. It is important to accurately recognize biomedical entity names in text and map the identified names to corresponding records in biomedical databases. Usually, a biomedical database provides a list of names either entered by curators or extracted from other databases. Those names could be used to retrieve records from databases or map names to database records by NLP systems. However, there are several characteristics associated with biomedical entity names, namely: synonymy (i.e., different names refer to the same database entry), ambiguity (i.e., one name is associated with different entries), and novelty (i.e., names or entities are not present in databases or knowledge bases) which make the task of retrieving database records using names and the task of associating names in text to database records very daunting. Additionally, biomedical entities can appear in text as short forms (SFs) abbreviated from their long forms (LFs). The prevalent use of SFs representing biomedical entities is another challenge faced by end users and NLP applications because of the high ambiguity of SFs.       Recently, ontology-based knowledge management is becoming increasingly popular since ontologies provide formal, machine-processable, and human-interpretable representations of the biomedical entities and their relations. We hypothesize that biomedical ontologies can be used to reduce the difficulty associated with retrieving records using names or mapping names in text to database records. Specific aims and the corresponding hypotheses are: i) develop onto-BioThesaurus by enriching BioThesaurus with gene/protein-related ontologies (Hypothesis: aligning gene/protein names to gene/protein-related ontologies can reduce the complexity associated with gene/protein names); ii) harvest synonyms for gene/protein classes and entities from online resources and text (Hypothesis: harvesting synonyms especially gene/protein SFs is critical since SFs are frequently used to represent gene/protein entities); iii) build a web user interface for gene/protein names and entries search and query through ontology-enabled onto-BioThesaurus (Hypothesis: enhancing BioThesaurus with gene/protein-related ontologies would enable us to build heuristic rules to enable machine reasoning); and iv) evaluate and distribute research methods/outcome (Hypothesis: evaluating and distributing research methods/outcome are critical to advance both basic and applied biomedical science.            The proposed research is critical for biomedical knowledge retrieval and management. It serves as one of the foundation for storing, retrieving, and extracting knowledge and information in the biomedical domain. Additionally, the proposed research will benefit biomedical researchers and general community for understanding and managing biomedical text through web interfaces and automated systems.",Onto-BioThesaurus: ontological representation of gene/protein names for biomedica,7654995,R01LM009959,"['Abbreviations', 'Biomedical Research', 'Characteristics', 'Communities', 'Computers', 'Databases', 'Expert Opinion', 'Foundations', 'Gene Proteins', 'Genes', 'Goals', 'Harvest', 'Human', 'Information Resources', 'Information Resources Management', 'Internet', 'Investigation', 'Knowledge', 'Literature', 'Manuals', 'Maps', 'Names', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Outcome', 'Peer Review', 'Process', 'Proteins', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Review Literature', 'Science', 'Services', 'System', 'Techniques', 'Terminology', 'Text', 'Thesauri', 'Time', 'acronyms', 'base', 'biomedical ontology', 'heuristics', 'knowledge base', 'tool', 'web interface', 'web site']",NLM,GEORGETOWN UNIVERSITY,R01,2009,608650,-0.06056835888574412
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,8133305,R01LM009153,[' '],NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2009,109860,-0.0044623663608457384
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7676864,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2009,3464579,-0.018591692803929485
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7495148,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2008,132030,-0.020195186748873127
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7675157,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Language', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'day', 'forgetting', 'innovation', 'preference', 'speech processing', 'symposium']",NLM,UNIVERSITY OF CHICAGO,R01,2008,354823,0.010682445297184755
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7498449,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2008,153203,0.02075137746696309
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7468470,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2008,286582,0.03268975865356211
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7432910,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Eukaryotic Cell', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'desire', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,273906,0.0446420052540029
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7465580,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,273993,0.0008530605877998769
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7468497,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,268274,0.014716139106556246
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7470146,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2008,278208,-0.0044623663608457384
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7488396,R01LM009254,"['Adoption', 'Affect', 'Agreement', 'Arts', 'Biomedical Research', 'Body of uterus', 'Collection', 'Computational Technique', 'Data', 'Development', 'Evaluation', 'Gold', 'Growth', 'Human', 'Judgment', 'Language', 'Lead', 'Literature', 'Machine Learning', 'Memory', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular Biology', 'Numbers', 'Pattern', 'Peer Review', 'Performance', 'Play', 'Process', 'Public Health', 'Publishing', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Retrieval', 'Review Literature', 'Role', 'Sampling', 'Scheme', 'Scientist', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'abstracting', 'base', 'concept', 'improved', 'information organization', 'interest', 'journal article', 'language processing', 'novel strategies', 'prototype', 'size', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2008,268713,0.045651149707010986
"Using Natural Language Processing to Monitor Product Claims Compliance for FDA    DESCRIPTION (provided by applicant): Linguastat, Inc. proposes to develop a means to automate the process of monitoring and identifying companies engaged in false advertising and deceptive practices in the marketing of drugs, dietary supplements, and/or food products. By leveraging state of the art approaches in computational linguistics such as Information Extraction and Natural Language Processing, it should be feasible, with some adaptation, to use this technology to: 1) automatically and continuously monitor the websites, TV transcripts, press releases and other electronic marketing text communications of tens of thousands of companies for various claims and product information 2) automatically ""red-flag"" instances in which claims have a high likelihood of potential harm to consumers, according to FDA priorities 3) automatically identify and extract the companies, products and claims embedded in electronic product information and electronic promotional materials to create a database easily searchable by the FDA and 4) automatically capture web-based or other electronic content for human review and store it as ""evidence."" Such automated technology would enable the FDA to significantly stretch its limited human resource to more effectively and comprehensively identify noncompliant product information, detect deceptive ads and other illegal practices, successfully prosecute offenders, and prevent harm to American consumers. For this Phase I SBIR project we propose to assess the feasibility of automated claims monitoring in three steps: In the first step, we will train information extraction and natural language processing algorithms to extract product marketing claims from text. In the second, step we will apply data mining and rules-based algorithms to assess which claims are likely to be non-compliant and merit further attention by FDA staff. In the third step, we will design and build a database of product claims that allows analysts to search, organize, and prioritize product claims based on the type of claim (e.g. what ailments does the product claim to treat), the type of product, and the likelihood of non- compliance. This technology will enable regulators and consumers to better monitor and detect cases of false, misleading, or deceptive advertising and product information. By enabling more effective enforcement of FDA regulations and giving consumers tools to make better buying decisions, the public health can be better protected by minimizing the impact of products that cause harm, give false hope, or entice consumers to forgo conventional remedies.          n/a",Using Natural Language Processing to Monitor Product Claims Compliance for FDA,7677599,R43FD003406,[' '],FDA,"LINGUASTAT, INC.",R43,2008,20000,-0.014947330329991751
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7474790,R01LM008111,"['Address', 'Biomedical Research', 'Body of uterus', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Numbers', 'Ontology', 'Phenotype', 'Rate', 'Readiness', 'Representations, Knowledge (Computer)', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'concept', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2008,614419,0.0008503873827542827
"Nation Center: Multi-Scale Study- Cellular Networks(RMI) A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genomewide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 1 investigators to combine molecular interaction clues from Core 2 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions. n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7674889,U54CA121852,"['Address', 'Algorithms', 'Area', 'Automobile Driving', 'Binding', 'Bioinformatics', 'Biological', 'Biological Process', 'Biomedical Research', 'Cell Adhesion', 'Cell physiology', 'Cells', 'Communities', 'Complex', 'Computational Science', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA-Protein Interaction', 'Databases', 'Development', 'Dissection', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Genes', 'Genetic Engineering', 'Genomics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Language', 'Literature', 'Machine Learning', 'Maps', 'Medical', 'Methodology', 'Molecular', 'Molecular Analysis', 'Ontology', 'Organism', 'Physics', 'Process', 'Published Comment', 'Range', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'Science', 'Signal Pathway', 'Skeleton', 'Source', 'Structure', 'Techniques', 'Testing', 'Tissues', 'Training', 'Transcriptional Activation', 'Work', 'base', 'computer framework', 'data mining', 'design', 'graphical user interface', 'improved', 'innovation', 'knowledge base', 'multidisciplinary', 'novel', 'response', 'software systems', 'structural biology', 'tool', 'usability']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2008,113826,-0.019641535141464946
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7475712,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,438476,0.0019789171043795703
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7671784,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2008,48482,0.0019789171043795703
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7414601,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2008,142400,0.005483716679816321
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7426246,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Depth', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Facility Construction Funding Category', 'Funding', 'GDF15 gene', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Numbers', 'Ontology', 'PLAB Protein', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,266213,-0.027345686340504334
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7502135,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2008,3570137,-0.018591692803929485
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,-0.03473292322382993
"Construction of a Full Text Corpus for Biomedical Text Mining    DESCRIPTION (provided by applicant):       There is a demonstrated community need for an annotated corpus consisting of the full texts of biomedical journal articles. There are many reasons to believe that the rate-limiting factor impeding progress in biomedical language processing today is the lack of availability of the right kind of expertly annotated data. An annotated corpus is a collection of texts with information about the meaning or structure associated with particular textual elements. Annotated corpora are a critical component of biomedical natural language processing research in two ways. First, most contemporary approaches to language processing rely at least in part on machine learning or statistical models. Such systems must be ""trained"" on sets of examples with known outputs, so annotated corpora provide the training data vital to the construction of modern NLP systems. Second, annotated corpora provide the gold standard by which various approaches to particular text mining tasks are evaluated. Due to their central roles in training and testing language processing systems, the quality of the design and operational creation of annotated corpora place fundamental limits on what can be accomplished with such systems. Although there has been valuable work done on annotating abstracts, there are important differences between abstracts and full-text articles from a text mining perspective, and annotation of full-text journal articles has been negligible. Workers in both the biological (especially model organism database curation) community and the text mining community have independently pointed out the importance of processing the full text of scientific publications if the biomedical world is to be able to fully utilize text mining. We propose to build a large, fully annotated corpus consisting of full texts of biomedical journal articles. Additionally, previous biomedical corpus annotation efforts have often utilized ad hoc ontologies that have limited their utility outside of the groups that created them. We will ensure community acceptability by annotating with respect to community-consensus ontologies such as the Gene Ontology and the UMLS. Since the task involves expensive human labor, efficiency is a key issue in creating corpora. For this reason, we propose to build a team that includes the builder of the largest semantically annotated corpus to date, one of the pioneers of the model organism databases, and an already-assembled cadre of experienced linguistic and domain-expert annotators.             n/a",Construction of a Full Text Corpus for Biomedical Text Mining,7301251,G08LM009639,"['Address', 'Agreement', 'Biological', 'Biology', 'Body of uterus', 'Collection', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Elements', 'Ensure', 'Facility Construction Funding Category', 'Feedback', 'Genes', 'Gold', 'Growth', 'Human', 'Light', 'Linguistics', 'Literature', 'MEDLINE', 'Machine Learning', 'Manuals', 'Measures', 'Metric', 'Monitor', 'Natural Language Processing', 'Nature', 'Numbers', 'Ontology', 'Output', 'Problem Solving', 'Procedures', 'Process', 'Publications', 'Published Comment', 'Rate', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scheme', 'Series', 'Standards of Weights and Measures', 'Statistical Models', 'Structure', 'System', 'Testing', 'Text', 'Today', 'Training', 'Unified Medical Language System', 'Work', 'abstracting', 'base', 'design', 'experience', 'indexing', 'information organization', 'innovation', 'journal article', 'language processing', 'model organisms databases', 'programs', 'quality assurance', 'text searching', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,G08,2007,130432,-0.020195186748873127
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7217497,R01LM008799,"['Address', 'Biological Models', 'Clinical', 'Complex', 'Data', 'Databases', 'Devices', 'Diagnosis', 'Face', 'Generations', 'Goals', 'Hand', 'Human', 'Information Resources', 'Knowledge', 'Language', 'Librarians', 'Machine Learning', 'Medical', 'Medical Errors', 'Medical Students', 'Modality', 'Modeling', 'Natural Language Processing', 'Patient Care', 'Patients', 'Physicians', 'Process', 'Reporting', 'Research', 'Resources', 'Retrieval', 'Software Tools', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'Translating', 'Voice', 'Work', 'base', 'day', 'forgetting', 'innovation', 'preference', 'speech processing', 'symposium']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2007,361696,0.010682445297184755
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7299922,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2007,181125,0.02075137746696309
"Technology Development for a MolBio Knowledge-Base Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges adsing from the proliferation of high- throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent iadvances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB. n/a",Technology Development for a MolBio Knowledge-Base,7473405,R01LM008111,"['Address', 'Area', 'Biology', 'Class', 'Clinical', 'Collection', 'Data', 'Databases', 'Evaluation', 'Facility Construction Funding Category', 'Genes', 'Genome', 'Genomics', 'Genotype', 'Human', 'Indium', 'Informatics', 'Information Resources', 'Information Resources Management', 'Investments', 'Knowledge', 'Knowledge Base (Computer)', 'Laboratories', 'Language', 'Link', 'Machine Learning', 'Manuals', 'Metabolism', 'Methods', 'Metric System', 'Molecular', 'Molecular Biology', 'Mus', 'Names', 'Natural Language Processing', 'Numbers', 'Ontology', 'Persons', 'Pharmaceutical Preparations', 'Plug-in', 'Proteins', 'Purpose', 'Semantics', 'Source', 'SwissProt', 'System', 'Taxonomy', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'base', 'concept', 'data integration', 'gene function', 'heuristics', 'instrumentation', 'knowledge base', 'success', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,234058,0.009859800500624405
"Assisting Systematic Review Preparation Using Automated Document Classification    DESCRIPTION (provided by applicant):       The work proposed in this new investigator initiated project studies the hypothesis that machine learning-based text classification techniques can add significant efficiencies to the process of updating systematic reviews (SRs). Because new information constantly becomes available, medicine is constantly changing, and SRs must undergo periodic updates in order to correctly represent the best available medical knowledge at a given time.       To support studying this hypothesis, the work proposed here will undertake four specific aims:   1. Refinement and further development of text classification algorithms optimized for use in classifying   literature for the update of systematic reviews on a variety of therapeutic domains. Comparative analysis using several different machine learning techniques and strategies will be studied, as well as various means of representing the journal articles as feature vectors input to the process.   2. Identification and evaluation of systematic review expert preferences and trade offs between high recall and high precision classification systems. There are several opportunities for including this technology in the process of creating SRs. Each of these applications has separate and unique precision and recall tradeoff thresholds that will be studied based on the benefit to systematic reviews.   3. Prospective evaluation of text classification algorithms. We will verify that our approach performs as   expected on future data.   4. Development of comprehensive gold standard test and training sets to motivate and evaluate the   proposed and future work in this area.      The long term relevance of this research to public health is that automated document classification will   enable more efficient use of expert resources to create systematic reviews. This will increase both the   number and quality of reviews for a given level of public support. Since up-to-date systematic reviews are essential for establishing widespread high quality practice standards and guidelines, the overall public health will benefit from this work.          n/a",Assisting Systematic Review Preparation Using Automated Document Classification,7242352,R01LM009501,"['Algorithms', 'Area', 'Classification', 'Data', 'Data Set', 'Development', 'Evaluation', 'Future', 'Gold', 'Guidelines', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Numbers', 'Paper', 'Performance', 'Preparation', 'Process', 'Public Health', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Review, Systematic (PT)', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Therapeutic', 'Time', 'Training', 'Triage', 'Update', 'Work', 'base', 'comparative', 'expectation', 'journal article', 'preference', 'programs', 'prospective', 'text searching', 'vector']",NLM,OREGON HEALTH AND SCI UNIVERSITY,R01,2007,292133,0.03268975865356211
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7264196,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,279300,0.0008530605877998769
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7299383,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2007,266852,0.014716139106556246
"Automatic Literature-based Protein Annotation    DESCRIPTION (provided by applicant):       Knowledge of protein function serves as a corner stone for biomedical research, which is fundamental for understanding biologic systems, the mechanism of disease and ultimately the human health. Decades of biomedical research has accumulated a great wealth of such knowledge available in the form of biomedical literatures. An important task of biomedical informatics is to acquire and represent the knowledge from free text of literatures and transform it to languages that are understandable by computational agents, so that the knowledge can be stored, retrieved and used for knowledge discovery. Currently, all protein annotations are assigned manually which, unfortunately, is extremely labor-intense and cannot keep up the pace of the growth of information. Indeed, with the completion of genome sequences of several model organisms, manual annotation of proteins has already become a major bottleneck between large number of proteins and exploding amount information in biomedical literatures. In this application, we propose to develop methods to facilitate automatic annotation of protein functions based on the functional information buried in the biomedical literature. The proposed methods adapt and extend the state of art probabilistic semantic analysis, information retrieval and machine learning methodologies, which serve as principled approaches to modeling uncertainties in natural language text. The project will develop algorithmic building blocks for a future automatic annotation system such that, when given a brief description of a protein (e.g., a protein name and symbol), it will be capable of retrieving relevant literature articles about the protein, extracting biological concepts from the articles and mapping the concept to a controlled vocabulary. We envision that achieving these goals will result in advances with broader impact which not only facilitate automatic protein annotation but also for biomedical literature indexing-one of the important area of biomedical informatics. The efficient knowledge acquisition and management will enhance biomedical research regarding the mechanisms of diseases and drug discovery.          n/a",Automatic Literature-based Protein Annotation,7260682,R01LM009153,"['Algorithms', 'Animal Model', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Body of uterus', 'Calculi', 'Classification', 'Controlled Vocabulary', 'Data', 'Disease', 'Future', 'Genes', 'Goals', 'Growth', 'Health', 'Human', 'Information Retrieval', 'Information Theory', 'Knowledge', 'Knowledge acquisition', 'Language', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Modeling', 'Names', 'Ontology', 'Organism', 'Performance', 'Proteins', 'Rate', 'Reporting', 'Research Personnel', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Uncertainty', 'base', 'biomedical informatics', 'concept', 'drug discovery', 'genome sequencing', 'human disease', 'improved', 'indexing', 'knowledge of results', 'markov model', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein function']",NLM,MEDICAL UNIVERSITY OF SOUTH CAROLINA,R01,2007,291350,-0.0044623663608457384
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7287359,R01LM009254,"['Adoption', 'Affect', 'Agreement', 'Arts', 'Biomedical Research', 'Body of uterus', 'Collection', 'Computational Technique', 'Data', 'Development', 'Evaluation', 'Gold', 'Growth', 'Human', 'Judgment', 'Language', 'Lead', 'Literature', 'Machine Learning', 'Memory', 'Methods', 'Metric', 'Mining', 'Modeling', 'Molecular Biology', 'Numbers', 'Pattern', 'Peer Review', 'Performance', 'Play', 'Process', 'Public Health', 'Publishing', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Retrieval', 'Review Literature', 'Role', 'Sampling', 'Scheme', 'Scientist', 'Standards of Weights and Measures', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Training', 'Work', 'abstracting', 'base', 'concept', 'improved', 'information organization', 'interest', 'journal article', 'language processing', 'novel strategies', 'prototype', 'size', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2007,350638,0.045651149707010986
"Using Natural Language Processing to Monitor Product Claims Compliance for FDA    DESCRIPTION (provided by applicant): Linguastat, Inc. proposes to develop a means to automate the process of monitoring and identifying companies engaged in false advertising and deceptive practices in the marketing of drugs, dietary supplements, and/or food products. By leveraging state of the art approaches in computational linguistics such as Information Extraction and Natural Language Processing, it should be feasible, with some adaptation, to use this technology to: 1) automatically and continuously monitor the websites, TV transcripts, press releases and other electronic marketing text communications of tens of thousands of companies for various claims and product information 2) automatically ""red-flag"" instances in which claims have a high likelihood of potential harm to consumers, according to FDA priorities 3) automatically identify and extract the companies, products and claims embedded in electronic product information and electronic promotional materials to create a database easily searchable by the FDA and 4) automatically capture web-based or other electronic content for human review and store it as ""evidence."" Such automated technology would enable the FDA to significantly stretch its limited human resource to more effectively and comprehensively identify noncompliant product information, detect deceptive ads and other illegal practices, successfully prosecute offenders, and prevent harm to American consumers. For this Phase I SBIR project we propose to assess the feasibility of automated claims monitoring in three steps: In the first step, we will train information extraction and natural language processing algorithms to extract product marketing claims from text. In the second, step we will apply data mining and rules-based algorithms to assess which claims are likely to be non-compliant and merit further attention by FDA staff. In the third step, we will design and build a database of product claims that allows analysts to search, organize, and prioritize product claims based on the type of claim (e.g. what ailments does the product claim to treat), the type of product, and the likelihood of non- compliance. This technology will enable regulators and consumers to better monitor and detect cases of false, misleading, or deceptive advertising and product information. By enabling more effective enforcement of FDA regulations and giving consumers tools to make better buying decisions, the public health can be better protected by minimizing the impact of products that cause harm, give false hope, or entice consumers to forgo conventional remedies.          n/a",Using Natural Language Processing to Monitor Product Claims Compliance for FDA,7326883,R43FD003406,[' '],FDA,"LINGUASTAT, INC.",R43,2007,99773,-0.014947330329991751
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts    DESCRIPTION (provided by applicant):  Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted readability levels with no critical information loss, using statistical natural language processing techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive impact on reader comprehension. We will use as a test bed for our system a general internal medicine clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public.              n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7303652,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,435682,0.0019789171043795703
"Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts Many studies have shown that the readability of the health information provided to consumers does not match their general reading levels (Rudd, Moeykens et al. 2000). Even with the efforts of healthcare providers and writers to make materials more readable, today most patient-oriented Web sites, pamphlets, drug-labels, and discharge instructions still require the consumer to have a tenth grade reading level or higher (Nielsen-Bohlman, Panzer et al. 2004). Not infrequently, however, consumer reading levels are as low as fourth grade. To address this problem, we propose developing a computer-based method for providing texts of appropriate readability to a consumer. Recent progress in statistical natural language processing techniques (Barzilay 2003; Barzilay and Elhadad 2003; Elhadad, McKeown et al. 2005) lead us to believe that computer programs can be developed to dramatically increase the range and amount of readable content available to consumers. It may also help improve comprehension, self management and eventually clinical outcome. The general goal of this project is to provide consumers with readable content through the automated translation of content from difficult to understand to easy to understand. The specific aims are 1. To develop a computerized instrument for assessing the readability of health texts. We will enhance  existing readability instruments (Zakaluk and Samuels March 1, 1988) by including measurements of  health term difficulty, text cohesion, and content organization and layout. 2. To develop a ""Plain English"" tool for translating complex health texts into new versions at targeted  readability levels with no critical information loss, using statistical natural language processing  techniques. 3. To conduct an evaluation study to verify that providing content with appropriate readability has a positive  impact on reader comprehension. We will use as a test bed for our system a general internal medicine  clinic and its diabetes patients. We will provide self-care materials to these patients. Relevance to public health: The proposed development of a readability assessment instrument and ""Plain English"" tool will help translating complex health materials into reader-appropriate texts. It has the potential of making the vast amount of available information in the health domain more accessible to the lay public. n/a",Improving Health Outcomes through Computer Generation of Reader-Appropriate Texts,7492453,R01DK075837,"['Address', 'Beds', 'Clinic', 'Clinical', 'Complex', 'Comprehension', 'Computers', 'Development', 'Diabetes Mellitus', 'Drug Labeling', 'Evaluation Studies', 'Exercise', 'Generations', 'Goals', 'Health', 'Health Personnel', 'Health education', 'Home environment', 'Instruction', 'Internal Medicine', 'Internet', 'Lead', 'Measurement', 'Metabolic Control', 'Methods', 'Natural Language Processing', 'Nursing Faculty', 'Outcome', 'Pamphlets', 'Patients', 'Public Health', 'Range', 'Readability', 'Reader', 'Reading', 'Score', 'Self Care', 'Self Management', 'Site', 'Smog', 'Software Tools', 'System', 'Teaching Materials', 'Techniques', 'Testing', 'Text', 'Today', 'Translating', 'Translations', 'Weight maintenance regimen', 'Work', 'Writing', 'base', 'cohesion', 'computer program', 'computerized', 'improved', 'instrument', 'literacy', 'patient oriented', 'prevent', 'programs', 'tool']",NIDDK,BRIGHAM AND WOMEN'S HOSPITAL,R01,2007,47853,0.0010062300355084437
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7195053,G08LM008983,"['Academy', 'Address', 'Basic Science', 'Collection', 'Computer Systems Development', 'County', 'Digital Libraries', 'Effectiveness', 'Elements', 'Ensure', 'Evaluation', 'Frequencies', 'Funding', 'Goals', 'Gray unit of radiation dose', 'Harvest', 'Health', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Improve Access', 'Information Systems', 'Intervention', 'Language', 'Librarians', 'Life', 'Literature', 'MEDLINE', 'Measures', 'Medicine', 'Metric', 'Mission', 'Modeling', 'Natural Language Processing', 'New York', 'Online Systems', 'Outcome Measure', 'Perception', 'Population', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Project Grants', 'Retrieval', 'Source', 'Specialist', 'Structure', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Today', 'Training', 'Translating', 'United States National Library of Medicine', 'Update', 'Wood material', 'Work', 'base', 'cost', 'design', 'digital', 'experience', 'feeding', 'improved', 'research to practice', 'tool']",NLM,SYRACUSE UNIVERSITY,G08,2007,145510,0.005483716679816321
"National Center: Multi-Scale Study of Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",National Center: Multi-Scale Study of Cellular Networks(RMI),7286779,U54CA121852,[' '],NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2007,3638557,-0.018591692803929485
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,7123058,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,611872,0.012544265048063039
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,-0.024744834654683624
"Computer System for Functional Analysis of Genomic Data    DESCRIPTION (provided by applicant): In two previous stages of this project, both funded by the National Institute of General Medical Sciences and carried out successfully, we developed GeneWays, a completely automated system that efficiently distills information about molecular interactions from an astronomical number of full-text biomedical articles. The next logical stage of the project is to carry this system from the computational laboratory into a practical, useful, and even indispensable tool that researchers can use to solve complex problems currently posed in experimental medicine and biology. The central hypothesis of our work on GeneWays has been that our computational tools will generate biological predictions of a quality sufficiently high that the biomedical community will invest in serious experimental validation. Specifically, we propose the following. 1. We will improve significantly the precision and recall of the GeneWays system. 2. We will develop and implement a probabilistic belief-network formalism?a belief-graph relative of the Bayesian network formalism that allows us to place and update beliefs on both the vertices and the edges of the graph for probabilistic reasoning over the large collection of facts in the GeneWays database. We will develop and implement a coordinated collection of methods for computing and updating beliefs on individual nodes and edges of the belief graph. 3. We will develop and implement a mathematical framework for incorporating pathway information into a genetic- linkage analysis formalism in such a way that each piece of pathway knowledge includes a specified degree of confidence. 4. We will process an enormous collection of texts, such as open-access biomedical journals, PubMed abstracts, and the GeneWays corpus, and thus will build a comprehensive GeneHighWays database. We will make the GeneHighWays database easily and freely accessible to academic researchers through a web interface. We will evaluate the new version of the GeneWays system and the GeneHighWays database for the quality of data, performance of the mathematical methods, and quality of the interface.           n/a",Computer System for Functional Analysis of Genomic Data,7148274,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,303688,0.019899313491093844
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,0.0257285435792743
"Answering Information Needs in Workflow    DESCRIPTION (provided by applicant): Needs for information arise continuously during the course of clinical practice, especially for physicians in training, e.g. when examining a patient or participating in rounds. In most of these situations, it is difficult or impossible for the clinician to immediately access appropriate information resources. Most needs are never adequately articulated or recorded, and consequently are forgotten by the end of the day. Moreover, when clinicians do recall information needs, they don't act on them, due to the significant limitations of current retrieval systems and the exigencies of clinical practice. The specific aims of the proposed project are: 1) to capture information needs in a convenient manner at the moment they occur using different modalities such as text input and voice capture on hand-held devices, 2) to translate high-level information needs into complex search strategies that adapt to user needs and are tuned to the capabilities of resources, and 3) to deliver relevant materials to the clinician in an accessible format and in a timely manner. The project will develop a central hub for addressing the information needs of medical students and residents, to be transmitted electronically as they occur in the field. A unique feature of the project is the use of natural language processing technology to identify context, goals and preferences in clinicians' questions. The major innovation is the generation of complex search strategies that exploit this contextual information, based on studies of human searching experts (reference librarians).              n/a",Answering Information Needs in Workflow,7101997,R01LM008799,"['clinical research', 'human', 'language', 'memory disorders', 'model', 'physicians', 'training', 'voice']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2006,372499,0.010682445297184755
"Improving Public Health Grey Literature Access for the Public Health Workforce    DESCRIPTION (provided by applicant): The long-term objective of the proposed project is to provide the public health (PH) workforce with improved access to high quality, relevant PH grey literature reports in order to positively impact the planning, conducting, and evaluating of PH interventions. The project will consist of two components: 1) continuation of user-focused technical system development, and 2) deployment and evaluation of this system's impact on the tasks of the PH workforce in county health departments.      The system will automatically harvest web-based grey literature reports (utilizing rules trained on input from PH professionals) and then produce rich summaries of PH grey literature reports using the model of essential elements of PH intervention reports validated by PH specialists (Turner et al, In Press). After developing a user interface that capitalizes on both natural language querying and the display of search results in a structured, model-based summary, the system will be introduced into several county health departments and evaluated to determine its impact on information flows and uses.      The project will include intrinsic evaluations of: 1) the appropriateness of the grey literature reports harvested by the system; 2) the quality and sufficiency of summaries representing the full reports based on the PH intervention model and; 3) retrieval results for users' queries using metrics of precision and recall. Extrinsic evaluation will be done in PH departments participating in the New York Academy of Medicine's ongoing study of the relationship between information and effectiveness. The research will provide baseline measures. We will evaluate: 1) the ease and frequency of use, perceptions of currency, accuracy, and completeness of retrieved reports by county PH personnel, and; 2) impact of the system's usage on information flows and uses based on internal outcome measures within the county health departments.      The work proposed here shares both the missions of public health and the National Library of Medicine   through the design of an information system that exploits natural language processing technology to   efficiently collect and provide access to quality public health grey literature for the public health workforce.         n/a",Improving Public Health Grey Literature Access for the Public Health Workforce,7019753,G08LM008983,"['clinical research', 'public health']",NLM,SYRACUSE UNIVERSITY,G08,2006,149472,0.005483716679816321
"Beyond Abstracts:  Issues in Mining Full Texts    DESCRIPTION (provided by applicant):     Biomedical language processing, the application of computational techniques to human-generated texts in biomedicine, is an increasingly important enabling technology for basic and applied biomedical research. The exponential growth of the peer-reviewed literature and the breakdown of disciplinary boundaries associated with high-throughput techniques have increased the importance of automated tools for keeping scientists abreast of all of the published material relevant to their work. However, despite decades of research, the performance of state-of-the-art tools for basic language processing tasks like information extraction and document retrieval remain below the level necessary for adequate utility and widespread adoption of this technology. The development, performance and evaluation of text mining systems depend crucially on the availability of appropriate corpora: collections of representative documents that have been annotated with human judgments relevant to a language-processing task. Corpora play two roles in the development of this technology: first, they act as ""gold standards"" by which alternative automated methods can be fairly compared, and second, they provide data for the training of statistical and machine learning systems that create empirical models of patterns in language use. The conventional view is that corpora are neutral, random samples of the domain of interest. Our preliminary work suggests that the restrictions in size, quality, genre, and representational schema of the small number of existing corpora are themselves a critical limiting factor for near-term breakthroughs in biomedical text processing technology. Therefore, we propose to test the following hypothesis: Creation of large, high-quality, biomedical corpora from multiple genres will lead to significant improvements in the performance of biomedical text mining systems and the creation of new approaches to text mining tasks. Specific aims include constructing several large corpora covering a range of genres and incorporating a rich knowledge representation; identifying factors that affect differential performance on full text versus abstracts; and developing new methods for language processing, especially of full text. Because improvements in the ability to automatically extract information from many textual genres will assist scientists and clinicians in the crucial task of keeping up with the burgeoning biomedical literature, the potential public health impact is quite large.          n/a",Beyond Abstracts:  Issues in Mining Full Texts,7135482,R01LM009254,"['abstracting', 'human', 'language', 'performance', 'training']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2006,369593,0.045651149707010986
"Nation Center: Multi-Scale Study- Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7126050,U54CA121852,"['NIH Roadmap Initiative tag', 'bioinformatics', 'cell biology', 'computational biology', 'cooperative study', 'genome']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2006,3747227,-0.018568184797957132
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,7033080,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2006,133459,0.023252776525259013
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6953701,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2005,613495,0.012544265048063039
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6929696,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2005,206378,0.011285071963432102
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The overall goal of this proposal is to develop an information integration architecture and associated tools to support rapid integration of data and knowledge from distributed heterogeneous data sources. The architecture aims to play a significant role in extracting coherent knowledge bases for biomedical research and improving the accuracy, completeness and quality of the extracted knowledge. Towards achieving these goals, the proposed scalable architecture includes new innovative generalized integration algorithms and tools for the generation of mediators to capture the functional behavior of data sources, semantic representation of data sources to support automated generation of integration agents, and optimization of integrated data queries. The information integration architecture keeps pace with the evolving Internet-based XML electronic data interchange, semantic web services, and web services discovery standards. Thus, leveraging the Internet technologies and standards for the purpose of providing lasting state-of-the-art solutions for information integration. In addition, the proposed architecture is inherently scalable in terms of the number of data sources that can be integrated, the number of users of the integrated system, and the range of biomedical problems that can be tackled. During phase I of the project, prototypes of the proposed integration algorithms and tools will be developed as proofs of concept and to form the foundation for evaluation and pilot testing of the proposed integration mechanisms, using private and public data sources, in terms of scalability and integration capabilities.         n/a",Information Integration of Heterogeneous Data Sources,6881960,R43RR018667,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'information system analysis', 'information systems', 'mathematics']",NCRR,"INFOTECH SOFT, INC.",R43,2005,260661,-0.0020348470882288173
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,-0.024744834654683624
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,0.02268393484262289
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,7000653,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,SPELMAN COLLEGE,R33,2005,260030,-0.020704452842696678
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,0.0257285435792743
"Extracting Semantic Knowledge from Clinical Data Sources DESCRIPTION (provided by applicant):    Electronic medical record systems (EMR) contain a wealth of clinical data that is invaluable for biomedical research, but because there are no satisfactory methods to build coherent specialized knowledge bases, which represent the information in free text medical records, data mining and clinical discovery are held back. Medical Reporting Solutions, Inc. has developed advanced technology, which we propose to extend, refine, and test for constructing specialized semantic knowledge bases. These knowledge bases will encode the clinical information in medical reports, and enable automated natural language processing systems for extracting clinical knowledge.      Our research and development uses methods in corpus linguistics and sentential logic to represent the knowledge in free-text medical reports in an efficient, codeable manner. We have created tools to map sentences in a medical domain to unique codeable propositions. Our method for creating knowledge ontologies makes it easy for biomedical researchers to get semantic information at the appropriate level of detail. The knowledge base and mapping tables allow us to analyze medical reports in near real-time. One knowledge base, under development, is derived from hundreds of thousands of reports in the radiology domain, and we intend to analyze other medical domains using the methods we have pioneered.      Our phase one project plan includes further improving our knowledge editing tools, substantially enlarging our semantic knowledge base to cover over 60-70% of the radiology domain, and extensively test our knowledge representation schema against actual radiology reports. We plan to make the knowledge base freely accessible to the biomedical research community, while providing commercial services to codify free text reports found in EMRs. n/a",Extracting Semantic Knowledge from Clinical Data Sources,6988908,R43LM008974,"['automated medical record system', 'clinical research', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'human data', 'informatics', 'information retrieval']",NLM,"LOGICAL SEMANTICS, INC.",R43,2005,100000,-0.004286682433556063
"Nation Center: Multi-Scale Study- Cellular Networks(RMI)    DESCRIPTION (provided by applicant):  A network of molecular interactions, involving many thousands of genes, their products, and other molecules, underlie cellular processes. Investigation of these interactions across a wide range of scales ranging from the formation/activation of transcriptional complexes, to the availability of a signaling pathway, all the way to macroscopic processes, such as cell adhesion, calls for a new level of sophistication in the design of genome- wide computational approaches. A homogeneous environment for the comprehensive mapping and analysis of molecular cellular interactions in would be a powerful resource for the biomedical research community. We propose the creation of a National Center for the Multiscale Analysis of Genomic and Cellular Networks (MAGNet). The Center will provide an integrative computational framework to organize molecular interactions in the cell into manageable context-dependent components and will develop interoperable computational models and tools that can leverage such a map of cellular interactions to elucidate important biological processes. Center activities will involve a significant, multidisciplinary effort of biological and computational sciences. Specific areas of expertise include natural language parsing (NLP), machine learning (ML), software systems and engineering, databases, computational structural biology, reverse engineering of genetic networks, biomedical literature datamining, and biomedical ontologies, among others. The Center will 1) construct an evidence integration framework to collect and fuse a variety of diverse cellular interaction clues based on their statistical relevance 2) assemble a comprehensive set of physics- and knowledge-based methodologies to fill this framework 3) provide a set of methodologies and filters, anchored in formal domain ontologies, to associated specific interactions to an organism, tissue, molecular, and cellular context. All relevant tools will be made accessible to the biomedical research community through a common, extensible, and interoperable software platform, geWorkbench. We will reach out to train and encourage researchers to use and/or develop new modules for, geWorkbench. An important element of the software platform will be the development of specific components that can exploit the evidence integration techniques developed by Core 9001 investigators to combine molecular interaction clues from Core 9002 algorithms and databases. Development will be both driven and tested by the biomedical community to ensure the usefulness of the tools and the usability of the graphical user interfaces to address biomedical problems in completely novel ways, to dissect the web of cellular interactions responsible for cellular processes and functions.         n/a",Nation Center: Multi-Scale Study- Cellular Networks(RMI),7012104,U54CA121852,"['bioinformatics', 'cell biology', 'computational biology', 'cooperative study', 'genome']",NCI,COLUMBIA UNIVERSITY HEALTH SCIENCES,U54,2005,3758967,-0.018568184797957132
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6865478,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2005,162750,0.023252776525259013
"Technology Development for a MolBio Knowledge-Base   DESCRIPTION (provided by applicant):     Since the introduction of the Mycin system more than 25 years ago, it has widely been hypothesized that extensive, well-represented computer knowledge-bases will facilitate a wide variety of scientific and clinical tasks. Driven by growing knowledge-management challenges arising from the proliferation of high throughput instrumentation, recently created knowledge-bases in areas related to genomics and related aspects of contemporary biology, such as the Gene Ontology, EcoCyc and PharmGKB, have begun to become integrated into the laboratory practices of a growing number of molecular biologists. However, these successful molecular biology knowledge-bases (MBKBs) have two drawbacks which impede their more general application: each has been narrowed to a particular special purpose, either in its domain of applicability or in the scope of knowledge represented, and each of these knowledge-bases was constructed largely on the basis of enormous human effort. Given the current state of molecular biology data and recent advances in database integration and information extraction technology, we proposed to test the following hypothesis: Current computational technology and existing human-curated knowledge resources are sufficient to build an extensive, high-quality computational knowledge-base of molecular biology. To test this hypothesis we propose to first create tools which can (a) automatically link incommensurate knowledge sources using semantic linking, and (b) use natural language processing techniques to extract new information from NCBrs GeneRIFs and from the GO definitions fields; and second, to evaluate the results of these methods by carefully quantifying the degree to which the induced linkages and extracted assertions are complete, consistent and correct. Although we propose to construct a broad and rich knowledge-base in order to develop and test the adequacy of largely automated methods to leverage existing human-curated collections, we do not propose to build a comprehensive MBKB.            n/a",Technology Development for a MolBio Knowledge-Base,6822280,R01LM008111,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computational biology', 'computer program /software', 'functional /structural genomics', 'information retrieval', 'molecular biology information system', 'technology /technique development']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2004,577307,0.012544265048063039
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,-0.024744834654683624
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6783420,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2004,200515,0.011285071963432102
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,0.02268393484262289
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,0.02268393484262289
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6697243,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,UNIVERSITY OF IOWA,R33,2004,252586,-0.020704452842696678
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,0.0257285435792743
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6709988,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2004,161668,0.023252776525259013
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6658916,R33GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2003,200824,0.011285071963432102
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,0.02268393484262289
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6744998,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2003,191306,0.046003544449467755
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6549357,R33EY013688,"['artificial intelligence', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' human subject', ' informatics', ' interdisciplinary collaboration', ' macular degeneration', ' mathematics', ' pathologic process', ' phenotype']",NEI,UNIVERSITY OF IOWA,R33,2003,259228,-0.020704452842696678
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6526728,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2002,372289,0.018331205508272545
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6549345,R21GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R21,2002,99510,0.011285071963432102
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6391275,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,215487,0.02788847295722112
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6388359,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2001,362594,0.018331205508272545
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6363593,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2001,306158,0.046003544449467755
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6185231,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,213046,0.02788847295722112
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6031661,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2000,354198,0.018331205508272545
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,0.019954525356583267
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,0.019954525356583267
"AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE It is becoming increasingly difficult for biologists to keep pace with           information being published within their own fields, let alone biology           as a whole. The ability to rapidly access specific and current                   biomedical information as well as to quickly gain an overview of current         knowledge in a given field is becoming more difficult while at the same          time more important. Traditional methods of keeping up with advances are         therefore becoming inadequate.                                                                                                                                    This project will involve a unique collaboration between a computational         linguist at Brandeis University and two biologists at Tufts University           School of Medicine. We propose to make use of recent advances in the             computational analysis of text to organize and summarize the biological          literature. Building on our previous language technology research at             Brandeis, we propose to integrate the domain knowledge of the National           Library of Medicine's Unified Medical Language System (UMLS) with                Brandeis' semantic lexicon, CoreLex, toward the development of                   normalized structured representations of the semantic content of                 abstracts in the Medline database. These data structures, called lexical         webs, accelerate the availability of information in a richly hyperlinked         index that facilitates rapid navigation and information access.                  Automated analysis of biological abstracts will be combined with                 information derived from sequence databases to provide an up-to-date and         comprehensive database of information regarding known genes and                  proteins. The results of this analysis will be used to construct a web           accessible database organized on a gene-by-gene basis.                                                                                                            Other unique aspects of this database will be the visualization of               motifs and features extracted from Medline abstracts through the                 generation of annotated structure-function maps of proteins and genes,           and the construction of gene-specific semantic indexes to the relevant           biological literature. This system, called MedStract, will reduce the            time required for biomedical researchers to find information of interest         and should facilitate the development of new research insights.                   n/a",AUTOMATED KNOWLEDGE EXTRACTION FOR BIOMEDICAL LITERATURE,6165092,R01LM006649,"['Internet', ' abstracting', ' artificial intelligence', ' computer assisted sequence analysis', ' computer system design /evaluation', ' informatics', ' information retrieval', ' information system analysis', ' molecular biology information system', ' nucleic acid sequence', ' protein sequence', ' semantics', ' syntax', ' vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2000,297119,0.046003544449467755
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,9952803,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2020,92359,0.022597503595082443
"Temporal relation discovery for clinical text Project Summary / Abstract The current proposal continues the investigation on the topic of temporal relation extraction from the Electronic Medical Records (EMR) clinical narrative funded by the NLM since 2010 (Temporal Histories of Your Medical Events, or THYME; thyme.healthnlp.org). Through our efforts so far, we have defined the topic as an active area of research attracting attention across the world. Since its inception, the project has pushed the boundaries of this highly challenging task by investigating new computational methods within the context of the latest developments in the fields of natural language processing (NLP), machine learning (ML), artificial intelligence (AI) and biomedical informatics (BMI) resulting in 60+ publications/presentations. We have made our best performing methods available to the community open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES; ctakes.apache.org). In 2015, 2016, 2017 and 2018, we organized an international shared task (Clinical TempEval) on the topic under the umbrella of the highly prestigious SemEval, thus inviting the international community to work with our THYME data and improve on our results. Clinical TempEval has been highly successful with many participants each year, resulting in new discoveries and many publications. We have made all our data along with our gold standard annotations available to the community through the hNLP Center (center.healthnlp.org).  The underlying theme of this renewal is novel methods for combining explicit domain knowledge (linguistic, semantic, biomedical ontological, clinical), readily available unlabeled data (health-related social media, EMRs), and modern machine learning techniques (e.g. neural networks) for temporal relation extraction from the EMR clinical narrative. Therefore, our renewal proposes a novel and much needed exploration of this line of research:  Specific Aim 1: Develop computational models for novel rich semantic representations such as the Abstract Meaning Representations to encapsulate a single, coherent, full-document graphical representation of meaning for temporal relation extraction  Specific Aim 2: Develop computational methods to infuse domain knowledge (linguistic, semantic, biomedical ontological, clinical) into modern machine learning techniques such as NNs for temporal relation extraction – through input representations, pre-trained vectors, or architectures  Specific Aim 3: Develop novel methods for combining labeled and unlabeled data from various sources (EMR, health-related social media, newswire) for temporal relation extraction from the clinical narrative  Specific Aim 4: Apply the best performing methods for temporal relation extraction developed in SA1-3 to temporally sensitive phenotypes for direct translational sciences studies. Dissemination efforts through publications and open source releases into Apache cTAKES. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EMR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,9949779,R01LM010090,"['Address', 'Apache', 'Architecture', 'Area', 'Artificial Intelligence', 'Attention', 'Clinical', 'Cognitive', 'Communities', 'Complex', 'Computer Models', 'Computerized Medical Record', 'Computing Methodologies', 'Coupled', 'Data', 'Development', 'Disease', 'Encapsulated', 'Engineering', 'Event', 'Fostering', 'Foundations', 'Funding', 'Goals', 'Gold', 'Health', 'Image', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Label', 'Linguistics', 'Link', 'Machine Learning', 'Medical', 'Methods', 'Modernization', 'Natural Language Processing', 'Nature', 'Participant', 'Patient Care', 'Patients', 'Phenotype', 'Publications', 'Recording of previous events', 'Research', 'Semantics', 'Signs and Symptoms', 'Solid', 'Source', 'Speed', 'Structure', 'System', 'Techniques', 'Text', 'Thyme', 'Time', 'TimeLine', 'Training', 'Translational Research', 'Vision', 'Work', 'advanced disease', 'base', 'biomedical informatics', 'biomedical ontology', 'clinically relevant', 'cohesion', 'electronic data', 'electronic structure', 'epidemiology study', 'improved', 'individualized medicine', 'learning community', 'neural network', 'next generation', 'novel', 'open source', 'programs', 'relating to nervous system', 'social media', 'support vector machine', 'symptom treatment', 'vector']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2020,545793,0.001766230567720322
"BOND: Benchmarking based on heterogeneous biOmedical Network and Deep learning novel drug-target associations Project Summary/Abstract The applicant’s goals are to develop the necessary skills to become an independent translational biomedical informatics researcher in the area of computational drug repurposing. Exploring novel drug-target interactions (DTI) plays a crucial role in drug development. In order to lower the overall costs and uncover more potential screening targets, computational (in silico) methods have become popular and are commonly applied to poly-pharmacology and drug repurposing. Although machine learning-based strategies have been studied for years, there is no standardized benchmark that provides large-scale training datasets as well as diverse evaluation tasks to test different methods. Furthermore, the existing methods suffer from remarkable limitations, where 1) results are often biased due to a lack of negative samples, 2) novel drug-target associations with new (or isolated) drugs/targets cannot be explored, and 3) the comprehensive topological structure cannot be captured by feature learning methods . Therefore, in the era of big data, the applicant proposes a study to tackle the challenges by achieving two aims. • Aim 1 (K99 Phase): Develop a large scale benchmark for evaluating drug-target prediction based on the  generation of a multipartite network from heterogeneous biomedical datasets. • Aim 2 (R00 Phase): Adapt a deep learning model to build an accurate predictive model based on a novel  feature learning algorithm that mines the multi-dimensional biomedical network (multipartite network). In the mentored phase, the applicant will integrate heterogeneous biomedical datasets and build a benchmark for evaluation of the drug-target prediction based on well-designed strategies. The applicant will receive training in standardization tools for data integration, tools, and skills for data management, evaluation methods for drug-target predictions, and state-of-the-art machine learning/deep learning methods in computer-aided pharmacology. Complementary didactic, intellectual, and professional training will help prepare the applicant for the R00 phase where he will develop a deep learning-based predictive model and multi-dimensional graph embedding methods for feature learning. Together, these novel studies will advance the current computational drug repurposing by providing 1) comprehensive benchmarking for testing and evaluation, and 2) a scalable and accurate predictive model based on a biomedical multi-partite network. The applicant will be mentored by senior, established investigators with substantial expertise in Semantic Web, computational biology, cancer genomics, drug development, and machine learning/deep learning. Importantly, this project will provide a foundation for the applicant to establish independent research programs in 1) computational drug repurposing in real cases, 2) investigation of the diverse hidden associations in system biology (e.g., associations between drugs, genetics, and diseases), and 3) precision medicine aimed applications leveraging biomedical knowledgebases and electronic health records. Project Narrative The field of computational drug repurposing lacks a large scale benchmark that provides comprehensive and standard evaluation tasks as well as a scalable and accurate prediction model that can handle large biomedical datasets. This study aims to utilize Semantic Web technology to construct a multi-partite network based on heterogeneous biomedical databases and develop a deep learning-based predictive model based on the network. The proposed investigation will advance this field by providing a large scale benchmark for evaluation as well as a predictive model based on state-of-the-art technology.",BOND: Benchmarking based on heterogeneous biOmedical Network and Deep learning novel drug-target associations,10054989,K99GM135488,"['Address', 'Algorithms', 'Area', 'Base Ratios', 'Benchmarking', 'Big Data', 'Clinic', 'Computational Biology', 'Computer Assisted', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dimensions', 'Disease', 'Drug Evaluation', 'Drug Targeting', 'Electronic Health Record', 'Employment', 'Evaluation', 'Foundations', 'Generations', 'Genetic', 'Goals', 'Graph', 'Investigation', 'Learning', 'Learning Module', 'Link', 'Machine Learning', 'Mentors', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Network-based', 'Pharmaceutical Preparations', 'Pharmacology', 'Phase', 'Play', 'Positioning Attribute', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Silver', 'Source', 'Standardization', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Training', 'Ursidae Family', 'Validation', 'base', 'biomedical informatics', 'cancer genomics', 'computer based Semantic Analysis', 'cost', 'data integration', 'data management', 'data tools', 'deep learning', 'deep neural network', 'design', 'drug development', 'flexibility', 'heterogenous data', 'improved', 'in silico', 'knowledge base', 'large scale data', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'new therapeutic target', 'novel', 'precision medicine', 'predictive modeling', 'predictive test', 'programs', 'repository', 'screening', 'skills', 'tool']",NIGMS,MAYO CLINIC ROCHESTER,K99,2020,100000,-0.008492195859304693
"Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points Project Summary Biomedical data science data modeling is relevant to a plethora of informatics research activities, such as natural language processing, machine learning, artificial intelligence, and predictive analytics. As Electronic Health Record systems become more advanced and more mature, with the potential to incorporate a wide and diverse array of data from genomics to mobile health (mHealth) applications, the scope and nature of the biomedical data science questions researchers ask become broader. Concomitantly, the answers to their questions have the potential to impact the care of millions of patients—getting the answers right, proactively, is high stakes. However, in data modeling currently, there is no bioethics framework to guide the process of mapping key decision points and recording the rationale for choices made. Making data modeling decision points, as well as the reasoning behind them, explicit would have a twofold impact on improving biomedical data science by: 1. Enhancing transparency and reproducibility and maximizing the value of data science research and 2. Supporting the ability to assess decision points and rationales in terms of their most crucial ethical ramifications. Research in this area is particularly timely amid the interest in, and enthusiasm for, leveraging Big Data sources in the service of improving patient population health and the health of the general public. The National Institutes of Health (NIH) recently released a strategic plan for data science; there is no better time than now to create an initial bioethical framework to inform common data modeling decision points. The improvements in data quality that will derive from decision point mapping and bioethical review will enhance efforts to apply data models across a range of high-impact areas, from predictive analytics to support clinical decision-making to robust trending models in population health to better inform local, regional, and national health policies and resource allocation. To develop this initial bioethics framework, we will use well- established qualitative research methods (interviews, focus groups, and in-person deliberation) to map the decision points in biomedical data modeling research and document the rationales invoked to support those decisions (Aim 1 key informant interviews); assess those data science decision points and decision-making rationales for their bioethical ramifications (Aim 2 focus groups); and create an initial bioethics data modeling framework (Aim 3 deliberative meeting). This study would be the first to provide a bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe. This approach directly supports core scientific values of inclusivity, transparency, accountability, and reproducibility that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global. Project Narrative This study would be the first to develop an initial bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe—not least because poorly developed data models have the potential to impact adversely the health of individuals, groups, and communities. Currently, there is limited conversation around potential bioethics issues in data modeling, and as yet no implementable guidance on how biomedical data science modeling research activities should occur. The initial ethics framework developed by this study would provide a roadmap to ensure that data modeling decision points are documented and their ethical ramifications considered at the outset of model creation, thus supporting core scientific values of inclusivity, accountability, reproducibility, and transparency that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global.",Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points,10039527,R21HG011277,"['Accountability', 'Address', 'Area', 'Artificial Intelligence', 'Big Data', 'Bioethical Issues', 'Bioethics', 'Bioethics Consultants', 'Caring', 'Clinical', 'Communities', 'Data', 'Data Science', 'Data Scientist', 'Data Sources', 'Decision Making', 'Development', 'Electronic Health Record', 'Ensure', 'Ethical Issues', 'Ethical Review', 'Ethics', 'Focus Groups', 'Fostering', 'General Population', 'Health', 'Health Resources', 'Health system', 'Individual', 'Informatics', 'Interview', 'Machine Learning', 'Maps', 'Methods', 'Mobile Health Application', 'Modeling', 'National Health Policy', 'Natural Language Processing', 'Nature', 'Output', 'Patients', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Qualitative Research', 'Reproducibility', 'Research', 'Research Activity', 'Research Methodology', 'Research Personnel', 'Resource Allocation', 'Role', 'Services', 'Social Environment', 'Strategic Planning', 'Structure', 'System', 'Time', 'Trust', 'United States National Institutes of Health', 'Walking', 'base', 'biomedical data science', 'clinical decision support', 'clinical decision-making', 'data modeling', 'data quality', 'data tools', 'ethical legal social implication', 'genomic data', 'high standard', 'improved', 'individual patient', 'informant', 'interest', 'interoperability', 'meetings', 'model development', 'patient population', 'population health', 'programs', 'public trust', 'tool', 'trend', 'usability']",NHGRI,"HASTINGS CENTER, INC.",R21,2020,100000,0.0018066089832271517
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9990898,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'machine learning method', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'structured data', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2020,291873,0.01605499985278066
"Advanced End-to-End Relation Extraction with Deep Neural Networks ABSTRACT Relations linking various biomedical entities constitute a crucial resource that enables biomedical data science applications and knowledge discovery. Relational information spans the translational science spectrum going from biology (e.g., protein–protein interactions) to translational bioinformatics (e.g., gene–disease associations), and eventually to clinical care (e.g., drug–drug interactions). Scientists report newly discovered relations in nat- ural language through peer-reviewed literature and physicians may communicate them in clinical notes. More recently, patients are also reporting side-effects and adverse events on social media. With exponential growth in textual data, advances in biomedical natural language processing (BioNLP) methods are gaining prominence for biomedical relation extraction (BRE) from text. Most current efforts in BRE follow a pipeline approach containing named entity recognition (NER), entity normalization (EN), and relation classiﬁcation (RC) as subtasks. They typically suffer from error snowballing — errors in a component of the pipeline leading to more downstream errors — resulting in lower performance of the overall BRE system. This situation has lead to evaluation of different BRE substaks conducted in isolation. In this proposal we make a strong case for strictly end-to-end evaluations where relations are to be produced from raw text. We propose novel deep neural network architectures that model BRE in an end-to-end fashion and directly identify relations and corresponding entity spans in a single pass. We also extend our architectures to n-ary and cross-sentence settings where more than two entities may need to be linked even as the relation is expressed across multiple sentences. We also propose to create two new gold standard BRE datasets, one for drug–disease treatment relations and another ﬁrst of a kind dataset for combination drug therapies. Our main hypothesis is that our end-to-end extraction models will yield supe- rior performance when compared with traditional pipelines. We test this through (1). intrinsic evaluations based on standard performance measures with several gold standard datasets and (2). extrinsic application oriented assessments of relations extracted with use-cases in information retrieval, question answering, and knowledge base completion. All software and data developed as part of this project will be made available for public use and we hope this will foster rigorous end-to-end benchmarking of BRE systems. NARRATIVE Relations connecting biomedical entities are at the heart of biomedical research given they encapsulate mech- anisms of disease etiology, progression, and treatment. As most such relations are ﬁrst disclosed in textual narratives (scientiﬁc literature or clinical notes), methods to extract and represent them in a structured format are essential to facilitate applications such as hypotheses generation, question answering, and information retrieval. The high level objective of this project is to develop and evaluate novel end-to-end supervised machine learning methods for biomedical relation extraction using latest advances in deep neural networks.",Advanced End-to-End Relation Extraction with Deep Neural Networks,10052028,R01LM013240,"['Adverse event', 'Architecture', 'Area', 'Benchmarking', 'Bioinformatics', 'Biology', 'Biomedical Research', 'Classification', 'Clinical', 'Code', 'Collaborations', 'Combination Drug Therapy', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Disease', 'Distant', 'Drug Interactions', 'Encapsulated', 'Etiology', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Gold', 'Growth', 'Hand', 'Heart', 'Information Retrieval', 'Information Sciences', 'Intramural Research', 'Joints', 'Knowledge Discovery', 'Label', 'Language', 'Lead', 'Link', 'Literature', 'Manuals', 'Maps', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Patients', 'Peer Review', 'Performance', 'Periodicity', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'Scientist', 'Semantics', 'Software Tools', 'Source', 'Standardization', 'Structure', 'Supervision', 'System', 'Terminology', 'Testing', 'Text', 'Training', 'Translational Research', 'Trees', 'base', 'biomedical data science', 'clinical care', 'deep neural network', 'improved', 'insight', 'interest', 'knowledge base', 'machine learning method', 'natural language', 'neural network', 'neural network architecture', 'new therapeutic target', 'novel', 'off-label use', 'protein protein interaction', 'relating to nervous system', 'side effect', 'social media', 'supervised learning', 'syntax']",NLM,UNIVERSITY OF KENTUCKY,R01,2020,358691,0.026180734699239497
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10057158,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'structured data', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2020,795873,0.019137829447402194
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9955351,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical data science', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'large scale data', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,517554,0.006427916634859874
"Scientific Questions: A New Target for Biomedical NLP Project Summary  Natural language processing (NLP) technology is now widespread (e.g. Google Translate) and has several important applications in biomedical research. We propose a new target for NLP: extraction of scientific questions stated in publications. A system that automatically captures and organizes scientific questions from across the biomedical literature could have a wide range of significant impacts, as attested to in our diverse collection of support letters from researchers, journal editors, educators and scientific foundations. Prior work focused on making binary (or probabilistic) assessments of whether a text is hedged or uncertain, with the goal of downgrading such statements in information extraction tasks—not computationally capturing what the uncertainty is about. In contrast, we propose an ambitious plan to identify, represent, integrate and reason about the content of scientific questions, and to demonstrate how this approach can be used to address two important new use cases in biomedical research: contextualizing experimental results and enhancing literature awareness. Contextualizing results is the task of linking elements of genome-scale results to open questions across all of biomedical research. Literature awareness is the ability to understand important characteristics of large, dynamic collections of research publications as a whole. We propose to produce rich computational representations of the dynamic evolution of research questions, and to prototype textual and visual interfaces to help students and researchers explore and develop a detailed understanding of key open scientific questions in any area of biomedical research. Project Narrative The scientific literature is full of statements of important unsolved questions. By using artificial intelligence systems to identify and categorize these questions, the proposed work would help other researchers discover when their findings might address an important question in another scientific area. This work would also make it easier for students, journal editors, conference organizers and others understand where science is headed by tracking the evolution of scientific questions.",Scientific Questions: A New Target for Biomedical NLP,10069773,R01LM013400,"['Address', 'Area', 'Artificial Intelligence', 'Awareness', 'Biomedical Research', 'Characteristics', 'Collection', 'Computerized Patient Records', 'Cues', 'Data', 'Elements', 'Environment', 'Evolution', 'Expert Systems', 'Foundations', 'Genes', 'Goals', 'Gold', 'Information Retrieval', 'Journals', 'Letters', 'Link', 'Literature', 'Manuals', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Phenotype', 'Proteomics', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Signal Transduction', 'Source', 'Students', 'System', 'Taxonomy', 'Technology', 'Text', 'Time', 'Translating', 'Uncertainty', 'Update', 'Visual', 'Work', 'design', 'dynamical evolution', 'experimental study', 'genome wide association study', 'genome-wide', 'graduate student', 'high throughput screening', 'innovation', 'journal article', 'news', 'novel', 'pharmacovigilance', 'prototype', 'symposium', 'text searching', 'tool', 'transcriptome sequencing', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,462393,0.02060446997815578
"National Resource for Network Biology (NRNB) OVERALL - PROJECT SUMMARY The mission of the National Resource for Network Biology (NRNB) is to advance the science of biological networks by creating leading-edge bioinformatic methods, software tools and infrastructure, and by engaging the scientific community in a portfolio of collaboration and training opportunities. Much of biomedical research is dependent on knowledge of biological networks of multiple types and scales, including molecular interactions among genes, proteins, metabolites and drugs; cell communication systems; relationships among genotypes and biological and clinical phenotypes; and patient and social networks. NRNB-supported platforms like Cytoscape are among the most widely used software tools in biology, with tens of thousands of active users, enabling researchers to apply network concepts and data to understand biological systems and how they are reprogrammed in disease.  NRNB’s three Technology Research and Development projects introduce innovative concepts with the potential to transform network biology, transitioning it from a static to a dynamic science (TR&D 1); from flat network diagrams to multi-scale hierarchies of biological structure and function (TR&D 2); and from descriptive interaction maps to predictive and interpretable machine learning models (TR&D 3). In previous funding periods our technology projects have produced novel and highly cited approaches, including network-based biomarkers for stratification of disease, data-driven gene ontologies assembled completely from network data, and deep learning models of cell structure and function built using biological networks as a scaffold.  During the next period of support, we introduce dynamic regulatory networks formulated from single-cell transcriptomics and advanced proteomics data (TR&D 1); substantially improved methodology for the study of hierarchical structure and pleiotropy in biological networks (TR&D 2); and procedures for using networks to seed machine learning models of drug response that are both mechanistically interpretable and transferable across biomedical contexts (TR&D 3). These efforts are developed and applied in close collaboration with outside investigators from 19 Driving Biomedical Projects who specialize in experimental generation of network data, disease biology (cancer, neuropsychiatric disorders, diabetes), single-cell developmental biology, and clinical trials. TR&Ds are also bolstered by 7 Technology Partnerships in which NRNB scientists coordinate technology development with leading resource-development groups in gene function prediction, mathematics and algorithm development, and biomedical databases. Beyond these driving collaborations, we continually support a broader portfolio of transient (non-driving) research collaborations; organize and lead international meetings including the popular Network Biology track of the Intelligent Systems for Molecular Biology conference; and deliver a rich set of training opportunities and network analysis protocols. OVERALL - PROJECT NARRATIVE We are all familiar with some of the components of biological systems – DNA, proteins, cells, organs, individuals – but understanding biological systems involves more than just cataloging its component parts. It is critical to understand the many interactions of these parts within systems, and how these systems give rise to biological functions and responses and determine states of health and disease. The National Resource for Network Biology provides the scientific community with a broad platform of computational tools for the study of biological networks and for incorporating network knowledge in biomedical research.",National Resource for Network Biology (NRNB),9937486,P41GM103504,"['Area', 'Automobile Driving', 'Beds', 'Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Biomedical Technology', 'Cataloging', 'Catalogs', 'Cell Communication', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Conceptions', 'DNA', 'Data', 'Data Set', 'Databases', 'Developmental Cell Biology', 'Diabetes Mellitus', 'Disease', 'Disease stratification', 'Drug Modelings', 'Ecosystem', 'Educational workshop', 'Event', 'Expert Systems', 'Feedback', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic Risk', 'Genomics', 'Genotype', 'Goals', 'Health', 'Individual', 'Infrastructure', 'International', 'Knowledge', 'Lead', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mentors', 'Methodological Studies', 'Methods', 'Mission', 'Modeling', 'Molecular Biology', 'National Institute of General Medical Sciences', 'Network-based', 'Ontology', 'Organ', 'Pathway Analysis', 'Patients', 'Pharmaceutical Preparations', 'Phase Transition', 'Phenotype', 'Positioning Attribute', 'Procedures', 'Proteins', 'Proteomics', 'Protocols documentation', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Running', 'Science', 'Scientist', 'Seeds', 'Services', 'Social Network', 'Software Tools', 'Structure', 'Students', 'System', 'Technology', 'Testing', 'Tissues', 'Training', 'Visual', 'Visualization', 'Work', 'algorithm development', 'biological systems', 'clinical phenotype', 'cloud storage', 'computational platform', 'computerized tools', 'deep learning', 'gene function', 'genomics cloud', 'improved', 'innovation', 'interoperability', 'lens', 'mathematical algorithm', 'meetings', 'method development', 'multi-scale modeling', 'neuropsychiatric disorder', 'next generation', 'novel', 'pleiotropism', 'prediction algorithm', 'programs', 'protein metabolite', 'response', 'scaffold', 'single cell analysis', 'software infrastructure', 'symposium', 'technology development', 'technology research and development', 'tool', 'training opportunity', 'transcriptome', 'transcriptomics']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",P41,2020,1995376,-0.015335633196956735
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,0.033919004170786665
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,9987133,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,313495,0.0351884332049787
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9920181,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,249000,0.018036650318522238
"Finding emergent structure in multi-sample biological data with the dual geometry of cells and features    A fundamental question in biomedical data analysis is how to capture biological heterogeneity and characterize the complex spectrum of health states (or disease conditions) in patient cohorts. Indeed, much effort has been invested in developing new technologies that provide groundbreaking collections of genomic information at a single cell resolution, unlocking numerous potential advances in understanding the progression and driving forces of biological states. However, these new biomedical technologies produce large volumes of data, quantified by numerous measurements, and often collected in many batches or samples (e.g., from different patients, locations, or times). Exploration and understanding of such data are challenging tasks, but the potential for new discoveries at a level previously not possible justifies the considerable effort required to overcome these difficulties.  In this project we focus on multi-sample single-cell data, e.g., from a multi-patient cohort, where data points represent cells, data features represent gene expressions or protein abundances, and samples (e.g., considered as separate batches or datasets) represent patients. We consider a duality or interaction between constructing an intrinsic geometry of cells (e.g., with manifold learning techniques) and processing data features as signals over it (e.g., with graph signal processing techniques). We propose the utilization of this duality for several data exploration tasks, including data denoising, identifying noise-invariant phenomena, cluster characterization, and aligning cellular features over multiple datasets. Furthermore, we expect the dual multiresolution organization of data points and features to allow us to compute aggregated signatures that represent patients, and then provide a novel data embedding that reveals multiscale structure from the cellular level to the patient level.  The proposed research combines recent advances in several fields at the forefront of data science, including geometric deep learning, manifold learning, and harmonic analysis. The methods developed in this project will provide novel advances in each of these fields, while also establishing new relations between them. Furthermore, the challenges addressed by these methods are a foundational prerequisite for new advances in genomic research, and more generally in empirical data analysis where data is collected in varying experimental environments. The developed algorithms and methods in this project will be validated in several biomedical settings, including characterizing Zika immunity in Dengue patients, tracking progress of Lyme disease, and predicting the effectiveness of immunotherapy. In this project we will develop new algorithms for biomedical data analysis that will characterize the complex spectrum of health states across various patient populations. These algorithms will leverage the large volumes of data collected by new biomedical technologies, focusing on single-cell data. Specific analysis will be carried out for characterizing Zika immunity in Dengue patients, tracking progress of Lyme disease, and predicting the effectiveness of immunotherapy.",Finding emergent structure in multi-sample biological data with the dual geometry of cells and features   ,10022130,R01GM135929,"['Address', 'Algorithms', 'Biological', 'Biomedical Technology', 'Cells', 'Cellular Structures', 'Collection', 'Complex', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Dengue', 'Disease', 'Effectiveness', 'Environment', 'Foundations', 'Gene Expression', 'Gene Proteins', 'Genomics', 'Geometry', 'Graph', 'Health', 'Immunity', 'Immunotherapy', 'Learning', 'Location', 'Lyme Disease', 'Measurement', 'Methods', 'Noise', 'Patients', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Structure', 'Techniques', 'Time', 'ZIKA', 'algorithmic methodologies', 'biological heterogeneity', 'cohort', 'computerized data processing', 'data exploration', 'deep learning', 'denoising', 'driving force', 'multiple datasets', 'new technology', 'novel', 'patient population', 'signal processing']",NIGMS,MICHIGAN STATE UNIVERSITY,R01,2020,353150,-0.01111402375881278
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domainagnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10057127,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2020,609068,-0.019201615417520034
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9894759,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Information Retrieval', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Medicine', 'Methods', 'Mining', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'machine learning method', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'structured data', 'tool', 'unstructured data']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2020,269500,0.023731959185028784
"Computational Methods for Enhancing Privacy in Biomedical Data Sharing Project Summary Data sharing is essential to modern biomedical data science. Access to a large amount of genomic and clinical data can help us better understand human genetics and its impact on health and disease. However, the sensitive nature of biomedical information presents a key bottleneck in data sharing and collection efforts, limiting the utility of these data for science. The goal of this project is to leverage cutting-edge advances in cryptography and information theory to develop innovative computational frameworks for privacy-preserving sharing and analysis of biomedical data. We will draw upon our recent success in developing secure pipelines for collaborative biomedical analyses to address the imminent need to share sensitive data securely and at scale.  Practical adoption of existing privacy-preserving techniques in biomedicine has thus far been largely limited due to two major pitfalls, which this project overcomes with novel technical advances. First, emerging cryptographic data sharing frameworks, which promise to enable collaborative analysis pipelines that securely combine data across multiple institutions with theoretical privacy guarantees, are too costly to support complex and large-scale computations required in biomedical analyses. In this project, we will build upon recent advances in cryptography (e.g., secure distributed computation, pseudorandom correlation, zero-knowledge proofs) to significantly enhance the scalability and security of cryptographic biomedical data sharing pipelines. Second, existing approaches that locally transform data to protect sensitive information before sharing (e.g. de-identification techniques) either offer insufficient levels of protection or require excessive perturbation in order to ensure privacy. We will draw upon recent tools from information theory to develop effective local privacy protection methods that achieve superior utility-privacy tradeoffs on a range of biomedical data including genomes, transcriptomes, and medical images by directly exploiting the latent correlation structure of the data.  To promote the use of our privacy techniques, we will create production-grade software of our tools and publicly release them. We will also actively participate in international standard-setting organizations in genomics, e.g. GA4GH and ICDA, to incorporate our insights into community guidelines for biomedical privacy. Successful completion of these aims will result in computational methods and software tools that open the door to secure sharing and analysis of massive sets of sensitive genomic and clinical data. Our long-term goal is to broadly enable data sharing and collaboration efforts in biomedicine, thus empowering researchers to better understand the molecular basis of human health and to drive translation of new biological insights to the clinic. Project Narrative Rapidly-growing volume of biomedical datasets around the world promises to enable unprecedented insights into human health and disease. However, increasing concerns for individual privacy severely limited the extent of data sharing in the field. This project draws upon cutting-edge tools from cryptography and information theory to develop effective privacy- preserving methods for collecting, sharing, and analyzing sensitive biomedical data to empower advances in genomics and medicine.",Computational Methods for Enhancing Privacy in Biomedical Data Sharing,10017554,DP5OD029574,"['Address', 'Adoption', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Clinic', 'Clinical Data', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Disease', 'Electronic Health Record', 'Engineering', 'Enhancement Technology', 'Ensure', 'Foundations', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Genetics', 'Image', 'Individual', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'International', 'Knowledge', 'Letters', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mainstreaming', 'Mathematics', 'Medical Genetics', 'Medical Imaging', 'Mentorship', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Pattern', 'Pharmacology', 'Policies', 'Polynomial Models', 'Preservation Technique', 'Privacy', 'Privatization', 'Production', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Science', 'Secure', 'Security', 'Software Tools', 'Structure', 'Techniques', 'Technology', 'Translations', 'Vision', 'Work', 'analysis pipeline', 'base', 'biomedical data science', 'computer framework', 'computing resources', 'cost', 'cryptography', 'data sharing', 'design', 'empowered', 'experience', 'experimental study', 'genetic analysis', 'genome wide association study', 'genomic data', 'infancy', 'innovation', 'insight', 'novel', 'privacy preservation', 'privacy protection', 'software development', 'statistics', 'structured data', 'success', 'task analysis', 'tool', 'transcriptome', 'transcriptomics', 'web server']",OD,"BROAD INSTITUTE, INC.",DP5,2020,392799,-0.002733597241769375

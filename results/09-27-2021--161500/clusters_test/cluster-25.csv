text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Human Face Representation in Deep Convolutional Neural Networks The human visual system can recognize a familiar face across wide variations of viewpoint, illumination, expression, and appearance. This remarkable computational feat is accomplished by large-­scale networks of neurons. We will test a face space theory of the representations that emerge at the top layer of deep learning convolutional neural networks (DCNNs) as a model of human visual representations of faces. Computer-based face recognition has improved in recent years due to DCNNs and the easy availability of labeled training data (faces and identities) from the web. Inspired by the primate visual system, DCNNs are feed­forward artificial neural networks that can map images of faces into representations that support recognition over widely variable images. Although the calculations executed by the simulated neurons are simple, enormous numbers of computations are used to convert an image into a representation. The end result of this processing is a highly compact representation of a face that retains image detail in an invariant, identity­-specific face code. This code is fundamentally different than any representation of faces considered in vision science. This theory we test combines key components of previous face space models (similarity, learning history) with new features (imaging conditions, personal face history) in a unitary space that represents both identity and facial appearance across variable images. We will test whether this model can account for human recognition of familiar faces, which is highly robust to image variability (pose, illumination, expression). The model will also be applied to understanding long standing difficulties humans (and machines) have with faces of other races. We aim to bridge critical gaps in our knowledge of how DCNNs work, linking psychological, neural, and computational perspectives. A fundamentally new theory of face representation will alter the questions we ask about face representations in all three fields. A new focus on understanding how we (or neural networks) “perceive” a single familiar identity in widely variable images will give rise to a search for representations that gracefully merge the properties of faces with the real-­world image conditions in which they are experienced. This project presents a unique opportunity to study, manipulate, and learn from these representations, and to apply the findings to broader questions about high-­level vision from neural and perceptual perspectives. Human recognition of familiar faces is highly robust to image variability (pose, illumination, expression)—a skill that is likely due to the quality and quantity of experience we have with the faces of people we know well. Deep convolutional neural networks are modeled after the primate visual system and have made impressive gains recently on the problem of robust face recognition. Understanding the visual nature of the face “feature” codes that emerge in these networks can give insight into long-standing questions about how the human visual system can, but does not always, represent a face in a way that generalizes across images that vary widely.",Human Face Representation in Deep Convolutional Neural Networks,10129967,R01EY029692,"['Affect', 'Appearance', 'Categories', 'Code', 'Computers', 'Data', 'Data Set', 'Face', 'Face Processing', 'Familiarity', 'Human', 'Image', 'Individual', 'Internet', 'Knowledge', 'Label', 'Learning', 'Lighting', 'Link', 'Maps', 'Methods', 'Modeling', 'Nature', 'Neural Network Simulation', 'Neurons', 'Performance', 'Persons', 'Primates', 'Property', 'Published Comment', 'Race', 'Recording of previous events', 'Space Models', 'Testing', 'Training', 'Variant', 'Vision', 'Visual', 'Visual system structure', 'Work', 'artificial neural network', 'base', 'convolutional neural network', 'deep learning', 'experience', 'feedforward neural network', 'human model', 'improved', 'insight', 'neural network', 'psychologic', 'relating to nervous system', 'representation theory', 'skills', 'theories', 'vision science']",NEI,UNIVERSITY OF TEXAS DALLAS,R01,2021,361998
"Shared and specific mechanisms of auditory and visual category learning PROJECT SUMMARY/ABSTRACT The ability to learn new perceptual categories enables some of the most complex human behaviors, from speech perception to visual object recognition. Current understanding of the mechanisms involved in perceptual category learning relies on the fundamental assumption that the processes underlying such learning are shared across the senses. However, the vast majority of this work has focused on the visual modality. As a consequence, the research regarding how humans learn to group complex auditory information into categories has relied greatly on conclusions from the research in the visual domain without testing this critical assumption. However, recent evidence from the attention literature suggests that even seemingly domain-general cognitive processes, such as working memory, are accomplished via sensory-biased regions in frontal cortex. The current investigation will directly compare the computational and neural mechanisms supporting auditory and visual category learning by training the same individuals on categories in both modalities while in an fMRI scanner. Aim #1 of this investigation will identify the shared and sensory-biased circuits supporting feedback processing during auditory and visual category learning. If the neural circuits supporting perceptual category learning are shared across the modalities, it is expected that similar regions will be recruited to a similar extent during feedback processing. If instead, the neural circuits are distinct for particular modalities, it is expected that sensory-biased regions will emerge as supporting category learning for auditory and visual modalities. Aim #2 will utilize advanced machine learning techniques (multivariate pattern classification and representational similarity analyses) to characterize the emergence of category-level neural representations over the course of learning. Aim #3 will identify the functional and structural connectivity of the circuits as they contribute to perceptual category learning. The proposed research will directly test the fundamental assumption about the nature of this complex problem that affects everyday behaviors. This research has the potential to impact understanding of cases where modality- specific learning abilities might be impaired, such as phonetic learning and language-related impairments in dyslexia, autism, and specific language impairment. The proposed research will provide the training foundation to support the PI’s long-term objective of developing theories of perceptual category learning that are constrained by neurobiology and behavior and will specify the behavioral, computational, and neural mechanisms of such learning. This project presents the opportunity to directly test a critical assumption underlying understanding of perceptual category learning. The proposed research will take place in an exceptional training environment and the PI will be mentored by a team of knowledgeable and accomplished scientists. The research will provide the PI with training in functional magnetic resonance experiment design and analysis which will prepare her well for a career as an independent scientist in computational cognitive neuroscience. PROJECT NARRATIVE The proposed research will contribute to fundamental knowledge about how seemingly general-purpose cognitive systems may demonstrate modality specificity. The goal of this investigation is to characterize the differences in cognitive processing during category learning when the information comes from the auditory or visual modalities. The findings from this work may inform mechanistic approaches to understanding modality- specific deficits in language-based disorders, such as dyslexia, autism, and specific language impairment.",Shared and specific mechanisms of auditory and visual category learning,10197776,F32DC018979,"['Affect', 'Area', 'Attention', 'Auditory', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Categories', 'Classification', 'Cognitive', 'Complex', 'Corpus striatum structure', 'Disease', 'Dyslexia', 'Environment', 'Esters', 'Feedback', 'Finches', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hobbies', 'Human', 'Hybrids', 'Impairment', 'Individual', 'Investigation', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Magnetic Resonance', 'Mentors', 'Modality', 'Modeling', 'Nature', 'Neurobiology', 'Participant', 'Pattern', 'Process', 'Property', 'Research', 'Scientist', 'Sensory', 'Short-Term Memory', 'Specific qualifier value', 'Specificity', 'Speech', 'Speech Perception', 'Structure', 'Techniques', 'Testing', 'Theoretical model', 'Training', 'Visual', 'Work', 'auditory stimulus', 'autism spectrum disorder', 'base', 'behavior measurement', 'career', 'cognitive neuroscience', 'cognitive process', 'cognitive system', 'design', 'experimental study', 'frontal lobe', 'individual variation', 'innovation', 'learning ability', 'neural circuit', 'neuromechanism', 'object recognition', 'programs', 'recruit', 'relating to nervous system', 'response', 'sound', 'specific language impairment', 'theories', 'visual learning']",NIDCD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,F32,2021,65994
"Cracking the Olfactory Code Project Summary (Overall: Cracking the Olfactory Code)  Sensation drives perception, which informs decisions and actions. Olfaction is the main sense used by most animals to interact with the environment. However, olfaction remains shrouded in mystery — we do not know which molecular odorant features matter to the olfactory system and which do not, how information about these features is recombined to create holistic odor representations within the brain, or how those representations relate to perception. As a consequence, we lack an empirical understanding of the core transformations taking place at each stage of olfactory processing, which ultimately lead to perception and behavior. In addition, we lack a clear theoretical framework for understanding how a stimulus space that is both discrete and high-dimensional yields a perceptual space that is continuous and low dimensional. Because the olfactory system is “shallow” — meaning that within two synapses information about complete odor objects is abstracted and generalized — understanding this specific circuit will also afford general insight both into architecturally-related allocortical brain regions critical to behavior (e.g., cerebellum, hippocampus), and into cortical centers that play a key role in integrating diverse sources of information (e.g., prefrontal cortex, posterior parietal cortex). Here we propose to reveal the computational logic of olfaction by collecting the first system-wide dataset of neural and perceptual responses to a large, principled set of odorants, and by applying a unified statistical and theoretical approach to its interpretation. This project will convene research groups with expertise that spans neurobiology, and will leverage recent technical advances in molecular genetics, neural imaging, electrophysiology, opto- and chemogenetics, human psychophysics, and machine learning to interrogate all levels (from peripheral receptors to cortex to perceptual and behavioral output) of the olfactory system. Taken together, these experiments will establish a reference dataset that reveals the key transformations performed by the olfactory system, test a key unifying theory for olfaction, and create a community-wide resource that will prompt new theory and experiment. This work will also have wide-ranging implications for our general understanding of how sensory information is organized in the brain to facilitate adaptive action. Project Narrative (Overall) The brain builds rich internal representations of the external world in order to support perception and behavior. Here we take advantage of the architectural simplicity of the mammalian olfactory system — and an interdisciplinary team whose expertise ranges from molecular genetics and optogenetic to machine learning and human psychophysics — to characterize how odor information is sequentially transformed by neural circuits to generate meaningful perception. This work will both address longstanding mysteries about the inner workings of the olfactory system, and reveal general principles that govern how the brain organizes and processes information.",Cracking the Olfactory Code,10200162,U19NS112953,"['Address', 'Affinity', 'Animals', 'Architecture', 'Area', 'Behavior', 'Behavioral', 'Behavioral Paradigm', 'Big Data', 'Biological', 'Brain', 'Brain region', 'Cerebellum', 'Chemicals', 'Code', 'Communities', 'Computer Models', 'Data', 'Data Analyses', 'Data Science Core', 'Data Set', 'Decision Making', 'Development', 'Dimensions', 'Electrophysiology (science)', 'Environment', 'Esthesia', 'Frequencies', 'Hippocampus (Brain)', 'Human', 'Image', 'Lead', 'Link', 'Logic', 'Machine Learning', 'Measures', 'Modality', 'Modeling', 'Molecular', 'Molecular Genetics', 'Mus', 'Nasal Epithelium', 'Neurobiology', 'Neurons', 'Odorant Receptors', 'Odors', 'Olfactory Pathways', 'Output', 'Parietal Lobe', 'Pattern', 'Perception', 'Peripheral', 'Physiology', 'Play', 'Prefrontal Cortex', 'Process', 'Psychophysics', 'Research', 'Resources', 'Sense Organs', 'Sensory', 'Smell Perception', 'Source', 'Stimulus', 'Structural Models', 'Structure', 'Synapses', 'System', 'Testing', 'Time', 'Vision', 'Work', 'base', 'combinatorial', 'data integration', 'design', 'experimental study', 'genetic manipulation', 'high dimensionality', 'imaging approach', 'in vivo', 'information processing', 'insight', 'meetings', 'neural circuit', 'new technology', 'novel', 'olfactory bulb', 'olfactory receptor', 'olfactory stimulus', 'optogenetics', 'piriform cortex', 'receptor', 'relating to nervous system', 'response', 'sensory stimulus', 'sensory system', 'stimulus processing', 'theories', 'working group']",NINDS,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,U19,2021,2774000
"Cortical computations underlying binocular motion integration PROJECT SUMMARY / ABSTRACT Neuroscience is highly specialized—even visual submodalities such as motion, depth, form and color processing are often studied in isolation. One disadvantage of this isolation is that results from each subfield are not brought together to constrain common underlying neural circuitry. Yet, to understand the cortical computations that support vision, it is important to unify our fragmentary models that capture isolated insights across visual submodalities so that all relevant experimental and theoretical efforts can benefit from the most powerful and robust models that can be achieved. This proposal aims to take the first concrete step in that direction by unifying models of direction selectivity, binocular disparity selectivity and 3D motion selectivity (also known as motion-in-depth) to reveal circuits and understand computations from V1 to area MT. Motion in 3D inherently bridges visual submodalities, necessitating the integration of motion and binocular processing, and we are motivated by two recent paradigm-breaking physiological studies that have shown that area MT has a robust representation of 3D motion. In Aim 1, we will create the first unified model and understanding of the relationship between pattern and 3D motion in MT. In Aim 2, we will construct the first unified model of motion and disparity processing in MT. In Aim 3, we will develop a large-scale biologically plausible model of these selectivities that represents realistic response distributions across an MT population. Having a population output that is complete enough to represent widely-used visual stimuli will amplify our ability to link to population read-out theories and to link to results from psychophysical studies of visual perception. Key elements of our approach are (1) an iterative loop between modeling and electrophysiological experiments; (2) building a set of shared models, stimuli, data and analysis tools in a cloud-based system that unifies efforts across labs, creating opportunities for deep collaboration between labs that specialize in relevant submodalities, and encouraging all interested scientists to contribute and benefit; (3) using model-driven experiments to answer open, inter-related questions that involve motion and binocular processing, including motion opponency, spatial integration, binocular integration and the timely problem of how 3D motion is represented in area MT; (4) unifying insights from filter-based models and conceptual, i.e., non-image- computable, models to generate the first large-scale spiking hierarchical circuits that predict and explain how correlated signals and noise are transformed across multiple cortical stages to carry out essential visual computations; and (5) carrying out novel simultaneous recordings across visual areas. This research also has potential long-term benefits in medicine and technology. It will build fundamental knowledge about functional cortical circuitry that someday may be useful for interpreting dysfunctions of the cortex or for helping biomedical engineers construct devices to interface to the brain. Insights gained from the visual cortex may also help to advance computer vision technology. NARRATIVE The processing of visual motion and depth information is essential for a wide variety of important human abilities, including navigating through the world, avoiding collisions, catching and grabbing objects and interpreting complex scenes. To understand how neurons in the visual cortex transform and represent the information that underlies these abilities, we aim to initiate the development of a more complete, biologically constrained and openly available computer model of motion and depth processing that will be used to guide, and to interpret and incorporate results from, primate visual neurophysiological and psychophysical experiments. Gaining an understanding of the normal function of cortical neural circuitry is an important step in building the fundamental knowledge that someday may help to improve the ability to assess dysfunctions of the cortex and may help bioengineers create devices that interface to cortical circuitry to treat disorders and overcome disabilities.",Cortical computations underlying binocular motion integration,10188534,R01EY027023,"['3-Dimensional', 'Affect', 'Architecture', 'Biological', 'Biomedical Engineering', 'Brain', 'Collaborations', 'Color', 'Complex', 'Computer Models', 'Computer Vision Systems', 'Cues', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disadvantaged', 'Discrimination', 'Disease', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Foundations', 'Frequencies', 'Functional disorder', 'Human', 'Joints', 'Knowledge', 'Link', 'Literature', 'Medicine', 'Modeling', 'Motion', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Primates', 'Production', 'Psychophysics', 'Reproducibility', 'Research', 'Role', 'Scientist', 'Signal Transduction', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Vision Disparity', 'Visual', 'Visual Cortex', 'Visual Motion', 'Visual Perception', 'area MT', 'base', 'cloud based', 'color processing', 'disability', 'experimental study', 'extrastriate visual cortex', 'fitness', 'improved', 'in vivo', 'insight', 'interest', 'neural circuit', 'neurophysiology', 'novel', 'predictive modeling', 'relating to nervous system', 'response', 'spatial integration', 'spatiotemporal', 'theories', 'tool', 'visual neuroscience', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF WASHINGTON,R01,2021,387514
"Causal role of higher-order thalamo-cortical oscillations in sustained attention PROPOSAL SUMMARY/ABSTRACT Sustained attention – the continuous allocation of cognitive resources to respond to infrequent but behaviorally relevant stimuli – is impaired in many psychiatric disorders and represents an important aspect of cognitive control. Sustained attention requires top-down control and engagement with the external world, which is linked to both frontoparietal and thalamic controlling signals on primary sensor cortices. Despite the extensive behavioral characterization of sustained attention in animal models using the five-choice serial reaction time task (5-CSRTT), very little is known about the oscillatory interaction between the dorsal attention network and thalamo-cortical dynamics and its potential as a stimulation target for enhancing sustained attention. The objective of this project is to dissect the causal role of higher-order thalamo-cortical oscillations in sustained attention via temporally-precise rhythmic stimulation. I will focus on three regions in the visual thalamo-cortical network: higher-order visual thalamus (lateral aspect of lateral posterior nucleus, LPl), posterior parietal cortex (PPC), and primary visual cortex (V1). I will test the central hypothesis that LPl-cortical theta (4-7 Hz) functional connectivity causally coordinates PPC-V1 functional connectivity to facilitate sustained attention. The rationale of this work is that the proposed temporally-precise rhythmic optogenetic perturbations will directly test the causal role of thalamo-cortical functional connectivity in sustained attention. Accordingly, the two specific aims are: (1) Delineate the functional role of higher-order thalamo-cortical oscillations in sustained attention, (2) Determine the causal role of thalamo-cortical functional connectivity in sustained attention via temporally-precise rhythmic optogenetics. This work is significant because it will causally test a convergent model in which higher-order visual thalamus coordinates the parietal top-down control signals onto visual cortex that is crucial for developing circuit- based therapies to enhance sustained attention. The work is innovative due to its integration of closed-loop optogenetics circuit interrogation, multisite electrophysiology, freely-moving sustained attention task, and machine-learning tools for the investigation of the causal role of oscillatory synchronization. The overall positive impact of the proposed study is to provide a more comprehensive map of how the higher-order visual thalamus interacts with the frontoparietal control signal to modulate V1, and thus mediates sustained attention, a transdiagnostic cognitive function that shows impairment in many psychiatric disorders including attention deficit hyperactivity disorder, bipolar disorder, and schizophrenia. The implication of this study is that it may reveal a general mechanism underlying the interaction between two higher-order processing structures signaling to lower sensory cortices during cognitive processing. PROJECT NARRATIVE Sustained attention enables us to focus our cognitive resources on an activity for a prolonged period of time, and when impaired causes profound deficits in both cognition and behavior. Sustained attention is modulated by the thalamo-cortical rhythms. The research generated in this proposal will investigate (1) what oscillatory feature in the higher-order thalamo-cortical circuit is crucial for sustained attention, and (2) how the temporally- precise enhancing or disrupting of a thalamo-cortical oscillatory feature alters the functional connectivity in the circuit and behavioral outcomes mediated by sustained attention.",Causal role of higher-order thalamo-cortical oscillations in sustained attention,10199754,F31MH118799,"['Anatomy', 'Animal Model', 'Animals', 'Attention', 'Attention deficit hyperactivity disorder', 'Attentional deficit', 'Behavior', 'Behavioral', 'Bipolar Disorder', 'Cognition', 'Cognitive', 'Cognitive deficits', 'Communication', 'Communities', 'Computers', 'Conflict (Psychology)', 'Couples', 'Data', 'Dorsal', 'Electrophysiology (science)', 'Exhibits', 'Ferrets', 'Foundations', 'Frequencies', 'Functional disorder', 'Impairment', 'Implant', 'Impulsivity', 'Investigation', 'Lasers', 'Lateral', 'Lateral posterior nucleus of thalamus', 'Length', 'Link', 'Machine Learning', 'Maps', 'Mediating', 'Mental disorders', 'Methods', 'Microelectrodes', 'Modeling', 'Neurons', 'Parietal', 'Parietal Lobe', 'Performance', 'Periodicity', 'Phase', 'Physiologic pulse', 'Prefrontal Cortex', 'Process', 'Psyche structure', 'Reaction Time', 'Research', 'Resources', 'Rewards', 'Role', 'Schizophrenia', 'Signal Transduction', 'Statistical Models', 'Stimulus', 'Structure', 'Techniques', 'Testing', 'Thalamic structure', 'Therapeutic', 'Time', 'Touch sensation', 'Training', 'Visual', 'Visual Cortex', 'Work', 'area striata', 'base', 'behavioral outcome', 'calmodulin-dependent protein kinase II', 'cognitive control', 'cognitive function', 'functional disability', 'innovation', 'neurotransmission', 'novel', 'optogenetics', 'recruit', 'relating to nervous system', 'sensor', 'sensory cortex', 'signal processing', 'sustained attention', 'theories', 'tool', 'visual stimulus']",NIMH,UNIV OF NORTH CAROLINA CHAPEL HILL,F31,2021,40048
"Novel Perceptual and Oculomotor Heuristics for Enhancing Radiologic Performance PROGRAM SUMMARY Radiological imaging is often the first step of the diagnostic pathway for many devastating diseases; thus, an erroneous assessment of “normal” can lead to death. Whereas a grayscale object in an image can be described by its first-order image statistics—such as contrast, spatial frequency, position, entropy, and orientation—none of these dimensions, by itself, indicates abnormal vs normal radiological findings. We are a highly diverse team proposing an empirical approach to determine the mixtures of the first-order statistics—the “visual textures”— that radiology experts explicitly and implicitly use to identify the locations of potential abnormalities in medical images. Our innovative approach does not rely on assumptions about which textures may or may not be im- portant to abnormality detection. Instead, we will track the oculomotor behavior of expert radiologists to deter- mine their conscious and unconscious targeting choices, and thus ascertain which textures are empirically in- formative. The ability of expert radiologists to rapidly find abnormalities suggests that they may be able to first identify them in their retinal periphery. Peripheral visual analysis skills are therefore potentially critical to radio- logic performance, despite being understudied. We will measure these skills and leverage the results to develop perceptual learning heuristics to improve peripheral abnormality texture detection. By comparing novices to ex- perts we will determine whether the first are inexpert due to a lack of sensitivity to diagnostically relevant textures (texture informativeness), or to a lack of knowledge about which textures are abnormal, or to a combined lack of both sensitivity and knowledge. Radiology also requires the acquisition of oculomotor skills through practice and optimization. Radiologic expertise thus changes the oculomotor system in predictable and detectable ways, in much the same way that an athlete’s body and brain change as a function of expertise acquisition in their sport. We will therefore analyze both the consistency between experts’ fixation choices in medical images, and the eye movement performance characteristics of experts vs novice radiologists, to create an objective oculomotor bi- omarker of radiological expertise. The differences between novices and experts will train a deep learning (DL) system, which will have human visual and oculomotor performance characteristics. Training the DL with the abnormalities identified by a panel of expert radiologists will allow it to pinpoint the possible solutions in the manner of a simulated human radiologist performing at peak accuracy, precision, and speed. The resulting rank- ordered list of possible optimal and suboptimal image-reading strategies will serve as a benchmarking tool to quantify the performance of actual clinicians and residents who read the same images, rested vs fatigued. Meas- uring the effects of both training and fatigue on radiology expertise will be a major interdisciplinary cross-cutting advance in performance assessment. Our proposal to quantify fatigue in terms of erosion of expertise represents a transformational advance towards objective fitness-for-duty and expertise measures in medicine and beyond. PROJECT NARRATIVE There are 25-32 million perceptual errors in radiological case studies worldwide each year, contributing to med- ical error, the third most common cause of death in the US. We seek to reduce detection errors in radiology with four innovations: (1) we will empirically and objectively determine the visual textures used by expert radiologists to identify abnormalities within medical images; (2) we will determine the ways in which expert radiologists use their eyes, and especially their peripheral vision, to scan images and target informative regions; (3) we will de- velop a perceptual learning paradigm to optimally train residents in both texture perception and oculomotor per- formance domains; and (4) we will construct a deep learning model bestowed with simulated human visual and oculomotor capabilities, to create a normative model of human radiological expertise. The combined results from these studies will quantify peak expert performance and be employed to track and enhance individual expertise acquisition during radiology training; thus, the proposed research will help reduce medical error and moreover provide objective fitness-for-duty measurement tools—based on quantified biomarkers—to evaluate and ame- liorate the effects of fatigue on radiologic performance.",Novel Perceptual and Oculomotor Heuristics for Enhancing Radiologic Performance,10220201,R01CA258021,"['Assessment tool', 'Benchmarking', 'Biological Markers', 'Brain', 'COVID-19', 'Cancer Detection', 'Case Study', 'Cause of Death', 'Cessation of life', 'Characteristics', 'Clinical/Radiologic', 'Collection', 'Conscious', 'Data', 'Data Analyses', 'Databases', 'Detection', 'Diagnostic', 'Dimensions', 'Disease', 'Elements', 'Ensure', 'Entropy', 'Exposure to', 'Eye', 'Eye Movements', 'Fatigue', 'Film', 'Foundations', 'Frequencies', 'Human', 'Image', 'Incentives', 'Individual', 'Instruction', 'Knowledge', 'Lead', 'Learning', 'Location', 'Measurement', 'Measures', 'Medical Errors', 'Medical Imaging', 'Medicine', 'Modeling', 'Nature', 'North America', 'Outcome', 'Participant', 'Pathway interactions', 'Perception', 'Perceptual learning', 'Performance', 'Peripheral', 'Positioning Attribute', 'Radiologic Finding', 'Radiology Specialty', 'Reading', 'Research', 'Residencies', 'Resolution', 'Rest', 'Retina', 'Scanning', 'Societies', 'Speed', 'Sports', 'Stress', 'System', 'Testing', 'Texture', 'Thoracic Radiography', 'Time', 'Training', 'Unconscious State', 'Vision', 'Visual', 'Workload', 'X-Ray Computed Tomography', 'base', 'cancer diagnosis', 'cancer imaging', 'cohort', 'deep learning', 'design', 'experience', 'fitness', 'heuristics', 'human error', 'human model', 'improved', 'innovation', 'learning network', 'lung imaging', 'meetings', 'novel', 'oculomotor', 'oculomotor behavior', 'pandemic disease', 'patient safety', 'programs', 'radiological imaging', 'radiologist', 'sample fixation', 'shift work', 'skills', 'statistics', 'tool', 'tool development']",NCI,SUNY DOWNSTATE MEDICAL CENTER,R01,2021,646124
"Cortical visual processing for navigation Project summary Vision plays a key role in our ability to navigate through the environment, from identifying landmarks and obstacles to determining location and heading. While studies of visual cortex have provided an understanding of properties such as orientation selectivity and object recognition, much less is known about how cortical circuitry extracts and processes features from the visual scene to support navigation. In particular, there are two challenges. First, the nature of the visual stimulus is dramatically different in navigation, where the subject's movement through the world creates a complex and dynamic visual input, in contrast to standard synthetic stimuli presented to stationary subjects. Second, the types of visual features and computations that must be performed are different in navigation than in standard detection or discrimination paradigms. Our goal in this proposal is to determine how the brain extracts relevant visual features from the rich, dynamic visual input that typiﬁes active exploration, and investigate how the neural representation of these features can support visual navigation.  We will investigate this through three parallel aims, that build up from the representation of the visual scene in V1 during freely moving navigation, to the computation of speciﬁc variables needed for navigation. In our ﬁrst aim, we will measure the visual input in freely moving mice using miniature head-mounted cameras, together with neural activity in V1, to determine how neural dynamics represent the visual scene during natural navigation. In our second aim, we will use large ﬁeld-of-view two-photon imaging of multiple cortical areas, while mice navigate in a naturalistic open-world virtual reality system, to determine how visual features are represented across visual cortical areas. In our third aim, we will use 2-photon imaging in mice in a rotational arena to determine how visual input is used to dynamically update a key navigational variable: heading direction. Together, this project bridges foundational measurements in freely moving animals with mechanistic circuit investigations, to provide insights into an important aspect of visual system function. Project Narrative This project will study how the brain processes visual information to support navigation, which is important for guiding goal-directed movement through the world. The results of this work will provide knowledge about normal visual function and insights for treating impaired vision via prosthetic or assistive devices.",Cortical visual processing for navigation,10208550,R01NS121919,"['Animals', 'Area', 'Behavior', 'Behavioral', 'Brain', 'Code', 'Complex', 'Conflict (Psychology)', 'Cues', 'Data', 'Detection', 'Discrimination', 'Electrophysiology (science)', 'Environment', 'Foundations', 'Frequencies', 'Goals', 'Head', 'Hippocampus (Brain)', 'Image', 'Investigation', 'Knowledge', 'Location', 'Measurement', 'Measures', 'Modeling', 'Motion', 'Movement', 'Mus', 'Nature', 'Neural Network Simulation', 'Play', 'Population', 'Process', 'Property', 'Prosthesis', 'Rotation', 'Sampling', 'Self-Help Devices', 'Sensory', 'Signal Transduction', 'Space Models', 'Stimulus', 'Structure', 'Testing', 'Update', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Visual system structure', 'Visuospatial', 'Work', 'deep neural network', 'entorhinal cortex', 'experimental study', 'high dimensionality', 'insight', 'novel', 'object recognition', 'optic flow', 'orientation selectivity', 'relating to nervous system', 'response', 'statistics', 'synergism', 'theories', 'two-photon', 'virtual reality', 'virtual reality system', 'virtual world', 'visual information', 'visual process', 'visual processing', 'visual stimulus']",NINDS,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R01,2021,2833387
"Elucidating novel features of visual processing and physiological connectivity from retina to primary visual cortex Project Summary The use of stimuli with increasingly naturalistic properties has become critical to advance our understanding of vision. Many studies demonstrate that simple artificial stimuli (e.g. sinusoidal gratings and white noise) fail to engage nonlinearities that profoundly alter responses in the retina, lateral geniculate nucleus (LGN), and primary visual cortex (V1). A recent and striking example comes from the use of naturalistic ‘flow’ stimuli, which engage robust responses in V1 that are not predicted from responses to gratings. This gap in understanding motivates the development of a stimulus ensemble and analysis framework that produces a quantitative understanding of visual processing to increasingly naturalistic stimuli and the nonlinearities that they engage. Our objective is to understand how flow stimuli are processed from retina through visual cortex. To meet this goal, we will make neural population recordings in retina (Aims 1 & 3), LGN (Aims 1 & 3) and V1 (Aim 3) using matched experimental conditions and a unified theoretical/modeling framework to map the transformations that occur across these stages of visual processing. Our central hypothesis is that V1 transforms a discrete and heavily light-level-de- pendent retinal representation of natural stimuli into a continuous (uniform) representation that is relatively in- variant to changes in the mean luminance. This invariance places a strong constraint on the class of nonlineari- ties that transform retinal responses to those observed in LGN and V1. We test this hypothesis in three aims: (1) determine early visual processing (retina & LGN) of naturalistic flow stimuli; (2) develop an encoding manifold to capture the population activity at each processing stage and transforms from one stage to the next; (3) test the ability of the manifold description to predict the impact of light adaptation on processing flow stimuli from retina to V1. Aim 1 will yield a matched experimental dataset to an interesting and novel class of ecologically-relevant stimuli. Aim 2 will yield a quantitative framework by which to understand the transformations that occur between retina, LGN, and V1. Aim 3 will provide a platform for globally perturbing the output of the retina by switching from photopic to mesopic and scotopic conditions, and thereby compare predictions of our model to measured changes in LGN and V1 activity. The primary significance of this research is that it will provide a computationally and experimentally unified framework for understanding the transformations that occur in the processing of stim- uli across multiple stages of visual processing. The major innovations are (1) presenting visual stimuli for retinal recordings that are matched to eye movements and pupil dynamics in alert animals; (2) creating a novel analysis framework that captures the responses of neurons at all three levels and the inter-level transformations to in- creasingly complex stimuli; (3) utilizing light adaptation as a method of perturbing retinal output to test our model and the stability (invariance) of LGN and V1 responses to adapting retinal signals. The expected outcome is a data-driven model of the processing from retina to LGN and V1 that generalizes from starlight to sunlight. Project Narrative Restoring vision to the blind likely requires understanding how retinal signals are communicated to the brain and how these signals are transformed in the thalamocortical pathway. This project aims to acquire an understanding of these transformations in the context of complex and more naturalistic visual stimuli.",Elucidating novel features of visual processing and physiological connectivity from retina to primary visual cortex,10229447,R01EY031059,"['Affect', 'Animals', 'Brain', 'Collaborations', 'Complex', 'Cone', 'Data', 'Data Set', 'Development', 'Environment', 'Eye Movements', 'Future', 'Goals', 'Lateral Geniculate Body', 'Light', 'Light Adaptations', 'Machine Learning', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Movement', 'Mus', 'Neurons', 'Noise', 'Optics', 'Outcome', 'Output', 'Pathway interactions', 'Physiological', 'Population', 'Process', 'Property', 'Pupil', 'Research', 'Retina', 'Retinal Ganglion Cells', 'Rod', 'Signal Transduction', 'Stimulus', 'Structure', 'Sunlight', 'Techniques', 'Testing', 'Theoretical model', 'Variant', 'Vision', 'Visual Cortex', 'Visual system structure', 'Work', 'area striata', 'base', 'blind', 'cell type', 'computational neuroscience', 'experimental study', 'in vivo', 'innovation', 'luminance', 'multi-electrode arrays', 'novel', 'predictive modeling', 'receptive field', 'relating to nervous system', 'response', 'retinal neuron', 'sight restoration', 'stimulus processing', 'visual processing', 'visual stimulus']",NEI,DUKE UNIVERSITY,R01,2021,487568
"GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN PROJECT SUMMARY & ABSTRACT  Human locomotion through natural environments requires the coordination of all levels of the sensorimotor hierarchy, from the cortical areas involved in processing of visual information and high level planning to the subcortical and spinal structures involved in the regulation of the gait and posture. However, despite the complex neural bases of human locomotion, the output is highly regular and well organized around the basic physical dynamics and biomechanics that define the stability and energetic costs of moving a bipedal body through space. There is a rich and growing body of literature describing detailed knowledge each of the individual components of human locomotion, including neural mechanisms, muscular neuromechanics, and biomechanics. However, very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion through a complex and dynamic world. This lack of integrative research not only restricts the breadth of impact of research from these individual disciplines, but also limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to to fill this critical gap in our knowledge about human locomotion, it is necessary to develop an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world. In service of this general goal, this proposal outlines research projects aimed at specific unanswered questions about locomotion over different terrains. This proposal comprises three specific research and training aims on the visual control of locomotion over rough terrain. Aim 1 focuses on the behavioral task itself, Aim 2 investigates the sensory stimulus experienced during real-world locomotion, and Aim 3 examines the motor integration of visually specified goals into the ongoing gait cycle. Aim 1 investigates effects of changing environmental uncertainty and task demands on gaze allocation strategies during locomotion over real-world rough terrain. Aim 2 analyzes and models the visual stimulus experienced during locomotion over real-world rough terrain. Aim 3 determines how visually specified target footholds and targets are integrated into the ongoing preferred steady-state gait. Together these aims will significantly advance our understanding of how humans use vision to control their movement through the natural world, which greatly increase our ability to develop clinical diagnosis and treatment for loss of locomotor function. PROJECT NARRATIVE  Very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion. This lack of integrative research limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to fill this critical gap in our knowledge about human locomotion, this proposal develops an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world.",GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN,10224830,R00EY028229,"['3-Dimensional', 'Aging', 'Algorithms', 'Area', 'Attention', 'Behavior', 'Behavioral', 'Biomechanics', 'Clinical Treatment', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Development', 'Discipline', 'Environment', 'Eye', 'Gait', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Link', 'Literature', 'Locomotion', 'Measures', 'Mechanics', 'Mentors', 'Modeling', 'Motion', 'Motor', 'Movement', 'Muscle', 'Musculoskeletal', 'Nature', 'Neuromechanics', 'Output', 'Parkinson Disease', 'Pattern', 'Phase', 'Photic Stimulation', 'Positioning Attribute', 'Postdoctoral Fellow', 'Posture', 'Protocols documentation', 'Regulation', 'Research', 'Research Activity', 'Research Project Grants', 'Research Training', 'Services', 'Signal Transduction', 'Specific qualifier value', 'Spinal', 'Stroke', 'Structure', 'System', 'Training', 'Training Programs', 'Uncertainty', 'Vision', 'Visual', 'Visual Fields', 'Walking', 'Wireless Technology', 'area MST', 'area MT', 'base', 'clinical Diagnosis', 'cost', 'design', 'environmental change', 'experience', 'experimental study', 'foot', 'gaze', 'insight', 'instrument', 'kinematics', 'multidisciplinary', 'neuromechanism', 'neuromuscular', 'novel', 'optic flow', 'programs', 'relating to nervous system', 'response', 'sensory stimulus', 'skills', 'statistics', 'treadmill', 'treatment planning', 'visual control', 'visual information', 'visual processing', 'visual stimulus', 'visual-motor integration']",NEI,NORTHEASTERN UNIVERSITY,R00,2021,229062
"Early representation of 3D volumetric shape in visual object processing Project Summary The goal of this project is to test a novel theoretical framework for understanding how the ventral pathway subserves object vision. In the standard framework, a series of neural operations on 2D image data through many intermediate cortical stages, including area V4, leads to high-level perceptual representations, including representation of object identity, at the final stages of the ventral pathway. However, our preliminary microelectrode data from a fixating monkey show that many neurons in V4 represent volumetric (volume- enclosing) 3D shape, not 2D image patterns. These neurons respond to many different 2D images that convey the same 3D shape with different shape-in-depth cues, including shading, reflection, and refraction. They even respond preferentially to random dot stereograms that convey 3D volumetric shape with no 2D cues whatsoever. Moreover, our preliminary results with 2-photon functional imaging in anesthetized monkey V4 show that 3D shape signals are grouped by their similarity, and also group with isomorphic (same outline) 2D shape signals (which could contribute evidence to corresponding 3D shape inferences). We propose to capitalize on these preliminary data by demonstrating the prevalence of 3D shape tuning in area V4, analyzing the 3D shape coding strategies used by these neurons, and measuring how 2D and 3D shape signals are arranged at a microscopic level across the surface of V4. We expect these results to provide strong evidence that extraction of 3D shape fragments is a primary goal of V4 processing. This early extraction of 3D shape information, just two synapses beyond primary visual cortex, would suggest a competing framework for understanding the ventral pathway, in which the initial goal is to represent 3D physical structure, independent of the various 2D image cues used to infer it. In this framework, object recognition would be based on preceding information about 3D physical structure, which would explain why human object recognition is so robust to image changes, in a way that the best computational vision systems are not. The scientific impact of this work would be to divert vision experiments toward understanding representation of real world 3D structure (rather than 2D planar stimuli) and to encourage computational vision scientists to incorporate early 3D shape processing into the deep convolutional network models that are the current state of the art. Narrative This study will help explain how the human brain achieves such remarkably robust perception of visual objects. The results will help guide future rehabilitative and prosthetic strategies for patients with visual impairments, by elucidating neural strategies that could be used to bring computer vision prosthetics up to human performance levels, and by discovering specific neural signals for object information that could be replicated by electrode array implants in higher-level visual cortex.",Early representation of 3D volumetric shape in visual object processing,10173788,R01EY029420,"['3-Dimensional', '3D world', 'Area', 'Biological', 'Brain', 'Calcium Signaling', 'Code', 'Computer Vision Systems', 'Cues', 'Data', 'Electrodes', 'Functional Imaging', 'Future', 'Genetic Programming', 'Glass', 'Goals', 'Human', 'Image', 'Implant', 'Lead', 'Learning', 'Measures', 'Medial', 'Microelectrodes', 'Microscopic', 'Microscopy', 'Modeling', 'Monkeys', 'Network-based', 'Neurons', 'Ocular Prosthesis', 'Pathway interactions', 'Patients', 'Pattern', 'Performance', 'Population', 'Prevalence', 'Prosthesis', 'Rehabilitation therapy', 'Scientist', 'Series', 'Shapes', 'Signal Transduction', 'Stimulus', 'Structure', 'Surface', 'Synapses', 'System', 'Testing', 'Texture', 'Three-dimensional analysis', 'Time', 'V4 neuron', 'Vision', 'Visual Cortex', 'Visual Perception', 'Visual impairment', 'Work', 'area V4', 'area striata', 'base', 'convolutional neural network', 'experimental study', 'individual response', 'network models', 'neurotransmission', 'nonhuman primate', 'novel', 'object perception', 'object recognition', 'operation', 'relating to nervous system', 'response', 'three dimensional structure', 'two-photon', 'virtual reality', 'visual object processing']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2021,532112
"Function and circuitry of adaptive inhibition in the retina Studies of the visual system face a number of challenges, two of which are the intricacy of the cell types and synaptic connections that comprise the nervous system, and the complexity of the computational processes that underlie vision. Although the retina is one of the most characterized and well understood neural circuits of the visual system, it nonetheless has a great diversity of cell types, connections and computations. The normal function of the retina is to convey information about natural visual scenes, which have complex spatial and temporal structure. The processing of natural scenes has the greatest relevance towards a fundamental understanding retinal function, and the greatest clinical relevance. Yet most studies of retinal visual processing and circuitry focus on responses to simple artificial stimuli rarely encountered normally, such as flashing spots, drifting stripes and flickering checkerboards. With respect to retinal cell types greatest diversity lies in a class of inhibitory interneurons known as amacrine cells. These cells make extensive lateral and feedback connections, and although they form stereotyped connections between each other, excitatory bipolar cells, and ganglion cells that transit signals in the optic nerve, the functional effects of nearly all of these cell types are poorly understood. This proposal aims towards a direct characterization of the functional effects of amacrine cells under ethologically relevant stimuli, including natural scenes. We combine approaches of perturbation and recording using electrical and optical methods as well as computational modeling to characterize the specific contributions of amacrine cells to stimuli that include the representation of moving objects. We take advantage of recently developed computational approaches that can simultaneously capture the retinal response to a broad range of stimuli including natural scenes, capture a wide range of phenomena previously characterized only with artificial stimuli, and that have internal units highly correlated with retinal interneurons. Our goals are to 1) Create a quantitative understanding of the functional contributions of a class of sustained amacrine cells in the salamander retina for specific stimuli including those that represent moving objects and natural scenes, and test hypotheses related to dynamic effects on visual sensitivity and sensory features generated by those amacrine cells 2) Use molecularly defined amacrine cells in the mouse to quantitatively characterize the functional contribution of specific amacrine cell types to specific stimuli including artificial moving objects and natural scenes. These studies create a new way to generate and test hypotheses related to the quantitative effect of any interneuron on retinal output under any visual stimulus. Understanding how retinal circuitry creates visual processing under natural scenes is critical to our understanding of retinal mechanisms and diseases involving the degeneration of the retinal circuitry. In addition, the computational descriptions of retinal responses will be directly useful in the design of electronic retinal prosthesis systems. The retina is a complex network of many cell types, including the most diverse but poorlyunderstood class of cells, amacrine cells. By understanding how inhibitory neurons change neural processing in the retina under natural visual scenes, we can begin to address how these cells and their connections degenerate during retinal diseases, an essential step in designing treatments for these diseases. Furthermore, by creating accurate computational models of the retinal response to natural scenes, this research will be immediately applicable to electronic retinal prosthesis systems that aim to restore vision in cases of photoreceptor degeneration.",Function and circuitry of adaptive inhibition in the retina,10086475,R01EY022933,"['Address', 'Amacrine Cells', 'Archives', 'Cells', 'Complex', 'Computer Models', 'Data', 'Disease', 'Electrophysiology (science)', 'Face', 'Feedback', 'Future', 'Goals', 'Injections', 'Interneurons', 'Lateral', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Molecular', 'Motion', 'Mus', 'Nervous system structure', 'Neural Network Simulation', 'Neurons', 'Optic Nerve', 'Optical Methods', 'Output', 'Pathway interactions', 'Periodicity', 'Population', 'Process', 'Property', 'Research', 'Retina', 'Retinal Degeneration', 'Retinal Diseases', 'Retinal Ganglion Cells', 'Salamander', 'Sensory', 'Signal Transduction', 'Spottings', 'Stereotyping', 'Stimulus', 'Structure', 'Synapses', 'System', 'Techniques', 'Testing', 'Theoretical Studies', 'Time', 'Vision', 'Visual', 'Visual system structure', 'biological systems', 'cell type', 'clinically relevant', 'computerized tools', 'computing resources', 'connectome', 'convolutional neural network', 'design', 'experimental study', 'extracellular', 'ganglion cell', 'inhibitory neuron', 'interest', 'neural circuit', 'novel strategies', 'photoreceptor degeneration', 'predicting response', 'receptive field', 'relating to nervous system', 'response', 'retinal prosthesis', 'sight restoration', 'therapy design', 'tool', 'visual processing', 'visual stimulus']",NEI,STANFORD UNIVERSITY,R01,2021,380427
"Access to parietal action representations after stroke lesions in visual cortex PROJECT SUMMARY  The ability to recognize and use objects according to their function (e.g., fork, hammer, pencil) requires integration of visual, semantic and action knowledge across occipital, temporal and parietal areas. Left parietal regions support critical aspects of object-directed action, such as grasping and object manipulation. This research activity uses a combination of fMRI and behavioral measures in patients with ischemic strokes to early and extrastriate visual areas to test the following hypotheses: Aim 1: There is a visual pathway to the parietal grasp region (aIPS) that bypasses processing in primary visual cortex. Aim 2: Left ventral extrastriate cortex is necessary to access manipulation information for visually presented objects. Aim 3A: Ballistic grasping actions to objects in the hemianopic field are influenced by volumetric properties (size, orientation) of targets. Aim 3B: Left ventral extrastriate lesions impair object function (e.g., `scissors used to cut') and disrupt access to manipulation knowledge from visual input. The research leverages strengths of fMRI (whole brain correlational measure) and neuropsychology (causal inference) to test new hypotheses about vision and action.  `Tools' (i.e., small manipulable objects) are an excellent domain in which to address broader questions about the integration of sensory, motor and cognitive processing. This is because tool recognition and tool use require the integration of distinct sensory, motor and cognitive representations, and the neural substrates of tool processing are well described. The research program emphasizes fresh perspectives on longstanding ideas about the dorsal and ventral visual pathways, by a) undertaking the first systematic investigation of the types of information about objects that are extracted by visual pathways that bypass primary visual cortex, and by b) studying how some parietal areas depend on inputs from the ventral stream in order to access the correct action for a given object. The research activity innovates by testing hypotheses about how lesions at different stages in the cortical visual hierarchy affect downstream processing in parietal cortex, combining neural and behavioral measures to study brain damaged patients (generating causal evidence), and by combining univariate and multivariate measures to `read out' the information content of brain regions (parietal cortex) that are anatomically remote from a lesion. The research advances understanding of how lesions in one brain region disrupt computations in other parts of the brain that depend on the damaged region for their inputs, a phenomenon (`dynamic diaschisis') that applies to brain injury generally. Advancing understanding of these basic issues using causal data has broad implications for understanding how the brain selects the correct action for the correct object, and more generally for theories of conceptual organization and causal reasoning. Understanding how the brain accesses actions from visual input has implications for related fields, such as robotics, neuroprosthetics, and evidenced based approaches for rehabilitating function after brain injury. Project Narrative Functional MRI and behavioral testing are used to test hypotheses about how strokes affecting occipital and temporal cortex disrupt access to action representations in parietal cortex. This research will advance understanding of how the brain processes visual information in support of everyday actions.",Access to parietal action representations after stroke lesions in visual cortex,10130533,R01EY028535,"['Address', 'Affect', 'Alexia', 'Anatomy', 'Area', 'Ballistics', 'Behavioral Assay', 'Brain', 'Brain Injuries', 'Brain region', 'Bypass', 'Cognitive', 'Data', 'Dorsal', 'Eating', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grain', 'Imaging Device', 'Impairment', 'Investigation', 'Ischemic Stroke', 'Knowledge', 'Left', 'Lesion', 'Literature', 'Location', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Neural Pathways', 'Neuropsychology', 'Occipital lobe', 'Parahippocampal Gyrus', 'Parietal', 'Parietal Lobe', 'Participant', 'Pathway interactions', 'Patients', 'Population', 'Process', 'Property', 'Prosopagnosia', 'Reading', 'Research', 'Research Activity', 'Robotics', 'Role', 'Semantics', 'Sensory', 'Stimulus', 'Stream', 'Stroke', 'Structure of supramarginal gyrus', 'Temporal Lobe', 'Testing', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual system structure', 'Work', 'area striata', 'base', 'behavior measurement', 'behavior test', 'blind', 'cognitive development', 'evidence base', 'extrastriate', 'extrastriate visual cortex', 'fovea centralis', 'grasp', 'information processing', 'innovation', 'lens', 'multisensory', 'neuroprosthesis', 'post stroke', 'programs', 'relating to nervous system', 'sensory integration', 'theories', 'tool', 'visual information', 'visual motor', 'visual object processing', 'visual process', 'visual processing']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2021,410316
"The cerebro-cerebellar-basal-gangliar network for visuomotor learning ABSTRACT Visual learning is critical to the lives of human and non-human primates. Visuomotor association, the assignment of an arbitrary symbol to a particular movement (like a red light to a braking movement), is a well- studied form of visual learning. This proposal tests the hypothesis that the brain accomplishes visuomotor associative learning using an anatomically defined closed-loop network, including the prefrontal cortex, the basal ganglia, and the cerebellum. In our preliminary work we have developed a task that studies how monkeys learn to associate one of two novel fractal symbols with a right hand movement, and the other symbol with a left hand movement. Every experiment begins with the monkeys responding to two overtrained symbols that they have seen hundreds of thousands of times. At an arbitrary time we change the symbols to two fractal symbols that the monkey has never seen. It takes the monkey 40 to 70 trials to learn the new associations. In our preliminary results we have discovered that Purkinje cells in the midlateral cerebellar hemisphere track the monkeys’ learning as they as they figure out the required associations. The neurons signal the result of the prior decision. Half of the neurons respond more when the prior decision was correct; the others respond more when the prior decision was wrong. The difference between the activity of these two types of neurons provides a cognitive error signal that is maximal when the monkeys are performing at a chance level, and gradually becomes not different from zero as the monkeys learn the task. The neurons do not predict the result of the impending decision. Although the neurons change their activity dramatically at the symbol switch, the kinematics of the movements do not change at all. This proposal takes this discovery as the starting point for four aims: 1) to use viral transynaptic tract tracing to discover the cortical and basal ganglia regions that project to the cerebellar visuomotor association area. 2) to record from the four nodes of the network as anatomically defined (midlateral cerebellar hemisphere, dentate nucleus, basal ganglia, prefrontal cortex), simultaneously, using multiple single neuron recordings, to see if these areas also have information about the process of visuomotor association 3) to inactivate each node, to see how their inactivation affects the monkey’s ability to learn new associations, and whether the inactivation affects the activity of the neurons at the other nodes. 4) to develop computational methods to analyze the activity of neural activity recorded simultaneously in all four nodes of the network (Aim 2) in the midlateral cerebellar cortex with regard to parameters such as prior outcome and movement, hand, symbol, and the intensity and epoch of the prior cognitive error signal. We will use dimensional reduction techniques to answer questions like whether hand or symbol can be decoded from network activity. We will model how the cerebellum simple spike cognitive error signal might propagate through the network and be used to facilitate visuomotor association learning and the processing of signals in the cerebellum, basal ganglia and cerebral cortex Project Narrative Learning that a particular object cues a particular action, as a red light makes us stop walking or brake the car, is critical for human behavior and can be degraded by human disease. This project will apply physiological, computational, and anatomical methods to investigate a brain network for visual learning. We will find the exact areas of the cerebral cortex, basal ganglia, and cerebellum that participate in this learning, and use machine learning techniques to understand how the activity of neurons recorded simultaneously in these brain areas can facilitate learning.",The cerebro-cerebellar-basal-gangliar network for visuomotor learning,10150927,R01NS113078,"['Affect', 'Agonist', 'Anatomy', 'Area', 'Association Learning', 'Basal Ganglia', 'Behavior', 'Brain', 'Cerebellar Cortex', 'Cerebellum', 'Cerebral cortex', 'Cognition', 'Cognitive', 'Computational Technique', 'Computer Analysis', 'Computing Methodologies', 'Cues', 'Data Set', 'Dentate nucleus', 'Dimensions', 'Electronic Medical Records and Genomics Network', 'Exhibits', 'Failure', 'Fractals', 'Grant', 'Hand', 'Human', 'Injections', 'Learning', 'Left', 'Light', 'Machine Learning', 'Methods', 'Modeling', 'Monkeys', 'Motor', 'Movement', 'Muscimol', 'Neurons', 'Outcome', 'Parietal', 'Pathway Analysis', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Physiological', 'Prefrontal Cortex', 'Process', 'Property', 'Purkinje Cells', 'Rabies virus', 'Reaction Time', 'Reading', 'Reflex action', 'Reporting', 'Role', 'Short-Term Memory', 'Signal Transduction', 'Site', 'Source', 'Suggestion', 'Techniques', 'Testing', 'Time', 'To specify', 'Viral', 'Virus', 'Visual', 'Walking', 'Work', 'classical conditioning', 'cognitive function', 'cognitive process', 'experimental study', 'gamma-Aminobutyric Acid', 'human disease', 'kinematics', 'motor behavior', 'motor control', 'motor learning', 'nonhuman primate', 'novel', 'relating to nervous system', 'signal processing', 'success', 'visual learning', 'visual motor']",NINDS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2021,1026631
"Imaging Functional Connectivity in Visual Cortex The mammalian visual cortex is critical for vision and has been used as a model for the rest of the cerebral cortex. In spite of this, little is known about the detailed operations of its microcircuits. In fact, the cortex is composed of many different cell types, and, it is likely that each cell type has a particular circuit function.  In the past cycle we focused on four major subtypes of cortical GABAergic interneurons (PV, SOM, VIP and chandeliers) in mouse visual cortex, using two- photon photoactivation and transgenic mice to map connections to and from interneurons in a systematic fashion. We found that interneurons often make synaptic connections with every single neighboring cell, and this promiscuity could be an important feature of the cortical inhibitory “circuit blueprint”.  In the course of these experiments we unexpectedly discovered that groups of coactive neurons, which we termed “neuronal ensembles”, account for the majority of the cortical response to visual stimuli. Moreover, ensembles also dominate spontaneous activity and have distinct spatiotemporal characteristics. Further, using two-photon optogenetics we found by chance that ensembles can be artificially imprinted into the cortex and they can be recalled by stimulating individual neurons, showing pattern completion, even days after imprinting. These intriguing results suggest that ensembles could be multicellular building blocks of cortical function, implementing a population neural code.  To pursue this discovery and test these ideas we now propose an experimental “deep dive” into the properties of these ensembles, in order to characterize their basic phenomenology, understand their synaptic circuit mechanisms and test their role in the behavior of the animal in sensory discrimination. The project will be carried out in primary visual cortex of awake behaving mice and make use of a novel two-photon holographic microscopy method that enables us to image and optically manipulate neurons in different cortical layers simultaneously.  Our work will provide a systematic anatomical and functional description of neuronal ensembles, how they regulate the activity of the cortex and how they can be manipulated and reconfigured. This could help in novel therapeutic strategies by reconfiguring pathological circuits and correct visual deficits. Imaging Functional Connectivity in Visual Cortex The role of coordinated neuronal activity by groups of neurons in the visual cortex is still poorly understood. In the last cycle of the grant we discovered that one can reconfigure cortical activity in mouse visual cortex using optogenetics, by imprinting and recalling artificial activity patterns, and we now propose to understand how these patterns are build and whether their manipulation can lead to changes in behavior. This work could help to understand how neural circuits in the visual cortex operate and can be altered and also help to design novel strategies to ameliorate amblyopia and cortical cerebral visual impairment by reconfiguring abnormal circuits.  ",Imaging Functional Connectivity in Visual Cortex,10169447,R01EY011787,"['3-Dimensional', 'Amblyopia', 'Anatomy', 'Animal Behavior', 'Animals', 'Behavior', 'Behavioral', 'Brain', 'Cells', 'Cerebral cortex', 'Characteristics', 'Child', 'Code', 'Computational algorithm', 'Discrimination', 'Disease', 'Electrophysiology (science)', 'Epidemic', 'Generations', 'Goals', 'Grant', 'Image', 'In Vitro', 'Individual', 'Injury', 'Interneurons', 'Lead', 'Machine Learning', 'Maps', 'Mediating', 'Methods', 'Microscopy', 'Modeling', 'Morphology', 'Mus', 'Neocortex', 'Neurons', 'Optical Methods', 'Optics', 'Outcome', 'Pathologic', 'Pattern', 'Perceptual learning', 'Population', 'Premature Infant', 'Preparation', 'Process', 'Property', 'Research', 'Research Personnel', 'Rest', 'Role', 'Slice', 'Structure', 'Synapses', 'Synaptic plasticity', 'Techniques', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Transgenic Mice', 'Vision', 'Visual', 'Visual Cortex', 'Work', 'area striata', 'awake', 'cell type', 'cortical visual impairment', 'design', 'developmental plasticity', 'excitatory neuron', 'experimental study', 'imprint', 'in vivo', 'mouse genetics', 'nerve supply', 'neural circuit', 'neural network', 'novel', 'novel strategies', 'novel therapeutic intervention', 'novel therapeutics', 'operation', 'optogenetics', 'phenomenological models', 'photoactivation', 'programs', 'relating to nervous system', 'response', 'sensory discrimination', 'spatiotemporal', 'two-photon', 'visual stimulus']",NEI,COLUMBIA UNIV NEW YORK MORNINGSIDE,R01,2021,384357
"Intelligent Intensive Care Unit (I2CU): Pervasive Sensing and Artificial Intelligence for Augmented Clinical Decision-making Project Summary Although close monitoring and dynamic assessment of patient acuity are key aspects of ICU care, both are limited by the time constraints imposed on healthcare providers. Currently, dynamic and precise assessment of patient’s acuity in ICU rely almost exclusively on physicians’ clinical judgment and vigilance. Furthermore, important visual assessment details, such as facial expressions, posture, and mobility, are captured sporadically by overburdened nurses or are not captured at all. However, these visual assessment details are associated with critical indices such as physical function, pain and subsequent clinical deterioration. The PIs’ long-term goal is to sense, quantify, and communicate patient’s clinical condition in an autonomous and precise manner. The overall objective of this application is to develop the novel tools for sensing, quantifying, and communicating any patient’s condition in an autonomous, precise, and interpretable manner. The central hypothesis is that deep learning models will be superior to existing acuity clinical scores by predicting acuity in a dynamic, precise, and interpretable manner, using autonomous assessment of pain, emotional distress and physical function, together with clinical and physiologic data. The hypothesis has been formulated based on preliminary data and is well-grounded in clinical care literature. The rationale is that autonomous and precise patient quantification can result in enhanced clinical workflow and early intervention. The overall objective will be achieved by pursuing three specific aims. (1) Developing and validating an interpretable deep learning algorithm for precise and dynamic prediction of the patient’s clinical status to determine if it is more accurate in predicting daily care transition outcomes, while providing interpretable information to the physician. (2) Developing a pervasive sensing system for autonomous visual assessment of critically ill patients to determine if it can provide accurate visual assessment of a patient compared to human expert, and if it can enrich acuity prediction when combined with clinical data. (3) Implementing and evaluating an intelligent platform for real- time integration of autonomous visual assessment and acuity prediction in clinical workflow to determine accuracy in real-time prospective evaluation and to determine physicians’ risk perception and satisfaction. The approach is innovative, because it represents the first attempt to (1) dynamically predict precise patient trajectory, (2) autonomously perform visual assessment in the ICU, and (3) implement artificial intelligence platform in real time in clinical workflow. The proposed research is significant since it will address several key problems and critical barriers in critical care, including (1) lack of precise and real-time prediction of clinical trajectory, (2) manual repetitive ICU assessments, and (3) uncaptured patient aspects. Ultimately, the results are expected to improve patient outcomes and decrease hospitalization costs, as well as lifelong complications. Project Narrative The proposed research is relevant to public health because it can result in enhanced critical care workflow and early critical care intervention, ultimately improving patient outcomes and decreasing hospitalization costs. Thus, the proposed research is relevant to the part of NIH’s mission that pertains to advancing disease diagnosis through medical applications of new tools and technologies, as the proposed research applies advanced computational methods and sensing technologies to automatically quantify and convey patient status in real-time.",Intelligent Intensive Care Unit (I2CU): Pervasive Sensing and Artificial Intelligence for Augmented Clinical Decision-making,10154047,R01EB029699,"['Address', 'Algorithms', 'Artificial Intelligence', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Informatics', 'Clinical assessments', 'Collaborations', 'Color', 'Complex', 'Computing Methodologies', 'Critical Care', 'Critical Illness', 'Cues', 'Data', 'Data Set', 'Decision Making', 'Deterioration', 'Early Diagnosis', 'Early Intervention', 'Electronic Health Record', 'Environment', 'Evaluation', 'Facial Expression', 'Fostering', 'Foundations', 'Frequencies', 'Goals', 'Health Personnel', 'Hospital Costs', 'Human', 'Image', 'Intelligence', 'Intensive Care Units', 'Intervention', 'Judgment', 'Literature', 'Manuals', 'Measurement', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Monitor', 'Nurses', 'Outcome', 'Pain', 'Pain Measurement', 'Patient-Focused Outcomes', 'Patients', 'Physical Function', 'Physicians', 'Physiological', 'Posture', 'Process', 'Process Assessment', 'Public Health', 'Reproducibility', 'Research', 'Risk Assessment', 'Sampling', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Visual', 'advanced disease', 'augmented intelligence', 'base', 'clinical care', 'clinical decision-making', 'clinical practice', 'cost', 'deep learning', 'deep learning algorithm', 'disease diagnosis', 'emotional distress', 'improved', 'indexing', 'innovation', 'novel', 'prediction algorithm', 'prospective', 'prospective test', 'risk perception', 'satisfaction', 'sensor', 'sensor technology', 'tool', 'vigilance']",NIBIB,UNIVERSITY OF FLORIDA,R01,2021,632088
"Perceptual integration of luminance, texture and color cues for visual boundary segmentation Project Summary One of the most essential computations performed by the visual system is segmenting images into regions corresponding to distinct surfaces. This in turn requires identifying the boundaries separating image regions, a process known as boundary segmentation. Computational analyses of natural images have revealed that many visual cues are available at region boundaries, including differences in luminance, texture, and color. It is known that these cues combine for tasks like edge localization and orientation discrimination. However, it remains unclear how these various cues are weighted and combined for boundary segmentation.  In collaborative work with Canadian colleagues at McGill University in Montreal, we have developed a novel machine learning framework for characterizing human performance on boundary segmentation tasks using naturalistic micro-pattern stimuli. Our method makes use of the Filter-Rectify-Filter (FRF) model often applied to characterizing texture boundary segmentation. The major innovation of our approach is that we fit the FRF model directly to thousands of psychophysical stimulus-response observations to estimate its major defining parameters. We have recently applied this approach to investigating spatial strategies for contrast boundary segmentation and comparing competing hypotheses of how contrast modulation is integrated across orientation channels. In this grant, we propose to apply both classical psychophysical techniques and our novel machine learning methodology to understanding the computations employed to combine luminance, texture and color cues for segmentation.  In Aim 1, we focus on modeling segmentation of luminance-defined boundaries, comparing the case where each surface has uniform luminance, giving rise to a sharp edge (luminance step), to the more naturalistic case where the two surfaces have differing proportions of dark and light micro-patterns on either side of the boundary with no sharp edge (luminance texture). We will apply our machine learning methodology to test the hypothesis that different neural mechanisms may be involved in segmenting these two different kinds of luminance boundaries. In Aim 2, we ask how observers integrate first-order (luminance) and second-order (texture) cues for boundary segmentation, and if there are differences in cue combination strategies for luminance steps and luminance textures. We will also compare models embodying competing hypotheses of the underlying neural mechanisms of cue combination. In Aim 3, we extend the analyses in Aims 1 and 2 beyond simple luminance differences to include differences in color. Finally, Aim 4 is a pedagogical aim of promoting undergraduate research. Project Narrative Segmenting natural images into regions corresponding to distinct surfaces is an essential visual task, yet the underlying computations for boundary segmentation remain poorly understood. In this project, we will apply classical psychophysical techniques and our novel machine learning methodology to understand how multiple cues, including luminance, texture, and color, combine to enable boundary segmentation. We hope to develop and test computational models of boundary segmentation, with the ultimate goal of gaining insight into the underlying neural mechanisms employed for this essential natural vision task.","Perceptual integration of luminance, texture and color cues for visual boundary segmentation",10201916,R15EY032732,"['Address', 'Biological', 'Color', 'Computational Biology', 'Computer Analysis', 'Computer Models', 'Cues', 'Data', 'Discrimination', 'Educational process of instructing', 'Environment', 'Goals', 'Grant', 'Human', 'Image', 'Journals', 'Laboratories', 'Light', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Positioning Attribute', 'Process', 'Psychophysics', 'Publications', 'Research', 'Response to stimulus physiology', 'Series', 'Side', 'Source', 'Stimulus', 'Surface', 'Techniques', 'Testing', 'Texture', 'Universities', 'Vision', 'Vision research', 'Visual', 'Visual system structure', 'Work', 'computer studies', 'experience', 'innovation', 'insight', 'interest', 'laboratory curriculum', 'luminance', 'neuromechanism', 'neurophysiology', 'novel', 'pedagogy', 'undergraduate research', 'undergraduate student', 'vision science']",NEI,FLORIDA GULF COAST UNIVERSITY,R15,2021,374345
"Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning PROJECT SUMMARY/ABSTRACT Selective attention is an essential cognitive ability that permits us to effectively process and act upon relevant information while ignoring distracting events. A network involving frontal and parietal cortex for top-down attentional control, referred to as the Dorsal Attention Network (DAN), is active during both spatial and non- spatial (feature-based) attention. However, we know very little about the fine structure of attentional control activity in the DAN, how this structure changes to represent different to-be-attended stimulus features, how the connectivity within the DAN, and between the DAN and sensory cortex shifts when attending different features, or how these top-down processes and their influence in sensory cortex unfold over time. This gap in our knowledge is a critical problem for our models and theories of attention, and because attentional deficits are involved in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia. The working model guiding this research is that top-down attentional control, based on different to-be-attended stimulus attributes, is guided by a smaller-scale neural fine structure within the DAN and prefrontal cortex that makes specific connections with specialized areas of visual cortex coding the attended attributes. Moreover, the time course of activity within the DAN in relation to that in sensory cortex follows a top-down cascading model, being earliest in frontal, then parietal cortex, and finally sensory cortex for preparatory, voluntary, attentional control. To identify the functional networks for attentional control for different forms of attention, and to define their time courses, this project uses innovative simultaneous recording of electroencephalographic (EEG) and functional magnetic resonance imaging (fMRI) data. Advanced signal processing and modeling, including multivariate pattern analysis (MVPA), graph theoretic connectivity analysis, and Granger causality analysis will be used to reveal the fine functional anatomy and time course of attentional control and selection. The project includes three experiments that vary the to-be-attended stimulus attributes from spatial location to stimulus features (color and motion), and pursues three aims. Aim 1 is to reveal the fine structure of top-down preparatory attentional control for different to-be-attended stimulus features. Aim 2 is to elucidate the specific connectivity between fine structures for preparatory attentional control in the DAN and their target sensory structures in sensory cortex. Aim 3 is to reveal the time course of top-down attentional control for different to-be-attended stimulus attributes. PROJECT NARRATIVE The capacity to focus attention is at the core of human mental functioning, and therefore, elucidating the neural bases of attention remains a central challenge for neuroscience, representing an essential aim in translational efforts to ameliorate attentional deficits in a wide variety of neuropsychiatric disorders including autism, attention deficit disorder, dementia, and schizophrenia.",Mechanisms of attentional control: Structure and dynamics from simultaneous EEG-fMRI and machine learning,10115818,R01MH117991,"['Anatomy', 'Attention', 'Attention Deficit Disorder', 'Attentional deficit', 'Brain', 'Code', 'Color', 'Cues', 'Data', 'Dementia', 'Discrimination', 'Dorsal', 'Etiology', 'Event', 'Face', 'Functional Magnetic Resonance Imaging', 'Graph', 'Human', 'Inferior', 'Knowledge', 'Location', 'Machine Learning', 'Modeling', 'Motion', 'Neurosciences', 'Parietal Lobe', 'Pattern', 'Perception', 'Physiological', 'Prefrontal Cortex', 'Process', 'Property', 'Research', 'Schizophrenia', 'Sensory', 'Specific qualifier value', 'Specificity', 'Stimulus', 'Structure', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Work', 'attentional control', 'autism spectrum disorder', 'base', 'cognitive ability', 'data integration', 'experimental study', 'extrastriate visual cortex', 'frontal lobe', 'indexing', 'innovation', 'mental function', 'neuropsychiatric disorder', 'predictive modeling', 'relating to nervous system', 'selective attention', 'sensory cortex', 'signal processing', 'theories', 'tool']",NIMH,UNIVERSITY OF CALIFORNIA AT DAVIS,R01,2021,530307
"Neural mechanisms of active vision in the fovea Neural mechanisms of active vision in the fovea In many ways human vision is like a camera, with a lens that forms an image on a spatially arranged sensor (the retina). However, it is unlike a camera because the sensor has uneven sampling and is constantly moving with the eyes. Recent behavioral and theoretical work suggest these eye movements serve a faciliatory role in high acuity vision – where the eye movements are part of the computations and enhance spatial resolution. However, the neurophysiological mechanisms to support this facilitation remain unknown. More broadly, little is known about the neural mechanisms that integrate across the retinal motion generated by eye movements, especially in the central visual field (the fovea). This is particularly important because over 8 million Americans suffer from central vision loss due to retinal disorders. Even if the retinal signals could be repaired, it is imperative to understand how the brain reads out foveal signals to ensure recovery of high-acuity visual processing, and fixational eye movements are a part of that process. The proposed career development plan aims to address these questions by measuring visual processing in the foveal representation of primary visual cortex (V1) during natural visual behavior. This proposal uses custom high-resolution eye-tracking, a novel visual foraging paradigm, largescale neurophysiology, and state-of-the-art machine learning to make these measurements possible. The proposed research will not only generate fundamental understanding of how eye-movements facilitate visual processing, but also will integrate the experimental and theoretical tools required to support neurophysiological studies of active visual processing without a loss of rigor or detail. The candidate has extensive expertise in awake- behaving neurophysiology and computational modeling and the training plan is designed to support his further training in statistical modeling, high-resolution eye-tracking, and modern machine-learning techniques for analyzing neural population data. The primary mentor, Dr. Daniel Butts, is a world expert in statistical models of neural activity during active vision; Co-mentor, Dr. Michele Rucci, is a world leader in high-resolution eye tracking and theoretical approaches to active vision; and Co-mentor, Dr. Jude Mitchell, is a pioneer in establishing the marmoset model of visual neuroscience and an expert in neurophysiology of visual attention. Together, they will provide the guidance to establish the candidate’s transition to a successful independent research career. The goal of this proposal is to identify the impact of fixational eye movements on neural representations in visual cortex in the central visual field. Over 9 million Americans have central vision loss from age-related macular degeneration. Results from this proposal will build a fundamental understanding of how retinal signals from the fovea are processed by visual cortex during natural visual behavior.",Neural mechanisms of active vision in the fovea,10106203,K99EY032179,"['Address', 'Age related macular degeneration', 'American', 'Behavior', 'Behavioral', 'Blindness', 'Brain', 'Callithrix', 'Code', 'Cognitive', 'Computer Models', 'Custom', 'Data', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Ensure', 'Esthesia', 'Eye', 'Eye Movements', 'Foundations', 'Frequencies', 'Funding', 'Goals', 'Grant', 'Human', 'Image', 'Laboratories', 'Machine Learning', 'Measurement', 'Measures', 'Mentors', 'Modeling', 'Modernization', 'Monkeys', 'Motion', 'Neurons', 'Outcome', 'Peripheral', 'Phase', 'Physiology', 'Population', 'Positioning Attribute', 'Primates', 'Process', 'Recovery', 'Research', 'Resolution', 'Retina', 'Retinal Diseases', 'Role', 'Saccades', 'Sampling', 'Series', 'Signal Transduction', 'Statistical Models', 'Stream', 'System', 'Techniques', 'Testing', 'Training', 'Universities', 'V1 neuron', 'V4 neuron', 'Vision', 'Visual', 'Visual Acuity', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual attention', 'Work', 'active vision', 'area striata', 'awake', 'base', 'career', 'career development', 'central visual field', 'computerized tools', 'design', 'extrastriate', 'extrastriate visual cortex', 'flexibility', 'fovea centralis', 'interest', 'lens', 'neural model', 'neuromechanism', 'neurophysiology', 'new technology', 'novel', 'professor', 'receptive field', 'relating to nervous system', 'repaired', 'response', 'retinal imaging', 'sample fixation', 'sensor', 'skills', 'spatial vision', 'spatiotemporal', 'statistics', 'tool', 'visual information', 'visual neuroscience', 'visual process', 'visual processing', 'visual tracking']",NEI,"UNIV OF MARYLAND, COLLEGE PARK",K99,2021,116969

text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Biomedical Data Translator Development of Autonomous Relay Agent: ARAX This project would continue collaborative work within the Translator consortium by a multi-site team (“Team X-ray”) at Oregon State University (PI Stephen Ramsey) and at two partner institutions, Pennsylvania State University (PI Koslicki) and the Institute for Systems Biology (PI Eric Deutsch; Co-I Jared Roach). Team X-ray was highly productive in Translator's feasibility assessment phase and the team brings critical expertise to Translator (see Resources). Component type: We propose to create, validate, and integrate an autonomous relay agent (ARA) called ARAX . ARAX will be a middleware component in the new Translator architecture that will extend significantly beyond the capabilities of the prototype reasoning tool (RTX) that we created in the feasibility assessment phase. Depending on the input request, ARAX's main output will be ranked subgraphs with clearly explained ranking basis. ARAX will leverage code and algorithms from RTX and will have an explicit application focus area, as described below. Main problems that ARAX is trying to address: Connections within a biomedical knowledge graph have highly variable degrees of (i) confidence (due to ambiguous predicates and/or due to highly variable degrees of reliability of knowledge types) and (ii) potential relevance to the user's query. Such edge-significance variability leads to both incorrect and difficult-to-interpret results which together pose a significant problem for creating broadly useful tools for computer-based biomedical reasoning. We propose to address this problem by explicitly accounting for these two types of edge variability in the reasoning algorithms–spanning a broad range of biomedical query types–that ARAX will provide to Translator. In addition to these broad capabilities, as described in the Project Plan, we will incorporate advanced algorithms in ARAX for responding to queries relating to disease therapy, including (1) drug repositioning for known disease, leveraging knowledge about the disease’s pathogenesis [1] ; and (2) therapeutic recommendations for rare diseases based on symptoms and the putative causal genetic variant. Plan for implementation of the project: In our Project Plan we describe a five-year timeline for creating, validating, and integrating ARAX within Translator, beginning with a three-month sprint leading to a prototype of ARAX by mid-March 2020. Key components of the plan include: (1) leveraging the BioThings Explorer software framework to enable ARAX to dynamically map between compounds, proteins, pathways, variants, phenotypes, and diseases based on knowledge source application programming interfaces in the Translator registry; (2) leveraging COHD and related Translator resources to obtain biomedical semantic distance information; (3) leveraging an application programming interface endpoint for the RTX biomedical knowledge graph, KG2; and (4) implementing probabilistic reasoning algorithms leveraging provenance information and dynamically determined edge relevance scores to improve reasoning. We will systematically use machine-learning to align ranking scores with measures of output quality. Collaboration strengths of our team include (i) developing technical standards for communications between Translator software agents (leveraging PI Deutsch’s extensive past experience); (ii) developing knowledge graph standards (leveraging PI Ramsey’s and PI Koslicki’s expertise); and (iii) deriving use-case vignettes that speak to the transformative potential of Translator (leveraging Co-I Roach’s and PI Ramsey’s expertise). In the development phase, our team would continue to collaborate with other teams and with NIH stakeholders in an adaptive, high-bandwidth, and team-boundary-agnostic fashion, as detailed in the Project Plan. Key challenges to building the proposed system are (1) the need to be able to ""chain"" together analytical steps between tools and (2) the need for cooperative development of standards that enable Translator components to interact; we address them in detail in the Project Plan. n/a",Biomedical Data Translator Development of Autonomous Relay Agent: ARAX,10333468,OT2TR003428,"['Accounting', 'Address', 'Algorithms', 'Architecture', 'Area', 'Code', 'Collaborations', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disease', 'Institutes', 'Institution', 'Knowledge', 'Machine Learning', 'Maps', 'Measures', 'Oregon', 'Output', 'Pathogenesis', 'Pathway interactions', 'Pennsylvania', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Proteins', 'Rare Diseases', 'Recommendation', 'Registries', 'Resources', 'Roentgen Rays', 'Semantics', 'Site', 'Software Framework', 'Source', 'Symptoms', 'System', 'Systems Biology', 'Therapeutic', 'TimeLine', 'United States National Institutes of Health', 'Universities', 'Variant', 'Work', 'application programming interface', 'base', 'experience', 'genetic variant', 'improved', 'knowledge graph', 'middleware', 'prototype', 'tool']",NCATS,OREGON STATE UNIVERSITY,OT2,2021,897051
"A Network Science Approach to Conflicts of Interest: Metrics, Policies, and Communication Design A Network Science Approach to Conflicts of Interest: Metrics, Policies, and  Communication Design Project Description 1. Overview Research on the effects of conflicts of interest (COI) in biomedical science have long- established, significant causes for concern. For example, industry-funded trials are significantly more likely to return results that support patient use of tested drugs. As early as 2003, a JAMA study of 370 randomized controlled trials found that industry-sponsored studies were 5.3 times more likely to return results favorable to industry.[1] Subsequent research has corroborated these sponsorship effects.[2] Furthermore, funding relationships come in the form of advisory board positions, speaking fees, consulting fees, free lunches, prescription pads, and ink pens, and these smaller, financial relationships also have significant, undesirable effects on biomedical research, including an 8.4 factor increase in the likelihood of results favorable to industry.[3] The primary purpose of this project is to develop new metrics and mechanisms for the evaluation and communication of COI risks in the biomedical research enterprise. The research will use a network science approach to understand the circulation and accumulation of COI in biomedical decision systems. Network science offers an ideal framework for assessing influence within the biomedical research enterprise. The approach also moves beyond understandings of COI as about individual researchers or funders to encompass systemic risk and network-level effects. To advance the science of COI mitigation, this research will improve and enhance a machine-learning-based system that identifies and classifies COI across published disclosure statements. We will apply this system to COI disclosure statements representing the current state of biomedical research for 240 of the most commonly used prescription drug products and distill COI networks for each drug. The team will test candidate COI and COI-network metrics against drug safety data drawn from the Food and Drug Administration's Adverse Events Reporting System. The results will inform the development of new recommendations for COI policies in biomedical research and recommendations for evidence-based disclosure practices. 2. Intellectual Merit 2.1 Significance Given the growing recognition of the problems of COI, many federal agencies, universities, professional medical associations, and biomedical journals have adopted policies to mitigate the potentially harmful effects of these relationships. However, despite the consensus that policy interventions are a necessary and appropriate response, policies are inconsistent.[4] In some cases, institutions have outright bans on any financial relationships with industry. In other cases, institutions have complex requirements for vetting each individual COI. The FDA, for example, assesses each potential advisory committee member COI for “directness” and “predictability.” So, if a potential advisory committee member owns stock in a company that is seeking a new drug approval, regulators can identify a direct and predictable COI (stock values may rise based on the regulatory decision). The most common COI intervention across institutions is some form 12 n/a","A Network Science Approach to Conflicts of Interest: Metrics, Policies, and Communication Design",10202242,R01GM141476,"['Adopted', 'Adverse event', 'Advisory Committees', 'Biomedical Research', 'Blood Circulation', 'Committee Members', 'Communication', 'Complex', 'Conflict of Interest', 'Consensus', 'Consult', 'Data', 'Development', 'Disclosure', 'Drug Prescriptions', 'Evaluation', 'Fees', 'Funding', 'Individual', 'Industry', 'Ink', 'Institution', 'Intervention', 'Journals', 'Machine Learning', 'Medical', 'New Drug Approvals', 'PF4 Gene', 'Patients', 'Pharmaceutical Preparations', 'Policies', 'Positioning Attribute', 'Publishing', 'Randomized Controlled Trials', 'Recommendation', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Science', 'System', 'Testing', 'Time', 'United States Food and Drug Administration', 'Universities', 'base', 'design', 'drug testing', 'evidence based guidelines', 'financial relationship', 'improved', 'medication safety', 'response']",NIGMS,"UNIVERSITY OF TEXAS, AUSTIN",R01,2021,215960
"Education Pathways for Biomedical Data Science (R25) Project Summary / Abstract This project brings together Drexel University and the Children’s Hospital of Philadelphia (CHOP) to create an adaptive Biomedical Data Science education program to empower researchers to learn and use emerging data science methods. We propose an in-line program to prepare researchers for data-driven work directly in their current field, while also identifying avenues for interdisciplinary collaboration. We will develop novel curricula pathways, leverage existing educational resources, create bridge materials, and provide practicum “lab” activities matching domain-specific projects to participants for hands-on experience. Educational Pathways in Biomedical Data Science will collaborate with the CHOP Office of Academic Training and Outreach Programs to engage a diverse learner audience in novel pedagogical research. Learners will be recruited from active participants in many existing CHOP training initiatives, including a novel data science education program, graduate student training, postdoc mentorship, physician fellowship research, and clinical research staff training. After enrollment, education program managers will cluster participants into collaborative communities of practice where they will receive mentorship and contribute to development of pathways. We acknowledge that we are still learning the most effective interventions for biomedical data science education, both within our specific communities and broadly within medical education (e.g. Federer et al., 2015; Rowhani-Faarid, Allen, and Barnett, 2017). We will develop new evidence of gaps in knowledge, skills, and attitudes among learners. We will develop and implement biomedical data science literacy instruments based on emerging scholarship. We will also gather learner feedback via mixed methods to adapt and evolve our modular resources, ensure robust learner outcomes, and align deliverables with the NIH Strategic Plan for Data Science. Data Science instruction for researchers outside of traditional computer and information sciences is a concrete step toward data-driven scientific literacy for all. We will ensure that participants emerge from this program with computational and algorithmic literacy solidified through hands-on experience. For non-computing researchers, we also will provide the foundational data fluencies necessary for individuals to contribute meaningfully to machine learning research, which will enable data-driven systems, insight-to-decision transformation, decision- making, and data-driven decision management. Finally, for all participants, we will strive to help individuals from a broad spectrum of backgrounds and identities identify new directions in which to develop their careers. Project Narrative “Educational Pathways in Biomedical Data Science” will create evidence-based pathways to guide and augment researcher skill in data-intensive science. Our learning pathways and supporting modules will equip researchers to perform nimble research with massive datasets that span institutional and disciplinary boundaries. We will model cross-disciplinary collaboration by building upon an existing partnership between the College of Computing and Informatics of Drexel University and the Research Institute of the Children’s Hospital of Philadelphia, grounding the project in both computing and biomedical research practice.",Education Pathways for Biomedical Data Science (R25),10199482,R25GM141501,"['Academic Training', 'Attitude', 'Biomedical Research', 'Certification', 'Clinical Research', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Community of Practice', 'Computational algorithm', 'Custom', 'Data', 'Data Science', 'Data Set', 'Decision Making', 'Degree program', 'Development', 'Discipline', 'Ecosystem', 'Education', 'Educational Curriculum', 'Effectiveness', 'Enrollment', 'Ensure', 'Feedback', 'Fellowship', 'Foundations', 'Goals', 'Grouping', 'Home environment', 'Individual', 'Informatics', 'Information Sciences', 'Instruction', 'Knowledge', 'Learning', 'Learning Module', 'Literature', 'Machine Learning', 'Medical Education', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Motivation', 'Outcome', 'Participant', 'Pathway interactions', 'Pediatric Hospitals', 'Peer Review', 'Philadelphia', 'Physicians', 'Plant Roots', 'Postdoctoral Fellow', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Institute', 'Research Personnel', 'Resources', 'Scholarship', 'Science', 'Strategic Planning', 'System', 'Training', 'Training Programs', 'Translating', 'United States National Institutes of Health', 'Universities', 'Work', 'acronyms', 'base', 'biomedical data science', 'biomedical informatics', 'career', 'cohort', 'college', 'computer science', 'education pathway', 'education resources', 'effective intervention', 'evidence base', 'experience', 'formative assessment', 'graduate student', 'insight', 'instrument', 'interdisciplinary collaboration', 'interest', 'literacy', 'novel', 'online course', 'outreach program', 'pedagogy', 'programs', 'science education', 'scientific literacy', 'skills', 'statistics', 'student training', 'success', 'user centered design']",NIGMS,CHILDREN'S HOSP OF PHILADELPHIA,R25,2021,85673
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domain-agnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10330627,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2021,332969
"High Performance Text Mining for Translator We propose to build a knowledge provider that will seek out, integrate and provide AI-ready, BioLink-compatible models via high-performance text-mining of the biomedical literature. Problems with Translator’s current mining of the biomedical literature that we intend to solve include: (1) weaknesses in framework extensibility and benchmarking that make integrating and validating new text-mining approaches difficult; (2) problematic licensing of software, terminologies and other resources that do not adequately support FAIR (and TLC) best practices; (3) processing only PubMed titles and abstracts, not full text publications; (4) Translator’s use of older NLP technology with relatively poor performance; (5) lack of a mechanism for community feedback regarding errors and other problems; (6) lack of continuous updates to add knowledge from new publications; (7) output knowledge representation that is simplistic and vague, failing to reflect the richness of what is expressed in scientific documents. Plan for implementation: Our team has a long history of productive NLP research, successful open source software projects, effective benchmarking and broad community engagement. We will build on the results of NLM-funded work in information extraction, our gold-standard Colorado Richly Annotated Full Text (CRAFT) corpus, a recent BioNLP Open Shared Task (BioNLP-OST) that we organized, and recent advances in state-of-the-art NLP. For Segment 1, we will: (1) Demonstrate BioStacks, an extensible, cloud-based text-mining framework that produces knowledge graphs grounded in the Open Biomedical Ontologies (OBOs). This BioStacks demo will include a state-of-the-art OBO concept recognizer for multiple ontologies, a state-of-the-art semantic relationship prediction tool, and a state-of-the-art structural analysis tool. All generated assertions will have provenance metadata linking the assertion to a particular text span in a document specified by PMCID. (2) Demonstrate CRAFTST, a cloud-based text-mining evaluation system that evaluates the performance of text-mining systems against the CRAFT gold standard. (3) Demonstrate an adaptive machine learning process illustrating how to efficiently create tools to extract BioLink association types. For Segment 2, we propose to extend the text-mining and evaluation frameworks to align with BioLink and the Translator community, improve text-mining quality and expand the collection of source documents mined. Specifically, we propose to target 10 long term milestones: (1) Align CRAFT to BioLink. (2) Develop new tools for extracting associations from text. (3) Develop and manage a community engagement process on text-mining for Translator. (4) Extend benchmarking. (5) Improve recall. (6) Improve precision. (7) Improve computational efficiency. (8) Expand BioStacks to include all available full text biomedical journal articles. (9) Expand document collections to include Patents & Regulatory filings. (10) Develop a scientist-based movement to improve document access for text-mining from non-open publishers. The types of questions the resulting knowledge graph can be used to address are extremely broad, as it is generated by mining a large part of the biomedical literature. Questions that can be answered include those about specific assertions (e.g. is this drug an agonist-activator of this protein?), general relations (are these two proteins often mentioned together?), and documents (which publications mention this gene, mutation and drug?). Integration: We are long-time contributors to the open-science community and have longstanding collaborations with existing awardees; we were participants in the NIH Data Commons Pilot. We propose to align the output of text-mining tools to the BioLink model via OBO terms. We propose to implement our frameworks in NIH Cloud Computing environments. We propose to adopt the CD2H Contributor Attribution Model to foreground community contributions. We plan to coordinate with the NLM’s nascent benchmarking activities and the SmartAPI effort to build Translator standard interfaces. Challenges and gaps: High-performance mining of rich, contextualized knowledge from the literature remains a difficult task, and is unlikely to be solved in the next five years. Many important publications remain inaccessible to text-mining due to restrictive licensing. n/a",High Performance Text Mining for Translator,10334356,OT2TR003422,"['Address', 'Adopted', 'Agonist', 'Benchmarking', 'Cloud Computing', 'Collaborations', 'Collection', 'Colorado', 'Communities', 'Computer software', 'Data Commons', 'Environment', 'Evaluation', 'Feedback', 'Funding', 'Gene Mutation', 'Gold', 'Information Retrieval', 'Knowledge', 'Legal patent', 'Licensing', 'Link', 'Literature', 'Machine Learning', 'Metadata', 'Mining', 'Modeling', 'Movement', 'Ontology', 'Output', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'PubMed', 'Publications', 'Recording of previous events', 'Research', 'Resources', 'Scientist', 'Semantics', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Terminology', 'Text', 'Time', 'United States National Institutes of Health', 'Update', 'Work', 'base', 'biomedical ontology', 'cloud based', 'improved', 'information organization', 'journal article', 'knowledge graph', 'knowledge of results', 'open data', 'open source', 'text searching', 'tool']",NCATS,UNIVERSITY OF COLORADO DENVER,OT2,2021,471239
"ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis Project Summary The morphology (or shape) of anatomical structures forms the common language among clinicians, where ab- normalities in anatomical shapes are often tied to deleterious function. While these observations are often quali- tative, ﬁnding subtle, quantitative shape effects requires the application of mathematics, statistics, and computing to parse the anatomy into a numerical representation that will facilitate testing of biologically relevant hypotheses. Particle-based shape modeling (PSM) and its associated suite of software tools, ShapeWorks, enable learning population-level shape representation via automatic dense placement of homologous landmarks on image seg- mentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. Despite its obvious utility for the research enterprise and highly permissive open-source license, ShapeWorks does not have a viable commercialization path due to the inherent trade-off between development and maintenance costs, and a specialized scientiﬁc and clinical market. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread ap- plicability to medicine and biology is hindered by several barriers that most existing shape modeling packages face. The most important roadblocks are (1) the complexity and steep learning curve of existing shape modeling pipelines and their increased computational and computer memory requirements; (2) the considerable expertise, time, and effort required to segment anatomies of interest for statistical analyses; and (3) the lack of interoperable implementations that can be readily incorporated into biomedical research laboratories. In this project, we pro- pose ShapeWorksStudio, a software suite that leverages ShapeWorks for the automated population-/patient-level modeling of anatomical shapes, and Seg3D – a widely used open-source tool to visualize and process volumet- ric images – for ﬂexible manual/semiautomatic segmentation and interactive manual correction of segmented anatomy. In Aim 1, we will integrate ShapeWorks and Seg3D in a framework that supports big data cohorts to enable users to transparently proceed from image data to shape models in a straightforward manner. In Aim 2, we will endow Seg3D with a machine learning approach that provides automated segmentations within a statisti- cal framework that combines image data with population-speciﬁc shape priors provided by ShapeWorks. In Aim 3, we will support interoperability with existing open-source software packages and toolkits, and provide bindings to commonly used programming languages in the biomedical research community. To promote reproducibility, we will develop and disseminate standard workﬂows and domain-speciﬁc test cases. This project combines an interdisciplinary research and development team with decades of experience in statistical analysis and image understanding, and application scientists to conﬁrm that the proposed developments have a real impact on the biomedical and clinical research communities. Our long-term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. ShapeWorks has been effective in a range of applications, including psychology, biological phenotyping, cardiology, and orthopedics. If funded, this application will ensure the viability of ShapeWorks in the face of the ever-increasing complexity of shape datasets and support its availability to biomedical researchers in the future, as well as provide opportunities for use in a wide spectrum of new biological and clinical applications, including anatomy reconstruction from sparse/low- dimensional imaging data, large-scale clinical trials, surgical planning, optimal designs of medical implants, and reconstructive surgery.","ShapeWorksStudio: An Integrative, User-Friendly, and Scalable Suite for Shape Representation and Analysis",10173765,U24EB029011,"['Address', 'Adoption', 'Anatomic Models', 'Anatomy', 'Applied Research', 'Area', 'Big Data', 'Binding', 'Biological', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Cardiology', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communities', 'Complex', 'Complex Analysis', 'Computer software', 'Computers', 'Consensus', 'Data', 'Data Set', 'Development', 'Dimensions', 'Electronic Mail', 'Ensure', 'Exhibits', 'Face', 'Funding', 'Future', 'Goals', 'Image', 'Interdisciplinary Study', 'Laboratory Research', 'Language', 'Learning', 'Licensing', 'Machine Learning', 'Maintenance', 'Manuals', 'Mathematics', 'Measures', 'Medical', 'Medicine', 'Memory', 'Methods', 'Modeling', 'Modernization', 'Modification', 'Morphology', 'Normalcy', 'Operative Surgical Procedures', 'Orthopedics', 'Phenotype', 'Population', 'Process', 'Programming Languages', 'Psychology', 'Reconstructive Surgical Procedures', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Shapes', 'Software Engineering', 'Software Tools', 'Statistical Data Interpretation', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Work', 'automated segmentation', 'base', 'clinical application', 'clinical care', 'clinical investigation', 'cohort', 'commercialization', 'computerized tools', 'cost', 'design', 'efficacy evaluation', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'interest', 'interoperability', 'medical implant', 'open source', 'outreach', 'particle', 'patient population', 'reconstruction', 'research and development', 'shape analysis', 'software development', 'statistics', 'tool', 'usability', 'user-friendly']",NIBIB,UNIVERSITY OF UTAH,U24,2021,256578
"The Metadata Powerwash - Integrated tools to make biomedical data FAIR Project Summary  The metadata that describe scientific data are fundamental resources to enable (1) the discovery and reuse of the data and (2) the reproducibility of the experiments that generated the data in the first place. Metadata are essential for scientists to understand the associated data and to reuse them, as well as for information technology to index the data, to make the data available, and to provide filters for scientists to search for the corresponding datasets. Currently, the scientific metadata hosted in public repositories suffer from multiple quality issues that limit scientists’ ability to find and reuse the experimental datasets to which they refer. It can take many weeks of a scientist’s time to identify a collection of datasets that fulfill specific criteria when the data are so poorly described—and the majority of the process is necessarily manual.  We propose to develop an end-to-end solution to standardize biomedical metadata with the help of ontologies—data structures that define the terms in an application domain and the relationships among them. There are hundreds of ontologies that provide standard terms for use in biomedicine, and they are essential resources to make biomedical metadata interoperable and reusable. Our approach also will build on the technology created by the Center for Expanded Data Annotation and Retrieval (CEDAR), which offers a library of building blocks and common data elements for defining computer-based metadata templates based on community standards.  Our plan involves three specific aims. First, we will develop a method and tool to standardize the multiple, ad hoc metadata field names that may appear in metadata to represent the same type of information by replacing those field names with the field names used in standard metadata templates or, if no appropriate template match is available, with terms from a relevant ontology. Second, we will develop methods and tools to standardize different types of metadata field values, for example, categorical values such as drugs or diseases, and numerical values such as age, or sample collection date. Third, we will evaluate the speed, precision, and recall of our metadata transformation pipeline—built out of the methods and tools to standardize field names and values—on a large corpus of metadata that we will manually curate based on existing public metadata. We will also carry out experiments to test the effect of the standardized metadata when biomedical scientists perform dataset search in the context of their work. Project Narrative Data that offer precise descriptions of data—metadata—are critical scientific resources that facilitate the discovery, reuse, and reproducibility of the data to which they refer. Our goal is to create methods and tools that improve the quality of scientific metadata hosted in public repositories, and thus enhance the discoverability and re-use of public biomedical datasets. Making data more accessible through scientifically rigorous metadata will accelerate the ability to make transformative data-driven biomedical discoveries using public data archives.",The Metadata Powerwash - Integrated tools to make biomedical data FAIR,10093841,R01LM013498,"['Age', 'Biological Specimen Banks', 'Categories', 'Collection', 'Common Data Element', 'Communities', 'Computers', 'Data', 'Data Science', 'Data Set', 'Disease', 'FAIR principles', 'Funding Agency', 'Goals', 'Gold', 'Information Technology', 'Knowledge', 'Libraries', 'Link', 'Manuals', 'Metadata', 'Methods', 'Names', 'Natural Language Processing', 'Numerical value', 'Ontology', 'Pharmaceutical Preparations', 'Problem Solving', 'Process', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Retrieval', 'Sampling', 'Science', 'Scientist', 'Specific qualifier value', 'Speed', 'Standardization', 'Structure', 'Technology', 'Testing', 'Time', 'Variant', 'Work', 'base', 'biomedical scientist', 'data archive', 'data repository', 'data reuse', 'experimental study', 'improved', 'indexing', 'information organization', 'interoperability', 'metadata standards', 'public repository', 'repository', 'sample collection', 'search engine', 'secondary analysis', 'tool']",NLM,STANFORD UNIVERSITY,R01,2021,334847
"Reproducible and FAIR Bioinformatics Analysis of Omics Data PROJECT SUMMARY/ABSTRACT Modern biomedical research is increasingly quantitative. The next generation of researchers will need an entirely new set of quantitative skills to fully take advantage of the data they create. In response to this need, the goal of the current R25 proposal is to transform an existing, 5-day bioinformatics techniques course into a new, two-week short course, Reproducible and FAIR Bioinformatics Analysis of Omics Data, to provide a unique educational opportunity for biomedical research scientists-in-training to begin to develop core competencies in bioinformatics and biostatistical analyses of large datasets. The new course will also address NIH priorities including rigor and reproducibility and Findable, Accessible, Interoperable and Reusable (FAIR) data principles. During the last five years, the Dartmouth faculty who are serving as Principal Investigators have taught a 5-day course at the MDI Biological Laboratory on bioinformatics and biostatistics to ~40 trainees/year (202 total). Based on overwhelmingly positive feedback, the current project will extend this 5-day course into a longer, two-week short course at the MDI Biological Laboratory that will feature a low student-to- instructor ratio (5:1), more hands-on experiential learning, and exceptional faculty who are highly experienced in teaching and performing big-data analyses. The new course is designed to accommodate ~35 trainees per year (175 total over the five-year R25 project). It will incorporate modules on biostatistics, scientific rigor and reproducibility, and FAIR data principles. The course design will also involve a short conceptual presentation, followed by an exercise in which students will gain confidence by applying a new skill. Each active-learning session will have three levels of difficulty (beginner, intermediate, and advanced) to allow each student to progress at their own pace. The low student-to-faculty ratio will allow course facilitators to guide participants through realistic challenges without causing frustration. The specific aims of the proposed course include: Specific Aim 1. Develop a two-week short course primarily for postdoctoral fellows and graduate students that improves their ability to design and analyze omics experiments such as RNA-seq, 16S (microbiome), metagenomics, and sc-RNA-seq data; Specific Aim 2. Enhance the impact of research by biomedical scientists by teaching them the Responsible Conduct of Research, the secure and ethical use of data, as well as rigor and reproducibility and FAIR data principles; Specific Aim 3. Disseminate the training curriculum to a broad audience; and Specific Aim 4. Evaluate the short- and long-term impacts of the course on students, including a long-term follow-up to determine students’ confidence in and actual integration of bioinformatics, biostatistics, and FAIR data principles into their research, and the reported impact of this course on their career trajectory and competitiveness in the job market. In summary, the proposed course will provide a unique cross- training, educational opportunity for biomedical research scientists-in-training to begin to develop core competency in bioinformatics and biostatistical analyses of large data sets. PROJECT NARRATIVE The goal of our two-week short course, Reproducible and FAIR Bioinformatics Analysis of Omics Data, is to provide a unique cross-training, experiential, educational opportunity for biomedical research scientists-in- training to begin to develop core competency in bioinformatics and biostatistical analyses of large data sets.",Reproducible and FAIR Bioinformatics Analysis of Omics Data,10087570,R25HG011447,"['Active Learning', 'Address', 'Advisory Committees', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Cancer Education Grant Program', 'Competence', 'Data', 'Data Analyses', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Environment', 'Ethics', 'Euclidean Space', 'Evaluation', 'Exercise', 'Experimental Designs', 'FAIR principles', 'Faculty', 'Feedback', 'Frustration', 'Goals', 'Guidelines', 'High Performance Computing', 'Laboratories', 'Learning', 'Linear Models', 'Long-Term Effects', 'Longterm Follow-up', 'Machine Learning', 'Measurement', 'Measures', 'Metagenomics', 'Modernization', 'Occupations', 'Outcome', 'Participant', 'Postdoctoral Fellow', 'Principal Component Analysis', 'Principal Investigator', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Scientist', 'Secure', 'Students', 'Surveys', 'Techniques', 'Testing', 'Training', 'United States National Institutes of Health', 'Visualization', 'base', 'biomedical scientist', 'career', 'data reuse', 'design', 'expectation', 'experience', 'experimental study', 'follow-up', 'graduate student', 'improved', 'instructor', 'interest', 'job market', 'large datasets', 'microbiome', 'next generation', 'open source', 'programs', 'response', 'responsible research conduct', 'skills', 'transcriptome sequencing']",NHGRI,MOUNT DESERT ISLAND BIOLOGICAL LAB,R25,2021,79877
"Knowledge Management Center for Illuminating the Druggable Genome The main goal of the Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program is to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG (“IDG List”). The long-term objective of the KMC is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins. With focus on the IDG List and human proteins, the KMC will enable support for expanded coverage for non-human proteins of therapeutic interest and other associated human health data, in order to catalyze novel biomedical discoveries. To support the overall IDG objective, and to maintain, update and improve these integrated resources, the KMC draws upon expertise from multiple knowledge domains, specifically biology, chemistry and medicine, as well as computer science, graphic design and web programming. Specifically, for the Phase 2 of the IDG KMC we propose 4 Aims:1. Create an automated workflow that captures relevant public data for the entire proteome and manual annotations for the IDG list. The KMC knowledge management system will be built around knowledge graphs, focused on five major branches of the target knowledge tree, tkt: Genotype, Phenotype, Expression, Structure & Function, and Interactions & Pathways, respectively. Aim 2: Design, develop and implement a protein knowledgebase with Data Analytics support. Our protein-centric biomedical knowledge base, TCKB (Target Central Knowledgebase) will be comprised of the data, knowledge and information container, together with its codebase and software pipelines. TCKB will be the repository for experimental, processed and computed data and reagents originating from the IDG DRGCs (Data and Resource Generation Centers). We will provide informatics and modeling support for DRGC activities. Aim 3: We will expand, improve and maintain Pharos. Particularly “knowledge packages,” support automated data summaries for Protein Dossiers, and actively seek feedback from our community. Aim 4. Outreach to scientific community. We will support a series of activities that will leverage TCKB, Pharos and other IDG resources to increase adoption of IDG work, while observing FAIR (findable, accessible, interoperable, reusable) principles for our knowledgebase, portal and pipelines. The KMC will engage in community outreach by leading tutorials and feedback sessions and dissemination of the Pharos system. To meet its goals, the KMC will coordinate all core activities in close coordination with the IDG Steering Committee and IDG Project Scientists (PS), and include members of the IDG Consortium (IDG- C), other NIH Common Fund programs, NIH Commons, as well as other initiatives. The Knowledge Management Center (KMC) for the Illuminating the Druggable Genome (IDG) program plans to aggregate, update and articulate protein-centric data, information and knowledge for the entire human proteome with emphasis on understudied proteins from the 3 families that are the focus of the IDG. The KMC long-term objective is to encourage and support biomedical research aimed at understudied proteins by providing an extensive resource of data, information, knowledge, methods and reagents for the entire human proteome, and to support the growing online community focused on understudied proteins.",Knowledge Management Center for Illuminating the Druggable Genome,10073482,U24CA224370,"['Address', 'Adoption', 'Alleles', 'Archives', 'Basic Science', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Chemistry', 'Communities', 'Community Outreach', 'Computer software', 'Coupled', 'Data', 'Data Analytics', 'Data Element', 'Data Set', 'Databases', 'Dependence', 'Disease', 'Documentation', 'FAIR principles', 'Family', 'Feedback', 'Funding', 'Future', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genome Components', 'Genotype', 'Goals', 'Grant', 'Healthcare', 'Human', 'Informatics', 'Information Resources Management', 'International', 'Internet', 'Knowledge', 'Knowledge Portal', 'Life', 'Link', 'Machine Learning', 'Manuals', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Mutation', 'Non-Human Protein', 'Ontology', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Process', 'Protein Isoforms', 'Proteins', 'Proteome', 'Public Domains', 'Publications', 'Reagent', 'Resources', 'Scientist', 'Series', 'Services', 'Source', 'Structure', 'System', 'TRD@ gene cluster', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Update', 'Work', 'analytical method', 'computer science', 'data ecosystem', 'data resource', 'design', 'disease classification', 'experience', 'gender difference', 'genome resource', 'health data', 'improved', 'interest', 'knowledge base', 'knowledge graph', 'member', 'novel', 'online community', 'outreach', 'programs', 'public repository', 'repository', 'therapeutic protein', 'tool', 'web site']",NCI,UNIVERSITY OF NEW MEXICO HEALTH SCIS CTR,U24,2021,1037175
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,10158538,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2021,93342
"Temporal relation discovery for clinical text Project Summary / Abstract The current proposal continues the investigation on the topic of temporal relation extraction from the Electronic Medical Records (EMR) clinical narrative funded by the NLM since 2010 (Temporal Histories of Your Medical Events, or THYME; thyme.healthnlp.org). Through our efforts so far, we have defined the topic as an active area of research attracting attention across the world. Since its inception, the project has pushed the boundaries of this highly challenging task by investigating new computational methods within the context of the latest developments in the fields of natural language processing (NLP), machine learning (ML), artificial intelligence (AI) and biomedical informatics (BMI) resulting in 60+ publications/presentations. We have made our best performing methods available to the community open source as part of the Apache Clinical Text Analysis and Knowledge Extraction System (cTAKES; ctakes.apache.org). In 2015, 2016, 2017 and 2018, we organized an international shared task (Clinical TempEval) on the topic under the umbrella of the highly prestigious SemEval, thus inviting the international community to work with our THYME data and improve on our results. Clinical TempEval has been highly successful with many participants each year, resulting in new discoveries and many publications. We have made all our data along with our gold standard annotations available to the community through the hNLP Center (center.healthnlp.org).  The underlying theme of this renewal is novel methods for combining explicit domain knowledge (linguistic, semantic, biomedical ontological, clinical), readily available unlabeled data (health-related social media, EMRs), and modern machine learning techniques (e.g. neural networks) for temporal relation extraction from the EMR clinical narrative. Therefore, our renewal proposes a novel and much needed exploration of this line of research:  Specific Aim 1: Develop computational models for novel rich semantic representations such as the Abstract Meaning Representations to encapsulate a single, coherent, full-document graphical representation of meaning for temporal relation extraction  Specific Aim 2: Develop computational methods to infuse domain knowledge (linguistic, semantic, biomedical ontological, clinical) into modern machine learning techniques such as NNs for temporal relation extraction – through input representations, pre-trained vectors, or architectures  Specific Aim 3: Develop novel methods for combining labeled and unlabeled data from various sources (EMR, health-related social media, newswire) for temporal relation extraction from the clinical narrative  Specific Aim 4: Apply the best performing methods for temporal relation extraction developed in SA1-3 to temporally sensitive phenotypes for direct translational sciences studies. Dissemination efforts through publications and open source releases into Apache cTAKES. Project Narrative Temporal relations are of prime importance in biomedicine as they are intrinsically linked to diseases, signs and symptoms, and treatments. Understanding the timeline of clinically relevant events is key to the next generation of translational research where the importance of generalizing over large amounts of data holds the promise of deciphering biomedical puzzles. The goal of our current proposal is to automatically discover temporal relations from clinical free text and structured EMR data and create an aggregated patient-level timeline.",Temporal relation discovery for clinical text,10176589,R01LM010090,"['Address', 'Apache', 'Architecture', 'Area', 'Artificial Intelligence', 'Attention', 'Clinical', 'Cognitive', 'Communities', 'Complex', 'Computer Models', 'Computerized Medical Record', 'Computing Methodologies', 'Coupled', 'Data', 'Development', 'Disease', 'Encapsulated', 'Engineering', 'Event', 'Fostering', 'Foundations', 'Funding', 'Goals', 'Gold', 'Health', 'Image', 'International', 'Investigation', 'Knowledge', 'Knowledge Extraction', 'Label', 'Linguistics', 'Link', 'Machine Learning', 'Medical', 'Methods', 'Modernization', 'Natural Language Processing', 'Nature', 'Participant', 'Patient Care', 'Patients', 'Phenotype', 'Publications', 'Recording of previous events', 'Research', 'Semantics', 'Signs and Symptoms', 'Solid', 'Source', 'Speed', 'Structure', 'System', 'Techniques', 'Text', 'Thyme', 'Time', 'TimeLine', 'Training', 'Translational Research', 'Vision', 'Work', 'advanced disease', 'base', 'biomedical informatics', 'biomedical ontology', 'clinically relevant', 'cohesion', 'electronic data', 'electronic structure', 'epidemiology study', 'improved', 'individualized medicine', 'learning community', 'neural network', 'next generation', 'novel', 'open source', 'programs', 'relating to nervous system', 'social media', 'support vector machine', 'symptom treatment', 'vector']",NLM,BOSTON CHILDREN'S HOSPITAL,R01,2021,531398
"Semi-Automating Data Extraction for Systematic Reviews Summary ​Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Evidence-based Medicine (EBM) aims to inform patient care using all available evidence. Realizing this aim in practice would require access to concise, comprehensive, and up-to-date structured summaries of the evidence relevant to a particular clinical question. Systematic reviews of biomedical literature aim to provide such summaries, and are a critical component of the EBM arsenal and modern medicine more generally. However, such reviews are extremely laborious to conduct. Furthermore, owing to the rapid expansion of the biomedical literature base, they tend to go out of date quickly as new evidence emerges. These factors hinder the practice of evidence-based care. In this renewal proposal, we seek to continue our ground-breaking efforts on developing, evaluating, and deploying novel machine learning (ML) and natural language processing (NLP) methods to automate or semi-automate the evidence synthesis process. This will extend our innovative and successful efforts developing RobotReviewer and related technologies under the current grant. Concretely, for this renewal we propose to move from extraction of clinically salient data elements from individual trials to synthesis of these elements across trials. Our first aim is to extend our ML and NLP models to produce (as one deliverable) a publicly available, continuously and automatically updated semi-structured evidence database, comprising extracted data for all evidence, both published and unpublished. Unpublished trials will be identified via trial registries. Taking this up-to-date evidence repository as a starting point, we then propose cutting-edge ML and NLP models that will generate first drafts of evidence syntheses, automatically. More specifically we propose novel neural cross-document summarization models that will capitalize on the semi-structured information automatically extracted by our existing models, in addition to article texts. These models will be deployed in a new version of RobotReviewer, called RobotReviewerLive, intended to be a prototype for “living” systematic reviews. To rigorously evaluate the practical utility of the proposed methodological innovations, we will pilot their use to support real, ongoing, exemplar living reviews. Semi-Automating Data Extraction for Systematic Reviews (​Renewal) Narrative We propose novel machine learning and natural language processing methods that will aid biomedical literature summarization and synthesis, and thereby support the conduct of evidence-based medicine (EBM). The proposed models and technologies will motivate core methodological innovations and support real-time, up-to-date, semi-automated biomedical evidence syntheses (“systematic reviews”). Such approaches are necessary if we are to have any hope of practicing evidence-based care in our era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,10199049,R01LM012086,"['American', 'Automation', 'Caring', 'Clinical', 'Collection', 'Consumption', 'Data', 'Data Element', 'Databases', 'Development', 'Elements', 'Evaluation', 'Evidence Based Medicine', 'Evidence based practice', 'Feedback', 'Grant', 'Hybrids', 'Individual', 'Informatics', 'Internet', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical Informatics', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Modern Medicine', 'Natural Language Processing', 'Outcome', 'Output', 'Paper', 'Patient Care', 'Population Intervention', 'Process', 'PubMed', 'Publications', 'Publishing', 'Registries', 'Reporting', 'Research', 'Resources', 'Risk', 'Stroke', 'Structure', 'Surveillance Methods', 'System', 'Technology', 'Text', 'Textbooks', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Vision', 'Work', 'base', 'cardiovascular health', 'database structure', 'design', 'evidence base', 'improved', 'indexing', 'innovation', 'machine learning method', 'natural language', 'neural network', 'novel', 'open source', 'programs', 'prospective', 'prototype', 'recruit', 'relating to nervous system', 'repository', 'search engine', 'structured data', 'study characteristics', 'study population', 'success', 'systematic review', 'tool', 'usability', 'working group']",NLM,NORTHEASTERN UNIVERSITY,R01,2021,292031
"Advanced End-to-End Relation Extraction with Deep Neural Networks ABSTRACT Relations linking various biomedical entities constitute a crucial resource that enables biomedical data science applications and knowledge discovery. Relational information spans the translational science spectrum going from biology (e.g., protein–protein interactions) to translational bioinformatics (e.g., gene–disease associations), and eventually to clinical care (e.g., drug–drug interactions). Scientists report newly discovered relations in nat- ural language through peer-reviewed literature and physicians may communicate them in clinical notes. More recently, patients are also reporting side-effects and adverse events on social media. With exponential growth in textual data, advances in biomedical natural language processing (BioNLP) methods are gaining prominence for biomedical relation extraction (BRE) from text. Most current efforts in BRE follow a pipeline approach containing named entity recognition (NER), entity normalization (EN), and relation classiﬁcation (RC) as subtasks. They typically suffer from error snowballing — errors in a component of the pipeline leading to more downstream errors — resulting in lower performance of the overall BRE system. This situation has lead to evaluation of different BRE substaks conducted in isolation. In this proposal we make a strong case for strictly end-to-end evaluations where relations are to be produced from raw text. We propose novel deep neural network architectures that model BRE in an end-to-end fashion and directly identify relations and corresponding entity spans in a single pass. We also extend our architectures to n-ary and cross-sentence settings where more than two entities may need to be linked even as the relation is expressed across multiple sentences. We also propose to create two new gold standard BRE datasets, one for drug–disease treatment relations and another ﬁrst of a kind dataset for combination drug therapies. Our main hypothesis is that our end-to-end extraction models will yield supe- rior performance when compared with traditional pipelines. We test this through (1). intrinsic evaluations based on standard performance measures with several gold standard datasets and (2). extrinsic application oriented assessments of relations extracted with use-cases in information retrieval, question answering, and knowledge base completion. All software and data developed as part of this project will be made available for public use and we hope this will foster rigorous end-to-end benchmarking of BRE systems. NARRATIVE Relations connecting biomedical entities are at the heart of biomedical research given they encapsulate mech- anisms of disease etiology, progression, and treatment. As most such relations are ﬁrst disclosed in textual narratives (scientiﬁc literature or clinical notes), methods to extract and represent them in a structured format are essential to facilitate applications such as hypotheses generation, question answering, and information retrieval. The high level objective of this project is to develop and evaluate novel end-to-end supervised machine learning methods for biomedical relation extraction using latest advances in deep neural networks.",Advanced End-to-End Relation Extraction with Deep Neural Networks,10200889,R01LM013240,"['Adverse event', 'Architecture', 'Area', 'Benchmarking', 'Bioinformatics', 'Biology', 'Biomedical Research', 'Classification', 'Clinical', 'Code', 'Collaborations', 'Combination Drug Therapy', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Disease', 'Distant', 'Drug Interactions', 'Encapsulated', 'Etiology', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Gold', 'Growth', 'Hand', 'Heart', 'Information Retrieval', 'Information Sciences', 'Intramural Research', 'Joints', 'Knowledge Discovery', 'Label', 'Language', 'Lead', 'Link', 'Literature', 'Manuals', 'Maps', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Patients', 'Peer Review', 'Performance', 'Periodicity', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'Scientist', 'Semantics', 'Software Tools', 'Source', 'Standardization', 'Structure', 'Supervision', 'System', 'Terminology', 'Testing', 'Text', 'Training', 'Translational Research', 'Trees', 'base', 'biomedical data science', 'clinical care', 'deep neural network', 'improved', 'insight', 'interest', 'knowledge base', 'machine learning method', 'natural language', 'neural network', 'neural network architecture', 'new therapeutic target', 'novel', 'off-label use', 'protein protein interaction', 'relating to nervous system', 'side effect', 'social media', 'supervised learning', 'syntax']",NLM,UNIVERSITY OF KENTUCKY,R01,2021,332681
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10330631,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2021,736476
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,10197219,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical data science', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'large scale data', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2021,506502
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,10149399,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2021,313018
"Audio Generation and Optimization from Existing Resources for Patient Education Project Summary/Abstract Health literacy is vital to achieving and maintaining good health. Several national programs have emphasized this goal and its importance. Text is generally much more efficient and cost-effective for presenting healthcare information on a large scale than interactive tools and videos. Over the past decade, therefore, most medical information has been provided as text, e.g., via printed pamphlets or on websites. We are entering a new era where a new similarly effective mode of information dissemination is becoming increasingly available: audio accessed with mobile devices. Millions of households have and use smart speakers and virtual assistants and they are increasingly used by patients and consumers to gather information. Hospitals also plan to gradually integrate them among their tools. However, there exist few if any guidelines on optimal generation and use of audio. The overall goal of this project is to discover how to support the creation of optimal audio from existing text sources for consumer and patient education. To accomplish this, four aims are proposed. The first aim is to identify audio features that affect information comprehension and retention. Here, features in audio content and style (e.g., word frequency or grammatical complexity) of the underlying information will be tested for impact. In addition, two groups of features specific to the audio medium will be tested: the delivery features (e.g., speed and pauses) as well as meta-features (e.g., speaker characteristics such as gender or accent and bias in listeners). This first aim will rely on large-scale datasets, semi-automatically generated and augmented with user scores for comprehension gathered using Amazon Mechanical Turk (MTurk). Statistical and machine learning approaches will be used to tease out the best features and combinations. The second aim focuses on discovering how to augment text for audio and finding the optimal combination of text and audio for information comprehension and retention. Different combinations will be tested online with MTurk participants using controlled user studies. The third aim is to update, test and provide the existing online free text editor to generate optimized audio. We will also start dissemination of the tool to potential users including API access to components. The project will conclude with a summative evaluation with representative consumers recruited at a local community health center and further dissemination of preferences, practical obstacles, and best practices for the medical community to help increase health literacy through this new, popular audio medium. If successful, this project will generate best practices for the medical community in using audio as an additional method for bringing healthcare information to the general public; it will provide an online, free tool to generate audio leveraging these best practices and will include API access so that other researchers can easily integrate tool components into their research and tools; and it will provide immediate practical lessons from working with consumers relevant for clinical practice. Project Narrative Improving health literacy is an important national goal and necessary for achieving and maintaining health. The increased adoption of smart speakers and virtual assistants by consumers and in medical settings has created novel opportunities to educate the population by delivering health information through audio or audio combined with text. Because few (if any) tools exist to improve content and generate optimal audio, we aim to discover supportive and adverse features of audio/text information provision and create a free, online software tool for optimizing health-related text with demonstrated impact on information comprehension and retention.",Audio Generation and Optimization from Existing Resources for Patient Education,10295641,R01LM011975,"['Accent', 'Adoption', 'Affect', 'Affordable Care Act', 'Age', 'Arizona', 'Characteristics', 'Collaborations', 'Communication', 'Communities', 'Comprehension', 'Computer software', 'Computers', 'Data', 'Development', 'Effectiveness', 'Evaluation', 'Frequencies', 'Gender', 'General Population', 'Generations', 'Goals', 'Government', 'Growth', 'Guidelines', 'Health', 'Healthcare', 'Hospitals', 'Household', 'Information Dissemination', 'Machine Learning', 'Measures', 'Mechanics', 'Medical', 'Methods', 'Modeling', 'Natural Language Processing', 'Neighborhood Health Center', 'Operative Surgical Procedures', 'Outcome', 'Pamphlets', 'Participant', 'Patient Education', 'Patients', 'Pilot Projects', 'Population', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Source', 'Specific qualifier value', 'Speed', 'Technology', 'Testing', 'Text', 'Update', 'Voice', 'Work', 'Work Simplification', 'Writing', 'application programming interface', 'clinical encounter', 'clinical practice', 'clinically relevant', 'cost effective', 'design', 'digital', 'experience', 'handheld mobile device', 'health literacy', 'improved', 'information processing', 'innovation', 'intelligent personal assistant', 'interactive tool', 'large scale data', 'novel', 'open source', 'preference', 'programs', 'real world application', 'recruit', 'skills', 'statistical and machine learning', 'symposium', 'tool', 'web site']",NLM,UNIVERSITY OF ARIZONA,R01,2021,257905
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,10133117,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2021,249000
"Data Science Applications in Communication andSwallowing Disorders PROJECT SUMMARY/ABSTRACT The emergence of electronic medical records, large data registries and readily accessible, protected servers have resulted in an explosion of digital information with potentially high clinical impact for improving patient management and outcomes. Big data warehouses that capture standardized information within the scope of clinical practices allow trained scientists to not only engage in traditional hypothesis testing, but to also uncover new hypotheses, refine existing theories and apply new discoveries to health assessments and interventions. Despite the accessibility and potential impact of these data platforms, clinician scientists have traditionally directed experiments that incorporate relatively small sample sizes and data from individual laboratories, and have not been trained in big data analytics or in engaging appropriate team scientists who work in this space, such as computer scientists, biostatisticians and engineers. The overarching goal of this proposal is to mentor early patient oriented communication and swallowing scientists in big data analytics and to mentor and involve early data science scholars in communication and swallowing research. The PI proposes four primary mentorship and research goals in this K24 renewal proposal: 1. Train a cadre of early stage communication and swallowing scientists in data science methods, including machine learning, by an expert, interdisciplinary, collaborative data science team, 2. Engage and introduce early career data scientists from fields of biostatistics, computer science and engineering to communication and swallowing sciences, and respective data sets, toward facilitating interdisciplinary data science teams and research productivity, 3. Apply novel data science methods to identify phenotypes of swallowing impairment and severity classifications in patient groups known to be at high risk for nutritional and health complications related to dysphagia, and 4. Develop a new area of research in machine learning applications toward improving reliability of physiologic swallowing assessment. The data science theme of the career development and research plan directly align with NIDCD's Strategic Plan for Data Science which lists as its mission: Storing, managing, standardizing and publishing the vast amounts of data produced by biomedical research. NIDCD recognizes that accessible, well-organized, secure and efficiently operated data resources are critical to modern scientific inquiry…and by maximizing the value of data generated through NIH-funded efforts, the pace of biomedical discoveries and medical breakthroughs for better health outcomes can be accelerated. PROJECT NARRATIVE The emergence of electronic health records exposes clinicians to massive amounts of information about the millions of patients who suffer from communication and swallowing disorders, yet most clinical scientists do not have the training or skill to apply meaning to the data toward improving patient care. The overarching mentorship goal of this proposal is to train early, patient-oriented communication and swallowing scientists in big data analyses, including computer machine learning approaches. The research project will uncover distinct patterns and severity of swallowing impairments in large groups of patients with high risk medical diagnoses, which will have high impact on patient care planning and identification of treatments that directly target these impairments for improved outcomes.",Data Science Applications in Communication andSwallowing Disorders,10073493,K24DC012801,"['Algorithms', 'Area', 'Award', 'Barium swallow', 'Big Data', 'Big Data Methods', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Communication', 'Communication impairment', 'Computer Vision Systems', 'Computerized Medical Record', 'Computers', 'Data', 'Data Analyses', 'Data Science', 'Data Scientist', 'Data Set', 'Deglutition', 'Deglutition Disorders', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Economics', 'Electronic Health Record', 'Engineering', 'Explosion', 'Funding', 'Geography', 'Goals', 'Grant', 'Head and Neck Cancer', 'Health', 'Hearing', 'Impairment', 'Individual', 'Instruction', 'Intervention', 'Laboratories', 'Machine Learning', 'Medical', 'Medical Records', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Mid-Career Clinical Scientist Award (K24)', 'Mission', 'Modernization', 'National Institute on Deafness and Other Communication Disorders', 'Nutritional', 'Outcome', 'Parkinson Disease', 'Patient Care', 'Patient Care Planning', 'Patients', 'Pattern', 'Phenotype', 'Physiological', 'Productivity', 'Publishing', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Sample Size', 'Science', 'Scientific Inquiry', 'Scientist', 'Secure', 'Severities', 'Speech', 'Standardization', 'Statistical Models', 'Strategic Planning', 'Stroke', 'Supervision', 'Techniques', 'Testing', 'Training', 'United States National Institutes of Health', 'Work', 'base', 'career', 'career development', 'clinical practice', 'clinically relevant', 'computer science', 'computerized', 'data registry', 'data resource', 'data warehouse', 'digital', 'experimental study', 'health assessment', 'high risk', 'human error', 'impression', 'improved', 'improved outcome', 'learning algorithm', 'novel', 'patient oriented', 'programs', 'research and development', 'skills', 'statistical learning', 'theories', 'uptake']",NIDCD,NORTHWESTERN UNIVERSITY,K24,2021,183209
"Transforming Analytical Learning in the Era of Big Data PROJECT SUMMARY In this dawning era of `Big Data' it is vital to recruit and train the next generation of biomedical data scientists in `Big Data'. The collection of `Big Data' in the biomedical sciences is growing rapidly and has the potential to solve many of today's pressing medical needs including personalized medicine, eradication of disease, and curing cancer. Realizing the benefits of Big Data will require a new generation of leaders in (bio)statistical and computational methods who will be able to develop the approaches and tools necessary to unlock the information contained in large heterogeneous datasets. There is a great need for scientists trained in this specialized, highly heterogeneous, and interdisciplinary new field of health big data. Thus, the recruitment of talented undergraduates in science, technology, engineering and mathematics (STEM) programs is vital to our ability to tap into the potential that `Big Data' offers and the challenges that it presents. The University of Michigan Undergraduate Summer Institute: Transforming Analytical Learning in the Era of Big Data will primarily draw from the expertise and experience of faculty from three different departments within three different schools at the University of Michigan: Biostatistics in the School of Public Health, Computer Science in the School of Engineering, Statistics in the College of Literature, Sciences and the Arts. The faculty instructors and mentors have backgrounds in Statistics, Computer Science, Information Science, Medicine, Population Health, Social and Biological Sciences. They have active research programs in a broad spectrum of methodological areas including statistical modeling, data mining, natural language processing, statistical and machine learning, large-scale optimization, matrix computation, medical computing, health informatics, high- dimensional statistics, distributed computing, missing data, causal inference, data management and integration, signal processing and medical imaging. The diseases and conditions they study include obesity, diabetes, cardiovascular disease, cancer, neurological disease, kidney disease, injury, macular degeneration and Alzheimer's disease. The areas of biology include neuroscience, genetics, genomics, metabolomics, epigenetics and socio-behavioral science. Undergraduate trainees selected will have strong quantitative skills and a background in STEM. The summer institute will consist of a combination of coursework, to raise the skills and interests of the participants to a sufficient level to consider pursuing graduate studies in `Big Data' science, along with an in depth mentoring component that will allow the participants to research a specific topic/project utilizing `Big Data'. We have witnessed tremendous enthusiasm and success with the current summer program on Big Data led by this team with 164 students trained in the last 4 years (2015-2018) including 90 female students and 30 students from underrepresented minority groups. Fourteen of these participants from the last three years are currently graduate students in Michigan Biostatistics. The ongoing program has gained traction in the national landscape of summer research programs with 20% rate of admission and 80% rate of acceptance among those who are offered this opportunity. The program has consistently received very strong evaluation and our past alumni have become brand ambassadors and advocates for our program. We plan to build on the success and legacy of this program in the next three year funding cycle of this grant (2019-2021). The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm. This six week long summer institute will recruit a group of approximately 45 undergraduates nationally and internationally, with 20 domestic students supported by the requested SIBS funding mechanism and others supported by supplementary institutional and foundation support. We propose to expose the trainees to diverse techniques, skills and problems in the field of health Big Data. They will be taught and mentored by a team of interdisciplinary faculty, reflecting the shared intellectual landscape needed for Big Data research. They will engage in mentored research projects in three primary areas of health big data: Electronic Health Records/Medical Claims, Genomics and Imaging. Some of the projects will be defined in the area of cardiovascular precision medicine, defined by a team of highly quantitative researchers engaged in cardiovascular research that uses big data. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by U-M researchers, outside guests and a professional development workshop to prepare the students for graduate school. We propose an inter-SIBS collaboration with Dordt College summer program trainees who will attend this concluding symposium. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material for undergraduate students in this new field across the world. We will offer multiple professional development opportunities and resources for graduate school preparation to our trainees so that they can reflect and plan beyond their senior year. All of our proposed activities are reflected through our three specific aims: Teaching, Mentoring and Dissemination. PROJECT NARRATIVE We propose a six week long undergraduate summer institute: “Transforming Analytical Learning in the Era of Big Data” to be held at the Department of Biostatistics, University of Michigan (U-M), Ann Arbor, with a group of approximately 45 undergraduate students recruited nationally and internationally, from 2019-2021. Funding is requested for 20 domestic students with supplementary funding expected to be garnered through institutional resources and private foundation support. The program builds on the success of our existing Big Data Summer Institute (BDSI) supported by a NIH BD2K Courses and Skills grant award that is ending in 2018. We plan to expose program students to diverse techniques, skills and problems in the field of Big Data and Human Health. We enhance our ongoing summer program structure in the current proposal by involving a team of researchers working at the intersection of cardiovascular research and data science with a focus on cardiovascular precision medicine where some of the new mentored research projects will be defined. We primarily focus on three genres of health Big Data arising in Electronic Health Records/Medical Claims, Genomics and Imaging. The trainees will be taught and mentored by a team of interdisciplinary faculty from Biostatistics, Computational Medicine and Bioinformatics, Statistics, Computer Science and Engineering, Information Sciences, Epidemiology and Medicine, reflecting the shared intellectual landscape needed for Big Data research. At the conclusion of the program there will be a concluding capstone symposium showcasing the research of the students via poster and oral presentation. There will be lectures by (U-M) researchers, outside guests and a professional development workshop to prepare the students for graduate school. The resources developed for the summer institute, including lectures, assignments, projects, template codes and datasets will be freely available through a Wiki page so that this format can be replicated anywhere in the world. This democratic dissemination plan will lead to access of teaching and training material in this new field across the world. The overarching goal of our summer institute in big data is to recruit and train the next generation of big data scientists using a non-traditional, action-based learning paradigm and engage them in influential research related to human health.",Transforming Analytical Learning in the Era of Big Data,10130608,R25HL147207,"['Admission activity', 'Adverse drug effect', 'Advocate', 'Alzheimer&apos', 's Disease', 'Area', 'Arts', 'Award', 'Basic Science', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biology', 'Biometry', 'Cardiovascular Diseases', 'Cardiovascular system', 'Case Study', 'Clinical', 'Clinical Sciences', 'Code', 'Collaborations', 'Collection', 'Computing Methodologies', 'Data', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Epidemiology', 'Epigenetic Process', 'Evaluation', 'Exposure to', 'Faculty', 'Female', 'Foundations', 'Funding', 'Funding Mechanisms', 'Generations', 'Genetic', 'Genomics', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Human', 'Image', 'Influentials', 'Information Sciences', 'Injury', 'International', 'Kidney Diseases', 'Learning', 'Literature', 'Macular degeneration', 'Malignant Neoplasms', 'Medical', 'Medical Imaging', 'Medical Records', 'Medicine', 'Mentors', 'Methodology', 'Methods', 'Michigan', 'Minority Groups', 'Natural Language Processing', 'Neurosciences', 'Obesity', 'Oral', 'Participant', 'Prevention', 'Privatization', 'Problem Sets', 'Public Health Informatics', 'Public Health Schools', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'STEM program', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics', 'Scientist', 'Social Behavior', 'Social Sciences', 'Statistical Methods', 'Statistical Models', 'Structure', 'Student recruitment', 'Students', 'Talents', 'Techniques', 'Traction', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Woman', 'Work', 'base', 'big-data science', 'burden of illness', 'cluster computing', 'college', 'computer science', 'data integration', 'data management', 'data mining', 'data visualization', 'design', 'experience', 'graduate school preparation', 'graduate student', 'heterogenous data', 'high dimensionality', 'instructor', 'interest', 'lectures', 'medical schools', 'member', 'metabolomics', 'nervous system disorder', 'network models', 'next generation', 'novel therapeutics', 'open source', 'personalized medicine', 'population health', 'posters', 'precision medicine', 'programs', 'recruit', 'signal processing', 'skills', 'statistical and machine learning', 'statistics', 'student training', 'success', 'summer institute', 'summer program', 'summer research', 'symposium', 'tool', 'undergraduate student', 'underrepresented minority student', 'wiki']",NHLBI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R25,2021,250974
"Training the next generation of leaders in biomedical engineering design Project Summary/Abstract The next generation of bioengineering and biomedical researchers will have unprecedented access to technologies including wireless health, big data, genetic sequencing, and machine learning approaches to enable modern diagnostic and therapeutic techniques. This presents individuals trained at the interface of technology and biomedicine with an enormous opportunity to address the world’s needs in health and medicine. In the Bioengineering Department at the University of California, Los Angeles, we aim to develop students into leaders able to seamlessly identify clinical needs that technology can address, design and validate solutions that address these needs, communicate with a variety of stakeholders to build teams invested in problem-oriented solutions, and to navigate the regulatory and commercial pathways necessary to enable their technologies to thrive. The Bioengineering Capstone Series at UCLA leverages resources available at UCLA to enable students to: 1) gain insight into clinical needs directly from clinicians and educators across the Ronald Reagan Medical Center, David Geffen School of Medicine, School of Dentistry and UCLA Health System, 2) design their solutions through mentorship from engineering professors, 3) understand the complexities of the biomedical industry with support from the UCLA Technology Development Group and members of the Department of Engineering Industry Advisory Board, 4) utilize modern technologies in wireless health and data science by collaborating with the Center of Excellence for Mobile Sensor Data- to-Knowledge (MD2K) and Mobile Health (mHealth) Institute at UCLA, and 5) work with the National Science Foundation Precise Advanced Technologies and Health Systems for Underserved Populations Engineering Research Center (NSF PATHS- UP ERC) to learn to target and communicate their technologies to maximize societal benefits. Statement of Public Health Relevance: To prepare the next generation of engineers with skills in the design of therapeutics and medical devices, this research education program will provide bioengineering students at UCLA with enhanced opportunities to engage in real world design for biomedical applications. We will guide students through the medical design process from identifying needs, creating solutions which address these needs, and communicating the significance of their contributions to the greater community, ultimately yielding a larger pool of well-trained engineers to address biomedical challenges. !",Training the next generation of leaders in biomedical engineering design,10142466,R25EB027626,"['Address', 'Applied Research', 'Area', 'Big Data', 'Biomedical Engineering', 'Biomedical Technology', 'California', 'Clinic', 'Clinical', 'Communication', 'Communities', 'Data', 'Data Science', 'Device or Instrument Development', 'Devices', 'Diagnostic', 'Educational workshop', 'Engineering', 'Event', 'Faculty', 'Fostering', 'Foundations', 'Freedom', 'Future', 'Genetic', 'Goals', 'Health', 'Health Sciences', 'Health Technology', 'Health system', 'Healthcare', 'High School Student', 'Home environment', 'Individual', 'Industry', 'Infrastructure', 'Institutes', 'Instruction', 'Intellectual Property', 'Investments', 'Laboratories', 'Learning', 'Los Angeles', 'Machine Learning', 'Medical', 'Medical Device', 'Medical center', 'Medicine', 'Mentorship', 'Modernization', 'Pathway interactions', 'Patient Monitoring', 'Physiological', 'Process', 'Recommendation', 'Regulation', 'Research', 'Research Personnel', 'Resources', 'STEM field', 'School Dentistry', 'Schools', 'Science', 'Science, Technology, Engineering and Mathematics Education', 'Series', 'Societies', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Technology Transfer', 'Therapeutic', 'Time', 'Training', 'Translations', 'Travel', 'Underrepresented Students', 'Underserved Population', 'Universities', 'Wireless Technology', 'Work', 'base', 'career', 'clinically actionable', 'cloud based', 'commercialization', 'data to knowledge', 'deep learning', 'design', 'education research', 'engineering design', 'experience', 'insight', 'interest', 'lectures', 'mHealth', 'medical schools', 'member', 'multidisciplinary', 'next generation', 'outreach', 'product development', 'professor', 'programs', 'public health relevance', 'sensor', 'skills', 'technology development', 'undergraduate student', 'underserved community']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R25,2021,21600
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,10093056,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'feature selection', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'machine learning method', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistical and machine learning', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2021,293003
"Breakthrough Molecular Dynamics Research via an Anton2 Supercomputer Project Summary In 2010, Pittsburgh Supercomputing Center (PSC) and D.E. Shaw Research (DESRES) partnered to make the Anton special-purpose molecular dynamics (MD) supercomputer available to the national biomedical research community for the first time. Anton enabled researchers to simulate biomolecular systems two orders of magnitude faster than any conventional supercomputer, allowing researchers access to critical multi- microsecond and longer timescales on which most biologically-significant molecular processes take place. In 2016, with operational support from NIH, DESRES made available a next generation Anton 2 system at PSC at no cost. This multimillion-dollar gift from DESRES, with NIH support, provided a unique opportunity for researchers to tackle even more groundbreaking biological questions by simulating systems as large as 700,000 atoms at a rate of multiple microseconds per day. The goal of this renewal project is to provide the national biomedical research community with continued access to this unique and powerful Anton 2 resource and to maximize the benefit of this resource to the community. Since 2010, PSC has supported 589 research projects conducted by 202 unique PIs on the Anton systems hosted at PSC. So far, these projects have resulted in 292 publications, many of which have had significant impact on their fields of research. These fields cover the entire spectrum of biomolecular processes that form the fundamental underpinnings of biology, including protein folding, ion channel selectivity and gating, membrane dynamics and organization, protein- ligand binding, and many others. Demand for Anton 2 is quite high, with 2-3x higher demand than can be met each year. Given this, our goals are focused not only on providing continued access but also on maximizing the value of this limited resource to the community—recognizing that the creativity of the broad, diverse Anton 2 research community is one of our most powerful resources for innovation. Anton 2 will be integrated as an XSEDE service provider, and the accessibility and impact of Anton 2 will significantly increase by engaging in their extensive broadening participation, outreach, and training programs. Through these outreach efforts, researchers at many more institutions will have the opportunity to use Anton 2, especially researchers from traditionally underrepresented groups in biomedical research. The hundreds of long-timescale MD trajectories researchers generate on Anton 2 constitute a unique and valuable dataset that can be used to gain additional biomedical knowledge and advance the application of machine learning protocols to augment molecular dynamics simulations. This important dataset will be available to researchers and educators worldwide through a web portal and co-located on the PSC's Bridges-2 system for classroom instruction and research, including reanalysis, machine learning, and data mining. Project Narrative The Anton 2 supercomputer made available at Pittsburgh Supercomputing Center through this award will enable researchers to simulate biomolecular systems orders of magnitude faster than any conventional supercomputer. This powerful resource will enable biomedical researchers to better understand the fundamental biomolecular processes of biology, leading to better ways to treat disease and improve quality of life.",Breakthrough Molecular Dynamics Research via an Anton2 Supercomputer,10211715,R01GM116961,"['Academy', 'Area', 'Award', 'Awareness', 'Binding Proteins', 'Biological', 'Biology', 'Biomedical Research', 'Collaborations', 'Communities', 'Community Health Education', 'Creativeness', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Disease', 'Evaluation Reports', 'FAIR principles', 'Funding', 'Gifts', 'Goals', 'Grant', 'Institution', 'Instruction', 'Ion Channel', 'Knowledge', 'Letters', 'Machine Learning', 'Membrane', 'Molecular', 'National Research Council', 'Process', 'Protocols documentation', 'Publications', 'Quality of life', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Running', 'Science', 'Secure', 'Supercomputing', 'System', 'Time', 'Training Programs', 'Underrepresented Populations', 'United States National Academy of Sciences', 'United States National Institutes of Health', 'Work', 'anxious', 'archive data', 'archived data', 'base', 'broadening participation research', 'computing resources', 'cost', 'data management', 'data mining', 'data sharing', 'improved', 'innovation', 'molecular dynamics', 'next generation', 'outreach', 'outreach program', 'price lists', 'protein folding', 'service providers', 'supercomputer', 'uptake', 'web portal']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2021,408856
"Developing novel technologies that ensure privacy and security in biomedical data science research Data science holds the promise of enabling new pathways to discovery and can improve the understanding, prevention and treatment of complex disorders such as cancer, diabetes, substance abuse, etc., which are significantly on the rise. The promise of data science can be fully realized only when collected data can be collaboratively shared and analyzed. However, the widespread increases in healthcare data breaches due to inappropriate access as well as the increasing number of novel privacy attacks restrict institutions from sharing data. Indeed, in some cases, the results of the analysis can themselves lead to significant privacy harm. The success of the data commons depends on ensuring the maximal access to data, subject to all of the patient privacy requirements including those mandated by legislation, and all of the constraints of the organization collecting the data itself. While there are existing solutions that can solve parts of the problem, there are significant challenges in truly incorporating these into comprehensive working solutions that are usable by the biomedical research community, and new challenges brought on by modern techniques such as deep learning. The long-term goal of this research is to develop technologies that can holistically enable data sharing while respecting privacy and security considerations and to ensure that they are implemented in existing platforms that have widespread acceptance in the research community. Towards this, the objective of this project is to develop complementary solutions for risk inference, distributed learning, and access control that can enable different modalities of data sharing. The problems studied are general in nature and will evolve depending on research successes and new impediments that arise. The proposed program of research is significant since lack of access to biomedical data can lead to fragmentation of care, resulting in higher economic and social costs, and is a significant impediment to biomedical research. The project will result in open-source, freely available software tools that will be integrated into widely used data collection, cohort identification, and distributed analytics platforms. There are several ongoing collaborations that will serve as initial pilot customers to provide use cases, identify the requirements, evaluate results, and in general validate the developed solutions. Project Narrative Statement of Relevance to Public Health Being able to ensure privacy and security while enabling data sharing and analysis is critical to pave the way forward for public health research and improve our understanding of diseases. The proposed work will address the challenges that impede the use of data across all of the different modalities of data sharing. The integration into existing platforms will ensure that the developed models, tools, and solutions directly impact the research community and improve public health interventions.",Developing novel technologies that ensure privacy and security in biomedical data science research,10077318,R35GM134927,"['Address', 'Biomedical Research', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Science', 'Diabetes Mellitus', 'Disease', 'Economics', 'Ensure', 'Goals', 'Healthcare', 'Institution', 'Lead', 'Learning', 'Malignant Neoplasms', 'Modality', 'Modeling', 'Modernization', 'Nature', 'Pathway interactions', 'Prevention', 'Privacy', 'Public Health', 'Research', 'Risk', 'Security', 'Software Tools', 'Statutes and Laws', 'Substance abuse problem', 'Techniques', 'Technology', 'Work', 'biomedical data science', 'care fragmentation', 'cohort', 'cost', 'data sharing', 'deep learning', 'improved', 'new technology', 'novel', 'open source', 'patient privacy', 'programs', 'public health intervention', 'public health research', 'social', 'success', 'tool']",NIGMS,RUTGERS THE STATE UNIV OF NJ NEWARK,R35,2021,383279

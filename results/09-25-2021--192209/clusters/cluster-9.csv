text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,9739919,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Base Management', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistics', 'subchondral bone', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,588354,-0.012220076971830595
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9713512,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2019,647706,-0.007862874086829579
"AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space Project Summary  An extensible analysis platform will be developed to accurately perform the automated genotyping of PCR/capillary electrophoresis (CE) traces for multiple disease-associated short tandem repeater (STR) assays. This study will evaluate the feasibility of developing generalizable and adaptive molecular analysis models, and will ultimately establish a new paradigm for deep learning analytical tools in the molecular diagnostic space.  Advanced machine learning strategies will be applied to interpret genotypes of inherited disorders caused by genetically unstable STR DNA sequences. STRs have traditionally been difficult to investigate due to their length (on the order of kilobases) and low sequence complexity, which elude detection by traditional and next- generation sequencing technologies. However, advances in PCR/CE technology have enabled the amplification and fragment sizing of STR DNA fragments, advancing clinical research and diagnostic test development for several neurodegenerative disorders, such as fragile X syndrome and amyotrophic lateral sclerosis. Despite these advances, the analysis of PCR/CE data from assays targeting STRs remains a manual, burdensome, and subjective process. There is a clear need to create a system that can scale with the development of new assays, and the proposed approach utilizes modern breakthroughs in artificial intelligence to fulfill that need.  This method will leverage recent advances in representation learning to establish a generalized and adaptive framework for automated PCR/CE annotation that can scale to new assays and improve automatically with the inclusion of new data. The project will benefit from Asuragen’s experience in optimizing repeat-primed chemistries to develop and commercialize multiple high performance assays including the AmplideX PCR/CE FMR1 kit. Importantly, the proposed modeling strategy will borrow-strength across multiple established PCR/CE assays and generalize to future PCR/CE assays for novel STR disease associated biomarkers. This system will be paramount to enabling a continuous learning platform wherein computationally-assisted annotation of PCR/CE assays can be continuously improved and integrated in to clinical research tools and diagnostics. Project Narrative  We are developing AmplideX DeepNet, an artificial intelligence-based analysis system that can accurately perform computationally-assisted analysis of molecular diagnostic assays. The proposed system will build upon recent breakthroughs in artificial intelligence to allow it to easily adapt to new assays and to continue to improve. The system will be applied to assays for several disorders, including fragile X syndrome, amyotrophic lateral sclerosis (ALS), myotonic dystrophy, and Huntington’s disease, and will provide a number of benefits over current analysis methods by reducing turn-around time for assay results and assuring reproducible reporting between operators and labs.","AmplideX DeepNet, a new paradigm for deep learning analytical tools in the molecular diagnostic space",9678895,R43GM128498,"['Alleles', 'American', 'Amyotrophic Lateral Sclerosis', 'Artificial Intelligence', 'Automated Annotation', 'Biological Assay', 'Biological Markers', 'C9ORF72', 'Capillary Electrophoresis', 'Chemistry', 'Clinical', 'Clinical Research', 'DNA', 'DNA Sequence', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnostic', 'Diagnostic tests', 'Disease', 'FMR1', 'FMR1 repeat', 'Fragile X Syndrome', 'Future', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Guidelines', 'Hand', 'Hereditary Disease', 'Heritability', 'Huntington Disease', 'Interruption', 'Learning', 'Length', 'Machine Learning', 'Manuals', 'Medical Genetics', 'Methods', 'Modeling', 'Modernization', 'Molecular Analysis', 'Myotonic Dystrophy', 'Neurodegenerative Disorders', 'Nucleotides', 'Pathogenicity', 'Performance', 'Phase', 'Process', 'Quality Control', 'Reagent', 'Reporting', 'Reproducibility', 'Running', 'Sampling', 'Short Tandem Repeat', 'System', 'Systems Analysis', 'Technology', 'Testing', 'Time', 'Training', 'analysis pipeline', 'analytical tool', 'automated analysis', 'base', 'clinical diagnostics', 'cohort', 'computer framework', 'deep learning', 'design', 'diagnostic assay', 'experience', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'heuristics', 'human-in-the-loop', 'improved', 'instrumentation', 'learning progression', 'learning strategy', 'medical schools', 'molecular diagnostics', 'nervous system disorder', 'next generation sequencing', 'novel', 'research and development', 'success', 'tool']",NIGMS,"ASURAGEN, INC.",R43,2019,269217,-0.009117851639047039
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,9887588,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2019,286435,-0.032098615991868924
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,9733308,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2019,388750,-0.006671953262419958
"Training a new generation of computational neuroscientists bridging neurobiology The Training Program in Computational Neuroscience (TPCN) will support integrated undergraduate and graduate training in computational neuroscience at New York University. The program will be hosted by the Center for Neural Science (CNS), with participation of faculty in the Departments of Psychology, Mathematics, and Computer Science, and the Institute of Neuroscience at the School of Medicine. The TPCN will fit well with NYU’s unique strengths and recent developments: (1) NYU is one of a few universities with a critical mass of computational neuroscientists. NYU has had a Sloan-Swartz Center for Theoretical Neuroscience since 1994. In the past three years alone, NYU has hired three computational neuroscientists. (2) CNS established an undergraduate major in neuroscience as early as 1992, and thus has a long track record in undergraduate education, it now has 136 students in the current academic year. (3) Recent faculty hiring in CNS, Psychology, and the School of Medicine has greatly expanded our teaching and research capabilities in the neuroscience of cognitive functions and their impairments associated with mental disorders. (3) As NYU is undertaking a merge of two historically separated neuroscience graduate programs (at CNS and the School of Medicine), this training grant will ensure that computational modeling, which has become indispensible in neuroscience, will be front-and-center in the integrated graduate program. (4) NYU is a major center of Artificial Intelligence and Data Science, with close links to Facebook’s AI Center and the Simons Center for Data Analysis. Our training faculty together with these connections will give our students ample opportunities to acquire machine learning techniques for data analysis and learn about brain-like AI algorithms. The proposed training program will support coherent undergraduate and graduate training in computational neuroscience at NYU. It will have several unique features: (1) Innovative mentorship methods: For example, (a) graduate trainees will mentor undergraduate trainees, (b) faculty will explicitly discuss human factors in academic practice; (c) there will be post-mortems after seminars by outside speakers. (2) Computational psychiatry: We propose new courses and research opportunities that are designed specifically to link cognitive function and the neurobiology of neural circuits. We propose innovative education in the nascent field of Computational Psychiatry, to bring theory and circuit modeling to clinical research in mental health. (3) Broad preparation: We aim to prepare trainees for jobs not only in academia, but also in medical and industry research. To achieve this, we will utilize our strength in machine learning and data science to broaden computational neuroscience training. The Program Directors have complementary strengths and will have complementary roles in the program. Wang will supervise graduate trainees and focus on training in mechanistic/circuit-level side of computational neuroscience as well as computational psychiatry. Ma will supervise undergraduate trainees and focus on the computational/behavioral side. This grant will support training of a new generation of graduate and undergraduate students in  computational neuroscience, which has become increasingly important to meet the challenges of  making discoveries with new data analysis tools and of understanding highly nonlinear complex  neural circuits. A salient component of our program is training in the nascent field  of   Computational  Psychiatry,  bridging  basic  neuroscience  and clinical research on mental  disorders. Therefore, the proposed program has the potential of making  a significant impact on  mental health.",Training a new generation of computational neuroscientists bridging neurobiology,9767751,R90DA043849,"['Academia', 'Algorithms', 'Artificial Intelligence', 'Behavioral', 'Brain', 'Clinical Research', 'Computer Simulation', 'Data Analyses', 'Data Science', 'Development', 'Education', 'Educational process of instructing', 'Ensure', 'Facebook', 'Faculty', 'Generations', 'Grant', 'Human', 'Impairment', 'Industry', 'Institutes', 'Learning', 'Link', 'Machine Learning', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Neurobiology', 'Neurosciences', 'New York', 'Occupations', 'Preparation', 'Psychiatry', 'Psychology', 'Research', 'Role', 'Science', 'Side', 'Students', 'Supervision', 'Teacher Professional Development', 'Techniques', 'Training', 'Training Programs', 'Universities', 'cognitive function', 'cognitive neuroscience', 'computational neuroscience', 'computer science', 'design', 'innovation', 'mathematical sciences', 'medical schools', 'neural circuit', 'programs', 'relating to nervous system', 'theories', 'undergraduate education', 'undergraduate student']",NIDA,NEW YORK UNIVERSITY,R90,2019,204981,-0.0012055586093287541
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9664620,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2019,573396,-0.013578602064109997
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9636581,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2019,489919,-0.005347690771447512
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9803774,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Custom', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2019,631809,-0.016641245648742406
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9724345,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'random forest', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2019,292500,-0.0028787097391207005
"Development of a novel photocatalytic system for direct deoxyfunctionalization of alcohols involving machine learning Project Summary Development of general and efficient methods for functionalization of alcohols is highly warranted due to the ubiquity and prominence of this functional group in natural products. Such methods would allow for late-stage diversification of complex molecules and, consequently, could have a broad impact in natural product synthesis and preparation of relevant pharmaceutical materials. However, owing to the chemical inertness of alcohols, most methods typically require installation of activating groups for functionalization, making them unattractive from an atom- and step-economical perspective. Nonetheless, many advances have been made. In particular, the Barton-McCombie reaction has become an indispensable tool for reductive functionalization of alcohols. Unfortunately, this transformation requires pre- functionalization of the alcohol substrate, employs highly toxic tin reagents, and invokes the use high reaction temperatures or harmful UV light for initiation of radical intermediates. Furthermore, the overall transformation is limited to H-atom incorporation or reductive coupling with alkenes. Lastly, only a few deoxygenation methods exist that are amenable for late-stage and site-selective deoxygenation in complex systems. Moreover, physical organic chemistry tools available to facilitate the selection of a set of conditions or parameters to afford site-selectivity are limited. In this proposal, we will develop a mild and practical photocatalytic deoxygenation of alcohols. Our strategy will focus on solving the inherent limitation of the Barton McCombie reaction by 1) avoiding the use of toxic tin reagents, 2) obviating the need for pre-functionalization of the alcohol substrate, and 3) allowing for modular coupling of formed alkyl radicals via Ni-catalysis. Specific aim 1 explores the development of a novel photoredox-catalyzed deoxygenation of alcohols. In addition, we outline a general protocol for deoxyfunctionalization of alcohols via inception of the alkyl radical intermediate, formed via β-scission, with various radical electrophiles. Moreover, we highlight an innovative method for the direct cross-coupling of alcohols via metallophotoredox catalysis in both racemic and enantioselective fashion. Specific aim 2 addresses the design strategy for implementing physical chemistry techniques such as Machine Learning in order to facilitate optimization and prediction of reaction performance in multi-dimensional chemical space. Also, we outline applying this strategy to identify a set of optimal conditions to confer site-selective functionalization in complex polyols. Project Narrative Mild and site-controlled deoxygenation of alcohols could significantly accelerate the late-stage synthesis/diversification of important organic molecules; however, current methods often employ toxic tin reagents, harsh reaction conditions, and require prefunctionalization of the alcohols employed. The strategy proposed would allow for a mild photocatalytic deoxygenation, as well as deoxyfunctionalization, of alcohols that solves the aforementioned limitations of prior art. Moreover, the proposed strategy outlines implementation of physical organic chemistry tools Machine Learning in order to facilitate optimization and prediction of reaction performance in multi-dimensional chemical space.",Development of a novel photocatalytic system for direct deoxyfunctionalization of alcohols involving machine learning,9759306,F32GM129910,"['Address', 'Alcohol consumption', 'Alcohols', 'Alkenes', 'Arts', 'Catalysis', 'Chemicals', 'Complex', 'Coupling', 'Development', 'Employment', 'Intercept', 'Machine Learning', 'Methods', 'Natural Products', 'Organic Chemistry', 'Organic Synthesis', 'Performance', 'Pharmacologic Substance', 'Phosphines', 'Physical Chemistry', 'Preparation', 'Protocols documentation', 'Reaction', 'Reagent', 'Site', 'System', 'Techniques', 'Temperature', 'Tin', 'Ultraviolet Rays', 'Visible Radiation', 'alcohol involvement', 'catalyst', 'design', 'functional group', 'innovation', 'novel', 'polyol', 'predictive tools', 'tool']",NIGMS,PRINCETON UNIVERSITY,F32,2019,61226,-0.03194663480516702
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9753295,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Machine Learning', 'Marines', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'contig', 'dark matter', 'deep learning', 'deep learning algorithm', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,279949,0.004751162392444918
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,9885663,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Traction', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'success', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2019,206139,-0.021230589961812065
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9610628,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Infrastructure', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multi-task learning', 'multitask', 'new technology', 'novel', 'pandemic disease', 'predictive modeling', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2019,224899,-0.01089269448222495
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9772541,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,264255,-0.016683818160472035
"Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations Q-Chem is a state-of-the-art commercial computational quantum chemistry program that has aided about 60,000 users in their modeling of molecular processes in a wide range of disciplines, including biology, chemistry, and materials science. In this proposal, we seek to significantly reduce the computational time (now around 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions. Specifically, we propose to use a multiple time step (MTS) simulation method, where a low-level (and less accurate) quantum chemistry method is used to propagate the system (i.e. move all atoms) at each time step (usually 0.5 or 1 fs), and then a high-level (i.e. more accurate and expensive) quantum chemistry method is used to correct the force on the atoms at longer time intervals. In this way, the simulation can be performed at the high-level energy surface in a fraction of time, compared with simulations performed only using the high-level quantum chemical method. In the Phase I proposal, our goal is to allow the high-level force update only once every 40—50 fs by identifying appropriate lower-level theories (Aim 1) and incorporating machine-learning techniques (Aim 2). This will accelerate accurate free energy simulations by 20—25 fold, reducing the overall computer time to around 25,000 CPU hours. Thus, our new MTS simulation method will make it feasible to routinely perform computational studies on enzymatic reaction mechanism. The addition of these new tools will also further strengthen Q-Chem's position as a global leader in the molecular modeling software market, making our program the most efficient and reliable computational quantum chemistry package for simulating large, complex chemical/biological systems. In this project, we seek to significantly reduce the computational time (ca. 500,000 CPU hours) required to obtain accurate free energy profiles of enzymatic reactions to ca. 25,000 CPU Hours. Building upon sophisticated quantum mechanics, this can lead to reliable and quick predictions of enzyme activities.",Multiscale ab initio QM/MM and machine learning methods for accelerated free energy simulations,9778517,R43GM133270,"['Acceleration', 'Accounting', 'Adopted', 'Back', 'Biochemical', 'Biochemical Reaction', 'Biology', 'Biomedical Research', 'Chemicals', 'Chemistry', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Development', 'Discipline', 'Enzymes', 'Foundations', 'Free Energy', 'Goals', 'Hour', 'Hybrids', 'Lead', 'Machine Learning', 'Maps', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular Conformation', 'Molecular Machines', 'Pathway interactions', 'Performance', 'Phase', 'Positioning Attribute', 'Potential Energy', 'Process', 'Protein Conformation', 'Proteins', 'Quantum Mechanics', 'Reaction', 'Recipe', 'Research', 'Research Personnel', 'Sampling', 'Scheme', 'Solvents', 'Surface', 'System', 'Techniques', 'Time', 'Update', 'biological systems', 'computer studies', 'cost', 'density', 'enzyme activity', 'enzyme model', 'improved', 'innovation', 'learning strategy', 'materials science', 'molecular mechanics', 'molecular modeling', 'programs', 'quantum', 'quantum chemistry', 'quantum computing', 'simulation', 'theories', 'time interval', 'tool']",NIGMS,"Q-CHEM, INC.",R43,2019,132011,-0.004786560486217169
"Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics The human microbiota plays an important role in health and disease, and its therapeutic manipulation is being actively investigated for a wide range of diseases that span every NIH institute. Our microbiota are inherently dynamic, and analyzing these time-dependent properties is key to robustly linking the microbiota to disease, and predicting the effects of therapies targeting the microbiota; indeed, longitudinal microbiome data is being acquired with increasing frequency, and is a major component of many NIH-funded projects. However, there is currently a dearth of computational tools for analyzing microbiome time-series data, which presents several special challenges including high measurement noise, irregular and sparse temporal sampling, and complex dependencies between variables. The objective of this proposal is to introduce new capabilities, improve on, and provide state-of-the-art implementations of tools for analyzing dynamics, or patterns of change in microbiome time-series data. The tools we develop will use Bayesian machine learning methods, which are well-recognized for their strong conceptual and practical advantages, particularly in biomedical domains. Tools will be rigorously tested and validated on synthetic and real human microbiome data, including publicly available datasets and those from collaborators providing 16S rRNA sequencing, metagenomic, and metabolomics data. We propose three specific aims. For Aim 1, we will develop integrated Bayesian machine learning tools for predicting population dynamics of the microbiome and its responses to perturbations. These tools will include a new model that simultaneously learns groups of microbes with similar interaction structure and predicts their behavior over time, and that incorporates prior phylogenetic information. The model will be further improved by incorporating stochastic microbial dynamics and errors in measurements throughout the model. For Aim 2, we will develop Bayesian machine learning tools to predict host status from microbiome dynamics. The tools will learn easily interpretable, human-readable rules that predict host status from microbiome time-series data, and will be further extended to handle a variety of longitudinal study designs. For Aim 3, we will engineer our microbiome dynamics analysis software tools for optimal performance, ease-of- use, maintainability, extensibility, and dissemination to the community. In total, the proposed work will yield a suite of contemporary software tools for analyzing microbiome dynamics, with expected broad use and major impact. The software will allow investigators to answer important scientific and translational questions about the microbiome, including discovering which microbial taxa or their metagenomes are affected over time by perturbations such as changes in diet or invasion by pathogens; predicting the effects of these perturbations over time, including changes in composition or stability of the gut microbiota; and finding temporal signatures in multi-‘omic microbiome data that predict disease risk in the human host. The human microbiota, or collection of micro-organisms living on and within us, plays an important role in health, and when disrupted or abnormal, may contribute to many types of diseases including infections, kidney diseases, bowel diseases, diabetes, heart diseases, arthritis, allergies, brain diseases, and cancer. Sophisticated computer-based tools are needed to make sense of human microbiota data, particularly time- series data, which can yield important insights into how our microbiomes change over time. This work will develop new and improved computer-based tools for analyzing microbiota time-series data, which will be made freely available and will enable scientists to increase our fundamental knowledge about how our microbiota affect us and ultimately to apply this knowledge to prevent and treat human illnesses.",Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics,9787546,R01GM130777,"['16S ribosomal RNA sequencing', 'Affect', 'Algorithms', 'Antibiotics', 'Arthritis', 'Autoimmunity', 'Bayesian learning', 'Behavior', 'Biological Markers', 'Biological Models', 'Brain Diseases', 'Cardiovascular Diseases', 'Childhood', 'Clostridium difficile', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Dependence', 'Diabetes Mellitus', 'Diet', 'Disease', 'Engineering', 'Environmental Exposure', 'Frequencies', 'Funding', 'Health', 'Heart Diseases', 'Human', 'Human Microbiome', 'Hypersensitivity', 'Infection', 'Institutes', 'Intervention', 'Intestines', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Learning', 'Link', 'Longitudinal Studies', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Metagenomics', 'Microbe', 'Modeling', 'Names', 'Noise', 'Oligosaccharides', 'Outcome', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Play', 'Population Dynamics', 'Property', 'Pythons', 'Readability', 'Recurrence', 'Research Design', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Series', 'Shotguns', 'Software Engineering', 'Software Tools', 'Speed', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Work', 'base', 'computerized tools', 'design', 'disorder risk', 'dynamic system', 'gut microbiota', 'human data', 'human microbiota', 'human subject', 'improved', 'insight', 'learning algorithm', 'learning strategy', 'man', 'metabolomics', 'metagenome', 'microbial', 'microbiome', 'microbiome analysis', 'microbiome sequencing', 'microbiota', 'microorganism', 'nervous system disorder', 'novel', 'open source', 'pathogen', 'predictive tools', 'prevent', 'response', 'software development', 'targeted treatment', 'tool']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R01,2019,312939,0.007627504950318887
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,-0.010273784864385424
"Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization Project Summary  This application is being submitted in response to NOT-RM-19-009 as a supplement to the parent award U54NS093793.  The Common Fund supports a number of resources that can significantly enhance gene and variant prioritization for study in the Model Organisms Screening Center of the Undiagnosed Diseases Network and beyond. To facilitate the use of these resources, we propose to create a tool that can be easily accessed by clinical geneticists and model organism scientists alike.  MARRVEL (Model organism Aggregated Resources for Rare Variant ExpLoration) was created two years ago because important data that is necessary for rare variant analysis for personalized medicine is spread throughout the internet in tens of different locations. To improve efficiency and streamline access to these data sources, we created a web-tool that allows users to query tens of data sources at once, including GTEx, and links to IMPC, the display portal for KOMP2.  In this proposal, our goal is to develop version 2 of MARRVEL to promote the use of Common Fund resources in the rare disease research community for manual and automated data analysis. This goal will be accomplished by developing MARRVEL 2.0 by integrating KOMP2 (IMPC) and PHAROS data and using the aggregated dataset to develop a machine-assisted gene and variant prioritization for diagnosis and animal model generation.  Our goals align with those of the NIH Common Fund to increase the utility of resources for broader use in the biomedical community. Project Narrative  We aim to promote the use of Common Fund resources and facilitate the diagnosis of rare diseases and the subsequent generation of animal models for the Undiagnosed Diseases Network and beyond. This goal will be accomplished by developing the web resource, MARRVEL 2.0.",Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization,9984757,U54NS093793,"['Affect', 'Animal Model', 'Artificial Intelligence', 'Award', 'Clinical', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Data Sources', 'Development', 'Diagnosis', 'Discipline', 'Disease', 'Disease model', 'Drosophila genus', 'Drug Targeting', 'Expert Systems', 'Family', 'Funding', 'Generations', 'Genes', 'Genetic Diseases', 'Genotype-Tissue Expression Project', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genetics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Manuals', 'Medical', 'Medical Genetics', 'Modeling', 'Mus', 'Parents', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Proteins', 'Rare Diseases', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Suggestion', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Time', 'Training', 'United States National Institutes of Health', 'Variant', 'Visit', 'Yeasts', 'Zebrafish', 'base', 'data wrangling', 'design', 'experimental study', 'feeding', 'fly', 'genetic disorder diagnosis', 'genetic variant', 'human data', 'improved', 'interest', 'learning community', 'machine learning algorithm', 'model organisms databases', 'online resource', 'personalized medicine', 'phenotypic data', 'rare genetic disorder', 'rare variant', 'response', 'screening', 'supervised learning', 'tool', 'web-based tool']",NINDS,BAYLOR COLLEGE OF MEDICINE,U54,2019,320000,-0.01319325638338078
"Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach Project Summary Biology is full of stunning examples of emergent behaviors – behaviors that arise from, but cannot be reduced to, the interactions of the constituent parts that make up the system under consideration. These behaviors span the full spectrum of length scales, from the emergence of distinct cell fates (e.g. neurons, muscle, etc.) due to the interactions of genes within cells, to the formation of complex ecological communities arising from the interactions of thousands of species. The overarching goal of my research is to develop new conceptual, theoretical, and computational tools to model such emergent, system-level behaviors in biology. To do so, we utilize an interdisciplinary approach that is grounded in Biological Physics, but draws heavily from Machine Learning, Information Theory, and Theoretical Ecology. Our work is unified and distinguished by our deep commitment to integrating theory with the vast amount of biological data now being generated by modern DNA sequencing-based techniques and quantitative microscopy. An important goal of the proposed research is to find common concepts and tools that transcend traditional biological sub-disciplines and models systems. The proposed research pursues four distinct but conceptually interrelated research directions: (1) understanding how distinct cell fates emerge from bimolecular interactions within mammalian cells (2) investigating how bimolecular networks within cells exploit energy consumption to improve computations, with applications to Synthetic Biology; (3) identifying the ecological principles governing community assembly in microbial communities and developing techniques for synthetically engineering ecological communities; and (4) developing new machine learning algorithms and techniques for biological data analysis. In addition to developing physics-based models for diverse biological phenomena, the proposed research will yield a series of practical important tools and algorithms which we will make publically available including: (1) a new linear-algebra based algorithm for assessing the fidelity of directed differentiation and cellular reprogramming protocols and visualizing reprogramming/differentiation dynamics and (2) improved algorithms for inferring microbial interactions in the human microbiome from high-throughput sequence data. These computational tools will allow scientists to realize the immense therapeutic potential of cellular reprogramming and microbial ecology-based techniques for studying and treating human disease. Project Narrative This project will develop new theoretical and computational tools that will allow scientists to realize the immense therapeutic potential of cellular reprogramming (the conversion of one cell type into another cell type) and microbial ecology-based techniques for studying and treating human diseases.",Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach,9745646,R35GM119461,"['Algorithms', 'Behavior', 'Biological', 'Biological Models', 'Biological Phenomena', 'Biology', 'Cells', 'Communities', 'Complex', 'Consumption', 'DNA sequencing', 'Data', 'Data Analyses', 'Discipline', 'Ecology', 'Engineering', 'Goals', 'Human Microbiome', 'Information Theory', 'Length', 'Linear Algebra', 'Machine Learning', 'Mammalian Cell', 'Modeling', 'Modernization', 'Muscle', 'Neurons', 'Physics', 'Protocols documentation', 'Quantitative Microscopy', 'Research', 'Scientist', 'Series', 'System', 'Systems Biology', 'Techniques', 'Therapeutic', 'Transcend', 'Work', 'base', 'cell type', 'computerized tools', 'gene interaction', 'human disease', 'improved', 'interdisciplinary approach', 'machine learning algorithm', 'microbial', 'microbial community', 'microorganism interaction', 'synthetic biology', 'theories', 'tool']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R35,2019,323318,-0.031055592645089218
"Accelerating phage evolution and tools via synthetic biology and machine learning Summary Phages, which are the naturally evolved predators of bacteria, may hold the key to combating bacterial pathogens, including the looming threat of multidrug resistant bacteria. Phages are viruses which while harmless to humans and have been successfully engineered as tools to separate, concentrate, and detect their bacterial hosts. Additionally, phages have been used as therapeutic agents to treat patients infected with pathogens resistant to known antibiotics. While the potential benefits of phages are numerous, certain limitations must be addressed in order to fully employ them. The central hypothesis of this proposal is that both top-down and bottom-up approaches can be utilized to design and synthesize novel phages, through a combination of synthetic biology and machine learning. This will result in phage-based tools with increased functionality and customizable host ranges. The rationale for the proposed research is that as the threat of bacterial infections including those with multi-drug resistance continues to grow, phages, which have evolved to efficiently recognize and kill bacteria, will become indispensable tools. Therefore, the ability to rapidly design and engineer new phages for biosensing and therapeutics will be a critical advantage to human health. The proposal contains three specific aims which are supported by preliminary data and cited literature. Aim 1: Site-directed conjugation for advanced phage-based biosensors and therapeutics. Under this aim, phages will be modified with alkyne-containing unnatural amino acids allowing their direct conjugation to 1) azide decorated magnetic nanoparticles, and 2) azide terminated polyethylene glycol. The modifications will allow the development of magnetic phages for bacteria separation and detection, and phages that are more effective therapeutics due to their ability to avoid a patient’s innate immune response, respectively. Aim 2: Decoding phage biorecognition elements using machine learning. In this aim, machine learning will be used to model the binding of phages and their bacterial hosts. The model will enable the prediction of host interactions as well as allow the design and synthesis of novel phage tail fibers which can target specific bacterial isolates. Aim 3: Repurposing phage biorecognition for a broader host ranges. Under the final aim, phage-binding proteins will be replaced with those known to recognize conserved regions of the bacterial LPS, resulting in a phage with a much broader host range. This approach is innovative because it uses top-down characterizations for bottom-up design and synthesis of novel phages. Traditional phage screening methods will be replaced with the rapid synthesis of phages, which are optimized for a particular bacterial isolate. Following the successful completion of the specific aims, the expected outcome is the design and synthesis of phages that can be used to target a selected group of bacteria within Enterobacteriaceae for advanced biosensing and therapeutics. A publically available computer model will allow rapid design of custom phage biorecognition elements which can be added to functionalized phages. These technologies will allow researchers to tip the scales of the co-evolutionary arms race between phage and bacteria. Narrative The project is relevant to public health because it accelerates the development of phage-based tools for the rapid detection of bacterial pathogens in human, food, and environmental samples, and the treatment of diseases from multidrug resistant bacteria by integrating machine learning and synthetic biology. Thus, it is specifically relevant to part of NIH's mission that pertains to the diagnosis, prevention, and cure of human diseases.",Accelerating phage evolution and tools via synthetic biology and machine learning,9714883,R01EB027895,"['Acinetobacter baumannii', 'Address', 'Alkynes', 'Amino Acid Sequence', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Azides', 'Bacteria', 'Bacterial Genome', 'Bacterial Infections', 'Bacteriophage T4', 'Bacteriophages', 'Binding', 'Binding Proteins', 'Biosensing Techniques', 'Biosensor', 'CRISPR/Cas technology', 'Capsid', 'Chemistry', 'Clinical', 'Computer Simulation', 'Consumption', 'Custom', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Drug resistance', 'Elements', 'Engineering', 'Enterobacteriaceae', 'Environment', 'Escherichia coli', 'Evolution', 'Family', 'Fiber', 'Food', 'Future', 'Genes', 'Genome', 'Goals', 'Health', 'Human', 'Infection', 'Innate Immune Response', 'Innate Immune System', 'Intervention', 'Life', 'Literature', 'Machine Learning', 'Magnetic nanoparticles', 'Magnetism', 'Methods', 'Mission', 'Modeling', 'Modification', 'Multi-Drug Resistance', 'Multidrug-resistant Acinetobacter', 'Multiple Bacterial Drug Resistance', 'Natural Immunity', 'Outcome', 'Patients', 'Phenotype', 'Polyethylene Glycols', 'Prevention', 'Process', 'Property', 'Public Health', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resistance', 'Sampling', 'Site', 'Specificity', 'Surface', 'System', 'Tail', 'Technology', 'Therapeutic', 'Therapeutic Agents', 'Time', 'Training', 'Treatment Efficacy', 'United States National Institutes of Health', 'Viral', 'Virus', 'arm', 'base', 'combat', 'design', 'human disease', 'innovation', 'next generation', 'novel', 'pathogen', 'pathogenic bacteria', 'rapid detection', 'receptor', 'resistance mechanism', 'screening', 'synthetic biology', 'tool', 'unnatural amino acids']",NIBIB,CORNELL UNIVERSITY,R01,2019,666637,-0.006118430229322919
"Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions PROJECT SUMMARY The research interests of my group are rooted in explorations of new and useful conceptual models to improve the control and prediction of noncovalent interactions. Our research involves the use of a variety of computational quantum chemical tools, applications of density functional theory (DFT), cheminformatics, and machine-learning methods. A premise of our research is that aromaticity may be used to modulate many types of noncovalent interactions (such as hydrogen bonding, π-stacking, anion-π interactions). The reciprocal relationship we find, between “aromaticity” in molecules and the strengths of “noncovalent interactions,” is surprising especially since they are typically considered as largely separate ideas in chemistry. The innovation of this research is that it will enable use of intuitive “back-of-the-envelope” electron-counting rules (such as the 4n+2πe Hückel rule for aromaticity) to make predictions of experimental outcomes regarding the impact of noncovalent interactions. A five-year goal is to realize the use of our conceptual models in real synthetic examples prepared by our experimental collaborators. My research vision is to bridge discoveries of innovative concepts to their practical impacts for biomedical and biomolecular research. PROJECT NARRATIVE This research proposal includes four projects that are jointly motivated by the challenge to control and predict noncovalent interactions in organic and biomolecular systems. The proposed work involves applications of a variety of computational quantum chemical tools and synergistic investigations with experimental collaborators. We seek to identify new and useful concepts to guide experimental designs of novel “non-natural” molecular systems (e.g., receptors, biosensors, and hydrogels) that have potential biomedical applications.",Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions,9798401,R35GM133548,"['Anions', 'Back', 'Biosensor', 'Chemicals', 'Chemistry', 'Electrons', 'Experimental Designs', 'Goals', 'Hydrogels', 'Hydrogen Bonding', 'Intuition', 'Investigation', 'Machine Learning', 'Modeling', 'Molecular', 'Outcome', 'Plant Roots', 'Research', 'Research Project Summaries', 'Research Proposals', 'System', 'Vision', 'Work', 'cheminformatics', 'density', 'improved', 'innovation', 'interest', 'learning strategy', 'novel', 'quantum computing', 'receptor', 'theories', 'tool']",NIGMS,UNIVERSITY OF HOUSTON,R35,2019,377200,-0.011886449679356216
"Image-guided robot for high-throughput microinjection of Drosophila embryos PROJECT SUMMARY This proposal is submitted in response to the NIH Development of Animal Models and Related Biological Materials for Research (R21) program. The proposal develops an image-guided robotic platform that performs the automated delivery of molecular genetic tools and non-genetically encoded reagents such as chemical libraries, fluorescent dyes to monitor cellular processes, functionalized magnetic beads, or nanoparticles into thousands of Drosophila embryos in a single experimental session. The proposed work builds on recent engineering innovations in our collaborative group which has developed image-guided robotic systems that can precisely interface with single cells in intact tissue. The two Specific Aims provide for a systematic development of the proposed technologies. AIM 1 first engineers a robotic platform (‘Autoinjector’) that can scan and image Drosophila embryos in arrays of egg laying plates. We will utilize machine learning algorithms for automated detection of embryos, followed by thresholding and morphology analysis to detect embryo centroids and annotate injection sites. In AIM 2, we will utilize microprocessor-controlled fluidic circuits for programmatic delivery of femtoliter to nanoliter volumes of reagents into individual embryos. We will quantify the efficacy of the Autoinjector by comparing the survival, fertility, and transformation rates of transposon or PhiC31-mediated transgenesis to manual microinjection datasets. Finally, we will demonstrate the efficient delivery of sgRNAs and mutagenesis in the presence of Cas9. This project fits very well within the goals of the program by engineering a novel tool for producing and improving animal models. The Autoinjector will accelerate Drosophila research and empower scientists to perform novel experiments and genome-scale functional genomics screens that are currently too inefficient or labor intensive to be conducted on a large scale and may additionally enable other novel future applications. PROJECT NARRATIVE This proposal develops a technology platform that will enable automated microinjection of molecular genetic tools and non-genetically encoded tools such as chemical libraries, fluorescent dyes, functionalized magnetic beads, or nanoparticles, into thousands of Drosophila embryos in a single experimental session. The successful development of this technology will empower Drosophila biologists to perform screens and develop new applications that are currently too inefficient or labor intensive to contemplate and will accelerate research into the function of the nervous system and the molecular and genetic underpinnings of numerous diseases in this important animal model.",Image-guided robot for high-throughput microinjection of Drosophila embryos,9806367,R21OD028214,"['Animal Model', 'Biocompatible Materials', 'Biological Assay', 'Caliber', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Collection', 'Computer Vision Systems', 'Cryopreservation', 'Data Set', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Drosophila melanogaster', 'Embryo', 'Engineering', 'Expenditure', 'Exploratory/Developmental Grant', 'Fertility', 'Fluorescent Dyes', 'Future', 'Gene Transfer Techniques', 'Genetic', 'Goals', 'Guide RNA', 'Image', 'Individual', 'Injections', 'Investigation', 'Laboratories', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Mediating', 'Methods', 'Microinjections', 'Microprocessor', 'Microscope', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Morphology', 'Motivation', 'Mutagenesis', 'Needles', 'Nervous System Physiology', 'Performance', 'Process', 'Reagent', 'Research', 'Resources', 'Robot', 'Robotics', 'Scanning', 'Scientist', 'Signaling Molecule', 'Site', 'Space Perception', 'System', 'Technology', 'Tissues', 'Transgenes', 'Transgenic Organisms', 'United States National Institutes of Health', 'Work', 'animal model development', 'base', 'biological research', 'cost', 'egg', 'experience', 'experimental study', 'functional genomics', 'gene product', 'genetic manipulation', 'genome-wide', 'image guided', 'improved', 'innovation', 'machine learning algorithm', 'magnetic beads', 'mutant', 'mutation screening', 'nanolitre', 'nanoparticle', 'novel', 'novel strategies', 'programs', 'response', 'robotic system', 'screening', 'small molecule libraries', 'stem', 'technology development', 'tool']",OD,UNIVERSITY OF MINNESOTA,R21,2019,184118,-0.01568456585551068
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9671422,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Ingestion', 'Kinetics', 'Laboratories', 'Locomotion', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'automated image analysis', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'machine learning algorithm', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2019,475637,-0.02272012899282942
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9702053,R21GM128020,"['Address', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'learning algorithm', 'machine learning algorithm', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'preservation', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2019,206250,-0.005312963130907486
"Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry PROJECT SUMMARY  Cellular interactions with the environment form the basis of health and disease for all organisms. Exposure to nutrients, toxins, and neighboring cells trigger coordinated molecular responses that impact cell function and metabolism in a beneficial, adaptive, or detrimental manner. Although the benefits of multicellularity for the formation of complex tissue structures or the function of entire organ systems has been long appreciated, it has only recently been understood that microbial inhabitants of vertebrates also have a tremendous impact on host cell function and dysfunction. Despite this, an understanding of these interactions has not moved beyond simple associations, and there are virtually no molecular technologies available that adequately define how a complex microbial ecosystem impacts host cell function, or how the host response to microbial colonization affects the bacterial community. This gap in knowledge is striking when one considers the broad and significant impact that microbes have on human health. In this application, we propose to expressly fill this knowledge gap through development of a novel multimodal imaging pipeline that will provide 3-dimensional information on the molecular heterogeneity of microbial communities and the immune response at the host-pathogen interface.  This proposal combines our expertise in immunology, infection biology, mass spectrometry, small animal imaging, machine learning, and computer vision to develop an integrated multimodal visualization method for studying infectious disease. Our unique approach will computationally combine ultra-high speed (~50px/s) MALDI-TOF images, ultra-high mass resolution (>200,000 resolving power) MALDI FTICR IMS, metal imaging by LA-ICP-IMS, high-spatial resolution optical microscopy, and MR imaging using data-driven image fusion. This strategy will enable 3-D molecular images to be generated for thousands of elements, metabolites, lipids, and proteins with an unprecedented combination of chemical specificity and spatial fidelity more than 50x faster than is currently possible. We will use this next-generation imaging capability to (i) define the heterogeneous microbial subpopulations throughout the 3-D volume of a S. aureus community, (ii) uncover the host molecules that form the abscess and accumulate to restrict microbial growth in murine models, and (iii) elucidate molecular markers that differentiate in vivo biofilms at the host-pathogen interface, between abscesses at various stages of progression, and under distinct degrees of nutrient stress. These studies will uncover new targets for therapeutic intervention and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research. PROJECT NARRATIVE This proposal will enable detailed views of the molecular components of infectious disease with unprecedented resolution through the development of a multimodal, 3-dimensional imaging platform. The proposed technologies will improve throughput and molecular specificity, enable automated high-precision and high-accuracy image alignment, and allow for descriptions of molecular signals in 3-D through the fusion of multi-modal imaging data. These studies will uncover targets for therapeutic intervention and antibiotic development and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research.",Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry,9788239,R01AI138581,"['3-Dimensional', 'Abscess', 'Affect', 'Animal Model', 'Animals', 'Anterior nares', 'Antibiotics', 'Antibodies', 'Architecture', 'Awareness', 'Bacteria', 'Bacterial Infections', 'Bacterial Proteins', 'Behavior', 'Biology', 'Biomedical Research', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Cellular Metabolic Process', 'Chemicals', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Custom', 'Data', 'Development', 'Diagnosis', 'Differentiation Antigens', 'Dimensions', 'Disease', 'Ecosystem', 'Elements', 'Environment', 'Exposure to', 'Fourier transform ion cyclotron resonance', 'Functional disorder', 'Genus staphylococcus', 'Glean', 'Growth', 'Health', 'Health Promotion', 'Heterogeneity', 'Histology', 'Human', 'Image', 'Imagery', 'Imaging technology', 'Immune', 'Immune response', 'Immunology', 'Imprisonment', 'Individual', 'Infection', 'Infectious Diseases Research', 'Integration Host Factors', 'Knowledge', 'Label', 'Lesion', 'Lipids', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mass Spectrum Analysis', 'Metals', 'Methodology', 'Methods', 'Microbe', 'Microbial Biofilms', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nutrient', 'Optics', 'Organism', 'Pathogenesis', 'Physiological', 'Population', 'Process', 'Proteins', 'Reagent', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Spatial Distribution', 'Specificity', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Speed', 'Staphylococcus aureus', 'Stress', 'Structure', 'Techniques', 'Technology', 'Therapeutic Intervention', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Tissues', 'Toxin', 'Vertebrates', 'Work', 'animal imaging', 'bacterial community', 'base', 'body system', 'commensal bacteria', 'experimental study', 'host colonization', 'imaging capabilities', 'imaging detection', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'innovation', 'interest', 'microbial', 'microbial colonization', 'microbial community', 'microscopic imaging', 'molecular imaging', 'molecular marker', 'mouse model', 'multimodality', 'neutrophil', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'protein expression', 'response', 'supervised learning', 'targeted treatment', 'virtual']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2019,562232,-0.023991320238286696
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9642618,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Effectiveness', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Imagery', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Universities', 'Virginia', 'absorption', 'artificial neural network', 'base', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2019,375602,-0.01651863740108497
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,9859232,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Dimensions', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'One-Step dentin bonding system', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2019,345016,-0.010604769574916632
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9773141,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2019,360843,-0.020803768594724898
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9750722,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Disease model', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Mathematics', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'experimental study', 'improved', 'infancy', 'insight', 'network models', 'novel', 'predictive modeling', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2019,527585,0.0027895398476973964
"A three-population three-scale social network model to assess disease dispersion DESCRIPTION (provided by applicant): Communicable diseases, such as influenza, are transmitted from individual to individual following a network of contacts in a population. Reports on recent outbreaks, such as SARS, the Bird flu, and the H1N1 flu, have repeatedly stressed the critical role of contact networks. We propose an innovative Three-population and Three-scale Social Network (3p3sNet) model to simulating the spatial and temporal dispersion of influenza in a metropolitan population in Buffalo, NY. The 3p3sNet aims to construct a realistic contact network by representing interacting and mobile behaviors of individuals at three scales and three types of places. These involve individual (microscopic) -> local network (mesoscopic) -> population (macroscopic) as nighttime population at homes, daytime population at workplaces, and pastime population at service places. Through this network, diseases disperse from infectious individuals to their local networks then to the population-wide network in a complex dynamic fashion. Modeling the disease dispersion through this network provides invaluable insights in who might be at risk, where and when this risk might occur, and with whom these at-risk individuals might be in contact. These insights lay the foundation of developing spatially and temporally sensitive intervention strategies targeted towards the most vulnerable individuals and social groups. Furthermore, the 3p3sNet can be applied in modeling the epidemiology of any disease where human contacts play a critical role.  In implementing 3p3sNet, we propose to use mobile phone data to extract the individual interaction and travel behaviors. We embrace recent developments in economics, geostatistics, econometrics, and machine learning to construct the network. We develop an innovative co-kriging approach to expanding local households to population-wide households and a novel distance-based GEV discrete choice model to link homes to workplaces and service places. It is anticipated that the assemblage of these advanced methods will enable new capabilities and bring transformative improvements in health-related studies in metropolitan areas. We will conduct a data-rich validation process for the three constructed populations, the links between them, and the simulated disease dispersion through the population. A comprehensive range of independent datasets will be used to support the proposed validation. These involve high-resolution population, workplace, and service place data, surveys of individual interaction and travel behavior, and reports on influenza infections.  The multidisciplinary team comprises world-renown leaders and scholars in epidemiology, agent-based and social network modeling, human mobility analysis, geographical information science, and machine learning. The proposed project represents emerging frontiers in the modeling of communicable diseases and will redefine the capabilities of epidemiological models. PUBLIC HEALTH RELEVANCE: This project addresses two issues of central importance to successfully capturing the complex, spatial and temporal dispersion process of a communicable disease through a population. They are: how to represent individuals as heterogeneous, mobile, and interacting and how to model the disease dispersion process from infectious individuals to their local networks then to the population-wide network. To address these issues, this project proposes to: (1) use mobile phone data to construct a three-population and three-scale social network (3p3sNet); (2) simulate the spatially and temporally dynamic dispersion of influenza through an urban population in Buffalo, NY; and (3) conduct an intensive, data-rich validation process on the simulated influenza dispersion.",A three-population three-scale social network model to assess disease dispersion,9656130,R01GM108731,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Behavior', 'Buffaloes', 'Car Phone', 'Communicable Diseases', 'Complex', 'Dangerousness', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Disease Outbreaks', 'Disease model', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Foundations', 'Geography', 'Health', 'Home environment', 'Household', 'Human', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Information Sciences', 'Intervention', 'Link', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Modeling', 'Outcome', 'Play', 'Population', 'Process', 'Records', 'Reporting', 'Resolution', 'Risk', 'Role', 'Sampling', 'Services', 'Severe Acute Respiratory Syndrome', 'Social Network', 'Stress', 'Surveys', 'Testing', 'Time', 'Travel', 'Urban Population', 'Validation', 'Workplace', 'base', 'disease transmission', 'econometrics', 'epidemiological model', 'flu', 'frontier', 'fundamental research', 'human disease', 'innovation', 'insight', 'metropolitan', 'multidisciplinary', 'network models', 'novel', 'pandemic disease', 'public health relevance', 'social group', 'transmission process']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2019,477166,-0.012396095687758504
"Quantifying causality for neuroscience Abstract: Causality is central to neuroscience. For example, we might ask about the causal effect of a neuron on another neuron, or its influence on perception, action, or cognition. Moreover, any medical approaches aim at producing a causal effect – effecting improvements for patients. Randomized controlled trials (RCTs) are the gold standard to establish causality, but they are not always practical. For example, while we can electrically or optogenetically activate entire areas, large-scale targeted stimulation of individual neurons is hard. Other ways of establishing causality are problematic: if we observe a correlation it is hard to know its cause. The problem is confounding: there are variables that we do not record that affect the variables we do. This also renders model comparisons problematic – a causally wrong model with few parameters may well fit the observed data better than a causally correct one with many parameters. We thus need data analysis tools that allow authoritatively asking causal questions without the need for random perturbation experiments.  Just like neuroscience now, the field of econometrics once focused on correlations. But since the 1980s, empirical economics has undergone a so-called credibility revolution, requiring the development of rigorous methods to establish causality. Several successful methods have emerged to become the workhorses of empirical economics. The idea underlying these methods is that if one can observe variables that approximate random perturbations, then one can still discover causal relations. This is what economists call a quasi-experiment. We here propose to carry over such quasi-experimental techniques to neuroscience. For example in neuroscience, if there is a random variable that affects only one neuron, then any activity in other neurons correlated with that variable must be causally affected by the neuron. Another famous quasi- experimental method is regression discontinuity design (RDD). This approach effectively uses the noise introduced at the threshold to identify causal relations. Importantly, such techniques have, thanks to decades of research in econometrics, very well understood statistical properties. These approaches promise to considerably enrich the approaches towards causality we have in neuroscience. We have a strong interdisciplinary team, spanning economics, experimental, and computational neuroscience, collaborating on adapting these quasi-experimental techniques to problems in neuroscience through a combination of machine learning and domain-specific engineering. This promises to be a major advance relative to current techniques that generally approach causality in neuroscience through model comparison. Project Narrative: The goal of this project is to develop a set of computational techniques that allow neuroscientists to quantify how neurons causally influence one another. To do so, it utilizes approaches popular in econometrics called quasiexperiments. Such approaches to quantify causality is important as medical perturbations of brains, e.g. treatments of epilepsy or depression are aimed at effecting or causing a change in the brain.",Quantifying causality for neuroscience,9775861,R01EB028162,"['Affect', 'Algorithms', 'Area', 'Brain', 'Code', 'Cognition', 'Communities', 'Computational Technique', 'Confounding Factors (Epidemiology)', 'Data', 'Data Analyses', 'Development', 'Economics', 'Engineering', 'Epilepsy', 'Etiology', 'Glean', 'Goals', 'Gold', 'Individual', 'Injections', 'Intervention', 'Learning', 'Machine Learning', 'Medical', 'Mental Depression', 'Methods', 'Modeling', 'Modernization', 'Neurons', 'Neurosciences', 'Noise', 'Organism', 'Output', 'Patients', 'Perception', 'Performance', 'Physiological', 'Property', 'Quasi-experiment', 'Randomized Controlled Trials', 'Refractory', 'Research', 'Synapses', 'Techniques', 'base', 'computational neuroscience', 'design', 'econometrics', 'experimental study', 'improved', 'optogenetics', 'phrases', 'relating to nervous system', 'theories', 'tool']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2019,803000,-0.02141030872539234
"INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE Project Summary  To understand the many disorders of the brain it is necessary to grapple with its complexity.  Increasingly large and complicated data sets are being collected, but the tools for analyzing and  modeling the data are not yet available. More researchers trained in computational neuroscience are  desperately needed. This project supports graduate and undergraduate training programs in  computational neuroscience (TPCN) at both Carnegie Mellon University (CMU) and the University of  Pittsburgh (Pitt), and a summer school in computational neuroscience for undergraduates, which are  available to students coming from colleges and universities throughout the United States.  The CMU-Pitt TPCN has 16 training faculty in computational neuroscience, 22 training faculty whose  laboratories are primarily experimental, and 20 training faculty whose laboratories are both  computational and experimental. At the graduate level the TPCN offers a PhD program in Neural  Computation (PNC) and joint PhD programs with CMU’s Department of Statistics (PNC-Stat) and its  Machine Learning Department (PNC- MLD), all set within a highly collegial, cross-disciplinary  environment of our Center for the Neural Basis of Cognition (CNBC), which is operated jointly by  CMU and Pitt. The CNBC was established in 1994 to foster interdisciplinary research on the neural  mechanisms of brain function, and now comprises 145 faculty having appointments in 22 departments.  At the undergraduate level a substantial pool of local students is supplemented during the summer  by a cohort of students from across the country. During this renewal funding period the project is  strengthening the role of statistics and machine learning throughout the training programs; (2) revising the summer undergraduate program by creating a didactic two-week “boot camp” at the  beginning, which includes a 20-lecture overview of computational neuroscience; (3) creating online  materials, in conjunction with the boot camp, that will serve not only our own students but also  the greater world of training in computational neuroscience; and (4) enhancing our minority  recruitment by (a) taking advantage of the boot camp and online materials, as well as making  promotional visits to targeted campuses, and (b) creating and running a one-year “bridge” program  to better prepare under-represented minorities for PhD programs.  TPCN trainees work in vertically integrated, cross-disciplinary research teams. Graduate students  take a year- long course in computational neuroscience that bridges modeling and modern statistical  machine learning approaches to neuroscience. To ensure their competency in core neuroscience  principles they also take courses in cognitive neuroscience, neurophysiology, and systems  neuroscience. They then pursue depth in a relevant quantitative discipline, such as computer  science, engineering, mathematics, or statistics. Graduate students have extended experience in at  least one experimental laboratory, and they take part in journal clubs and seminars within the  large Pittsburgh neuroscience community. Year-long undergraduates take courses in mathematics,  computer programming, statistics, and neuroscience; they take an additional course in neuroscience  or psychology and a course in computational neuroscience; and they complete a year-long research  project. In addition, they complete the TPCN summer program. Undergraduate trainees in the summer  program go through the boot camp on topics in computational neuroscience, including tutorials in  Matlab, statistical methods, fundamentals of differential equations, and ideas of neural coding;  they then complete a research project under careful guidance. All trainees will receive training in  responsible conduct of research. Across 5 years of funding, the TPCN supports 20 NRSA graduate  students, 10 non-NRSA graduate students, 30 undergraduate year-long fellows, and 60 undergraduate  summer fellows. Project Narrative  Research in neuroscience is crucial for attacking the causes of neurological and mental health  disorders. If the field of neuroscience is to continue its rapid advance, neuroscientists must use,  understand, and develop new technologies, acquire and analyze ever larger data sets, and grapple  more directly with the complexity of neurobiological systems. The primary goal of these training  programs will be to help train a new generation of interdisciplinary neuroscientists with strong  quantitative skills.",INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE,9763514,R90DA023426,"['Appointment', 'Brain', 'Brain Diseases', 'Code', 'Cognition', 'Communities', 'Competence', 'Country', 'Data Set', 'Differential Equation', 'Discipline', 'Doctor of Philosophy', 'Educational Status', 'Engineering', 'Ensure', 'Environment', 'Faculty', 'Fostering', 'Funding', 'Interdisciplinary Study', 'Joints', 'Journals', 'Laboratories', 'Machine Learning', 'Mathematics', 'Minority Recruitment', 'Modeling', 'Modernization', 'National Research Service Awards', 'Neurosciences', 'Psychology', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Running', 'Schools', 'Statistical Methods', 'Students', 'System', 'Teacher Professional Development', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States', 'Universities', 'Visit', 'Work', 'bridge program', 'cognitive neuroscience', 'cohort', 'college', 'computational neuroscience', 'computer program', 'computer science', 'data modeling', 'experience', 'graduate student', 'lectures', 'neuromechanism', 'neurophysiology', 'programs', 'relating to nervous system', 'responsible research conduct', 'statistics', 'summer program', 'tool', 'undergraduate student']",NIDA,CARNEGIE-MELLON UNIVERSITY,R90,2019,305372,-0.0020936381406555563
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or “position” on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9988039,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,49496,-0.02366431172992844
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9654021,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2019,1354555,-0.012678116277285309
"High-Resolution Inference for Correlates of Vaccine Protection Project Summary/Abstract Statisticians play in a key role in quantifying the uncertainty in ﬁndings from clinical trials, observational studies, and other data sources, thereby enabling rational decision making based on the ﬁndings and protecting against the human tendency to see signal in noise. Ideally the validity of the uncertainty quantiﬁcation, also known as statistical inference, will be agnostic in the sense that it avoids reliance on implausible assumptions, thereby improving the interpretability and credibility of the resulting ﬁnd- ings. There has been much progress over the last several decades in obtaining agnostic inference of population-level quantities, such as (i) the percent reduction in infection in vaccinees compared to place- bos or (ii) the average treatment effect if the entire population receives treatment versus control. There has been relatively little progress, on the other hand, in obtaining agnostic inference of higher-resolution quantities, such as (i) the percent reduction in the probability of infection in vaccinees compared to place- bos, conditional on a continuous measure of immune response or (ii) the average treatment effect for an individual based on a high-dimensional set of observed covariates. These high-resolution quantities are function-valued in the sense that they are respectively deﬁned as functions of immune response and subject covariates. While inference for high-resolution quantities can be obtained when one is willing to commit to strong assumptions on the observed data distribution, these methods are plagued by the same deﬁciencies of poor interpretability and damaged credibility faced by non-agnostic inferential procedures for population-valued quantities. In contrast to the limited progress on obtaining inference for more reﬁned quantities, there has been considerable progress towards obtaining inference-free point estimates of (i) the percent reduction in infection probability conditional on immune response markers and (ii) the average treatment effect conditional on covariates. This progress has come from several ﬁelds, including statistics, machine learning, and computer science. This proposal outlines a uniﬁed methodology for obtaining inference for high-resolution quantities, where the proposal draws inspiration from a framework developed for population-level quantities, namely targeted minimum loss-based estimation, but features highly original developments that enable inference for higher-resolution, function-valued quantities. This work has the potential to make a major impact on the identiﬁcation of individual-level variables that correlate with vaccine efﬁcacy, including in identifying individuals who will be harmed by the only licensed dengue vaccine and in identify vaccine-induced immune responses that correlate with prevention in two ongoing HIV vaccine efﬁcacy trials. The potential for broad impacts to other areas of biomedical research, including precision medicine, are also described. Project Narrative The statistics and machine learning communities have made remarkable progress in developing highly ﬂexible data mining procedures that, for example, enable (i) the identiﬁcation of potentially protective vaccine-induced immune responses and (ii) the prediction of the optimal treatment for a given individual. Despite this progress, there has been little progress in quantifying the uncertainty in the output of these ﬂexible procedures without making implausible assumptions on the mechanism that generated the data, where the likely failure of these assumptions renders it difﬁcult to make (i) informed vaccine development decisions based on the identiﬁed immune response or (ii) treatment decisions based on the predicted optimal treatment. The proposed work develops a uniﬁed methodology for quantifying the uncertainty in estimates obtained using these data mining techniques, thereby enabling informed decision making.",High-Resolution Inference for Correlates of Vaccine Protection,9774603,DP2LM013340,"['Area', 'Biomedical Research', 'Breathing', 'Clinical Trials', 'Data', 'Data Sources', 'Decision Making', 'Dengue Vaccine', 'Development', 'Failure', 'HIV vaccine', 'Human', 'Immune response', 'Individual', 'Infection', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Noise', 'Observational Study', 'Output', 'Placebos', 'Play', 'Population', 'Prevention', 'Probability', 'Procedures', 'Resolution', 'Signal Transduction', 'Techniques', 'Uncertainty', 'Vaccines', 'Work', 'base', 'computer science', 'data mining', 'efficacy trial', 'flexibility', 'high dimensionality', 'immune function', 'improved', 'learning community', 'optimal treatments', 'precision medicine', 'response biomarker', 'statistics', 'treatment effect', 'vaccine development', 'vaccine efficacy']",NLM,UNIVERSITY OF WASHINGTON,DP2,2019,2332500,-0.006250630828000521
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9823400,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2019,583885,-0.012678116277285309
"Big Omics Data Engine 2 Supercomputer Computational and data science has transformed biomedical scientific discovery: its approaches are embedded into a wide range of workflows for diseases such as schizophrenia, depression, Alzheimer's, epilepsy, influenza, autism, drug addiction, pediatric cardiac care, Inflammatory Bowel Disease, prostate cancer and multiple myleloma. Sixty-one basic and translational researchers at Mount Sinai representing over $100 million in NIH funding, along with their collaborators from 75 external institutions, have utilized the Big Omics Data Engine (BODE) supercomputer to elucidate significant scientific findings in over 167 publications, including high impact journals such as Nature and Science, with 2,427 citations in three years. These researchers have also shared the data generated on BODE throughout their consortia and into national data sharing repositories. BODE is nearing the end of its vendor maintainable life, and researchers need increased computational throughput and storage space. To empower researchers to not only continue their inquiries, but to also tackle more complex scientific questions with decreased time to solution, we propose the Big Omics Data Engine 2 Supercomputer (BODE2). BODE2 will contain a total of 3,200 Intel Cascade Lake cores with 15 terabytes of memory and 14 petabytes of raw storage, and will leverage an existing 250 terabytes of SSDs. An instrument of this size is not available elsewhere affordably. With the proposed instrument, researchers will be able to take advantage of three major benefits: (1) the ability to receive results faster for overall greater scientific throughput; (2) the ability to increase the fidelity of their simulations and analyses; and (3) the ability to migrate research applications seamlessly to the software environment for greater scientific productivity. As with data produced on BODE, BODE2 data products will also be shared with the broader scientific community. BODE2 will provide the critical infrastructure needed by the wide range of researchers and clinicians for the genetics and population analysis, gene expression, machine learning and structural and chemical biology approaches used to make advances in these diseases. A specialized Big Omics Data Engine 2 Supercomputer instrument will provide necessary computational and data science infrastructure for 61 research projects with 75 collaborating institutions in diverse areas such as Alzheimer's, autism, schizophrenia, drug addiction, influenza, pediatric cardiac care, depression, epilepsy, prostate cancer and multiple myeloma. Data generated from this instrument will be shared in national databases.",Big Omics Data Engine 2 Supercomputer,9708160,S10OD026880,"['Alzheimer&apos', 's Disease', 'Biology', 'Cardiac', 'Caring', 'Chemicals', 'Childhood', 'Communities', 'Complex', 'Computational Science', 'Computer software', 'Data', 'Data Science', 'Disease', 'Drug Addiction', 'Environment', 'Epilepsy', 'Funding', 'Gene Expression', 'Inflammatory Bowel Diseases', 'Influenza', 'Infrastructure', 'Institution', 'Journals', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Memory', 'Mental Depression', 'Nature', 'Population Analysis', 'Productivity', 'Publications', 'Research', 'Research Personnel', 'Schizophrenia', 'Science', 'Structure', 'Time', 'United States National Institutes of Health', 'Vendor', 'autism spectrum disorder', 'data sharing', 'genetic analysis', 'instrument', 'petabyte', 'repository', 'simulation', 'supercomputer', 'terabyte', 'translational scientist']",OD,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,S10,2019,1998264,-0.05610034007028169
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9911854,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2019,15000,-0.009969527488571573
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9755666,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,24201,0.013572157472697763
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9668156,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2019,381513,0.013572157472697763
"Interactions between Gut Microbiome Natural Products and Intestinal Helminths Project Summary / Abstract This project will identify the molecular mechanisms through which the gut microbiome interacts with infectious intestinal helminths. Intestinal helminthic parasites present one of the most pressing global health problems due to the abundance of infection, a limited understanding of etiology, and increasing drug resistance. Prior work has established that the gut microbiome can influence infection, but the specific mechanisms through which it does so are unknown. We hypothesize that the gut microbiome produces anti-helminthic metabolites that can prevent and control infection. We propose a series of experiments that integrate multi'omics data and machine learning techniques to discover natural products of the gut microbiome that predict infectious burden or associate with infection control. Our studies leverage a high-throughput zebrafish model of infection to precisely resolve these factors. Additionally, we will develop bioinformatic methods that (1) connect natural products to the genetic pathways and microbiota that produce them, and (2) impute the presence of these natural products in the human gut. Consequently, this project will produce novel anti-helminthic drug leads that can be efficiently isolated and tested, and will clarify the role of gut microbiome natural products in the etiology of infection. Project Narrative Intestinal helminthic infections are a pressing global health problem due to the number of infected individuals, a limited understanding of disease mechanisms, and an increase in resistance to anti-helminthic drugs. This project will discover gut microbiota, their genes, and their metabolites that are linked to the prevention and control of parasitic infection. The long-term goals of this project are to determine if the gut microbiome is a factor in the etiology of helminthic intestinal infection, and to identify microbiome-sourced candidates for the discovery of novel anti-helminthic drugs. !",Interactions between Gut Microbiome Natural Products and Intestinal Helminths,9701928,R21AI135641,"['Animals', 'Anthelmintics', 'Bioinformatics', 'Data', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Etiology', 'Exposure to', 'Fishes', 'Future', 'Generations', 'Genes', 'Genetic', 'Goals', 'Growth and Development function', 'Helminths', 'Human', 'Individual', 'Infection', 'Infection Control', 'Inflammation', 'Informatics', 'Intestinal parasite', 'Intestines', 'Link', 'Machine Learning', 'Mammals', 'Measurement', 'Metabolic', 'Methods', 'Microscopic', 'Minority', 'Modeling', 'Molecular', 'Monitor', 'Natural Products', 'Outcome', 'Parasites', 'Parasitic infection', 'Pathway interactions', 'Pharmaceutical Preparations', 'Population', 'Prevention', 'Probiotics', 'Research', 'Research Design', 'Resistance', 'Resources', 'Role', 'Sample Size', 'Series', 'Source', 'Taxonomy', 'Techniques', 'Testing', 'Therapeutic', 'Work', 'Zebrafish', 'base', 'enteric infection', 'expectation', 'experimental study', 'global health', 'gut microbes', 'gut microbiome', 'gut microbiota', 'helminth infection', 'infection risk', 'inquiry-based learning', 'insight', 'learning strategy', 'metabolome', 'metagenome', 'microbial', 'microbiome', 'microbiota', 'neglected tropical diseases', 'novel', 'prevent']",NIAID,OREGON STATE UNIVERSITY,R21,2019,183750,-0.009289143907511276
"Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack Project Summary: Tufts University physics professor Allan Cormack pioneered the field of tomography. His seminal work, from 1963 and 1964, provided both the mathematical foundations of computerized tomography (CT), and tangible proof-of-concept by engineering a rudimentary CT scanner. Taken together, this effort represented the first practical method to ""see into"" an object without physically breaking it open. Along with the engineer Godfrey Hounsfield, he won the 1979 Nobel Prize in Physiology or Medicine for these contributions. Since then, tomography has broadened to include a wide range of modalities and problems. This field is unique for the rich interplay among applications in medicine, security, earth sciences, industry, physics, and the mathematics required to solve these problems. This international conference at Tufts, “Modern Challenges in Imaging: In the Footsteps of Allan Cormack” will honor the achievements of Cormack and reflect this diversity in the field by gathering top international researchers in mathematics, engineering, science, and medicine to communicate the most current research and challenges in the field. This will include work on mathematical models of emerging modalities, tomographic machine learning, dynamic methods, and spectral imaging with applications include medicine and security. The best research from the conference will be disseminated in a special issue of the journal Inverse Problems. Talks will be posted on the conference website. The organizers will recruit a diverse set of experienced participants and trainees, and the conference will be advertised in a range of publications reflecting the scientific and demographic diversity of the field. This conference is unique in that it combines high-level mathematical participants with experts in medical and industrial CT. It is structured to encourage participants from different fields to talk with each other, broaden their horizons, and make connections between problems and methodologies in the various fields. Several of the plenary talks will provide introductions to the areas. Trainees will be integrated into the conference through an informal welcome lunch and a poster session to introduce them to researchers in the field. This supports goals 1, 4, and 5, of the NIBIB: Researchers will present innovative biomedical technologies, engineering solutions, and mathematical methods to better image the body and objects more generally. The synergy between research areas will support the translation of technologies from the academic sphere to medical utility. The training opportunities for graduate students and beginners support the training of the next generation of diverse scientists. Project Narrative This conference will bring together medical, scientific, engineering, and applied mathematical researchers to present their newest research for a range of tomographic problems. Graduate students and beginners will be encouraged to participate and learn by being offered introductory talks, a student poster session, a welcome event, and an informal atmosphere. The conference will be structured so researchers will learn about important challenges in practical tomography as well as new techniques and methods, thereby creating synergies and research connections among the areas.",Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack,9837131,R13EB028700,"['Achievement', 'Advertising', 'Algorithms', 'Area', 'Biomedical Technology', 'Communication', 'Development', 'Earth science', 'Engineering', 'Environment', 'Event', 'Fertilization', 'Foundations', 'Goals', 'Image', 'Individual', 'Industrialization', 'Industry', 'International', 'Journals', 'Lead', 'Learning', 'Lightning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modality', 'Modernization', 'National Institute of Biomedical Imaging and Bioengineering', 'Nobel Prize', 'Outcome', 'Participant', 'Physics', 'Physiology', 'Population', 'Problem Solving', 'Publications', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Security', 'Seminal', 'Societies', 'Structure', 'Students', 'Techniques', 'Technology', 'Time', 'Training Support', 'Translations', 'Underrepresented Groups', 'Universities', 'Work', 'X-Ray Computed Tomography', 'cohort', 'demographics', 'design', 'experience', 'graduate student', 'higher level mathematics', 'informal atmosphere', 'innovation', 'mathematical methods', 'mathematical model', 'meetings', 'member', 'next generation', 'posters', 'professor', 'recruit', 'spectrograph', 'symposium', 'synergism', 'tomography', 'training opportunity', 'web site']",NIBIB,TUFTS UNIVERSITY MEDFORD,R13,2019,10000,-0.015675691961546365
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9739188,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Simulation', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in vivo', 'insight', 'intracranial artery', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'multiple omics', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'single photon emission computed tomography', 'spatiotemporal', 'supervised learning', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2019,506426,-0.005582933117973528
"An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography SUMMARY – OVERALL Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function. Although the basic electron cryo-tomography technique has been used for several decades, the technology is being revolutionized by recent advances in sample preparation, electron cryo-microscopy hardware, improved capabilities for automatic data collection, direct electron detection imaging devices, and phase plate technologies. Combined, these advances led to the ability to generate extraordinarily large numbers of cellular cryo-tomograms of exquisite quality. In principle, such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. However, collection of cellular data is now at a far faster rate than can currently be analyzed with existing methods, producing a serious barrier to progress: to match the data production rates of a single laboratory, at least 50 experienced scientists would need to handle the data analysis. The primary goal of this Program Project is to establish quantitative and highly automated tools for the reconstruction and interpretation of highly complex cellular tomographic data. We have assembled a highly synergistic team of PIs with complimentary expertise in cutting-edge computational and experimental electron microscopy techniques to achieve this goal through collaborative efforts. Project 1 (Hanein & Penczek) focuses on development and implementation of tomogram quality assessment and validation techniques and on experimentally guided optimization of data collection strategies. Project 2 focuses on automatic tomographic reconstruction technology, extraction of various features from the tomograms, and the analysis of distribution patterns derived from the extracted features. Project 3 focuses on development of quantitative tools for tomogram annotation through deep learning and sub-tomogram alignment as well as interactive visualization tools. The set of highly automated tools developed in this Program Project will permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study. NARRATIVE Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function and—with recent instrumental advances—it is now possible to image hundreds of cells per months, enabling collection of cellular data at a far faster rate than can currently be analyzed. Such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. This Program Project brings together an accomplished team of investigators to develop new strategies for effectively processing and interpreting this massive influx of data, developing a set of highly automated tools to permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study.",An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography,9769773,P01GM121203,"['Address', 'Algorithms', 'Artificial Intelligence', 'Big Data', 'Biological', 'Biology', 'Cancer Diagnostics', 'Cell physiology', 'Cells', 'Classification', 'Collection', 'Complex', 'Computing Methodologies', 'Cryo-electron tomography', 'Cryoelectron Microscopy', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Disease', 'Electron Microscopy', 'Electrons', 'Environment', 'Floods', 'Goals', 'Health', 'Heart', 'Human', 'Image', 'Imaging Device', 'Individual', 'Knowledge', 'Laboratories', 'Methodology', 'Methods', 'Molecular', 'Molecular Structure', 'Morphology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Preparation', 'Process', 'Production', 'Real-Time Systems', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Stimulus', 'Structure', 'System', 'Techniques', 'Technology', 'Tomogram', 'Validation', 'Variant', 'Visualization software', 'base', 'computer framework', 'convolutional neural network', 'deep learning', 'electron tomography', 'experience', 'imaging detection', 'improved', 'insight', 'knowledge base', 'learning strategy', 'nanomachine', 'novel diagnostics', 'particle', 'programs', 'reconstruction', 'response', 'software development', 'statistics', 'tomography', 'tool', 'virtual']",NIGMS,SANFORD BURNHAM PREBYS MEDICAL DISCOVERY INSTITUTE,P01,2019,928444,-0.027233047244888904
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9789907,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Infrastructure', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2019,366616,-0.003015586397542168
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,9851583,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'learning strategy', 'mHealth', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2019,741688,-0.02790796069246297
"Acquisition of a next-generation computing cluster We request funds to purchase our next-generation computing cluster to support computationally intensive NIH-funded research at Washington University in St. Louis. This system will become the foundation of the Center for High Performance Computing (CHPC) to support our active, diverse user community. It has been designed to meet our current and future computing needs. It adds additional capabilities to support emerging fields such as “Deep Learning”. The CHPC currently supports over 775 users from 300 different groups across 33 departments. 58 papers have cited the CHPC. The Center has a proven funding model and is economically sustainable. The Center has partnered with other University organizations to offer training workshops, not only on the use of the cluster, but also on introductory programming for users with no prior programming experience. If this proposal is funded, we will be able to continue to support this ever-growing diverse community of researchers. The proposed system would replace critical components including the management node, the login nodes, the storage, and upgrade the Infiniband networking. We would add substantial upgrades to our computing power with state-of-the-art processors, increased memory capacity for growing jobs, General Purpose Graphical Processing units (GPGPUs), and new capabilities for “Deep Learning”. Nearly all fields of NIH-funded research are faced with increasingly large data sets that require additional computing power to analyze. We propose building a next-generation computing cluster to support this research. Our Center has a proven track record in supporting a large, diverse group of users in all aspects of their computationally demanding research.",Acquisition of a next-generation computing cluster,9707936,S10OD025200,"['Communities', 'Educational workshop', 'Foundations', 'Funding', 'Future', 'High Performance Computing', 'Memory', 'Modeling', 'Occupations', 'Paper', 'Research', 'Research Personnel', 'System', 'Training', 'United States National Institutes of Health', 'Universities', 'Washington', 'cluster computing', 'deep learning', 'design', 'experience', 'next generation']",OD,WASHINGTON UNIVERSITY,S10,2019,597200,-0.009117898706593387
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9638561,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computational platform', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'human microbiota', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2019,364865,0.0010379243160592254
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9626416,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Infrastructure', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2019,2000000,-0.029924869771353186
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,9882822,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2019,390806,-0.0034030235252328167
"Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!) PROJECT SUMMARY/ABSTRACT Our society faces significant challenges in providing quality health care that is accessible by each person and is sensitive to each person's individual lifestyle and individual health needs. Due to recent advances in sensing technologies that have improved in accuracy, increased in throughput, and reduced in cost, it has become relatively easy to gather high resolution behavioral and individualized health data at scale. The resulting big datasets can be analyzed to understand the link between behavior and health and to design healthy behavior interventions. In this emerging area, however, very few courses are currently available for teaching researchers and practitioners about the foundational principles and best practices behind collecting, storing, analyzing, and using behavior- based sensor data. Teaching these skills can help the next generation of students thrive in the increasingly digital world.  The goal of this application is to design online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to WSU faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.  This contribution is significant because not only large research groups but even individual investigators can create large data sets that provide valuable, in-the-moment information about human behavior. They need to be able to handle the challenges that arise when working with sensor- based behavior data. Because students will receive hands-on training with actual sensor datasets and analysis tools, they will know how to get the best results from available tools and will be able to interpret the significance of analysis results.  Our proposed online course program, called AHA!, builds on the investigators' extensive experience and ongoing collaboration at Washington State University on the development of smart home and mobile health app design, activity recognition, scalable biological data mining, and the use of these technologies for clinical applications. Our approach will be to design online course modules to train individuals in the analysis of behavior-based sensor data using clinical case studies (Aim 1). We will design an educational program that involves students from diverse backgrounds and that is findable, accessible, interoperable, and reusable (Aim 2). Finally, we will conduct a thorough evaluation to monitor success and incrementally improve the program (Aim 3). All of the materials will be designed for continued use beyond the funding period of the program. PROJECT NARRATIVE  This program focuses on the development and dissemination of online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to Washington State University faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.",Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!),9696381,R25EB024327,"['Address', 'Aging', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological', 'Case Study', 'Charge', 'Chronic Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Development', 'Discipline', 'E-learning', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Evaluation', 'FAIR principles', 'Face', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'General Population', 'Goals', 'Health', 'Human', 'Immersion Investigative Technique', 'Individual', 'Interdisciplinary Study', 'Life Style', 'Link', 'Longevity', 'Machine Learning', 'Methods', 'Mobile Health Application', 'Monitor', 'Performance', 'Persons', 'Precision Medicine Initiative', 'Pythons', 'Rehabilitation Nursing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Site', 'Societies', 'Structure', 'Students', 'Suggestion', 'Techniques', 'Technology', 'Training', 'Universities', 'Washington', 'Work', 'base', 'behavior influence', 'behavioral health', 'biocomputing', 'career networking', 'clinical application', 'cognitive rehabilitation', 'cost', 'course development', 'course module', 'data mining', 'design', 'digital', 'experience', 'health care quality', 'health data', 'improved', 'innovation', 'learning materials', 'learning strategy', 'mHealth', 'next generation', 'online course', 'programs', 'recruit', 'responsible research conduct', 'scale up', 'sensor', 'sensor technology', 'skills', 'smart home', 'statistics', 'success', 'synergism', 'tool', 'web page']",NIBIB,WASHINGTON STATE UNIVERSITY,R25,2019,150529,-0.0027900914013791324
"The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers Project Summary  One of the most glaring yet easily addressable gaps in our current scientific workflow and publication system is improving the way that methods are reported, in particular, the lack of key methodological details necessary for interpreting and reproducing a study. Most authors continue to cite the name of the reagent, like an antibody using the vendor, and the city where the vendor is located, but omit the catalog and lot number making antibodies very difficult to track down, thereby reducing reproducibility of the paper. The Resource Identification, RRID, Initiative has successfully implemented a solution to this lack of identification, by asking authors to include a persistent unique identifier (RRID) for each antibody along with a standard syntax that includes the lot information. This syntax is now required in about 100 journals and accepted in at least 400, was accepted into the EQUATOR network of standards and is under consideration by the JATS committee, the NISO standard for journal article metadata. For antibodies, RRIDs are assigned by the AntibodyRegistry.org, which accepts full catalogs from antibody companies and individual antibody records from authors, who are unable to locate the record for the antibody that they used in a study or one which they created in their lab. This process should be made as easy as possible for authors, and through text analysis we have devised a set of tools that should help authors create better records with less work.  The AntibodyRegistry.org was created as part of an academic project and it has successfully incorporated millions of antibody records, thousands of which have now been cited in scientific papers using the RRID syntax. The use of RRID is growing, and in order to support the longer term sustainability of the AntibodyRegistry.org, a core community authority for RRIDs, this resource needs to be enhanced to align with the available commercial and non-commercial funding sources. We propose the addition of features, valuable to antibody companies and journals, to improve market intelligence and reporting around antibodies. We also propose to auto-generate antibody entries for authors and curators when submitting/curating an antibody to decrease the time it takes to complete the task, thereby reducing the barrier to entry and cost. Project Narrative The AntibodyRegistry.org is a catalog of antibodies used in research and serves as the antibody identifier source for the Research Resource Identifier (RRID) initiative. The use of these identifiers improves rigor and transparency in compliance with both journal and NIH standards. As the RRID initiative grows, the AntibodyRegistry.org is becoming an increasingly well-used and valuable resource; requiring increased attention from curation staff. Improvements to the AntibodyRegistry.org are needed to increase commercial value and thereby its sustainability, decrease curation cost, and provide a higher level of service to the scientific community.",The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers,9680073,R41GM131551,"['Adopted', 'Antibodies', 'Asia', 'Attention', 'Award', 'Businesses', 'California', 'Catalogs', 'Cities', 'Cloud Computing', 'Communities', 'Data', 'Data Analytics', 'Databases', 'Electronic Mail', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Funding', 'Funding Agency', 'Generations', 'Glare', 'Goals', 'Individual', 'Intelligence', 'International', 'Journals', 'Machine Learning', 'Marketing', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Names', 'Neurosciences', 'Paper', 'Performance', 'Phase', 'Process', 'Provider', 'Publications', 'Reagent', 'Records', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Services', 'Small Business Technology Transfer Research', 'Source', 'System', 'Text', 'Time', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Work', 'authority', 'base', 'cost', 'improved', 'information framework', 'journal article', 'syntax', 'text searching', 'tool']",NIGMS,"SCICRUNCH, INC.",R41,2019,149373,-0.012504976136714658
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9700030,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric infection', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2019,26919,-0.015752223679611717
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9744013,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Algorithms', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelium', 'Esophageal', 'Esophageal Adenocarcinoma', 'Esophagitis', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Intelligence', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stem cells', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2019,171720,0.000807322072237718
"Successful Clinical Response In Pneumonia Therapy (SCRIPT) Systems Biology Center Overall Project Summary: This innovative integrated systems biology application seeks to delineate the complex host/pathogen interactions occurring at the alveolar level that lead to unsuccessful response to therapy in serious pneumonia. To achieve this objective, we will leverage our unique access to alveolar fluid collected as part of routine clinical care in mechanically ventilated patients with suspected pneumonia in our medical intensive care unit. Bronchoalveolar lavage fluid will be obtained serially from well characterized mechanically ventilated patients with Pseudomonas or Acinetobacter pneumonia. Both of these CDC- designated serious hazard level pathogens have clinical failure rates as high as 50%. A robust clinical definition will allow comparison of both host and pathogen signatures associated with failure of therapy vs. success. These clinical specimens and extensive patient phenomics will anchor two mutually supportive and iterative research projects. Project One will deploy robust tools for flow sorting macrophage and lymphocyte subset populations, isolating RNA from these populations, and performing transcriptomic and epigenomic analysis to compare successful and unsuccessful host responses to infection. Project Two will focus on both specific pathogen genomic profiles associated with unsuccessful outcome. Changes in microbiome communities will be comprehensively assessed by shotgun deep sequencing to detect bacteriophage, other virus, and fungal DNA, in addition to bacterial. The Technology Core will perform cell sorting of NBBAL macrophage and lymphocyte subsets, RNA sequencing, and whole genome methylation, and perform parallel studies in a unique humanized alveolar macrophage mouse model. A Data Management and Bioinformatics Core will develop tools to reduce the dimensionality of these large comprehensive datasets, including the clinical phenomics, and provide them to the Modeling Core. The Modeling Core will then use innovative modeling approaches including a model of the alveolus during pneumonia as an ecosystem out of balance combined with unique machine learning tools and neural networks to generate biomarkers of host, pathogen and/or microbiome patterns predictive of successful pneumonia outcome. Predictive biomarkers developed in the Modeling Core will then be validated in a prospective confirmatory cohort of patients in whom analogous data will be generated. The Administrative Core will perform the outward-facing role of education and outreach to the community and sponsor, as well as regularly exchanging datasets, analytic tools, and specimens with NIH-sponsored/approved repository sites. Project Narrative: The Successful Clinical Response In Pneumonia Treatment (SCRIPT) systems biology center seeks to delineate the complex host/pathogen interactions occurring at the alveolar level that lead to unsuccessful response to therapy in serious pneumonia. We will leverage our unique access to alveolar fluid collected as part of routine clinical care in mechanically ventilated patients to generate clinical phenomic, transcriptomic, epigenomic and metagenomic data that describe the host response, pathogen characteristics and microbiome of the alveolar space during pneumonia. We will then integrate this comprehensive phenotypic data into an ecosystem-based model to generate predictive biomarkers of pneumonia outcome for subsequent validation in a second cohort and tested for causality in a humanized alveolar macrophage mouse model.",Successful Clinical Response In Pneumonia Therapy (SCRIPT) Systems Biology Center,9626377,U19AI135964,"['Acinetobacter', 'Address', 'Alveolar', 'Alveolar Macrophages', 'Alveolus', 'Animal Model', 'Antibiotic Therapy', 'Bacteriophages', 'Bioinformatics', 'Biological Markers', 'Bronchoalveolar Lavage', 'Bronchoalveolar Lavage Fluid', 'Cause of Death', 'Cell Separation', 'Cells', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Microbiology', 'Clinical Trials', 'Collection', 'Communities', 'Complex', 'DNA Viruses', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Education and Outreach', 'Equilibrium', 'Etiology', 'Failure', 'Fungal DNA', 'Generations', 'Genes', 'Genetic', 'Goals', 'Human', 'Immune response', 'Infection', 'Intensive Care Units', 'Lead', 'Link', 'Liquid substance', 'Lymphocyte Subset', 'Lymphoid Cell', 'Machine Learning', 'Mechanics', 'Medical', 'Metagenomics', 'Methylation', 'Modeling', 'Multiomic Data', 'Myeloid Cells', 'Nosocomial Infections', 'Nosocomial pneumonia', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Pneumonia', 'Population', 'Preparation', 'Prospective cohort', 'Protocols documentation', 'Pseudomonas', 'Pseudomonas aeruginosa', 'Pseudomonas aeruginosa infection', 'RNA', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Secondary to', 'Shotguns', 'Site', 'Sorting - Cell Movement', 'Specimen', 'Standardization', 'Stream', 'System', 'Systems Biology', 'Technology', 'Testing', 'Treatment Failure', 'United States National Institutes of Health', 'Validation', 'Virulence', 'analytical tool', 'base', 'clinical care', 'cohort', 'data management', 'data pipeline', 'deep sequencing', 'epigenomics', 'experience', 'genomic data', 'genomic profiles', 'hazard', 'humanized mouse', 'improved', 'innovation', 'macrophage', 'microbiome', 'microbiome composition', 'mouse model', 'multiple omics', 'neural network', 'new therapeutic target', 'novel', 'pathogen', 'pathogen genomics', 'phenomics', 'phenotypic data', 'pneumonia model', 'predictive marker', 'predictive tools', 'prospective', 'repository', 'response', 'success', 'tool', 'transcriptome sequencing', 'transcriptomics', 'viral DNA', 'whole genome']",NIAID,NORTHWESTERN UNIVERSITY AT CHICAGO,U19,2019,2400000,0.0025797422099569285
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,9864664,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2019,269849,-0.004044851289175831
"Administrative Supplement to the OAIC Pepper Center Coordinating Center We wish to advantage of 2 new key opportunities that could significantly enhance achievement of the overall goals of the OIAC Coordinating Center (OAIC CC) and 2 key, unexpected administrative needs. Project 1) Develop, test and implement an innovative set of tools to perform Integrative Data Analysis (IDA) for combining and analyzing independent data sets across the OAIC network An over-arching goal of the OAIC CC is to build collaborations between OAICs that unlock synergy. Each of the OAICs has many small/medium-sized completed studies relevant to the OAIC theme, and that have measured key domains of physical function. Combining these studies could provide large, powerful databases for answering critical questions not possible with individual studies. However, this is currently not possible because different measurement instruments are often used across centers and across studies. This project overcomes this critical limitation by taking advantage of 2 newly available technologies and an ongoing study. IDA is a set of strategies in which two or more independent data sets which contain measures addressing similar domains but using different measurement instruments are combined into one and then statistically analyzed. The proposed project is timely because it leverages an ongoing clinical study to validate new procedures for harmonizing measures of physical and cognitive function across 20 Pepper center studies. The resources created by the project will significantly enhance collaboration across the OAIC program network, benefiting researchers at all OAICs, and can be disseminated to other NIA center programs. Project 2) Develop a robust, interactive database of OAIC Program accomplishments that will automatically be updated via an efficient, streamlined, electronic annual reporting process.  It is widely believed that the NIA-funded Pepper Center program has been highly productive. However, there is no means of assessing the overall effectiveness of the Pepper Center, or of ‘cataloging’ its impressive accomplishments. This project will take advantage of new open-source technology to efficiently develop a robust, comprehensive, searchable, interactive database of past accomplishments. It will also develop a streamlined electronic Annual Directory Report template, and link it to the new OAIC database so that it is automatically updated each year. Achieving the goals of this project will reduce administrative burden for sites, facilitate NIA review of performance of centers, and create an annually updated database of OAIC accomplishments, projects, publications, and outcomes, and facilitate collaborations between centers and investigators across NIA programs. This application also requests support for 2 key, unexpected administrative needs that have arisen: 1) Increase in funding amount for the annual OAIC CC Multi-center pilot project. 2) Support for additional Pepper Centers that will soon be added to the OAIC network. Relevance Statement for OAIC Coordinating Center Administrative Supplement The Coordinating Center of the OAIC coordinates the activities of all the individual centers in the NIA- funded, OAIC network; its over-arching goal is to build collaborations between the individual OAICs and thereby unlock synergy and enable projects that could not be undertaken by any single OAIC center. This administrative supplement application proposes 2 developmental projects that will significantly enhance the capabilities of the OAIC to achieve these goals and which takes advantage of newly available methods and technology. This also includes additional support for the possible increase in the number of Pepper Centers and an increase in the pilot award budget.",Administrative Supplement to the OAIC Pepper Center Coordinating Center,9961004,U24AG059624,"['Achievement', 'Address', 'Administrative Supplement', 'Aging', 'Annual Reports', 'Award', 'Budgets', 'Capsicum', 'Cataloging', 'Catalogs', 'Clinical', 'Clinical Research', 'Cognition', 'Collaborations', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Directories', 'Effectiveness', 'Elderly', 'Equipment and supply inventories', 'Evaluation', 'Funding', 'Goals', 'Health', 'Individual', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Online Systems', 'Outcome', 'Participant', 'Performance', 'Physical Function', 'Pilot Projects', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Update', 'Walking', 'analytical method', 'analytical tool', 'base', 'cognitive function', 'cost effective', 'data modeling', 'forest', 'innovation', 'instrument', 'interest', 'lifestyle intervention', 'new technology', 'novel', 'open source', 'programs', 'recruit', 'response', 'synergism', 'theories', 'tool']",NIA,WAKE FOREST UNIVERSITY HEALTH SCIENCES,U24,2019,149775,-0.00910042743336549
"Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity   Project Summary    For something as complex and multifaceted as bacterial antibiotic resistance (AR), our drug evaluation  paradigm is strikingly narrow and homogenous: MIC/MBC testing in standardized bacteriologic media. We  have shown that this drug evaluation paradigm is inadequate, even misleading, as changes in the media  conditions of the procedure lead to dramatically different results. A more holistic definition of antibiotic therapy  that centers on understanding antibiotic activity in synergy with host innate immune factors such as cationic  antimicrobial peptides (AMPs), serum and phagocytic cells (e.g. neutrophils) reveals therapeutic options  unrecognized in standard testing. The proposed U01 program represents a groundbreaking approach to use  systems biology approaches and inform more effective antibiotic utilization in the context of host innate  immunity. We propose to: 1) build an iterative systems biology workflow that integrates multiple experimental  and computational approaches to give a comprehensive assessment of AR; and 2) apply this workflow to high  priority pathogens to systematically elucidate AR mechanisms and their condition­dependency. The iterative  workflow includes: (i) omics and physiological data generation.  Clinically isolated strains of the selected  pathogens will be grown under conventional testing (bacteriologic media) and more physiologic conditions  (tissue culture media, serum, and in presence of AMPs and neutrophils) to probe for advantageous gain of  activity.  The omics data types collected are: DNA resequencing, RNAseq, and metabolomics.  (ii)  Bioinformatics and data modeling analysis involves three approaches: big data analysis for data set  dimensionality and coarse grained variable dependencies assessment, genome­scale modeling for  mechanistic elucidation and analysis, and machine learning that uses AR­related metadata to classify the  overall biological functions. This analysis will lead to understanding of AR mechanisms.  (iii) Multi­scale  validation from animal models, to laboratory evolution, to cytology, to gene expression alteration, to structural  protein analysis of putative targets. The validation thus ranges from host behavior to atomistic detail of  ligand­target interactions. The iterative loop then closes, comparing computational prediction to experimental  outcomes. False­negative and false­positive predictions are then algorithmically analyzed by a hypothesis  generating family of algorithms that then makes suggestions about what conditions to use in the next iteration  of the loop.  The pathogens that we will focus on are methicillin­resistant ​Staphylococcus aureus ​(MRSA), the  carbapenem­resistant Enterobacteriaceae (CRE) Klebsiella ​pneumoniae ​and ​Acinetobacter baumannii,​ and  Pseudomonas aeruginosa​. The team of investigators has made the foundational observations and led the  development of the technologies on which the iterative workflow is based. A multi­ and genome­scale methods  of systems biology fulfills requirements of RFA­AI­14­064 to which it responds.              Narrative    The current evaluation of antibiotic drug candidates in drug discovery and in clinical medicine is conducted in  laboratory media that ignores the actual physiologic conditions in the host and the host immune system.  We  have discovered potent antimicrobial activities of existing antibiotics against highly “drug­resistant superbugs”  that are currently ignored but revealed in synergy with the human immune system. This program proposes a  holistic and comprehensive systems biology approach to systematically discover novel treatment opportunities  and underlying mechanisms using a novel iterative data generation, analysis, and modeling workflow.       ",Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity,9653953,U01AI124316,"['Acinetobacter baumannii', 'Algorithmic Analysis', 'Algorithms', 'Animal Model', 'Animals', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Antimicrobial Cationic Peptides', 'Bacterial Antibiotic Resistance', 'Bacteriology', 'Behavior', 'Big Data', 'Bioinformatics', 'Biological Assay', 'Biological Process', 'Biological Products', 'Biology', 'Clinical', 'Clinical Medicine', 'Collection', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Culture Media', 'Cytology', 'DNA Resequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Drug Evaluation', 'Drug resistance', 'Effectiveness', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Future', 'Gene Expression Alteration', 'Generations', 'Genome', 'Grain', 'Growth', 'Human', 'Immune', 'Immune system', 'Immunity', 'Immunologic Factors', 'Infection', 'Integration Host Factors', 'Klebsiella pneumonia bacterium', 'Knowledge', 'Laboratories', 'Lead', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular Biology', 'Molecular Profiling', 'Mus', 'Natural Immunity', 'Network-based', 'Organism', 'Outcome', 'Output', 'Participant', 'Phagocytes', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Physiological', 'Predisposition', 'Procedures', 'Process', 'Protein Analysis', 'Pseudomonas aeruginosa', 'Research Personnel', 'Resistance', 'Resistance development', 'Risk Assessment', 'Series', 'Serum', 'Standardization', 'Statistical Data Interpretation', 'Structural Protein', 'Structure', 'Suggestion', 'Superbug', 'Systems Biology', 'Testing', 'Therapeutic', 'Update', 'Validation', 'antimicrobial', 'antimicrobial peptide', 'bacterial genetics', 'base', 'carbapenem-resistant Enterobacteriaceae', 'data modeling', 'design', 'drug candidate', 'drug discovery', 'experience', 'genome-wide', 'human tissue', 'improved', 'in vivo', 'macromolecule', 'metabolomics', 'methicillin resistant Staphylococcus aureus', 'microbial', 'microbial host', 'multi-drug resistant pathogen', 'multidrug-resistant Pseudomonas aeruginosa', 'neutrophil', 'novel', 'pathogen', 'priority pathogen', 'product development', 'programs', 'protein structure', 'reconstruction', 'resistance mechanism', 'response', 'screening', 'synergism', 'technology development', 'tissue culture', 'tool', 'transcriptome sequencing', 'transcriptomics', 'treatment response']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2019,1859004,-0.00880788892583872
"Development of mosaic mouse models of HCC for genetic interspecies inference DESCRIPTION (provided by applicant): Hepatocellular carcinoma (HCC) remains a menace for human health for the lack of any effective treatment. HCC is usually the end result of chronic liver diseases associated with diverse risk factors. Furthermore, non- alcoholic steatohepatitis (NASH) induced HCC, which is projected to be the leading cause of new cases, remains poorly characterized. Although many mouse models of HCC have been developed, it is unclear how well they represent different subgroups of human HCCs. This research plan proposes to transform HCC animal modeling by establishing a novel in vivo platform to accurately replicate the somatic molecular profiles of human HCC in mice. To do this, three independent HCC mouse models will be exhaustively characterized through exome sequencing and gene expression profiling. Using bioinformatics techniques including machine- learning and network analysis, these datasets will be compared with human HCC datasets from TCGA and the ICGC to identify subgroups of patients with similar somatic molecular profiles to the HCC mouse models as well as refined minimum sets of characteristic genetic aberrations. A derived transposon system will be used to generate mosaic mouse models replicating human HCC genetic subgroups faithfully. These models will enable 1) experimental dissection of the molecular mechanisms underlying distinct etiologies of HCC, 2) systematic assessment of candidate HCC therapies and 3) investigation of therapeutic resistances. This K99/R00 career development award proposal describes a two-year mentored and three-year independent research program essential for the development of Dr. Font-Burgada as an independent investigator. Dr. Font-Burgada received his PhD at University of Barcelona, Spain, for the work he performed to investigate basic chromatin regulatory and epigenetic mechanisms. He then moved to University of California, San Diego where he joined Dr. Michael Karin's laboratory to train in mouse models of cancer and signal transduction. For the accomplishment of this research proposal, Dr. Font-Burgada has designed a strong training and career development plan consisting of: 1- the continued mentorship of Dr. Michael Karin to gain additional expertise in mouse models of HCC and signal transduction, 2- Training in bioinformatics, specifically in methods to identify cancer driver genetic aberrations and network-based approaches for comparative genomic analysis of mouse and human HCCs, to be overseen by co-mentor Dr. Hannah Carter, an Assistant Professor of Medicine, at UCSD. 3- Training in application of emerging transposon vector technologies to generate mosaic mouse models for in vivo analysis of oncogenic pathways. 4- Career development courses and seminars in a supportive academic environment in the Department of Pharmacology at UCSD to complement other aspects of the training program. This training plan will be overseen by an advisory committee comprising 4 members, mentor, co-mentor, and additional experts in mouse models of cancer and bioinformatics, Inder Verma and Trey Ideker, providing key scientific insights and essential guidance in critical steps in Dr. Font-Burgada transition to independence. PUBLIC HEALTH RELEVANCE: Hepatocellular carcinoma (HCC) has a 5-year survival rate of less than 10% and leads to 700000 deaths globally each year. The genetic causes of this disease are poorly understood and the only available targeted therapy extends life expectancy by a mere 3 months. In order to improve these dismal statistics, I am developing a novel class of mouse models capable of reproducing the spectrum of mutations observed in a given human tumor, to enable study of HCC biology as well as systematic testing of therapeutic combinations.",Development of mosaic mouse models of HCC for genetic interspecies inference,9646340,R00CA191152,"['Advisory Committees', 'Animal Model', 'BRAF gene', 'Beauty', 'Bioinformatics', 'Biology', 'California', 'Cancer Etiology', 'Cancer Model', 'Catalogs', 'Cell Culture Techniques', 'Cells', 'Cessation of life', 'Characteristics', 'Chromatin', 'Clinical', 'Comparative Genomic Analysis', 'Complement', 'Complex', 'Data', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Dissection', 'Doctor of Philosophy', 'Drowsiness', 'Drug Targeting', 'Ensure', 'Environment', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Expression Profiling', 'Gene Expression Profiling', 'Genetic', 'Geography', 'Health', 'Hepatocyte', 'Heterogeneity', 'High Fat Diet', 'Human', 'Human Pathology', 'Investigation', 'K-Series Research Career Programs', 'Knowledge', 'Label', 'Laboratories', 'Life Expectancy', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mosaicism', 'Mouse Protein', 'Mus', 'Mutate', 'Mutation', 'Network-based', 'Oncogenic', 'Orthologous Gene', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Phase', 'Phenotype', 'Physiological', 'Population', 'Pre-Clinical Model', 'Primary carcinoma of the liver cells', 'Reproducibility', 'Research', 'Research Personnel', 'Research Proposals', 'Resistance', 'Risk', 'Risk Factors', 'Signal Transduction', 'Somatic Mutation', 'Spain', 'Subgroup', 'Survival Rate', 'System', 'Techniques', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Therapeutic', 'Training', 'Training Programs', 'Transgenes', 'Translating', 'Universities', 'Viral Vector', 'Work', 'Xenograft Model', 'accurate diagnosis', 'base', 'career development', 'chronic liver disease', 'combinatorial', 'comparative', 'course development', 'design', 'effective therapy', 'exhaustion', 'exome sequencing', 'human disease', 'improved', 'in vivo', 'in vivo Model', 'insight', 'member', 'mouse model', 'neoplastic cell', 'nonalcoholic steatohepatitis', 'novel', 'novel therapeutics', 'patient subsets', 'pre-clinical', 'professor', 'programs', 'public health relevance', 'response', 'statistics', 'success', 'targeted treatment', 'therapeutic evaluation', 'therapy resistant', 'tool', 'tumor', 'tumor progression', 'vector']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R00,2019,249000,-0.01471170853979563
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9751297,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Consumption', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2019,77750,-0.006219890559378438
"IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP The purpose of this contract is to provide bioinformatic support to researchers in the Divisions of National Toxicology Program (DNTP) and Intramural Research (DIR) at the National Institute of Environmental Health Sciences (NIEHS). NIEHS researchers conduct studies that produce large amounts of data, varying in size and complexity. Fields of scientific study are diverse and include toxicology, genomics, transcriptomics, high throughput screening (HTS) data and data extraction from diverse text resources. The variety and complexity of NIEHS scientific studies dictates the need for innovative analytical techniques and the development of new software tools. Bioinformatic data analyses are required to support accurate and precise interpretation of study results. Specific bioinformatics needs include data analysis, data mining, creating bioinformatics pipelines for gene expression and pathway analysis and computational support for the vast amount of data collected through studies conducted at NIEHS and NIEHS contract laboratories. n/a",IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP,9915697,73201700001C,"['Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'ChIP-seq', 'Chemical Exposure', 'Chemicals', 'Contractor', 'Contracts', 'DNA Methylation', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Epigenetic Process', 'Evaluation', 'Exons', 'Gene Expression', 'Genes', 'Genomics', 'Informatics', 'Intramural Research', 'Knowledge', 'Laboratories', 'Literature', 'Measures', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Output', 'Pathway Analysis', 'Peer Review', 'Privatization', 'Programming Languages', 'Proteomics', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scientific Evaluation', 'Scientist', 'Series', 'Software Tools', 'Specific qualifier value', 'Technology', 'Text', 'Toxicogenomics', 'Toxicology', 'analysis pipeline', 'bioinformatics tool', 'bisulfite sequencing', 'cheminformatics', 'computational intelligence', 'data integration', 'data mining', 'differential expression', 'high throughput screening', 'innovation', 'meetings', 'metabolomics', 'method development', 'next generation sequencing', 'physical property', 'programs', 'screening', 'technique development', 'transcriptomics', 'whole genome']",NIEHS,"SCIOME, LLC",N01,2019,2464037,-0.010656831281033894
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9724498,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2019,67704,-0.010306998741517122
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),9788202,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Infrastructure', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Privacy', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data management', 'data portal', 'data sharing', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'preservation', 'social', 'success', 'theories', 'therapy design', 'tool']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2019,114747,-0.0039192263057880065
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9817000,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2019,334891,-0.01267720700458476
"AUGS/DUKE UrogynCREST program PROJECT SUMMARY Health Services Research (HSR) and predictive analytics are rapidly growing fields and will have enormous implications for women’s health research in pelvic floor disorders (PFDs). The AUGS/DUKE Urogynecology Clinical Research Educational Scientist Training (UrogynCREST) program will prepare participants to recognize the critical role that data play in delivering high quality health care. It brings together expertise in health service and women’s health research, medical informatics and prediction modeling. This program will target Urogynecology Faculty at the Assistant Professor level who seek successful careers in health services research (HSR) and analytics. Participants will obtain skills through a combination of didactic and interactive coursework; hands-on manipulation of data through extraction, cleaning, and analysis; and project-based one on one mentoring. The UrogynCREST program will be an interactive, hands-on educational program with centralized activities organized and delivered by distance through a popular on-line learning platform called Sakai, with educational software designed to support teaching, research and collaboration. A diverse faculty with expertise in data sciences teaches courses and the advanced methodology required to perform HSR. Yearly in-person meetings at the annual American Urogynecologic Society meeting enhance networking and the development of partnerships between participants from various institutions, as well as, interactions with the mentors and other HSR in the field. The program’s strategy allows national leaders with particular skills in the field to provide their knowledge to the participants and help mentor them through development of a relevant research question and identification of an appropriate and existing database(s) to address the question. With the guidance of a dedicated statistician and analyst programmer, participants will learn and perform the necessary computer programming needed to extract, clean and analyze these data. Participants whose projects involve the development of prediction models in the form of scores, nomograms or other tools will learn how to build and validate such tools in the existing project. Each participant’s project will culminate in the completion of a submitted manuscript to a peer- reviewed journal or study proposal and publicly available tools when relevant. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and resources for invigorating data discovery and tools for investigations in HSR specifically addressing (PFDs). PROJECT NARRATIVE: The AUGS/DUKE UrogynCREST program will prepare participants to recognize the critical role that data play in delivering high quality health care for pelvic floor disorders. It will add structure to the health data science education for Assistant Professor Level Faculty in Urogynecology by bringing together expertise in health service and women’s health research, medical informatics, and prediction modeling. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and mentorship for invigorating data discovery and tools for investigations in health service research specifically addressing pelvic floor disorders.",AUGS/DUKE UrogynCREST program,9703267,R25HD094667,"['Address', 'Age', 'American', 'Area', 'Caring', 'Clinical Research', 'Collaborations', 'Communities', 'Connective Tissue', 'Data', 'Data Discovery', 'Data Science', 'Databases', 'Development', 'E-learning', 'Educational process of instructing', 'Faculty', 'Fecal Incontinence', 'Fostering', 'Future', 'Goals', 'Health Services', 'Health Services Research', 'Healthcare', 'Infrastructure', 'Institution', 'Instruction', 'Investigation', 'Journals', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Manuscripts', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modernization', 'Muscle', 'Nomograms', 'Participant', 'Peer Review', 'Pelvic Floor Disorders', 'Pelvis', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Public Health', 'Research', 'Resources', 'Role', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Shapes', 'Societies', 'Software Design', 'Structure', 'Techniques', 'Testing', 'Training Programs', 'Urinary Incontinence', 'Woman', 'Women&apos', 's Health', 'base', 'career', 'clinical decision-making', 'clinical development', 'computer program', 'computer science', 'design', 'health care quality', 'health data', 'improved', 'injured', 'innovation', 'meetings', 'pelvic organ prolapse', 'predictive modeling', 'professor', 'programs', 'recruit', 'science education', 'skills', 'social', 'tool']",NICHD,DUKE UNIVERSITY,R25,2019,161462,-0.014486999503101937
"COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data",9938885,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'Intelligence', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'commune', 'computational platform', 'computer framework', 'computing resources', 'connectome', 'cost', 'data anonymization', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'preservation', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2019,585151,-0.0017209774338024072
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,9676860,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Simulation', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2019,7168390,0.0001349757359138521
"Mapping host-microbe-metabolite interactions in 3D to find diet-derived enhancers of immunity Project Summary Antimicrobial resistance is an increasing problem, and current drug pipelines are not keeping pace with the rise of antimicrobial resistance. An alternative strategy is to boost host immunity. An often overlooked side-effect of the vitamin and mineral supplementation projects of the 1940s is that these supplements greatly reduced infectious disease burden. Recent work has shown that further gains may be possible, especially in adding phytochemicals back to highly processed diets typically consumed in the United States. However, we lack a fundamental understanding of how these components are processed by the microbiome, and how diet-derived molecules, microbiome and host immune system work together to resist infectious disease. A key barrier preventing us from making these discoveries is that each individual assay (microbiome, host gene expression metabolome, dietary compounds) is expensive and highly multivariate. Three key insights that enable the current project are the miniaturization of DNA and RNA sequencing assays on advanced nanoliter- scale liquid handling robots, greatly reducing the cost, the combination of untargeted and targeted mass spectrometry on the same samples in high throughput to enable discovery of a much greater chemical space, and the ability to use explicitly spatial maps on multiple scales to integrate the dataset throughout the body and enable both visual analytics and deep learning approaches based on spatial data. These breakthroughs will provide a fundamentally new understanding of how dietary metabolites promote disease resistance, and will allow us to develop a new infrastructure to integrate results from many investigators in different laboratories studying various aspects of these systems. Additionally, the results will allow us to choose biomaterials and biomarkers in human subjects that provide maximum information about internal nutritional and immune status. Results will be tested against the NHANES and American Gut cohorts. The results of this project will therefore be: 3D maps of mouse models showing how the microbiome, diet, and host gene expression produce immunity; an infrastructure for creating and sharing these maps; and a preliminary test of whether the results extend to large human populations. Project Narrative The public health relevance of this proposal is that it will provide an infrastructure for analyzing impacts of dietary components and microbiomes throughout the body. The analysis of these maps will help us identify dietary components that improve resistance to infectious disease.",Mapping host-microbe-metabolite interactions in 3D to find diet-derived enhancers of immunity,9785812,DP1AT010885,"['3-Dimensional', 'American', 'Antimicrobial Resistance', 'Back', 'Biocompatible Materials', 'Biological Assay', 'Biological Markers', 'Chemicals', 'Communicable Diseases', 'Consumption', 'DNA sequencing', 'Data', 'Data Set', 'Diet', 'Dietary Component', 'Disease Resistance', 'Enhancers', 'Gene Expression', 'Human', 'Immune system', 'Immunity', 'Individual', 'Infrastructure', 'Laboratory Study', 'Liquid substance', 'Maps', 'Mass Spectrum Analysis', 'Metabolite Interaction', 'Microbe', 'Minerals', 'Miniaturization', 'National Health and Nutrition Examination Survey', 'Nutritional status', 'Pharmaceutical Preparations', 'Phytochemical', 'Population', 'Process', 'Research Personnel', 'Resistance', 'Robot', 'Sampling', 'Supplementation', 'System', 'Testing', 'United States', 'Visual', 'Vitamins', 'Work', 'base', 'burden of illness', 'cohort', 'cost', 'deep learning', 'human subject', 'immunological status', 'improved', 'insight', 'metabolome', 'microbiome', 'mouse model', 'nanolitre scale', 'prevent', 'public health relevance', 'side effect', 'transcriptome sequencing']",NCCIH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",DP1,2019,1085000,-0.006749430494026114
"Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions Project Summary/Abstract DNA sequencing has spawned the “microbiome revolution” -- thousands of microbes and a dizzying number of microbial interactions that are associated with human health and disease. Unfortunately, most species in the microbiome are known only by a (partial) genome. The limited phenotypic data on newly discovered bacteria reveal species that behave unlike any of our model organisms. While genome-scale modeling plays an important role in understanding the microbiome, the paucity of phenotypic data for most species prevents detailed simulation of the microbial communities that affect our health. This project will develop an automated system for profiling, synthesizing, and modeling microbial communities. The center of our approach is Deep Phenotyping, an automated robotic platform that performs complex growth experiments on demand. Data from Deep Phenotyping will be used to train metabolic and statistical models of the oral pathogens Streptococcus mutans and Candida albicans to predict conditions that keep both microbes in a nonpathogenic state. Project Narrative The microbiome revolution has uncovered thousands of species of bacteria with roles in health and disease. This project automates the identification of interactions between environments, genes, and the microbes that live in and around us. Understanding these interactions is a critical step in re-engineering the microbiome to improve human health.","Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions",9651956,R21EB027396,"['Affect', 'Animal Model', 'Antibiotics', 'Area', 'Bacteria', 'Biochemical Pathway', 'Bioinformatics', 'Biological', 'Biological Assay', 'Candida albicans', 'Carbon', 'Coculture Techniques', 'Collaborations', 'Combinatorics', 'Communities', 'Complex', 'Computer software', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Disease', 'Engineering', 'Environment', 'Environmental Risk Factor', 'Exposure to', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Human Microbiome', 'Hybrids', 'Image', 'Individual', 'Knock-out', 'Knowledge', 'Link', 'Liquid substance', 'Machine Learning', 'Maps', 'Metabolic', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral candidiasis', 'Pathway interactions', 'Phenotype', 'Play', 'Positioning Attribute', 'Regulator Genes', 'Research Personnel', 'Robotics', 'Role', 'Shapes', 'Source', 'Statistical Models', 'Streptococcus mutans', 'Structure', 'System', 'Technology', 'Time', 'Training', 'Work', 'base', 'combinatorial', 'cost', 'design', 'dysbiosis', 'experience', 'experimental study', 'fitness', 'fungus', 'genome-wide', 'improved', 'instrumentation', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'network models', 'open source', 'oral pathogen', 'pathogenic fungus', 'phenotypic data', 'preference', 'prevent', 'screening', 'simulation', 'transcription factor', 'transcriptome sequencing', 'transposon sequencing']",NIBIB,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R21,2019,219161,-0.00882375188880822
"Joint Analysis of Microbiome and Other Genomic Data Types PROJECT SUMMARY In the same way that the human genome project created invaluable genomic maps, the objective of this project is to develop methods for eventual construction of comprehensive genetic and metabolomic by microbome relationship maps. Such maps would be an invaluable resource for improving our understanding as to the underlying mechanisms by which microbes and –omics features influence human diseases and conditions, potentially leading to identification of novel therapeutic targets. To these ends, this proposal seeks to develop statistical and computational tools for mapping associations and interactions between microbes and other – omic features and for further utilizing other –omics to improve microbiome based prediction models. Specifically, motivated by studies examining the role of the vaginal microbiome and other –omics in birth outcomes and menopause, we aim to develop statistical methodology for (1) mapping genetic variants that influence microbiome composition so as to understand the innate component of the microbiome as well as learn mechanisms by which genetics influence outcomes; (2) creating global metabolic maps integrating both microbes and metabolites which will enable understanding of how perturbations might influence the system and identify key pathways for therapeutic target; (3) exploiting other –omics in constructing more accurate microbiome based prediction models for preterm birth; (4) developing, distributing and supporting software packages for the proposed methods. The methods are based on frameworks in which we have considerable experience, but novel technical contributions are made to accommodate features of the data such as population stratification and relatedness in genetics, phylogenetic structure, and compositionality, as well as practical considerations such as availability of samples and other –omics data. Consequently, these new methods have the potential for accelerating mechanistic and translational microbiome studies, developing vital resources for enabling systematic achievement of many biological, clinical, and public health problems that have eluded researchers for decades. PROJECT NARRATIVE The methods developed in this proposal will enable improved understanding of the interactions between microbes and other –omics, thus aiding in elucidation of the mechanisms by which microbes and –omic features influence health outcomes and aiding in identification of potential molecular targets. Further emphasis is placed on utilization of other –omics to develop microbiome based prediction models in pregnancy outcomes, improving early detection of women who are at risk of preterm delivery.",Joint Analysis of Microbiome and Other Genomic Data Types,9763572,R01GM129512,"['Achievement', 'Address', 'Area', 'Automobile Driving', 'Biological', 'Birth', 'Chromosome Mapping', 'Clinical', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Early Diagnosis', 'Future', 'Genes', 'Genetic', 'Genomics', 'Grouping', 'Health', 'Human Genome Project', 'Joints', 'Lasso', 'Learning', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Menopause', 'Metabolic', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Modification', 'Molecular Target', 'Network-based', 'Outcome', 'Pathway interactions', 'Performance', 'Phylogenetic Analysis', 'Pregnancy Outcome', 'Premature Birth', 'Procedures', 'Public Health', 'Quantitative Trait Loci', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'Sampling', 'Scientific Advances and Accomplishments', 'Structure', 'System', 'Taxonomy', 'Training', 'Woman', 'Work', 'base', 'biological systems', 'computerized tools', 'experience', 'genetic variant', 'genomic data', 'human disease', 'improved', 'interest', 'loss of function', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiome analysis', 'microbiome components', 'microbiome composition', 'microbiome research', 'new therapeutic target', 'novel', 'open source', 'population stratification', 'predictive modeling', 'simulation', 'software development', 'therapeutic target', 'tool', 'translational study', 'user friendly software', 'vaginal microbiome']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,396000,-0.012579079785698019
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9724344,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Streptococcus vaccine', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2019,872121,-0.007978375344881931
"Dopaminergic encoding of counterfactual information in human striatum ﻿    DESCRIPTION (provided by applicant): Diseases and disorders directly affected by dopamine systems (e.g., drug addiction and Parkinson's disease) highlight the importance of these systems in motivated human behavior and cognition. The dopamine system is known to be a critical component of normal learning, reward processing, and decision-making (reviewed in Montague et al., 2004). Unfortunately, our present knowledge of dopamine systems in human brains is relatively sparse compared to the wealth of experimentation and computational modeling on these systems in rodents and non-human primates. Previously, technological constraints have limited direct experimentation in human brains. This proposal capitalizes on our group's recent technological innovation, which was supported by the NIH R21 mechanism - CEBRA: R21DA024140 - and resulted in the successful completion of the first sub-second measurements of dopamine release in a human brain. Furthermore, these measurements took place during an active decision-making task that was framed by computational models of learning and reward processing (Kishida et al., 2011 and Kishida et al., under review). We propose to pursue three specific aims, which combine our technological advance with active learning tasks designed to probe the role of dopamine in human behavior. Our aims incorporate three learning signals, where actual and counterfactual experience will each be examined in human striatal responses. The proposed work will inform on the controversial role for dopamine in reward/movement interactions. The experiments proposed will yield unprecedented insight into the function of the dopamine system in the humans afflicted with Parkinson's disease and Essential Tremor. With the support of the NIH (R21DA024140), our team successfully developed a complete prototype system for making electrochemical measurements of dopamine delivery in the human brain. Feasibility has been demonstrated by obtaining the first dopamine measurements in the striata of subjects with Parkinson's during a decision-making task. This substantial preliminary work is now ready for a larger scale with specific hypothesis testing about the role of dopamine systems in Parkinson's disease, Essential tremor, and human decision-making and behavior. PUBLIC HEALTH RELEVANCE: Experiments and computational models (primarily investigated in rodents and non-human primates) suggest dopamine systems in the brain are essential for normal learning, reward-processing, and ongoing decision- making. This proposal builds upon our group's innovative work, which - for the first time - recorded sub- second measurements of dopamine in human subjects and amalgamated these measurements with computational models of dopamine function. Herein we propose to use this newly developed technology to generate unprecedented insight into the function of dopamine systems directly in humans.",Dopaminergic encoding of counterfactual information in human striatum,9749260,R01NS092701,"['Active Learning', 'Acute', 'Affect', 'Animal Model', 'Behavior', 'Brain', 'Brain Diseases', 'Cells', 'Cessation of life', 'Cognition', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Coupling', 'Data', 'Decision Making', 'Disease', 'Dopamine', 'Drug Addiction', 'Electrodes', 'Essential Tremor', 'Exploratory/Developmental Grant', 'Funding', 'Human', 'Knowledge', 'Learning', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement', 'Movement Disorders', 'Operative Surgical Procedures', 'Parkinson Disease', 'Participant', 'Patients', 'Periodicity', 'Phase', 'Play', 'Replacement Therapy', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Sample Size', 'Scanning', 'Signal Transduction', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'blood oxygenation level dependent response', 'cognitive function', 'design', 'dopamine system', 'dopaminergic neuron', 'experience', 'experimental study', 'extracellular', 'human subject', 'implantation', 'innovation', 'insight', 'nonhuman primate', 'prototype', 'public health relevance', 'putamen', 'reduce symptoms', 'relating to nervous system', 'response', 'reward processing', 'social', 'symptomatic improvement', 'technological innovation', 'temporal measurement']",NINDS,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2019,564964,-0.0040301608457890626
"Pro-inflammatory activation of human macrophages regulated by lncRNAs Project Summary Macrophage activation promotes major inflammatory disorders, including arterial diseases. Its underlying mechanisms, however, remain obscure. The present study will establish a systems approach, involving computational prediction analyses, multi-omics, network science, and in vitro and in vivo validation, to discover long noncoding RNA (lncRNA)-mediated mechanisms for pro-inflammatory activation of macrophages and arterial disease. In Specific Aim 1, we will involve omics studies of human macrophages to identify lncRNAs and their interacting proteins and develop computational analyses to predict human lncRNAs that regulate macrophage activation. Specific Aim 2 will examine the functionality of candidate lncRNAs in macrophage activation in vitro and in vivo. The findings from the study will help to identify new mechanisms for macrophage activation and may provide molecular bases for new therapies. Project Narrative Inflammation plays a key role in coronary artery disease and other major vascular diseases, global health threats. Even with potent risk modifiers, e.g., statins, many patients still suffer vascular events. Long noncoding RNAs (lncRNAs) regulate various biological processes. We aim to discover lncRNAs that promotes vascular inflammation. The potential outcomes will offer new targets for much needed therapies for vascular diseases.",Pro-inflammatory activation of human macrophages regulated by lncRNAs,9840015,R01HL149302,"['Address', 'Biological', 'Biological Process', 'Biology', 'Blood', 'Blood Vessels', 'Cells', 'Communities', 'Complex', 'Computational Biology', 'Computer Analysis', 'Coronary Arteriosclerosis', 'Data', 'Development', 'Discipline', 'Disease', 'Drug usage', 'Endotoxemia', 'Event', 'Gene Expression Profiling', 'Goals', 'Hematopoietic Stem Cell Transplantation', 'Heterogeneity', 'Human', 'In Vitro', 'Inflammation', 'Inflammatory', 'Laboratories', 'Lesion', 'Leukocytes', 'Life', 'Link', 'Machine Learning', 'Macrophage Activation', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Myocardial', 'NF-kappa B', 'Network-based', 'Outcome', 'Pathway Analysis', 'Patients', 'Plasma', 'Play', 'Protein Analysis', 'Proteins', 'Proteomics', 'RNA', 'Reporting', 'Residual state', 'Risk', 'Risk Factors', 'Role', 'Science', 'Signal Transduction', 'Small Interfering RNA', 'Splenocyte', 'System', 'Systems Biology', 'Tissues', 'Untranslated RNA', 'Validation', 'Vascular Diseases', 'arterial lesion', 'base', 'cytokine', 'experimental study', 'femoral artery', 'gain of function', 'global health', 'human disease', 'humanized mouse', 'in vivo', 'injured', 'loss of function', 'macrophage', 'modifiable risk', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'overexpression', 'protein protein interaction', 'single cell analysis', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'vascular inflammation']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2019,767374,-0.013103874768953293
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9690782,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Dysregulation', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'bioinformatics tool', 'circadian', 'circadian regulation', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'informatics\xa0tool', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2019,328155,-0.005138159122516431
"Computational and Experimental Resources for Virome Analysis in Inflammatory Bowel Disease (CERVAID) SUMMARY  While the role of the bacterial microbiome in human health and disease is well established, few studies have evaluated the contribution of the virome. Recently, we demonstrated that alterations in the enteric virome in adulthood are associated with diseases such as inflammatory bowel disease (IBD) and AIDS. In a cross- sectional comparison of IBD cases and household controls, a significant expansion of the Caudovirales, an order of phages, and anelloviruses, a family of eukaryotic DNA viruses, was observed. Advancing understanding of the IBD virome beyond this finding is limited by: (1) A lack of well defined longitudinal human cohorts to enable discovery of temporal associations of the virome with health and disease; (2) The challenge of viral “dark matter”. Dark matter refers to the typically >50% of the sequences present in purified virus preparations cannot be classified due to a lack of statistically significant alignment to known reference sequences. Thus, current virome studies effectively assess < 50% of the virome, thereby compromising our ability to detect important associations between the virome and disease; (3) Inadequate experimental systems to manipulate the virome. Although sequencing has identified many novel eukaryotic viruses, there are only cell culture systems for a limited number of viruses; moreover, there are no small animal infection models for newly described viruses. In addition while a tremendous diversity of phage has been identified, only a tiny fraction have known hosts and an even smaller fraction has been cultured. Thus, there are significant barriers that must be overcome to be able to experimentally test the impact of either eukaryotic viruses or phages in murine IBD models. These barriers to understanding the role of the IBD virome will be addressed as follows: Aim 1 will define the enteric virome and bacterial microbiome in a longitudinal cohort of IBD patients and household controls and identify virome associations with IBD. In Aim 2 novel computational tools to identify and characterize viruses present in enteric viromes will be developed, including approaches to classify dark matter. In Aim 3 novel experimental systems for functional virome analysis, including novel cultures of both eukaryotic viruses and phages as well as animal infection models, will be developed with the end goal of evaluating causal roles for the viruses and phage in existing muring IBD models. Together, these Aims will not only address significant barriers in understanding of IBD, but will provide a wealth of tangible computational and experimental resources to advance the general field of virome studies.   NARRATIVE The overall goal of this project is to develop novel computational and experimental tools to address challenges in understanding the role of viruses in inflammatory bowel disease. These tools will facilitate studies of not only inflammatory bowel disease, but also broader studies of the relationship of the human virome to other diseases.  ",Computational and Experimental Resources for Virome Analysis in Inflammatory Bowel Disease (CERVAID),9707307,RC2DK116713,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adult', 'Animal Model', 'Animals', 'Bacteria', 'Bacterial Infections', 'Bacteriophages', 'Bioinformatics', 'Biology', 'Caudovirales', 'Cell Culture System', 'Cell Culture Techniques', 'Chronic', 'Classification', 'Clinical', 'Communities', 'Computer Analysis', 'Computer software', 'DNA Viruses', 'Databases', 'Defect', 'Development', 'Digestive System Disorders', 'Disease', 'Disease model', 'Enteral', 'Family', 'Feces', 'Genome', 'Gnotobiotic', 'Goals', 'Health', 'High-Throughput Nucleotide Sequencing', 'Household', 'Human', 'Immune system', 'Infection', 'Inflammatory Bowel Diseases', 'Longitudinal cohort', 'Machine Learning', 'Metabolic Diseases', 'Metadata', 'Metagenomics', 'Modeling', 'Mus', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Norovirus', 'Open Reading Frames', 'Parasitic infection', 'Pathogenesis', 'Pathogenicity', 'Pathology', 'Patients', 'Play', 'Population', 'Preparation', 'Resource Development', 'Resources', 'Ribosomal RNA', 'Role', 'System', 'Taxonomy', 'Testing', 'Time', 'Viral', 'Viral Genes', 'Viral Genome', 'Virus', 'Virus Diseases', 'bacterial resistance', 'bacteriome', 'base', 'case control', 'cohort', 'computerized tools', 'contig', 'dark matter', 'early childhood', 'experimental analysis', 'genome annotation', 'gut microbiome', 'human virome', 'improved', 'metagenome', 'microbial', 'molecular sequence database', 'multidisciplinary', 'novel', 'phenomenological models', 'protein function', 'software development', 'stool sample', 'tool', 'virology', 'virome']",NIDDK,WASHINGTON UNIVERSITY,RC2,2019,1872216,-0.0008730562971066025
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9785367,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2019,247413,0.0066395078112832595
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,-0.026243741200565565
"Dietary modulation of gut microbiome and host gene expression across human evolution and the emergence of modern human disease Evidence suggests that lifestyle changes, concordant with the adoption of agriculture and industrialization, have impacted the emergence of the so-called diseases of modern civilization in humans (e.g. metabolic disorders, cardiovascular disease etc.). The incidence of these diseases in contemporary, industrialized populations is believed to be associated with a lack of adaptation of our genomes to the rapid dietary and lifestyle changes that occurred across human evolution. However the usefulness and resolution of this evolutionary model of disease are limited. Moreover, although the dietary and genetic markers of human evolution have been studied, we still lack understanding on how the microbiome, our second genome, has interacted with nutritional and host- genomic axes to confer increased disease risk in modern humans. Preliminary data by our group show that dietary shifts significantly modulate the gut microbiome and metabolome of wild primates, our closest evolutionary relatives. Additionally, we have identified gut microbiome markers only found in populations representing Paleolithic lifestyles (hunter-gatherers) and distinguishing them from traditional agriculturalists and industrialized populations. Thus, given 1) the potential role of diet in human evolution, 2) the critical impact of the gut microbiome on the nutritional and immune landscape of mammals, and 3) the existence of gut microbiome patterns exclusive of hunter-gatherers, we hypothesize that the emergence of metabolic disease in modern humans was significantly mediated by interactions between diet, the gut microbiome and the human genome across evolution. These issues are still unexplored. Thus, in Aim 1 of this proposal we will use a multi- OMIC approach (gut metabolomics, metagenomics and transcriptomics of the host colonic tissue) to identify metabolic and genetic markers that emerged and/or were lost when humans transitioned from hunter-gatherer to agricultural and industrialized lifestyles, and in humans affected by metabolic disease phenotypes. In Aim 2, we will use integrated meta-OMICs and network theory approaches to predict metabolic disease phenotypes, from hunter-gatherers to, populations in transition to agriculture to modern populations at risk. This system-level study will broaden our understanding of the extrinsic (environmental/nutritional) and intrinsic factors (genetic/metabolic) impacting the evolution of modern human disease. Additionally, the evolutionary approach proposed will shed light on potentially novel diet and microbe-based translational strategies to mitigate the incidence of metabolic disease in contemporary human populations. PROJECT NARRATIVE The high incidence of metabolic disorders (disorders of glucose, lipid and energy metabolism) is a significant public health threat in industrialized populations, affecting up to 25% of the adult population. Despite extensive work on characterizing the nutritional and genetic backgrounds of common metabolic disorders, we still have limited understanding as to how these factors interact with each other and with the gut microbiome, our second genome. This proposal interrogates the evolutionary baseline of modern human disease by exploring associations between nutritional, (host)genetic and microbiome markers in hunter-gatherers, traditional agriculturalists and industrialized human populations susceptible to metabolic disorders. The implementation of an evolutionary, system-level model improves our understanding of modern human disease, and, validates existing and novel dietary interventions to lessen their incidence in industrialized societies.",Dietary modulation of gut microbiome and host gene expression across human evolution and the emergence of modern human disease,9518889,R01DK112381,"['Adoption', 'Adult', 'Affect', 'African', 'Agriculture', 'Americas', 'C-reactive protein', 'Cardiovascular Diseases', 'Civilization', 'Data', 'Diet', 'Dietary Intervention', 'Disease', 'Disease model', 'Energy Metabolism', 'Evolution', 'Expression Profiling', 'Feces', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genome', 'Genomics', 'Glucose Metabolism Disorders', 'Glycosylated hemoglobin A', 'Goals', 'High Density Lipoproteins', 'Human', 'Human Genome', 'Immune', 'Incidence', 'Industrialization', 'Inflammation', 'Inflammatory', 'Intrinsic factor', 'Jamaica', 'Life Style', 'Light', 'Machine Learning', 'Mammals', 'Measures', 'Mediating', 'Metabolic', 'Metabolic Diseases', 'Metabolic Marker', 'Metabolic syndrome', 'Metagenomics', 'Microbe', 'Modeling', 'Modernization', 'Nutrient', 'Nutritional', 'Pattern', 'Population', 'Populations at Risk', 'Primates', 'Public Health', 'Resolution', 'Risk Factors', 'Role', 'Sampling', 'Societies', 'Structure', 'System', 'TNF gene', 'TNFRSF1A gene', 'Therapeutic Intervention', 'Tissue-Specific Gene Expression', 'Tissues', 'Triglycerides', 'Trinidad', 'Variant', 'Work', 'base', 'cohort', 'density', 'disease phenotype', 'disorder risk', 'fecal metabolome', 'glycemic control', 'gut microbiome', 'human disease', 'improved', 'learning strategy', 'lipid metabolism', 'metabolome', 'metabolomics', 'metagenome', 'microbial', 'microbial host', 'microbiome', 'microbiome composition', 'multiple omics', 'novel', 'nutrition related genetics', 'theories', 'transcriptome sequencing', 'transcriptomics', 'translational approach']",NIDDK,"J. CRAIG VENTER INSTITUTE, INC.",R01,2019,920435,-0.02157019210066562
"Mathematical ecology models of host-microbiota interaction in auto microbiota transplants (auto-FMT) Project Summary Mathematical ecology models of host-microbiota interaction  in auto microbiota transplants (auto-FMT) We aim to develop mathematical models for the rational design of microbiota transplants that can restore compositional diversity and function to the damaged microbiota of antibiotic-treated patients. We will focus on hospitalized cancer patients receiving allogeneic hematopoietic stem cell transplants (allo-HSCT). Allo-HSCT is a potentially curative cancer treatment that compromises the immune system, and requires that patients receive massive antibiotic treatments to prevent and treat life-threatening infections. We will build on a vast clinical database, in vitro experiments in bioreactors and in vivo experiments with mice to develop dynamic mathematical models that describe how antibiotics cause changes in the microbial composition, and how that can impact the recovery of the host's immune system after allo-HSCT. The model expands approaches pioneered by our team—the Generalized Lotka Volterra Ecological Regression (GLOVER) and agent-based models—towards a model that can assist in the development of microbiota therapies for patients undergoing allo-HSCT.  In aim 1 we will use data from a unique clinical resource available at the Memorial Sloan Kettering Cancer center—a sample bank obtained from >1,500 allo-HSCT patients (including microbiome 16S rRNA and shotgun sequencing) and extensive clinical metadata (including time series of complete blood counts and time and doses of all drugs given while the patients are hospitalized); we will also use data from a first-of-its-kind controlled randomized trial of autologous fecal microbiota transplant (auto-FMT) undergoing in allo-HSCT patients. We will use these unique resources to parameterize our models and investigate how the microbiota composition influences the recovery of the host immune system. In aim 2 we will validate the microbial component of our mathematical model using experimental data from anaerobic laboratory reactors that recreate—in vitro—the human microbiota dynamics during antibiotic treatment and auto-FMT in the absence of a living host. In aim 3 we will develop mouse models to investigate those same microbiota dynamics experimentally but now in the context of a living host.  The data obtained from these clinical studies, in vitro experiments and in vivo models will refine our mathematical models in close cycles of simulation and quantitative experimentation. Our ultimate goal is to develop models that can define optimal microbial cocktails and reconstitute the perturbed microbiota of allo- HSCT patients. In the process we hope to uncover general principles of microbiota ecology for future therapies in other patient populations whose microbiota is damaged by antibiotic treatments. Project Narrative Mathematical ecology models of host-microbiota interaction  in auto microbiota transplants (auto-FMT) The intestinal microbiota is an ecosystem with thousands of bacterial species that interact with each other and with their living host. We seek to develop and validate new mathematical models of host- microbiota ecology—using clinical data from hospitalized patients, in vitro experiments with bioreactors and in vivo experiments with mouse models—towards our ultimate goal of a predictive model that can assist in the rational design of microbiota therapies.",Mathematical ecology models of host-microbiota interaction in auto microbiota transplants (auto-FMT),9738403,R01AI137269,"['Address', 'Allogenic', 'Anaerobic Bacteria', 'Antibiotic Prophylaxis', 'Antibiotic Therapy', 'Antibiotics', 'Autologous', 'Bioreactors', 'Blood Cells', 'Cancer Patient', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clostridium difficile', 'Communities', 'Complement', 'Complete Blood Count', 'Complication', 'Computational Technique', 'Data', 'Data Science', 'Data Set', 'Databases', 'Development', 'Dose', 'Ecology', 'Ecosystem', 'Engraftment', 'Enrollment', 'Experimental Models', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Health', 'Hematopoietic', 'Hematopoietic Stem Cell Transplantation', 'Human', 'Immune system', 'In Vitro', 'Infection', 'Intervention', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Mathematics', 'Medical', 'Memorial Sloan-Kettering Cancer Center', 'Metadata', 'Modeling', 'Monitor', 'Mus', 'Neutropenia', 'Patients', 'Pharmaceutical Preparations', 'Play', 'Process', 'Randomized', 'Randomized Controlled Trials', 'Recovery', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Series', 'Shotgun Sequencing', 'Stem cells', 'Time', 'Transplantation', 'Validation', 'base', 'cancer therapy', 'clinical database', 'commensal bacteria', 'design', 'experimental study', 'fecal transplantation', 'gut microbiota', 'host microbiome', 'host microbiota', 'human microbiota', 'improved', 'improved outcome', 'in vivo', 'in vivo Model', 'indexing', 'mathematical model', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'microbiota transplantation', 'microorganism interaction', 'mortality', 'mouse model', 'next generation', 'patient population', 'patient subsets', 'predictive modeling', 'prevent', 'prophylactic', 'reconstitution', 'response', 'simulation', 'success']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,R01,2019,580221,-0.005637672488334788
"Fluomics: The Next Generation Influenza A virus is a major human respiratory pathogen, and available vaccines and antivirals are of limited efficacy. In order to identify novel targets for therapeutic intervention during influenza virus infection, we have assembled an interdisciplinary team that uses a highly integrated systems level approach to identify and validate key genes/networks involved in virus pathogenesis. The overarching theme of our multidisciplinary proposal “FluOMICS: The NEXT Generation” is to obtain multiple OMICS-based systems level measurements and integrate them using modeling approaches and machine learning algorithms to identify and validate 1) host-virus networks that modulate influenza A virus disease severity, 2) biomarkers in blood that reflect the activation states of these networks and 3) novel host targets for therapeutic interventions. Our underlying main hypothesis is that host networks involved in viral replication and early host responses regulate disease outcomes and represent targets for therapeutic intervention. The proposed studies leverage on our previous collaborations that generated global datasets and models that predict severity of disease caused by three influenza virus strains with different levels of virulence. While our previous studies gave many novel insights in influenza pathogenesis, they likely provide a narrow window on the determinants of disease severity in humans. Thus, it is necessary expand beyond the specific virus strains that were used to study pathogenesis, and explore a broader context of viral and host perturbations linked to clinical outcomes. In order to identify clinically relevant networks involved in influenza virus pathogenesis we propose to integrate into predictive and comprehensive models global responses during influenza virus infection in three systems 1) human blood from a human cohort of patients with documented influenza virus infection and diverse clinical outcomes (Project 1); 2) mouse blood and tissues from experimentally infected animals under a variety of conditions and perturbations resulting in diverse disease outcomes (Project 1) and 3) relevant primary human cells (Project 2). Samples will be processed and send to the Technology Core for global transcriptomics, proteomics and metabolomics analysis. OMICS data sets will be integrated and compared by the Modeling Core to generate network models of disease, uncover blood biomarkers and identify key drivers as targets for therapeutic intervention. Predicted network regulators will be used as a source for subsequent iterative rounds of perturbations to refine existing and to identify new network disease models. Data and models will be managed and disseminated by the Data Management and Bioinformatics Core. We expect that these studies will uncover and validate novel pathogenic networks, blood biomarkers associated with them, and specific therapeutic targets to revert pathogenic networks. In summary, our modeling approaches will find correlates and associations between diverse experimental systems that will help us define human blood biomarkers, and link them to in vivo and ex vivo signatures for both companion diagnostics and personalized therapies. We propose a systematic approach (FluOMICS) to generate predictive models of influenza virus pathogenesis which will a) allow us to identify biomarkers for predicting disease outcome, and b) provide avenues to explore for new, host-directed, therapeutic interventions.",Fluomics: The Next Generation,9627935,U19AI135972,"['Animals', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Biological Markers', 'Blood', 'CRISPR screen', 'Cells', 'Clinical', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Complementary DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Disease', 'Disease Outcome', 'Disease model', 'Environmental Risk Factor', 'Experimental Models', 'Genes', 'Genetic Polymorphism', 'Human', 'Immune response', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A virus', 'Integration Host Factors', 'Intervention', 'Link', 'Lung', 'Lung diseases', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Profiling', 'Mus', 'Network-based', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Post-Translational Protein Processing', 'Predisposition', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Sampling', 'Severities', 'Severity of illness', 'Small Interfering RNA', 'Source', 'System', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic Intervention', 'Tissues', 'Vaccines', 'Viral', 'Viral Pathogenesis', 'Virulence', 'Virus', 'Virus Diseases', 'Virus Replication', 'base', 'clinically relevant', 'cohort', 'companion diagnostics', 'data integration', 'data management', 'early detection biomarkers', 'epigenome', 'epigenomics', 'functional genomics', 'human pathogen', 'in vivo', 'influenza virus strain', 'influenzavirus', 'insight', 'machine learning algorithm', 'metabolome', 'metabolomics', 'mouse model', 'multidisciplinary', 'multiple omics', 'network models', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'personalized medicine', 'predictive marker', 'predictive modeling', 'programs', 'protein protein interaction', 'resilience', 'respiratory', 'response', 'specific biomarkers', 'targeted treatment', 'therapeutic target', 'transcriptome', 'transcriptomics']",NIAID,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U19,2019,2648073,0.007618075548388424
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",9919259,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Work', 'base', 'cell type', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,600000,-0.023802396514722077
"Genome Based Influenza Vaccine Strain Selection using Machine Learning No abstract available PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection using Machine Learning,10044945,R01AI116744,[' '],NIAID,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2019,147704,-0.00852252146051613
"PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry Project Summary Paralleling the growth of neuroscience research, there has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. Such sharing, evaluation, and integration are necessary if computational modeling efforts are to be useful not only in generating reliable and accurate accounts of how brain subsystems operate, but also of how they interact to give rise to higher cognitive functions, and how disruptions of such interactions may give rise to disturbances of mental function observed in psychiatric and neurological disorders. This proposal seeks to meet this need by developing PsyNeuLink: an open source, Python-based software environment that makes it easy to create new models, import and/or re-implement existing ones, integrate these within a single software environment that will facilitate head-to-head comparison of comparable models, the assembly of complementary models into system-level models, and serve as a common repository for the documentation and dissemination of such models for both research and didactic purposes (i.e., publication, education, etc.). These goals will be pursued under two Specific Aims: 1) Extend the scope of modeling efforts that PsyNeuLink can accommodate by: i) enhancing its application programmer interface (API) used to add new components and interfaces to statistical analysis tools and other modeling environments (such as PyTorch, Emergent and ACT-R; ii) enriching its Library by adding PsyNeuLink implementations of influential models of neural subsystems; and iii) developing a publicly available workbook of simulation exercises as both an introduction to PsyNeuLink and for use in Cognitive Neuroscience and Computational Psychiatry curricula. 2) Accelerate PsyNeuLink by developing a custom compiler that preserves its simplicity and flexibility, while dramatically increasing its speed, to make it suitable for simulation of large and complex system-level models, and for parameter estimation, model fitting, and model comparison. This project will exploit the power and accelerating use of Python, and modern just-in-time compilation methods to develop a tool designed specifically for the needs of systems-level Cognitive Neuroscience and Computational Psychiatry. This promises to open up new opportunities for research at the systems-level — a level of analysis that is crucial both for understanding how human mental function emerges from the interplay among neural subsystems, and how disturbances of individual neural subsystems impact this interplay, disruptions of which are almost certainly a critical factor in neurologic and psychiatric disorders. Project Narrative Paralleling the growth of neuroscience research has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. This proposed project seeks to address this need by developing a standard software platform for the construction, documentation, sharing, and integration of computational models of brain function, that promises to accelerate the study of how system-level interactions give rise to mental function and, critically, the kinds of disruptions of such system-level interactions produced by disturbances of individual subsystems — disruptions that are sure to be a complex but critical factor in neurological and psychiatric disorders.",PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry,9824928,R21MH117548,"['Acceleration', 'Address', 'Architecture', 'Attention', 'Basal Ganglia', 'Biological', 'Biological Models', 'Brain', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Custom', 'Data', 'Development', 'Documentation', 'Education', 'Educational Curriculum', 'Environment', 'Episodic memory', 'Evaluation', 'Exercise', 'Explosion', 'Foundations', 'Goals', 'Grain', 'Growth', 'Hippocampus (Brain)', 'Human', 'Individual', 'Influentials', 'Libraries', 'Literature', 'Maintenance', 'Manuals', 'Mental disorders', 'Methods', 'Modeling', 'Modernization', 'Neurosciences Research', 'Perceptual learning', 'Play', 'Prefrontal Cortex', 'Procedures', 'Psychiatry', 'Publications', 'Publishing', 'Pythons', 'Research', 'Role', 'Seeds', 'Short-Term Memory', 'Speed', 'Statistical Data Interpretation', 'System', 'Time', 'Writing', 'base', 'cognitive function', 'cognitive neuroscience', 'cognitive process', 'deep learning', 'deep neural network', 'design', 'flexibility', 'head-to-head comparison', 'improved', 'learning network', 'memory encoding', 'memory retrieval', 'mental function', 'nervous system disorder', 'neural model', 'open source', 'parallelization', 'preservation', 'programs', 'relating to nervous system', 'repository', 'simulation', 'tool', 'tool development']",NIMH,PRINCETON UNIVERSITY,R21,2019,253798,-0.017040406928221615
"Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression Project Summary/Abstract This project will develop a new technological approach for the comprehensive analysis of adaptive immune responses, which holds the potential to catalyze new strategies to prevent and treat disease. Here we will apply immune profiling techniques recently invented by the PI to investigate the mechanisms of Epstein-Barr virus (EBV) adaptive immune control in clinical cohorts of infected patients. EBV is a highly prevalent pathogen infecting >90% of the world’s population. Primary EBV infection often causes infectious mononucleosis (IM) and long-term sequelae include numerous malignancies, lymphoproliferative disorders, and a strong association with multiple sclerosis. No EBV vaccine is approved to date, and the molecular mechanisms of immune protection from EBV-associated diseases are unclear. Unfortunately, prior technical barriers in high- throughput immune profiling methods have prevented a comprehensive understanding of adaptive immune protection against EBV diseases. A technological approach that identifies the critical features of EBV immune protection will advance new solutions for vaccine and therapeutic development. Therefore, we developed an experimental pipeline to enable rapid and cost-effective analysis of B- and T-cell responses to EBV that is scalable to dozens of human patients per experiment. We hypothesize that a comprehensive B- and T-cell analysis of carefully selected patient cohorts that either can or cannot suppress symptomatic infection will reveal function-based correlates of EBV control. To test this hypothesis, we will apply quantitative immune profiling technologies to analyze cryopreserved longitudinal samples from recently completed prospective clinical studies of IM. Patient samples in our cohort span pre- and post-infection through convalescence and encompass the full range of clinical IM severity scores (from 0, asymptomatic primary infection, to 6, essentially bedridden with IM). Immune profile data will be used to establish adaptive immune correlates of IM disease severity. In addition, we will analyze immune responses in apparently immunocompetent patients with chronic active EBV (CAEBV) disease, or patients who do not adequately suppress EBV infection, to gain insight regarding adaptive immune function and dysfunction in CAEBV. Finally, we will develop a new computational toolkit to rapidly identify immune correlates from high-throughput datasets. Successful completion of this project will constitute the first comprehensive functional B- and T-cell receptor analysis in a human clinical cohort. Our efforts will provide a repertoire-scale, mechanistic understanding of adaptive immunity to EBV and suggest new strategies for treatment and prevention of EBV-associated diseases. Our long-term goal is to develop human immune profiling techniques as a platform approach to accelerate the rational design of vaccines and therapeutics against pathogens of high public health importance, beginning with EBV. Project Narrative This project will apply new high-throughput immune profiling technologies to elucidate the features of effective Epstein-Barr virus (EBV) immune control. EBV causes a range of human diseases including infectious mononucleosis and several forms of cancer; however, limited EBV treatment options are available and no approved preventive EBV vaccines exist. Our long-term objective is to apply enhanced understanding of adaptive immunity to accelerate the rational development of new vaccines and therapeutics.",Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression,9780369,DP5OD023118,"['Address', 'Antibodies', 'Antibody titer measurement', 'B cell repertoire', 'B-Lymphocytes', 'Burkitt Lymphoma', 'CD8-Positive T-Lymphocytes', 'Cells', 'Chronic', 'Clinical', 'Clinical Research', 'Convalescence', 'Cost Effectiveness Analysis', 'Cryopreservation', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'EBV-associated disease', 'Ensure', 'Epstein-Barr Virus Infections', 'Exhibits', 'Fatigue', 'Future', 'Goals', 'Herpesviridae', 'Herpesviridae Infections', 'Hodgkin Disease', 'Human', 'Human Herpesvirus 4', 'Immune', 'Immune System Diseases', 'Immune response', 'Immunocompetent', 'Immunologic Receptors', 'Incidence', 'Individual', 'Infection', 'Infectious Mononucleosis', 'Intervention', 'Knowledge', 'Lymphoproliferative Disorders', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Multiple Sclerosis', 'Nasopharynx Carcinoma', 'Oncogenic', 'Patients', 'Phase', 'Population', 'Prevention', 'Preventive', 'Primary Infection', 'Public Health', 'Receptors, Antigen, B-Cell', 'Recovery', 'Research Personnel', 'Risk', 'Sampling', 'Serum', 'Severities', 'Severity of illness', 'Stomach Carcinoma', 'Symptoms', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Techniques', 'Technology', 'Teenagers', 'Testing', 'Therapeutic', 'Therapeutic antibodies', 'Vaccine Design', 'Vaccines', 'Viral', 'Virus Diseases', 'Work', 'adaptive immune response', 'adaptive immunity', 'base', 'career', 'cohort', 'cost', 'design', 'disorder control', 'experimental study', 'high dimensionality', 'human disease', 'immune function', 'insight', 'lead candidate', 'multiple sclerosis patient', 'neutralizing monoclonal antibodies', 'next generation', 'novel', 'novel therapeutics', 'novel vaccines', 'pathogen', 'pressure', 'prevent', 'prospective', 'response', 'therapeutic development', 'treatment strategy', 'vaccine development', 'vaccine trial', 'virology', 'young adult']",OD,UNIVERSITY OF KANSAS LAWRENCE,DP5,2019,363625,-0.008479642874375939
"Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens ﻿    DESCRIPTION (provided by applicant): Infections caused by antibiotic-resistant bacterial pathogens are exceedingly common in immunocompromised hosts. Patients undergoing allogeneic hematopoietic stem cell transplantation (allo-HSCT) are particularly susceptible to these infections and are the patient population our studies will focus upon. Our goal is to extend and further develop systems biology approaches that our group has pioneered to identify mechanisms by which the intestinal microbiota confers resistance to infection by Vancomycin-resistant Enterococcus (VRE), antibiotic-resistant Klebsiella pneumoniae (arKp) and Clostridium difficile (C. diff). Aim 1 of our project is to establish a clinical database from the hospital recrds of allo-HSCT patients during their initial hospitalization that will include all laboratory values,vital signs, pharmacy data, dietary data, symptoms and physical exam findings. Aim 2 will expand our fecal bank by collecting fecal samples from approximately 160 allo-HSCT patients per year and determining the presence/absence of VRE, arKp and C. diff by culture and PCR. We will use NGS of 16S rRNA genes to determine microbiota composition on each sample, will perform metagenomic and RNA sequencing to determine the bacterial transcriptome and perform metabolomic analyses on a selected subset of fecal samples. Aim 3 is to extend our mathematical modeling to identify specific members of the microbiota, metabolic pathways and metabolic products that correlate with resistance to VRE or arKp expansion in the GI tract or are associated with resistance to C. diff infection. The clinical database will be used to establish correlations between clinical treatments or events and changes in the intestinal microbiota or the expression of bacterial metabolic pathways. Ultimately, the computational platforms developed in aim 3 will identify bacterial species or consortia that are associated with resistance to infection and Aim 4 will test these associations in germ-free mouse models. We will culture bacterial species associated with resistance, colonize mice with these protective bacteria and test for resistance against VRE, arKp and C. diff. Samples obtained from these experimental studies will be subjected to metagenomic and metabolomic analyses to further refine, in an iterative fashion, computational models developed in aim 3. Our proposed studies will develop new and extend existing computational models to identify bacterial species and molecular mechanisms that confer resistance to antibiotic-resistant bacterial infections. PUBLIC HEALTH RELEVANCE: The normal bacteria inhabiting the human intestine provide a high level of resistance against antibiotic-resistant bacterial pathogens. We are investigating the intestinal flora of hospitalized patients and using mathematical modeling to identify bacterial species and their metabolic products that reduce the risk of infection by three prevalent antibiotic-resistant bacteria. These studies may lead to the development of new approaches to treat and prevent antibiotic-resistant infections.",Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens,9657637,U01AI124275,"['16S ribosomal RNA sequencing', 'Allogenic', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bacterial Infections', 'Categories', 'Clinical Treatment', 'Clostridium difficile', 'Collection', 'Computer Simulation', 'Data', 'Deposition', 'Development', 'Diagnostic radiologic examination', 'Diet', 'Dietary intake', 'Enrollment', 'Event', 'Feces', 'Gastrointestinal tract structure', 'Germ-Free', 'Gnotobiotic', 'Goals', 'Growth', 'Hematopoietic Stem Cell Transplantation', 'Hospitalization', 'Hospitals', 'Human', 'Immune', 'Immunocompromised Host', 'Infection', 'Integration Host Factors', 'Intestinal Content', 'Intestines', 'Klebsiella pneumonia bacterium', 'Laboratories', 'Lead', 'Machine Learning', 'Measurable', 'Mediating', 'Medical Records', 'Memorial Sloan-Kettering Cancer Center', 'Metabolic', 'Metabolic Pathway', 'Metagenomics', 'Modeling', 'Modification', 'Molecular', 'Mus', 'Patient risk', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Population', 'Predisposition', 'Resistance', 'Resistance to infection', 'Risk', 'Sampling', 'Symptoms', 'Systems Biology', 'Testing', 'Toxic effect', 'Transplantation', 'Uncertainty', 'Vancomycin Resistance', 'Vancomycin resistant enterococcus', 'antimicrobial drug', 'clinical database', 'clinically relevant', 'colonization resistance', 'commensal bacteria', 'commensal microbes', 'computational platform', 'computer studies', 'data warehouse', 'design', 'drug resistant pathogen', 'experimental study', 'falls', 'gut microbiota', 'host microbiome', 'immune activation', 'infection risk', 'mathematical model', 'member', 'metabolome', 'metabolomics', 'metagenome', 'metagenomic sequencing', 'microbiome', 'microbiota', 'microorganism interaction', 'mouse model', 'network models', 'novel', 'novel strategies', 'parallel computer', 'pathogenic bacteria', 'patient population', 'prevent', 'public health relevance', 'rRNA Genes', 'resilience', 'resistance mechanism', 'text searching', 'transcriptome', 'transcriptome sequencing', 'transcriptomics']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,U01,2019,1697054,-0.008919107381441535
"Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology PROJECT SUMMARY/ABSTRACT Candidate After the completion of my Ph.D. in Bioinformatics, I joined the Center for Statistical Genetics at the University of Michigan (U-M) as research fellow and acquired extensive training in analysis on high- dimensional biological data. I uncovered a strong interest in studying the genetics and genomics of psoriasis when working with Dr. James Elder at the U-M, and I developed a fascination in understanding the functional roles of long non-coding RNAs (lncRNAs) in cutaneous diseases. I joined the Department of Dermatology at the U-M as a faculty in summer of 2015, with secondary appointments in the Department of Computational Medicine & Bioinformatics and the Department of Biostatistics. In addition, I direct the new Center for Cutaneous Bioinformatics within the Department of Dermatology, and serve to supervise and implement an analysis pipeline for studies investigating the immunological mechanisms for different skin diseases. Career Development Plan I aim to become a future leader in combining in silico discovery and bench experiments to advance biomedical research in autoimmune skin disorders. My objective in seeking a Mentored Research Scientist Development Award is to acquire the additional knowledge, training, and experience necessary for me to become an independent scholar in developing novel systems biology approaches to decipher the pathology and mechanisms of cutaneous diseases. The five year training proposed will provide knowledge and experience in aspects that are critical to my success, and they are: i) To develop knowledge and experimental skills in cutaneous biology --- achieved by guidance from Dr. Elder (investigative dermatology), intense research meetings/conferences, and practical laboratory experience in cutaneous research; ii) To develop knowledge and skills to study immunological systems of autoimmune skin diseases --- achieved by supervision from Dr. Johann Gudjonsson (skin immunology), attending formal Immunology courses and seminars, and earning laboratory experience from immunology experiments; iii) To advance skills in developing statistical and computational approaches --- accomplished by mentoring from Dr. Goncalo Abecasis (computational biologist), research meetings, and conducting research projects requiring advanced skills and knowledge in quantitative science; iv) To cultivate my professional development through enhancing scientific connections, grantsmanship skills, and educator portfolio --- achieved by establishing connections with colleagues during meetings, visiting King’s College London as scholar, attending a grantsmanship workshop and bootcamp, and learning mentoring skills through teaching formal classes and mentoring research students. Through the intensive and comprehensive training, I will be well grounded in conducting basic science experiments and also be able to capitalize my advanced knowledge in quantitative science to model mechanisms in cutaneous diseases. Research Project The research project will use psoriasis as a disease model to study the roles of lncRNAs in complex cutaneous disorders. I will test the hypotheses that (i): some lncRNAs are key causal elements and potentiate pathogenic inflammatory reactions in psoriasis development and (ii) by combining in silico predictions and in vitro validations we are able to provide comprehensive characterization of skin-expressing lncRNAs in keratinocytes and lymphocytes to infer their pathological implications for psoriasis. This work will demonstrate how we can take advantages of the genomic data to develop an integrative biology framework to provide novel biological insights and understand pathological roles of lncRNAs. Significance Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture. It is estimated that over 4 million Americans and 100 million people worldwide suffer from this disease. While genetic association studies have revealed the disease loci are highly enriched in non-coding regions, it is very challenging to translate genetic signals to biologic effects. In fact, most of the causal genes have not yet been identified. Our preliminary results showed that lncRNA is a class of gene that has largely been understudied for their roles in psoriasis, and both genetic and transcriptomic data suggested they can play important functions in psoriasis pathogenesis. By combining in silico analysis and in vitro validation we can expand our knowledge of lncRNAs in skin biology, and generate important hypotheses for future experiments. The results of this project can also identify novel biomarkers, and ultimately assist in the therapeutic drug discovery. PROJECT NARRATIVE Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture, and affects over 4 million Americans and 100 million people worldwide. Long non-coding RNAs (lncRNAs) is a class of gene that has largely been understudied, and recent studies suggested their potential roles in autoimmune diseases. This project aims to expand our knowledge of lncRNAs in skin biology, and advance identification of lncRNAs that play functional roles in psoriasis pathogenesis.",Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology,9761992,K01AR072129,"['Affect', 'American', 'Appointment', 'Autoimmune Diseases', 'Autoimmune Process', 'Basic Science', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Research', 'Biometry', 'Catalogs', 'Cellular Structures', 'Chromatin', 'Chronic', 'Comorbidity', 'Complex', 'Computer Simulation', 'Coronary Arteriosclerosis', 'Cutaneous', 'Data', 'Dermatology', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Disease susceptibility', 'Doctor of Philosophy', 'Economic Burden', 'Educational process of instructing', 'Educational workshop', 'Elderly', 'Elements', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Epigenetic Process', 'Expression Profiling', 'Faculty', 'Flow Cytometry', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genetic study', 'Genomics', 'Genotype', 'Homing', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunologics', 'Immunology', 'In Vitro', 'Inflammatory', 'Inflammatory Response', 'Knowledge', 'Learning', 'London', 'Lymphocyte', 'Machine Learning', 'Mediating', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Michigan', 'Modeling', 'Molecular', 'Pathogenesis', 'Pathogenicity', 'Pathologic', 'Pathology', 'Patients', 'Play', 'Process', 'Production', 'Proteins', 'Psoriasis', 'Public Health', 'Quality of life', 'Quantitative Reverse Transcriptase PCR', 'Reaction', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Signal Transduction', 'Skin', 'Societies', 'Statistical Methods', 'Students', 'Supervision', 'Susceptibility Gene', 'Systems Biology', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Transcript', 'Translating', 'United States', 'Universities', 'Untranslated RNA', 'Validation', 'Visit', 'Work', 'analysis pipeline', 'base', 'candidate identification', 'career development', 'causal variant', 'cell type', 'cohort', 'college', 'cytokine', 'drug discovery', 'experience', 'experimental study', 'fascinate', 'genetic architecture', 'genetic association', 'genomic data', 'high dimensionality', 'human disease', 'human tissue', 'immunoregulation', 'insight', 'interest', 'keratinocyte', 'knock-down', 'laboratory experience', 'learning strategy', 'meetings', 'novel', 'novel marker', 'recruit', 'scaffold', 'skills', 'skin disorder', 'small hairpin RNA', 'statistical center', 'success', 'symposium', 'tool', 'transcriptome sequencing', 'transcriptomics']",NIAMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,K01,2019,98982,-0.01829290657322149
"New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis ﻿    DESCRIPTION (provided by applicant)    Maha R Farhat, MD is an Instructor of Medicine at Harvard Medical School on the tenure track and a staff physician in the Department of Pulmonary and Critical Care Medicine at Massachusetts General Hospital. She is completing a masters of biostatistics at the Harvard School of Public Health in 5/2015. She has spent the last 4.5 years acquiring skills in Mycobacterium tuberculosis biology, epidemiology, bioinformatics and biostatistics. She has experience in the analysis of whole genome sequence data, drug resistance data and patient clinical outcome data with the focus of identifying Mycobacterium tuberculosis genetic determinants of drug resistance. She has also developed new methods in this area. Dr. Farhat has 11 publications 5 of which are first author including high impact and highly cited work in the journals Nature Genetics, Genome Medicine and the International Journal of Tuberculosis and Lung Disease. The short term goals of this K01 award are to provide training for Dr. Farhat in critical aspects of data science, computational and evolutionary biology, advanced biostatistics and network science. Dr. Farhat's long term goal is to become a leader in the field of Big Data analysis for infectious diseases. The proposed research as well as the training activities outlined in the proposal will successfully position Dr. Farhat for her first R01 and an independent career as a physician scientist. Environment: Dr. Farhat will perform the interdisciplinary work outlined in this proposal at the distinguished Harvard Departments of Global Health Social Medicine, Biostatistics, Evolutionary biology and the Institute for Quantitative Social Sciences. Dr. Farhat' mentorship team will include two world renowned leaders in the fields of infectious diseases and Big Data, Dr. Megan Murray and Dr. Gary King; and two rising stars in the fields of network Science and evolutionary Biology, Dr. JP Onnela and Dr. Michael Desai. Dr. Murray, the principal mentor on this proposal has mentored over 38 trainees, 9 of which have went on to have independent research careers, and 6 competed successfully for K awards. She is also PI on two recently awarded NIH/NIAID grants a CETR U19 and a TBRU U19 and has over 350 peer reviewed publications. To complement the expertise of her mentors Dr. Farhat will be advised by Dr. Christiani a practicing pulmonary and critical care physician and world renowned researcher in the field of lung and environmental genetics. She will also collaborate and consult with Dr. Merce Crosas, a data scientist, and Dr. Pardis Sabeti, a computational biologist. She will rotate through Dr. Soumya Raychaudhuri's bioinformatics laboratory to diversify her exposure to biomedical Big Data. In addition, she will receive formal training in evolutionary biology, Bayesian and mixed-model biostatistics, computer science, leadership skills and grant writing. The collaborative opportunities, intellectual environment and resources available to Dr. Farhat are outstanding. Research: Infectious diseases continue to be a major cause of morbidity and mortality. Despite the availability of effective antimicrobials, pathogens are successfully evolving new disease phenotypes that allow them to resist killing by these drugs or in other instances cause more severe disease manifestations or wider chains of transmission. Drug resistance (DR) is now common and some bacteria have even become resistant to multiple types or classes of antibiotics6. A key strategy in the fight against emerging pathogen phenotypes in infectious diseases is surveillance, and early personalized therapy to prevent transmission and propagation of these strains. The timely initiation of antibiotic therapy to which the pathogen is sensitive has been shown to be the key factor influencing treatment outcome for a diverse array of infections. Molecular tests that rely on the detection of microbial genetic mutations are particularly promising for surveillance and diagnosis of these pathogen phenotypes but rely on a comprehensive understanding of how mutations associate with these pathogen phenotypes. Currently there is an explosion of data on pathogen whole genome sequences (WGS) that is increasingly generated from clinical laboratories. Data on disease phenotype may also be available, but methods for the analysis and interpretation of these Big Data are lagging. Here I propose tools to aid in this analysis leveraging Big Data sets from Mycobacterium tuberculosis (MTB) and my prior work. Specifically I propose to (1) develop a web-based public interface to several analysis tools, including a statistical learning model that can predict the MTB DR phenotype from its genomic sequence, (2) to develop and study an MTB gene-gene network, based on WGS data, to improve our understanding of the effect of mutation-mutation interactions on the DR phenotype, and (3) study the performance of methods in current use for the association of genotype and phenotype in pathogens, and develop a generalizable power calculator for the best performing method. PUBLIC HEALTH RELEVANCE    Infectious agents of disease are successfully evolving drug resistance and other adaptations that threaten human health. Understanding the genetic mutations that underlie these disease phenotypes can inform surveillance and diagnostic strategies to combat this threat. Here I propose to develop accessible tools for pathogen genomic analysis that will help identify which genetic mutations are relevant to disease.",New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis,9747894,K01ES026835,"['Affect', 'Antibiotic Therapy', 'Antibiotics', 'Area', 'Award', 'Bacteria', 'Big Data', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Cessation of life', 'Clinical', 'Collaborations', 'Communicable Diseases', 'Complement', 'Consult', 'Critical Care', 'DNA Sequence Alteration', 'Data', 'Data Analyses', 'Data Science', 'Data Scientist', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug resistance', 'Drug resistance in tuberculosis', 'Drug-sensitive', 'Environment', 'Epidemiology', 'Explosion', 'Exposure to', 'Future', 'Gene Structure', 'General Hospitals', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Epistasis', 'Genome', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Grant', 'Health', 'Heritability', 'Human', 'Infection', 'Infectious Agent', 'Infectious Diseases Research', 'Institutes', 'International', 'Journals', 'K-Series Research Career Programs', 'Laboratories', 'Leadership', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Massachusetts', 'Measures', 'Medical Genetics', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Mentorship', 'Methods', 'Microbial Genetics', 'Modeling', 'Molecular', 'Monitor', 'Morbidity - disease rate', 'Mutation', 'Mycobacterium tuberculosis', 'National Institute of Allergy and Infectious Disease', 'Nature', 'Network-based', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Phylogeny', 'Physicians', 'Population', 'Positioning Attribute', 'Public Health', 'Public Health Schools', 'Publications', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Resources', 'Science', 'Scientist', 'Severity of illness', 'Social Medicine', 'Social Sciences', 'Standardization', 'Statistical Data Interpretation', 'Structural Genes', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Tuberculosis', 'United States National Institutes of Health', 'Variant', 'Virulent', 'Work', 'Writing', 'analytical tool', 'antimicrobial', 'base', 'big biomedical data', 'burden of illness', 'career', 'combat', 'computer science', 'design', 'disease phenotype', 'experience', 'fight against', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'global health', 'human pathogen', 'improved', 'instructor', 'medical schools', 'microbial', 'microbial genome', 'mortality', 'pathogen', 'pathogen genome', 'pathogen genomics', 'personalized medicine', 'prevent', 'prospective', 'public health relevance', 'simulation', 'skills', 'tenure track', 'tool', 'transmission process', 'whole genome']",NIEHS,HARVARD MEDICAL SCHOOL,K01,2019,215352,-0.013874960631570248
"Learning Dynamics of Biological Processes from Time Course Omics Datasets Complex biological processes, including organ development, immune response and disease progression, are inherently dynamic. Learning their regulatory architecture requires understanding how components of a large system dynamically interact with each other and give rise to emergent behavior. Recent experimental advances have made ii possible to investigate these biological systems in a data-driven fashion al high temporal resolution, allowing identification of new genes and their regulatory interactions. Longitudinal omics data sets are becoming increasingly common in clinical practice as well. Information on these collections of interacting genes can be integrated to gain systems-level insights into the roles of biological pathways and processes, including progression of diseases. Consequently, developing interpretable methods for learning functional relationships among genes, proteins or metabolites from high-dimensional time series data has become a timely research problem. The nature of these time-course data sets presents exciting opportunities and interesting challenges from a statistical perspective. Typical time-course omics data sets are challenging because of their high-dimensionality and non-linear relationships among system components. To tackle these challenges, one needs sophisticated dimension-reduction techniques that are biologically meaningful, computationally efficient and allow uncertainty quantification. Methods that incorporate prior biological information (e.g., pathway membership, protein-protein interactions) into the data analysis are good candidates for analyzing such high-dimensional systems using small samples. Here, we will develop three core methods to address the above challenges - (Aim 1): an empirical Bayes framework for clustering high-dimensional omics time-course data using prior biological knowledge; (Aim 2): a quantile-based Granger causality framework for learning interactions among genes or metabolites from their lead-lag relationships; and (Aim 3): a decision tree ensemble framework for searching cascades of interactions among genes from their temporal expression profiles. Our interdisciplinary team of statisticians and scientists will analyze time-course omics data from three research projects: (i) innate immune response systems in Drosophila, (ii) developmental process in mouse models, and (ii) longitudinal metabolite profiling of TB patients. These insights will be used to build and validate our methodology, which will be implemented in a publicly available software. This proposal is innovative in its incorporation of prior biological knowledge in the framework of novel dimension reduction techniques for interrogating high-dimensional time-course omics data. This research is significant in that it will impact basic sciences by elucidating data-driven, testable hypotheses on the regulatory architecture of biological processes, and clinical practice by monitoring disease progression and prognosis. n/a",Learning Dynamics of Biological Processes from Time Course Omics Datasets,9903643,R01GM135926,"['Biological Process', 'Data Set', 'Instruction', 'Learning', 'Time']",NIGMS,CORNELL UNIVERSITY,R01,2019,351443,-0.01288260704072481
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),9850367,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip region structure', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2019,644134,-0.008696463578333513
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9752596,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,323659,-0.023372206811977856
"Precision Medicine in Sarcoidosis ABSTRACT Sarcoidosis is a systemic inflammatory disease of unknown etiology characterized by non-caseating granulomas in affected organs, primarily in the lungs. Approximately 30% of patients with sarcoidosis progress to debilitating disease; however, the drivers of susceptibility or resilience to disease remain poorly understood. An inflammatory response to an undefined antigen is postulated as the etiology of granuloma formation, and the pathogenesis has been suggested to involve gene-pathogen interaction, yet analysis of single genes or microbes has not proven applicable to diagnosis of all forms of sarcoidosis. Indeed, rather than a single organism, the disease may represent an interaction between the community of organisms that comprise the lung microbiome (community of organisms that live in and on us) and the host immune response. We propose that understanding the microbiome/host interaction will suggest strategies for precision medicine approaches to sarcoidosis. This proposal addresses this significant gap by investigating interactions between the lung microbiome, host immune and clinical responses in sarcoidosis using multiomics approaches – a critically innovative strategy. Our preliminary data support our novel hypotheses. First, we identified distinct lung microbiomes that differentiated patients with sarcoidosis versus controls. Second, our results identified biomarkers of disease severity that were associated with decreased lung function. Third, a recurrent analytic theme that emerged, regardless of the type of -omic analysis, was that sarcoidosis is characterized by pathways related to apoptosis and autophagy, which is consistent with our observation of decreased abundance of peripheral lymphocytes and functional immune anergy. These data led us to our Overall Hypothesis: Lung microbiome and host immune interactions characterized by apoptosis and autophagy pathways influence sarcoidosis clinical course. This hypothesis will be tested by an observational prospective and validation study of sarcoidosis patients at 5 time points to facilitate time series analyses. Aims 1 and 2 focus on lung microbiome or host immune responses, respectively, in relation to clinical course of sarcoidosis. Using these data in Aim 3, predictive models will be constructed based on integrated data of metagenomic and host-immune interactions. The novelty and significance of our multiomics strategy is to construct models for precision medicine therapies to harness bioinformatic strategies into focused, patient-specific approaches. The long-term significance of this study is to define pathways for sarcoidosis progression or resolution, and to develop database of these findings to further develop more precise, testable, models. PROJECT NARRATIVE Sarcoidosis is a disease of unknown etiology that predominately affects the lung and may affect other organs. We propose to construct a model to predict sarcoidosis progression or resolution by identifying microbial and immune interactions. We postulate that these models will be helpful in designing therapeutic options.",Precision Medicine in Sarcoidosis,9632837,R01HL138628,"['Address', 'Affect', 'Antigens', 'Apoptosis', 'Apoptotic', 'Autophagocytosis', 'Bioinformatics', 'Biological Markers', 'Biological Process', 'Blood', 'Bronchoalveolar Lavage', 'Clinical', 'Communities', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Etiology', 'Feces', 'Genes', 'Granuloma', 'Immune', 'Immune response', 'Immunity', 'Inflammatory', 'Inflammatory Response', 'Lung', 'Lymphocyte', 'Messenger RNA', 'Metagenomics', 'Methods', 'MicroRNAs', 'Microbe', 'Modeling', 'Organ', 'Organism', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Peripheral', 'Phenotype', 'Predisposition', 'Prospective Studies', 'Recurrence', 'Resolution', 'Respiratory physiology', 'Sarcoidosis', 'Severity of illness', 'Taxonomy', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'Tissue-Specific Gene Expression', 'anergy', 'base', 'clinical predictors', 'clinically relevant', 'cytokine', 'deep sequencing', 'design', 'host microbiome', 'indexing', 'inflammatory marker', 'innovation', 'lung microbiome', 'machine learning algorithm', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbiome', 'multiple omics', 'novel', 'novel strategies', 'outcome forecast', 'pathogen', 'precision medicine', 'predictive modeling', 'resilience', 'response', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'validation studies']",NHLBI,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2019,914660,-0.020226994388651734
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9731544,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'repository', 'research and development', 'software development', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,350620,-0.00973322338599481
"Research and development of an open, extensible, web-based information extraction workbench for systematic review Project Summary  1 More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence-based  2 medicine, with each review requiring, on average, between six months to one year of effort to complete. One of the most  3 time consuming and repetitive aspects of this endeavor involves extraction of detailed information from a large number  4 of scientific documents. The specific data items extracted differ among disciplines, but within a given scientific domain,  5 certain data points are extracted repeatedly for each review conducted. Research on use of natural language processing  6 (NLP) for extracting individual data elements has shown that it has the potential to greatly reduce the laborious, time  7 intensive, and repetitive nature of this step. However, there is currently no integrated, automatic data extraction platform  8 that meets the needs of the systematic review community. We propose a web-based data extraction software platform  9 specifically designed for usage in the domain of systematic review. By combining multiple state-of-the-art data extraction 10 methods utilizing NLP, text mining and machine learning, into a single, unified user interface, we will thereby empower 11 the end-user with a powerful and novel tool for automating an otherwise arduous task. 12 The research we propose encompasses three specific aims: (1) develop new data extraction models using deep learning 13 and a new technique called “data programming”; (2) develop a web-based platform to semi-automate the process; (3) 14 design protocols and standards for packaging extraction models as software components and integrating work done by 15 other research groups and vendors. In the first aim, we will contribute novel data extraction modules designed and 16 trained specifically to extract data elements of interest to those conducting systematic reviews in the domain of 17 environmental health. For this research, we will employ state-of-the-art machine learning, NLP and text mining 18 methodologies to train and evaluate several novel extraction components. In our second aim, we will develop a web- 19 based workbench which will allow users to upload scientific documents for automated data extraction. Our system will 20 also be designed to allow for integration of data extraction approaches (components) from other research groups, thus 21 enabling end users to choose from a wide variety of advanced data extraction methodologies within one unified and 22 intuitive software environment. In our third aim, we will develop new protocols to standardize the inputs and outputs 23 for data extraction components. The resulting interface, which will enable seamless integration of third party extraction 24 components into the workbench, will also facilitate the incorporation of feedback from users such that extraction 25 components can be continuously improved based on real-time data. 26 Our overarching goal is to translate emerging semi-automated extraction technologies out of the lab and into practical 27 software and to bring to market both the software itself as well as several premium data extraction components. The 28 results of the research conducted for Aims 1-3 represent the first step in this direction and will provide the foundation for 29 future developments. These result will take us one step closer to the dream of creating “living systematic reviews,” which 30 are maintained using automated or semi-automated methods and updated regularly as new evidence becomes available. Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a flexible, extensible software system that automates the crucial and resource-intensive process of extracting key data elements from scientific documents, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.","Research and development of an open, extensible, web-based information extraction workbench for systematic review",9623091,R43ES029901,"['Communities', 'Computer software', 'Consensus', 'Data', 'Data Element', 'Development', 'Discipline', 'Dreams', 'Environment', 'Environmental Health', 'Evidence Based Medicine', 'Feedback', 'Foundations', 'Future', 'Goals', 'Health', 'Individual', 'Internet', 'Intuition', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Online Systems', 'Output', 'Process', 'Protocols documentation', 'Research', 'Resources', 'Source', 'Standardization', 'Supervision', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Translating', 'Update', 'Vendor', 'Work', 'artificial neural network', 'base', 'data integration', 'deep learning', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'interest', 'learning strategy', 'model design', 'novel', 'research and development', 'software systems', 'systematic review', 'text searching', 'tool']",NIEHS,"SCIOME, LLC",R43,2018,225000,-0.03519036971441448
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,9573854,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2018,388750,-0.006671953262419958
"Training a new generation of computational neuroscientists bridging neurobiology The Training Program in Computational Neuroscience (TPCN) will support integrated undergraduate and graduate training in computational neuroscience at New York University. The program will be hosted by the Center for Neural Science (CNS), with participation of faculty in the Departments of Psychology, Mathematics, and Computer Science, and the Institute of Neuroscience at the School of Medicine. The TPCN will fit well with NYU’s unique strengths and recent developments: (1) NYU is one of a few universities with a critical mass of computational neuroscientists. NYU has had a Sloan-Swartz Center for Theoretical Neuroscience since 1994. In the past three years alone, NYU has hired three computational neuroscientists. (2) CNS established an undergraduate major in neuroscience as early as 1992, and thus has a long track record in undergraduate education, it now has 136 students in the current academic year. (3) Recent faculty hiring in CNS, Psychology, and the School of Medicine has greatly expanded our teaching and research capabilities in the neuroscience of cognitive functions and their impairments associated with mental disorders. (3) As NYU is undertaking a merge of two historically separated neuroscience graduate programs (at CNS and the School of Medicine), this training grant will ensure that computational modeling, which has become indispensible in neuroscience, will be front-and-center in the integrated graduate program. (4) NYU is a major center of Artificial Intelligence and Data Science, with close links to Facebook’s AI Center and the Simons Center for Data Analysis. Our training faculty together with these connections will give our students ample opportunities to acquire machine learning techniques for data analysis and learn about brain-like AI algorithms. The proposed training program will support coherent undergraduate and graduate training in computational neuroscience at NYU. It will have several unique features: (1) Innovative mentorship methods: For example, (a) graduate trainees will mentor undergraduate trainees, (b) faculty will explicitly discuss human factors in academic practice; (c) there will be post-mortems after seminars by outside speakers. (2) Computational psychiatry: We propose new courses and research opportunities that are designed specifically to link cognitive function and the neurobiology of neural circuits. We propose innovative education in the nascent field of Computational Psychiatry, to bring theory and circuit modeling to clinical research in mental health. (3) Broad preparation: We aim to prepare trainees for jobs not only in academia, but also in medical and industry research. To achieve this, we will utilize our strength in machine learning and data science to broaden computational neuroscience training. The Program Directors have complementary strengths and will have complementary roles in the program. Wang will supervise graduate trainees and focus on training in mechanistic/circuit-level side of computational neuroscience as well as computational psychiatry. Ma will supervise undergraduate trainees and focus on the computational/behavioral side. This grant will support training of a new generation of graduate and undergraduate students in  computational neuroscience, which has become increasingly important to meet the challenges of  making discoveries with new data analysis tools and of understanding highly nonlinear complex  neural circuits. A salient component of our program is training in the nascent field  of   Computational  Psychiatry,  bridging  basic  neuroscience  and clinical research on mental  disorders. Therefore, the proposed program has the potential of making  a significant impact on  mental health.",Training a new generation of computational neuroscientists bridging neurobiology,9544939,R90DA043849,"['Academia', 'Algorithms', 'Artificial Intelligence', 'Behavioral', 'Brain', 'Clinical Research', 'Computer Simulation', 'Data Analyses', 'Data Science', 'Development', 'Education', 'Educational process of instructing', 'Ensure', 'Faculty', 'Generations', 'Grant', 'Human', 'Impairment', 'Industry', 'Institutes', 'Learning', 'Link', 'Machine Learning', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Neurobiology', 'Neurosciences', 'New York', 'Occupations', 'Preparation', 'Psychiatry', 'Psychology', 'Research', 'Role', 'Science', 'Side', 'Students', 'Supervision', 'Teacher Professional Development', 'Techniques', 'Training', 'Training Programs', 'Universities', 'cognitive function', 'cognitive neuroscience', 'computational neuroscience', 'computer science', 'design', 'innovation', 'mathematical sciences', 'medical schools', 'neural circuit', 'programs', 'relating to nervous system', 'theories', 'undergraduate education', 'undergraduate student']",NIDA,NEW YORK UNIVERSITY,R90,2018,210165,-0.0012055586093287541
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9520706,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2018,591130,-0.013578602064109997
"Machine Learning for Generalized Multiscale Modeling Project Summary/Abstract  This project develops machine learning approaches that describe statistical systems in biology. By combining analytic results calculated from the exact probabilistic description of the system with machine learning inference, our new methods present exciting opportunities to model previously inaccessible complex dynamics. The resulting Boltzmann machine-like learning algorithms present a new class of modeling techniques based on the powerful in- ference of arti cial neural networks. Further development of this approach will bring the groundbreaking advances from the surge of recent interest in machine learning into the biological modeling eld. The mathematical methods we develop will be used to derive e cient algorithms for multiscale simulation, directly applicable to large scale biological modeling. In particular, the algorithms will be used to study the dynamics of stochastic biochemistry at synapses, with direct relevance to learning and memory formation in the brain. Current studies of these processes are limited by the long timescales involved and the highly spatially organized structures featured. In addition to leveraging the machine learning expertise we are developing, we also employ new electron microscopy datasets to produce 3D reconstructions of neural tissue with unprecedented accuracy. Consequentially, we will be able to study the fundamental mechanisms underlying synaptic plasticity, as well as the biochemical basis of oscillatory behavior in networks of neurons that occurs during sleep. Furthermore, the interactions of these highly stochastic ion channels with electrical in neurons will be explored through groundbreaking hybrid simulation environments. The software that we will develop combines existing popular simulation tools into multiscale approaches, and will be distributed as a powerful tool to the broader biological modeling community. Its usage in further computational experiments can present a key advancement in the development of pharmaceuticals, allowing the direct study of the interactions of biochemistry and whole neuron electrophysiology without making limiting assumptions to sim- plify the simulations. This has promising implications for intervening in age-related learning de cits, as well as in neurological disorders such as Alzheimers. Finally, this proposal will bring together our existing multiscale modeling community, the National Center for Multi-scale Modeling of Biological Systems (MMBioS), with the MSM consortium. The interactions of these organizations and their communities of expert researchers will foster new collaborative work on exciting multiscale problems in biology, including applications of the machine learning frameworks and software we are developing. 1 Project Narrative  A wide variety of biological systems can be described statistically, from molecular biochemistry up to the network level activity of neurons. This work develops machine learning approaches to approximate these systems, enabling new simulation methods that bridge di erent levels of description. The resulting computational studies aim to shed light on the basis of learning and computation in the brain, and will enable the development of pharmaceutical targets for learning de cits associated with aging and neurological disorders such as Alzheimers. 1",Machine Learning for Generalized Multiscale Modeling,9791802,R56AG059602,"['Aging', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Area', 'Behavior', 'Biochemical', 'Biochemistry', 'Biological Models', 'Biological Neural Networks', 'Biology', 'Brain', 'Calcium', 'Cells', 'Chemicals', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Consequentialism', 'Coupling', 'Data Set', 'Development', 'Dimensions', 'Electron Microscopy', 'Electrophysiology (science)', 'Environment', 'Equation', 'Equilibrium', 'Evolution', 'Fostering', 'Hybrids', 'Image', 'Investigation', 'Ion Channel', 'Learning', 'Libraries', 'Light', 'Machine Learning', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Morphology', 'National Institute of General Medical Sciences', 'Neurons', 'Neuropil', 'Neurosciences', 'Pharmacologic Substance', 'Physics', 'Population', 'Potassium Channel', 'Process', 'Pythons', 'Reaction', 'Research Personnel', 'Sleep', 'Structure', 'Synapses', 'Synaptic plasticity', 'System', 'Techniques', 'Time', 'Tissues', 'United States National Institutes of Health', 'Vertebral column', 'Work', 'age related', 'base', 'biological systems', 'calmodulin-dependent protein kinase II', 'computer studies', 'experimental study', 'information processing', 'insight', 'interest', 'mathematical methods', 'men who have sex with men', 'microscopic imaging', 'multi-scale modeling', 'nervous system disorder', 'particle', 'postsynaptic', 'reconstruction', 'relating to nervous system', 'simulation', 'software development', 'success', 'tool', 'working group']",NIA,UNIVERSITY OF CALIFORNIA-IRVINE,R56,2018,619053,-0.010344759408281216
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9523638,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2018,478806,-0.005347690771447512
"Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms Project Summary The nematode worm Caenorhabditis elegans has proven valuable as a model for many high-impact medical conditions. The strength of C. elegans derives from the extensive homologies between human and nematode genes (60-80%) and the many powerful tools available to manipulate genes in C. elegans, including expressing human genes. Researchers utilizing medical models based on C. elegans have converged on two main quantifiable measures of health and disease: locomotion and feeding; the latter is the focus of this proposal. C. elegans feeds on bacteria ingested through the pharynx, a rhythmic muscular pump in the worm’s throat. Alterations in pharyngeal activity are a sensitive indicator of dysfunction in muscles and neurons, as well as the animal’s overall health and metabolic state. C. elegans neurobiologists have long recognized the utility of the elec- tropharyngeogram (EPG), a non-invasive, whole-body electrical recording analogous to an electrocardiogram (ECG), which provides a quantitative readout of feeding. However, technical barriers associated with whole- animal electrophysiology have limited its adoption to fewer than fifteen laboratories world-wide. NemaMetrix Inc. surmounted these barriers by developing a turn-key, microfluidic system for EPG acquisition and analysis called the the ScreenChip platform. The proposed research and commercialization activities significantly expand the capabilities of the ScreenChip platform in two key respects. First, they enlarge the phenotyping capabilities of the platform by incorporating high-speed video of whole animal and pharyngeal movements. Second they develop a cloud database compatible with Gene Ontology, Open Biomedical Ontologies and Worm Ontology standards, allowing data-mining of combined electrophysiological, imaging and other data modalities. The machine-readable database will be compatible with artificial intelligence and machine learning algorithms. It will be accessible to all researchers to enable discovery of relationships between genotypes, phenotypes and treatments using large-scale analysis of multidimensional phenotypic profiles. The research and commercialization efforts culminate in an unprecedented integration of genetic, cellular, and organismal levels of analysis, with minimal training and effort required by users. Going forward, we envision the PheNom platform as a gold standard for medical research using C. elegans. n/a",Advancing Human Health by Lowering Barriers to Electrophysiology in Genetic Model Organisms,9467327,R44GM119906,"['Adopted', 'Adoption', 'Aging', 'Algorithms', 'Amplifiers', 'Animal Model', 'Animals', 'Artificial Intelligence', 'Bacteria', 'Biomedical Research', 'Caenorhabditis elegans', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Electrocardiogram', 'Electrodes', 'Electrophysiology (science)', 'Equipment', 'Face', 'Familiarity', 'Feeds', 'Functional disorder', 'Genes', 'Genetic', 'Genetic Models', 'Genotype', 'Gold', 'Health', 'Health Status', 'Human', 'Image', 'Image Analysis', 'Kinetics', 'Laboratories', 'Locomotion', 'Machine Learning', 'Market Research', 'Measures', 'Medical', 'Medical Research', 'Metabolic', 'Metabolic dysfunction', 'Metadata', 'Methods', 'Microfluidic Microchips', 'Microfluidics', 'Microscope', 'Microscopy', 'Modality', 'Modeling', 'Molecular', 'Movement', 'Muscle', 'Nematoda', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Neurons', 'Ontology', 'Optics', 'Periodicity', 'Pharyngeal structure', 'Phase', 'Phenotype', 'Physiological', 'Pre-Clinical Model', 'Pump', 'Readability', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Signal Transduction', 'Speed', 'Statistical Data Interpretation', 'Stream', 'Structure', 'Surveys', 'System', 'Training', 'United States National Institutes of Health', 'Video Microscopy', 'addiction', 'base', 'biomedical ontology', 'commercialization', 'data mining', 'design', 'feeding', 'human disease', 'human model', 'member', 'phenotypic data', 'prevent', 'software development', 'success', 'tool', 'trait', 'web app']",NIGMS,"NEMAMETRIX, INC.",R44,2018,510448,-0.02272012899282942
"A Modular Automated Platform for Large-scale Drosophila Experiments and Handling PROJECT SUMMARY / ABSTRACT Animal model systems are a powerful tool researchers use to investigate almost all aspects of biology: genetics, development, neuroscience, disease, and more. And fruit flies – Drosophila melanogaster – with their small size, easy care, and remarkable array of available genetic toolkits, occupy a sweet spot on the model organism spectrum. Over 75% of human diseases with a genetic basis have an analogue in the fly, and Drosophila have been a part of the research for six Nobel prizes. Furthermore, the advent of CRISPR/cas9 and other modern genetic tools has opened the door to modeling other diseases and pathways, leading to greater use of Drosophila for drug screens. A great deal of the work (and the majority of the budget) involved in fly experiments is tedious manual labor, and with advances in computer vision, machine learning, and other analytic techniques, the stage is set to automate many phenotypic screens. In this Phase I SBIR, we propose a robotic system – modular automated platform for large-scale experiments (MAPLE) – that can accomplish a wide variety of fly-handling tasks in Drosophila labs. This robot is the fruit fly version of a liquid handling robot, with a large, open workspace that can house a plethora of modules and several manipulators that can move small parts and animals around that workspace. Building on a collaboration between the de Bivort Lab and FlySorter completed in 2017, we will design, fabricate and validate a commercial system that can collect virgin flies, run behavioral assays, conduct drug screens, and adapt to the needs of fly labs through easy-to-code Python scripts. By strategically combining modules and instructions to the robot, MAPLE can perform a wide variety of tasks in a fly lab, saving experimentalists from repetitive chores, cutting labor costs, and increasing scientific output. Just as pipette robots have become standard equipment in wet labs, we envision our fly handling robot will be the engine that powers Drosophila labs in academia and pharma, enabling new kinds of experiments and freeing researchers from the drudgery of fly pushing. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are a powerful model organism used in the study of disease, neuroscience, development, genetics, and recently in drug screens, too, largely through phenotypic screening. This labor-intensive work is time consuming and expensive, and ripe for automation. We propose a fly-handling robot – analogous to a liquid pipetting robot in a wet lab – that can perform a variety of tasks in Drosophila labs, free researchers from the drudgery of fly pushing, and enable a broader spectrum of experiments that will increase scientific knowledge.",A Modular Automated Platform for Large-scale Drosophila Experiments and Handling,9623017,R43MH119092,"['Academia', 'Address', 'Affect', 'Air', 'Anesthesia procedures', 'Animal Model', 'Animals', 'Architecture', 'Automation', 'Basic Science', 'Behavior', 'Behavioral Assay', 'Biological Models', 'Biology', 'Budgets', 'CRISPR/Cas technology', 'Carbon Dioxide', 'Caring', 'Code', 'Collaborations', 'Computer Vision Systems', 'Computer software', 'Computers', 'Custom', 'Data Collection', 'Deposition', 'Detection', 'Development', 'Disease', 'Disease Pathway', 'Drosophila genus', 'Drosophila melanogaster', 'Drug Screening', 'Drug usage', 'Ensure', 'Equipment', 'Feedback', 'Genetic', 'Genetic Screening', 'Genetic study', 'Grant', 'Hand', 'Human', 'Instruction', 'Knowledge', 'Libraries', 'Liquid substance', 'Machine Learning', 'Manuals', 'Modeling', 'Modernization', 'Neurosciences', 'Nobel Prize', 'Organism', 'Output', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Protocols documentation', 'Pythons', 'Reagent', 'Research', 'Research Personnel', 'Robot', 'Robotics', 'Running', 'Savings', 'Scanning', 'Small Business Innovation Research Grant', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Transgenic Organisms', 'Travel', 'Universities', 'Update', 'Vacuum', 'Work', 'analog', 'bone', 'cost', 'design', 'drug discovery', 'experimental study', 'flexibility', 'fly', 'graduate student', 'health science research', 'human disease', 'improved', 'operation', 'programs', 'repository', 'robot control', 'screening', 'tool', 'touchscreen']",NIMH,"FLYSORTER, LLC",R43,2018,348007,-0.01521031598360499
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9543557,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,264277,-0.016683818160472035
"Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics The human microbiota plays an important role in health and disease, and its therapeutic manipulation is being actively investigated for a wide range of diseases that span every NIH institute. Our microbiota are inherently dynamic, and analyzing these time-dependent properties is key to robustly linking the microbiota to disease, and predicting the effects of therapies targeting the microbiota; indeed, longitudinal microbiome data is being acquired with increasing frequency, and is a major component of many NIH-funded projects. However, there is currently a dearth of computational tools for analyzing microbiome time-series data, which presents several special challenges including high measurement noise, irregular and sparse temporal sampling, and complex dependencies between variables. The objective of this proposal is to introduce new capabilities, improve on, and provide state-of-the-art implementations of tools for analyzing dynamics, or patterns of change in microbiome time-series data. The tools we develop will use Bayesian machine learning methods, which are well-recognized for their strong conceptual and practical advantages, particularly in biomedical domains. Tools will be rigorously tested and validated on synthetic and real human microbiome data, including publicly available datasets and those from collaborators providing 16S rRNA sequencing, metagenomic, and metabolomics data. We propose three specific aims. For Aim 1, we will develop integrated Bayesian machine learning tools for predicting population dynamics of the microbiome and its responses to perturbations. These tools will include a new model that simultaneously learns groups of microbes with similar interaction structure and predicts their behavior over time, and that incorporates prior phylogenetic information. The model will be further improved by incorporating stochastic microbial dynamics and errors in measurements throughout the model. For Aim 2, we will develop Bayesian machine learning tools to predict host status from microbiome dynamics. The tools will learn easily interpretable, human-readable rules that predict host status from microbiome time-series data, and will be further extended to handle a variety of longitudinal study designs. For Aim 3, we will engineer our microbiome dynamics analysis software tools for optimal performance, ease-of- use, maintainability, extensibility, and dissemination to the community. In total, the proposed work will yield a suite of contemporary software tools for analyzing microbiome dynamics, with expected broad use and major impact. The software will allow investigators to answer important scientific and translational questions about the microbiome, including discovering which microbial taxa or their metagenomes are affected over time by perturbations such as changes in diet or invasion by pathogens; predicting the effects of these perturbations over time, including changes in composition or stability of the gut microbiota; and finding temporal signatures in multi-‘omic microbiome data that predict disease risk in the human host. The human microbiota, or collection of micro-organisms living on and within us, plays an important role in health, and when disrupted or abnormal, may contribute to many types of diseases including infections, kidney diseases, bowel diseases, diabetes, heart diseases, arthritis, allergies, brain diseases, and cancer. Sophisticated computer-based tools are needed to make sense of human microbiota data, particularly time- series data, which can yield important insights into how our microbiomes change over time. This work will develop new and improved computer-based tools for analyzing microbiota time-series data, which will be made freely available and will enable scientists to increase our fundamental knowledge about how our microbiota affect us and ultimately to apply this knowledge to prevent and treat human illnesses.",Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics,9640012,R01GM130777,"['16S ribosomal RNA sequencing', 'Affect', 'Algorithms', 'Antibiotics', 'Arthritis', 'Autoimmunity', 'Behavior', 'Biological Markers', 'Biological Models', 'Brain Diseases', 'Cardiovascular Diseases', 'Childhood', 'Clostridium difficile', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Dependence', 'Diabetes Mellitus', 'Diet', 'Disease', 'Engineering', 'Environmental Exposure', 'Frequencies', 'Funding', 'Health', 'Heart Diseases', 'Human', 'Human Microbiome', 'Hypersensitivity', 'Infection', 'Institutes', 'Intervention', 'Intestines', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Learning', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Metagenomics', 'Microbe', 'Modeling', 'Names', 'Noise', 'Oligosaccharides', 'Outcome', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Play', 'Population Dynamics', 'Property', 'Pythons', 'Readability', 'Recurrence', 'Research Design', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Series', 'Shotguns', 'Software Engineering', 'Software Tools', 'Speed', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Work', 'base', 'computerized tools', 'design', 'disorder risk', 'dynamic system', 'gut microbiota', 'human data', 'human microbiota', 'human subject', 'improved', 'insight', 'learning strategy', 'man', 'metabolomics', 'metagenome', 'microbial', 'microbiome', 'microbiome analysis', 'microbiome sequencing', 'microbiota', 'microorganism', 'multiple omics', 'nervous system disorder', 'novel', 'open source', 'pathogen', 'predictive tools', 'prevent', 'response', 'software development', 'targeted treatment', 'tool']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R01,2018,327382,0.007627504950318887
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9406205,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'predictive modeling', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2018,372603,-0.01089269448222495
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9674585,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Machine Learning', 'Marines', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'dark matter', 'deep learning', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,289700,0.004751162392444918
"Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach Project Summary Biology is full of stunning examples of emergent behaviors – behaviors that arise from, but cannot be reduced to, the interactions of the constituent parts that make up the system under consideration. These behaviors span the full spectrum of length scales, from the emergence of distinct cell fates (e.g. neurons, muscle, etc.) due to the interactions of genes within cells, to the formation of complex ecological communities arising from the interactions of thousands of species. The overarching goal of my research is to develop new conceptual, theoretical, and computational tools to model such emergent, system-level behaviors in biology. To do so, we utilize an interdisciplinary approach that is grounded in Biological Physics, but draws heavily from Machine Learning, Information Theory, and Theoretical Ecology. Our work is unified and distinguished by our deep commitment to integrating theory with the vast amount of biological data now being generated by modern DNA sequencing-based techniques and quantitative microscopy. An important goal of the proposed research is to find common concepts and tools that transcend traditional biological sub-disciplines and models systems. The proposed research pursues four distinct but conceptually interrelated research directions: (1) understanding how distinct cell fates emerge from bimolecular interactions within mammalian cells (2) investigating how bimolecular networks within cells exploit energy consumption to improve computations, with applications to Synthetic Biology; (3) identifying the ecological principles governing community assembly in microbial communities and developing techniques for synthetically engineering ecological communities; and (4) developing new machine learning algorithms and techniques for biological data analysis. In addition to developing physics-based models for diverse biological phenomena, the proposed research will yield a series of practical important tools and algorithms which we will make publically available including: (1) a new linear-algebra based algorithm for assessing the fidelity of directed differentiation and cellular reprogramming protocols and visualizing reprogramming/differentiation dynamics and (2) improved algorithms for inferring microbial interactions in the human microbiome from high-throughput sequence data. These computational tools will allow scientists to realize the immense therapeutic potential of cellular reprogramming and microbial ecology-based techniques for studying and treating human disease. Project Narrative This project will develop new theoretical and computational tools that will allow scientists to realize the immense therapeutic potential of cellular reprogramming (the conversion of one cell type into another cell type) and microbial ecology-based techniques for studying and treating human diseases.",Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach,9507677,R35GM119461,"['Algorithms', 'Behavior', 'Biological', 'Biological Models', 'Biological Phenomena', 'Biology', 'Cells', 'Communities', 'Complex', 'Consumption', 'DNA sequencing', 'Data', 'Data Analyses', 'Discipline', 'Ecology', 'Engineering', 'Goals', 'Human Microbiome', 'Information Theory', 'Length', 'Linear Algebra', 'Machine Learning', 'Mammalian Cell', 'Modeling', 'Modernization', 'Muscle', 'Neurons', 'Physics', 'Protocols documentation', 'Quantitative Microscopy', 'Research', 'Scientist', 'Series', 'System', 'Systems Biology', 'Techniques', 'Therapeutic', 'Transcend', 'Work', 'base', 'cell type', 'computerized tools', 'gene interaction', 'human disease', 'improved', 'interdisciplinary approach', 'microbial', 'microbial community', 'microorganism interaction', 'synthetic biology', 'theories', 'tool']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R35,2018,316544,-0.031055592645089218
"Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry PROJECT SUMMARY  Cellular interactions with the environment form the basis of health and disease for all organisms. Exposure to nutrients, toxins, and neighboring cells trigger coordinated molecular responses that impact cell function and metabolism in a beneficial, adaptive, or detrimental manner. Although the benefits of multicellularity for the formation of complex tissue structures or the function of entire organ systems has been long appreciated, it has only recently been understood that microbial inhabitants of vertebrates also have a tremendous impact on host cell function and dysfunction. Despite this, an understanding of these interactions has not moved beyond simple associations, and there are virtually no molecular technologies available that adequately define how a complex microbial ecosystem impacts host cell function, or how the host response to microbial colonization affects the bacterial community. This gap in knowledge is striking when one considers the broad and significant impact that microbes have on human health. In this application, we propose to expressly fill this knowledge gap through development of a novel multimodal imaging pipeline that will provide 3-dimensional information on the molecular heterogeneity of microbial communities and the immune response at the host-pathogen interface.  This proposal combines our expertise in immunology, infection biology, mass spectrometry, small animal imaging, machine learning, and computer vision to develop an integrated multimodal visualization method for studying infectious disease. Our unique approach will computationally combine ultra-high speed (~50px/s) MALDI-TOF images, ultra-high mass resolution (>200,000 resolving power) MALDI FTICR IMS, metal imaging by LA-ICP-IMS, high-spatial resolution optical microscopy, and MR imaging using data-driven image fusion. This strategy will enable 3-D molecular images to be generated for thousands of elements, metabolites, lipids, and proteins with an unprecedented combination of chemical specificity and spatial fidelity more than 50x faster than is currently possible. We will use this next-generation imaging capability to (i) define the heterogeneous microbial subpopulations throughout the 3-D volume of a S. aureus community, (ii) uncover the host molecules that form the abscess and accumulate to restrict microbial growth in murine models, and (iii) elucidate molecular markers that differentiate in vivo biofilms at the host-pathogen interface, between abscesses at various stages of progression, and under distinct degrees of nutrient stress. These studies will uncover new targets for therapeutic intervention and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research. PROJECT NARRATIVE This proposal will enable detailed views of the molecular components of infectious disease with unprecedented resolution through the development of a multimodal, 3-dimensional imaging platform. The proposed technologies will improve throughput and molecular specificity, enable automated high-precision and high-accuracy image alignment, and allow for descriptions of molecular signals in 3-D through the fusion of multi-modal imaging data. These studies will uncover targets for therapeutic intervention and antibiotic development and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research.",Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry,9659850,R01AI138581,"['3-Dimensional', 'Abscess', 'Affect', 'Animal Model', 'Animals', 'Anterior nares', 'Antibiotics', 'Antibodies', 'Architecture', 'Awareness', 'Bacteria', 'Bacterial Infections', 'Bacterial Proteins', 'Behavior', 'Biology', 'Biomedical Research', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Cellular Metabolic Process', 'Chemicals', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Custom', 'Data', 'Development', 'Diagnosis', 'Differentiation Antigens', 'Dimensions', 'Disease', 'Ecosystem', 'Elements', 'Environment', 'Exposure to', 'Fourier transform ion cyclotron resonance', 'Functional disorder', 'Glean', 'Growth', 'Health', 'Health Promotion', 'Heterogeneity', 'Histology', 'Human', 'Image', 'Imagery', 'Imaging technology', 'Immune', 'Immune response', 'Immunology', 'Imprisonment', 'Individual', 'Infection', 'Infectious Diseases Research', 'Integration Host Factors', 'Knowledge', 'Label', 'Lesion', 'Lipids', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mass Spectrum Analysis', 'Metals', 'Methodology', 'Methods', 'Microbe', 'Microbial Biofilms', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nutrient', 'Optics', 'Organism', 'Pathogenesis', 'Physiological', 'Population', 'Process', 'Proteins', 'Reagent', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Spatial Distribution', 'Specificity', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Speed', 'Staphylococcus aureus', 'Stress', 'Structure', 'Supervision', 'Techniques', 'Technology', 'Therapeutic Intervention', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Tissues', 'Toxin', 'Vertebrates', 'Work', 'animal imaging', 'bacterial community', 'base', 'body system', 'commensal bacteria', 'experimental study', 'host colonization', 'imaging capabilities', 'imaging detection', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'innovation', 'interest', 'microbial', 'microbial colonization', 'microbial community', 'microscopic imaging', 'molecular imaging', 'molecular marker', 'mouse model', 'multimodality', 'neutrophil', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'protein expression', 'response', 'targeted treatment', 'virtual']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2018,593526,-0.023991320238286696
"Development of label-free computational flow cytometry for high-throughput micro-organism classification The purpose of flow cytometers is to enable the classification of cells or organisms at high throughput. Label-free optical flow cytometers not based on fluorescence are generally based on scattering. The most common of these compares the amount of forward (FS) versus side (SS) scattering. Such two-parameter information permits rudimentary classification based on size or granularity, but it misses more subtle features that can be critical in defining organism identity. Nevertheless, FS/SS flow cytometry remains popular, largely because of its simplicity and capacity for high throughput.  We propose to develop a label-free computational flow cytometer that preserves much of the simplicity and high-throughput capacity of FS/SS flow cytometry, but provides significantly enhanced information. Instead of characterizing organisms based on scattering direction (as does FS/SS flow cytometry), we will characterize based on scattering patterns. We will insert a reconfigurable diffractive element in the imaging optics of a flow cytometer to route user-defined basis patterns to independent detectors. The basis patterns will be optimally matched to specific sample features. The respective weights of these basis patterns will serve as signatures to identify organisms of interest. The basis patterns themselves will be determined by machine learning algorithms. Both the device and the learning algorithms will be developed from scratch.  We anticipate that our flow cytometer will be able to operate at flow rates on the order of meters per second, commensurate with state-of-the-art FS/SS flow cytometers, while providing significantly more information for improved classification capacity. While our technique should be advantageous for any label-free flow cytometry application requiring high throughput, we will test it here by demonstrating high-throughput classification of microbial communities. NARRATIVE Our goal is to improve the information extraction capacity of label-free flow cytometers, while maintaining high throughput capacity. As such, our device should have a broad range of applications.",Development of label-free computational flow cytometry for high-throughput micro-organism classification,9510096,R21GM128020,"['Address', 'Algorithms', 'Awareness', 'Bioinformatics', 'Biological', 'Biology', 'Categories', 'Cells', 'Classification', 'Communities', 'Custom', 'Detection', 'Development', 'Devices', 'Elements', 'Flow Cytometry', 'Fluorescence', 'Goals', 'Image', 'Image Compression', 'Label', 'Learning', 'Light', 'Machine Learning', 'Measurement', 'Microbe', 'Modernization', 'Optics', 'Organism', 'Pattern', 'Performance', 'Pupil', 'Resolution', 'Route', 'Sampling', 'Side', 'Signal Transduction', 'Specificity', 'Speed', 'Techniques', 'Testing', 'Traction', 'Validation', 'Weight', 'base', 'cellular imaging', 'cost', 'cost effective', 'design', 'detector', 'improved', 'interest', 'meter', 'microbial community', 'microorganism', 'microorganism classification', 'optical imaging', 'prototype', 'recruit']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R21,2018,239527,-0.005312963130907486
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9553816,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2018,368020,-0.020803768594724898
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9530668,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Disease model', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Mathematics', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'experimental study', 'improved', 'infancy', 'insight', 'network models', 'novel', 'predictive modeling', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2018,584485,0.0027895398476973964
"Adapting the Berkeley Big Data Analytics Stack to Genomics and Health Project Summary We propose building a computational platform based on the high performance Berkeley Big Data Analytics Stack (BDAS) to support a new ecosystem of Clinical Decision Support (CDS) applications. This platform will make it faster, easier, and less expensive to develop molecular Clinical Decision Support Systems. These systems require real-time queries of globally distributed data, efficient machine learning on large genomic datasets, and must be secure, fault-tolerant and scalable. BDAS and associated technologies are designed to help us meet these challenges and are therefore ideal building blocks to help us create our computational platform. To encourage the adoption of standards for the querying and sharing of large genomic datasets, we will adapt the BDAS stack to support the standards of the Global Alliance for Genomics and Health (GA4GH). Project Narrative Funding this work will help establish a production quality FOSS implementation of the important Global Alliance for Genomics and Health standards. Without such open-source implementations, a fragmented and proprietary platform ecosystem would slow down innovation as well as divert resources away from the practice of medicine.",Adapting the Berkeley Big Data Analytics Stack to Genomics and Health,9566212,R44GM119858,"['Adoption', 'Algorithms', 'Apache', 'Big Data', 'Big Data to Knowledge', 'Businesses', 'Capital', 'Clinical Decision Support Systems', 'Cloud Computing', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Contractor', 'Data', 'Data Analytics', 'Data Set', 'Distributed Systems', 'Ecosystem', 'Ensure', 'Feedback', 'Funding', 'Genome', 'Genomics', 'Health', 'Individual', 'Industrialization', 'Industry', 'Ingestion', 'Institutes', 'International', 'Leadership', 'Letters', 'Machine Learning', 'Maintenance', 'Measures', 'Medicine', 'Molecular', 'Performance', 'Phase', 'Phenotype', 'Policies', 'Production', 'Provider', 'Publications', 'Resources', 'Running', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Technology', 'Time', 'Training', 'Variant', 'Work', 'base', 'clinical decision support', 'cloud platform', 'cluster computing', 'commercialization', 'design', 'distributed data', 'genomic data', 'health care delivery', 'individual patient', 'innovation', 'open source', 'operation', 'petabyte', 'precision medicine', 'symposium', 'web services', 'whole genome']",NIGMS,"CUROVERSE INNOVATIONS, INC.",R44,2018,1069680,-0.01674030862199678
"Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data Project Summary The immunology database and analysis portal (ImmPort, http://immport.niaid.nih.gov) is the NIAID-funded public resource for data archive and dissemination from clinical trials and mechanistic research projects. Among the current 291 studies archived in ImmPort, 114 are focused on vaccine responses (91 for influenza vaccine responses), which is the largest category when organized by research focus. As the most effective method of preventing infectious diseases, development of the next-generation vaccines is faced with the bottleneck that traditional empirical design becomes ineffective to stimulate human protective immunity against HIV, RSV, CMV, and other recent major public health threats. This project will focus on three important aspects of informatics approaches to secondary analysis of ImmPort data for influenza vaccination research: a) expanding the data analytical capabilities of ImmPort and ImmPortGalaxy through adding innovative computational methods for user-friendly unsupervised identification of cell populations, b) processing and analyzing a subset of the existing human influenza vaccination study data in ImmPort to identify cell-based biomarkers using the new computational methods, and c) returning data analysis results with data analytical provenance to ImmPort for dissemination of derived data, software tools, as well as semantic assertions of the identified biomarkers. Each aspect is one specific research aim in the proposed work. The project outcome will not only demonstrate the utility of the ImmPort data archive but also generate a foundation for the Human Vaccine Project (HVP) to establish pilot programs for influenza vaccine research, which currently include Vanderbilt University Medical Center; University of California San Diego (UCSD); Scripps Research Institute; La Jolla Institute of Allergy and Immunology; and J. Craig Venter Institute (JCVI). Once such computational analytical workflow is established, it can be applied to the secondary analysis of other ImmPort studies as well as to support the user-driven analytics of their own cytometry data. Each of the specific aims contains innovative methods or new applications of the existing methods. The computational method for population identification in Aim 1 is a newly developed constrained data clustering method, which combines advantages of unsupervised and supervised learning. Cutting-edge machine learning approaches including random forest will be used in Aim 2 for the identification of biomarkers across study cohorts, in addition to the traditional statistical hypothesis testing. Standardized knowledge representation to be developed in Aim 3 for cell-based biomarkers is also innovative, as semantic networks with inferring and deriving capabilities can be built based on the machine-readable knowledge assertions. The proposed work, when accomplished, will foster broader collaboration between ImmPort and the existing vaccine research consortia. It will also accelerate the deployment of up-to-date informatics software tools on ImmPortGalaxy. Project Narrative Flow cytometry (FCM) plays important roles in human influenza vaccination studies through interrogating immune cellular functions and quantifying the immune responses in different conditions. This project will extend the current data analytical capabilities of the Immunology Database and Analysis Portal (ImmPort) through adding novel data analytical methods and software tools for user-friendly identification of cell populations from FCM data in ImmPort influenza vaccine response studies. The derived data and the knowledge generated from the secondary analysis of the ImmPort vaccination study data will be deposited back to ImmPort and shared with the Human Vaccines Project (HVP) consortium for dissemination.",Reproducible Analytics for Secondary Analyses of ImmPort Vaccination-Related Cytometry Data,9577591,UH2AI132342,"['Academic Medical Centers', 'Address', 'Archives', 'Back', 'Biological Markers', 'California', 'Categories', 'Cells', 'Characteristics', 'Clinical Trials', 'Cohort Studies', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer Analysis', 'Computing Methodologies', 'Cytomegalovirus', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Databases', 'Deposition', 'Development', 'Disease', 'Failure', 'Flow Cytometry', 'Fostering', 'Foundations', 'Funding', 'Genetic Transcription', 'HIV', 'Human', 'Hypersensitivity', 'Imagery', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunology', 'Incidence', 'Influenza', 'Influenza vaccination', 'Informatics', 'Institutes', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant neoplasm of cervix uteri', 'Maps', 'Measles', 'Medical', 'Meta-Analysis', 'Metadata', 'Methods', 'Mumps', 'Names', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Play', 'Poliomyelitis', 'Population', 'Population Statistics', 'Prevalence', 'Prevention strategy', 'Process', 'Public Health', 'Readability', 'Reporting', 'Reproducibility', 'Research', 'Research Design', 'Research Institute', 'Research Project Grants', 'Respiratory Syncytial Virus Vaccines', 'Respiratory syncytial virus', 'Role', 'Secondary to', 'Semantics', 'Smallpox', 'Software Tools', 'Source', 'Standardization', 'Supervision', 'Technology', 'Testing', 'Therapeutic', 'Universities', 'Vaccination', 'Vaccine Design', 'Vaccine Research', 'Vaccines', 'Work', 'analytical method', 'base', 'biomarker discovery', 'biomarker identification', 'catalyst', 'cohort', 'comparative', 'computer infrastructure', 'computerized tools', 'data archive', 'data mining', 'data portal', 'data resource', 'design', 'experience', 'experimental study', 'forest', 'immune function', 'improved', 'influenza virus vaccine', 'information organization', 'innovation', 'neoplastic', 'news', 'novel', 'novel strategies', 'novel vaccines', 'prevent', 'programs', 'public-private partnership', 'response', 'response biomarker', 'secondary analysis', 'statistics', 'success', 'tool', 'user-friendly', 'vaccine development', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity']",NIAID,"J. CRAIG VENTER INSTITUTE, INC.",UH2,2018,243750,-0.0028787097391207005
"A three-population three-scale social network model to assess disease dispersion DESCRIPTION (provided by applicant): Communicable diseases, such as influenza, are transmitted from individual to individual following a network of contacts in a population. Reports on recent outbreaks, such as SARS, the Bird flu, and the H1N1 flu, have repeatedly stressed the critical role of contact networks. We propose an innovative Three-population and Three-scale Social Network (3p3sNet) model to simulating the spatial and temporal dispersion of influenza in a metropolitan population in Buffalo, NY. The 3p3sNet aims to construct a realistic contact network by representing interacting and mobile behaviors of individuals at three scales and three types of places. These involve individual (microscopic) -> local network (mesoscopic) -> population (macroscopic) as nighttime population at homes, daytime population at workplaces, and pastime population at service places. Through this network, diseases disperse from infectious individuals to their local networks then to the population-wide network in a complex dynamic fashion. Modeling the disease dispersion through this network provides invaluable insights in who might be at risk, where and when this risk might occur, and with whom these at-risk individuals might be in contact. These insights lay the foundation of developing spatially and temporally sensitive intervention strategies targeted towards the most vulnerable individuals and social groups. Furthermore, the 3p3sNet can be applied in modeling the epidemiology of any disease where human contacts play a critical role.  In implementing 3p3sNet, we propose to use mobile phone data to extract the individual interaction and travel behaviors. We embrace recent developments in economics, geostatistics, econometrics, and machine learning to construct the network. We develop an innovative co-kriging approach to expanding local households to population-wide households and a novel distance-based GEV discrete choice model to link homes to workplaces and service places. It is anticipated that the assemblage of these advanced methods will enable new capabilities and bring transformative improvements in health-related studies in metropolitan areas. We will conduct a data-rich validation process for the three constructed populations, the links between them, and the simulated disease dispersion through the population. A comprehensive range of independent datasets will be used to support the proposed validation. These involve high-resolution population, workplace, and service place data, surveys of individual interaction and travel behavior, and reports on influenza infections.  The multidisciplinary team comprises world-renown leaders and scholars in epidemiology, agent-based and social network modeling, human mobility analysis, geographical information science, and machine learning. The proposed project represents emerging frontiers in the modeling of communicable diseases and will redefine the capabilities of epidemiological models. PUBLIC HEALTH RELEVANCE: This project addresses two issues of central importance to successfully capturing the complex, spatial and temporal dispersion process of a communicable disease through a population. They are: how to represent individuals as heterogeneous, mobile, and interacting and how to model the disease dispersion process from infectious individuals to their local networks then to the population-wide network. To address these issues, this project proposes to: (1) use mobile phone data to construct a three-population and three-scale social network (3p3sNet); (2) simulate the spatially and temporally dynamic dispersion of influenza through an urban population in Buffalo, NY; and (3) conduct an intensive, data-rich validation process on the simulated influenza dispersion.",A three-population three-scale social network model to assess disease dispersion,9443636,R01GM108731,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Behavior', 'Buffaloes', 'Car Phone', 'Communicable Diseases', 'Complex', 'Dangerousness', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Disease Outbreaks', 'Disease model', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Foundations', 'Geography', 'Health', 'Home environment', 'Household', 'Human', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Information Sciences', 'Intervention', 'Link', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Modeling', 'Outcome', 'Play', 'Population', 'Process', 'Records', 'Reporting', 'Resolution', 'Risk', 'Role', 'Sampling', 'Services', 'Severe Acute Respiratory Syndrome', 'Social Network', 'Stress', 'Surveys', 'Testing', 'Time', 'Travel', 'Urban Population', 'Validation', 'Workplace', 'base', 'disease transmission', 'epidemiological model', 'flu', 'frontier', 'fundamental research', 'human disease', 'innovation', 'insight', 'metropolitan', 'multidisciplinary', 'network models', 'novel', 'pandemic disease', 'public health relevance', 'social group', 'transmission process']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,496444,-0.012396095687758504
"INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE Project Summary  To understand the many disorders of the brain it is necessary to grapple with its complexity.  Increasingly large and complicated data sets are being collected, but the tools for analyzing and  modeling the data are not yet available. More researchers trained in computational neuroscience are  desperately needed. This project supports graduate and undergraduate training programs in  computational neuroscience (TPCN) at both Carnegie Mellon University (CMU) and the University of  Pittsburgh (Pitt), and a summer school in computational neuroscience for undergraduates, which are  available to students coming from colleges and universities throughout the United States.  The CMU-Pitt TPCN has 16 training faculty in computational neuroscience, 22 training faculty whose  laboratories are primarily experimental, and 20 training faculty whose laboratories are both  computational and experimental. At the graduate level the TPCN offers a PhD program in Neural  Computation (PNC) and joint PhD programs with CMU’s Department of Statistics (PNC-Stat) and its  Machine Learning Department (PNC- MLD), all set within a highly collegial, cross-disciplinary  environment of our Center for the Neural Basis of Cognition (CNBC), which is operated jointly by  CMU and Pitt. The CNBC was established in 1994 to foster interdisciplinary research on the neural  mechanisms of brain function, and now comprises 145 faculty having appointments in 22 departments.  At the undergraduate level a substantial pool of local students is supplemented during the summer  by a cohort of students from across the country. During this renewal funding period the project is  strengthening the role of statistics and machine learning throughout the training programs; (2) revising the summer undergraduate program by creating a didactic two-week “boot camp” at the  beginning, which includes a 20-lecture overview of computational neuroscience; (3) creating online  materials, in conjunction with the boot camp, that will serve not only our own students but also  the greater world of training in computational neuroscience; and (4) enhancing our minority  recruitment by (a) taking advantage of the boot camp and online materials, as well as making  promotional visits to targeted campuses, and (b) creating and running a one-year “bridge” program  to better prepare under-represented minorities for PhD programs.  TPCN trainees work in vertically integrated, cross-disciplinary research teams. Graduate students  take a year- long course in computational neuroscience that bridges modeling and modern statistical  machine learning approaches to neuroscience. To ensure their competency in core neuroscience  principles they also take courses in cognitive neuroscience, neurophysiology, and systems  neuroscience. They then pursue depth in a relevant quantitative discipline, such as computer  science, engineering, mathematics, or statistics. Graduate students have extended experience in at  least one experimental laboratory, and they take part in journal clubs and seminars within the  large Pittsburgh neuroscience community. Year-long undergraduates take courses in mathematics,  computer programming, statistics, and neuroscience; they take an additional course in neuroscience  or psychology and a course in computational neuroscience; and they complete a year-long research  project. In addition, they complete the TPCN summer program. Undergraduate trainees in the summer  program go through the boot camp on topics in computational neuroscience, including tutorials in  Matlab, statistical methods, fundamentals of differential equations, and ideas of neural coding;  they then complete a research project under careful guidance. All trainees will receive training in  responsible conduct of research. Across 5 years of funding, the TPCN supports 20 NRSA graduate  students, 10 non-NRSA graduate students, 30 undergraduate year-long fellows, and 60 undergraduate  summer fellows. Project Narrative  Research in neuroscience is crucial for attacking the causes of neurological and mental health  disorders. If the field of neuroscience is to continue its rapid advance, neuroscientists must use,  understand, and develop new technologies, acquire and analyze ever larger data sets, and grapple  more directly with the complexity of neurobiological systems. The primary goal of these training  programs will be to help train a new generation of interdisciplinary neuroscientists with strong  quantitative skills.",INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE,9548637,R90DA023426,"['Appointment', 'Brain', 'Brain Diseases', 'Code', 'Cognition', 'Communities', 'Competence', 'Country', 'Data Set', 'Differential Equation', 'Discipline', 'Doctor of Philosophy', 'Educational Status', 'Engineering', 'Ensure', 'Environment', 'Faculty', 'Fostering', 'Funding', 'Interdisciplinary Study', 'Joints', 'Journals', 'Laboratories', 'Machine Learning', 'Mathematics', 'Minority Recruitment', 'Modeling', 'Modernization', 'National Research Service Awards', 'Neurosciences', 'Psychology', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Running', 'Schools', 'Statistical Methods', 'Students', 'System', 'Teacher Professional Development', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States', 'Universities', 'Visit', 'Work', 'bridge program', 'cognitive neuroscience', 'cohort', 'college', 'computational neuroscience', 'computer program', 'computer science', 'data modeling', 'experience', 'graduate student', 'lectures', 'neuromechanism', 'neurophysiology', 'programs', 'relating to nervous system', 'responsible research conduct', 'statistics', 'summer program', 'tool', 'undergraduate student']",NIDA,CARNEGIE-MELLON UNIVERSITY,R90,2018,305372,-0.0020936381406555563
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or “position” on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9687220,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Research Infrastructure', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2018,330000,-0.02366431172992844
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9451318,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2018,1354554,-0.012678116277285309
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,-0.008542411977654804
"Interactions between Gut Microbiome Natural Products and Intestinal Helminths Project Summary / Abstract This project will identify the molecular mechanisms through which the gut microbiome interacts with infectious intestinal helminths. Intestinal helminthic parasites present one of the most pressing global health problems due to the abundance of infection, a limited understanding of etiology, and increasing drug resistance. Prior work has established that the gut microbiome can influence infection, but the specific mechanisms through which it does so are unknown. We hypothesize that the gut microbiome produces anti-helminthic metabolites that can prevent and control infection. We propose a series of experiments that integrate multi'omics data and machine learning techniques to discover natural products of the gut microbiome that predict infectious burden or associate with infection control. Our studies leverage a high-throughput zebrafish model of infection to precisely resolve these factors. Additionally, we will develop bioinformatic methods that (1) connect natural products to the genetic pathways and microbiota that produce them, and (2) impute the presence of these natural products in the human gut. Consequently, this project will produce novel anti-helminthic drug leads that can be efficiently isolated and tested, and will clarify the role of gut microbiome natural products in the etiology of infection. Project Narrative Intestinal helminthic infections are a pressing global health problem due to the number of infected individuals, a limited understanding of disease mechanisms, and an increase in resistance to anti-helminthic drugs. This project will discover gut microbiota, their genes, and their metabolites that are linked to the prevention and control of parasitic infection. The long-term goals of this project are to determine if the gut microbiome is a factor in the etiology of helminthic intestinal infection, and to identify microbiome-sourced candidates for the discovery of novel anti-helminthic drugs. !",Interactions between Gut Microbiome Natural Products and Intestinal Helminths,9436228,R21AI135641,"['Animals', 'Anthelmintics', 'Bioinformatics', 'Data', 'Development', 'Diagnostic', 'Disease', 'Drug resistance', 'Etiology', 'Exposure to', 'Fishes', 'Future', 'Generations', 'Genes', 'Genetic', 'Goals', 'Growth and Development function', 'Helminths', 'Human', 'Individual', 'Infection', 'Infection Control', 'Inflammation', 'Informatics', 'Intestinal parasite', 'Intestines', 'Link', 'Machine Learning', 'Mammals', 'Measurement', 'Metabolic', 'Methods', 'Microscopic', 'Minority', 'Modeling', 'Molecular', 'Monitor', 'Natural Products', 'Outcome', 'Parasites', 'Parasitic infection', 'Pathway interactions', 'Pharmaceutical Preparations', 'Population', 'Prevention', 'Probiotics', 'Research', 'Research Design', 'Resistance', 'Resources', 'Risk', 'Role', 'Sample Size', 'Series', 'Source', 'Taxonomy', 'Techniques', 'Testing', 'Therapeutic', 'Work', 'Zebrafish', 'base', 'expectation', 'experimental study', 'global health', 'gut microbes', 'gut microbiome', 'gut microbiota', 'helminth infection', 'inquiry-based learning', 'insight', 'learning strategy', 'metabolome', 'metagenome', 'microbial', 'microbiome', 'microbiota', 'multiple omics', 'neglected tropical diseases', 'novel', 'prevent']",NIAID,OREGON STATE UNIVERSITY,R21,2018,220500,-0.009289143907511276
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9465505,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,375463,0.013572157472697763
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9704539,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2018,57657,0.013572157472697763
"Device for real-time streaming of preclinical research data into a central cloud-based platform Project Summary BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. Traditionally researchers collect data from many disparate instruments and store data on PCs running single license software. Raw data is stored across several locations, analysis is restricted to the PC used to collect the data, and opportunities for collaboration, remote-participation, and data sharing are limited. BehaviorCloud is leveraging cloud data streaming and storage to overcome these barriers to discovery. In 2017 BehaviorCloud released a first version of the underlying cloud platform as well as BehaviorCloud Camera, an open-source “reference implementation” that demonstrates automated video tracking of animal behavior on the BehaviorCloud platform using a consumer-grade smartphone. This tool and the underlying platform are both in active use across academic and pharmaceutical labs. The aim of this Phase I SBIR application is to develop patent-pending “Bridge” technology that allows data streaming from third-party instrumentation into the central web platform. Researchers will bypass the original software and PCs associated with their instruments to control trials through their BehaviorCloud account and receive data back in real-time. BehaviorCloud will provide a public repository to aggregate all of these data and accelerate discovery by providing computational tools for large-scale meta-analysis and machine learning based predictive analytics. Project Narrative BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. The goal of this Phase I SBIR application is to develop the technology to enable streaming of data from all kinds of behavioral and phenotyping instrumentation into the BehaviorCloud platform. BehaviorCloud will aggregate these data into a repository and accelerate discovery by providing tools for collaboration and meta-analysis.",Device for real-time streaming of preclinical research data into a central cloud-based platform,9621228,R43OD025448,"['Adoption', 'Animal Behavior', 'Animal Experimentation', 'Animal Model', 'Area', 'Back', 'Behavioral', 'Bypass', 'Carbon Dioxide', 'Cellular Phone', 'Collaborations', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Devices', 'Goals', 'Heart Rate', 'Information Systems', 'Internet', 'Intervention', 'Legal patent', 'Licensing', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Meta-Analysis', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Process', 'Research', 'Research Contracts', 'Research Personnel', 'Resources', 'Running', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Standardization', 'Stimulus', 'Stream', 'System', 'Technology', 'Temperature', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'cloud based', 'cloud platform', 'computerized tools', 'control trial', 'data management', 'data sharing', 'data warehouse', 'design', 'experimental study', 'instrument', 'instrumentation', 'laptop', 'open source', 'phenotypic data', 'pre-clinical', 'pre-clinical research', 'prototype', 'repository', 'tool', 'wasting', 'web interface']",OD,"BEHAVIORCLOUD, LLC",R43,2018,220420,-0.01448342729638333
"Robust Control of the Stem Cell Niche Diana Arguijo has a unique background with a double major in biomedical engineering (BME) and electrical and computer engineering (ECE). Leveraging her strong mathematical background, she will develop computational techniques to identify patterns of epigenetic reprogramming during epithelial development and patterns of real- time electrical recoding of the GI tract reflective of sacral nerve modulation. Her work will provide insights into the robustness and plasticity of underlying biological control schemes. Diana Arguijo will develop machine-learning based computational techniques to analyze epigenetic reprogramming of epithelial development and electrical activities of the enteric nervous system. Her analyses will provide insights into the robustness and plasticity of tissue regulation.",Robust Control of the Stem Cell Niche,9731853,R35GM122465,"['Biological', 'Biomedical Engineering', 'Computational Technique', 'Computers', 'Development', 'Engineering', 'Enteric Nervous System', 'Epigenetic Process', 'Epithelial', 'Gastrointestinal tract structure', 'Machine Learning', 'Mathematics', 'Pattern', 'Regulation', 'Sacral nerve', 'Scheme', 'Time', 'Tissues', 'Work', 'base', 'insight', 'stem cell niche']",NIGMS,DUKE UNIVERSITY,R35,2018,42822,-0.021211120040343163
"Vaccine Beliefs and Decision Making Project Summary This project will use methods from quantitative anthropology to describe the social space of vaccine beliefs that circulate among the general public and to provide an initial assessment of how different belief variations influence decisions to vaccinate. The results will establish, for the first time, the patterns of co-variation in the wide variety of pro- and anti- vaccine beliefs, and which axes of this variation appear associated with decisions to vaccinate. Vaccination is a key public health defense against infectious disease, but the lay public largely does not fully appreciate scientific evidence when making decisions for or against vaccination. Understanding the inter-correlations of these beliefs, therefore, is imperative for designing effective educational interventions that can directly interface with the cultural beliefs that surround vaccination and influence the public's decision making on this issue. The project will leverage insights from two very different but complementary data sources: responses to a nationally representative survey (fielded on the RAND American Life Panel) and social media data from Twitter. Our analytic approach will begin with systematic coding techniques from mixed-methods research to classify vaccine beliefs into a comprehensive set of belief variants. Manual coding will be validated through inter-observer reliability checks and replicated at scale with machine-learning algorithms. Having systematically coded the data, we will then assess whether nationally representative survey data and data mined from Twitter produce similar results using Cultural Consensus Analysis, a technique from quantitative cultural anthropology. From the survey data we will test whether vaccine beliefs are correlated with decisions to vaccinate after controlling for demographic attributes. To ensure completion of this innovative and methodologically expansive project, the project team combines expertise from anthropology, decision science, clinical medicine, and biomathematics. The principal investigator brings to this project multiple years of both academic and industry experience in statistical modelling of cultural data. Project Narrative Vaccination is a key public health defense against infectious disease, but patients' vaccination decisions may be more influenced by broadly circulated cultural beliefs than they are influenced by scientific evidence. This proposed research will systematically map the diversity of the publics' vaccination beliefs, assess how these beliefs influence vaccination decisions, and advise policy makers how to interface more directly with these popular belief systems that are critical to effective vaccination efforts.",Vaccine Beliefs and Decision Making,9557573,R21HD087749,"['Achievement', 'Adolescent', 'Adopted', 'Adult', 'Algorithms', 'American', 'Anthropology', 'Autistic Disorder', 'Behavior', 'Belief', 'Belief System', 'Childhood', 'Clinical Medicine', 'Code', 'Cognitive', 'Communicable Diseases', 'Communities', 'Consensus', 'Cultural Anthropology', 'Data', 'Data Sources', 'Decision Making', 'Disease', 'Educational Intervention', 'Ensure', 'Environment', 'Fright', 'General Population', 'Health Communication', 'Health behavior', 'Immunization', 'Individual', 'Industry', 'International', 'Intervention', 'Lead', 'Life', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Maps', 'Measles', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Parents', 'Patients', 'Pattern', 'Persons', 'Policies', 'Policy Maker', 'Positioning Attribute', 'Principal Component Analysis', 'Principal Investigator', 'Probability', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Methodology', 'Resistance', 'Role', 'Safety', 'Sampling', 'Science', 'Statistical Models', 'Structure', 'Survey Methodology', 'Surveys', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'Vaccinated', 'Vaccination', 'Vaccines', 'Variant', 'Work', 'authority', 'biomathematics', 'cognitive process', 'design', 'efficacy testing', 'experience', 'innovation', 'insight', 'interest', 'prevent', 'response', 'social', 'social media', 'social space', 'theories', 'therapy design', 'vaccine safety']",NICHD,RAND CORPORATION,R21,2018,242266,-0.003996785644550197
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9474101,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'human microbiota', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'multidisciplinary', 'novel', 'open source', 'operational taxonomic units', 'oral behavior', 'oral microbial community', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2018,350321,-0.0023458727199927006
"An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography SUMMARY – OVERALL Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function. Although the basic electron cryo-tomography technique has been used for several decades, the technology is being revolutionized by recent advances in sample preparation, electron cryo-microscopy hardware, improved capabilities for automatic data collection, direct electron detection imaging devices, and phase plate technologies. Combined, these advances led to the ability to generate extraordinarily large numbers of cellular cryo-tomograms of exquisite quality. In principle, such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. However, collection of cellular data is now at a far faster rate than can currently be analyzed with existing methods, producing a serious barrier to progress: to match the data production rates of a single laboratory, at least 50 experienced scientists would need to handle the data analysis. The primary goal of this Program Project is to establish quantitative and highly automated tools for the reconstruction and interpretation of highly complex cellular tomographic data. We have assembled a highly synergistic team of PIs with complimentary expertise in cutting-edge computational and experimental electron microscopy techniques to achieve this goal through collaborative efforts. Project 1 (Hanein & Penczek) focuses on development and implementation of tomogram quality assessment and validation techniques and on experimentally guided optimization of data collection strategies. Project 2 focuses on automatic tomographic reconstruction technology, extraction of various features from the tomograms, and the analysis of distribution patterns derived from the extracted features. Project 3 focuses on development of quantitative tools for tomogram annotation through deep learning and sub-tomogram alignment as well as interactive visualization tools. The set of highly automated tools developed in this Program Project will permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study. NARRATIVE Cellular cryo-tomography has emerged as a critical tool for the visualization and structural study of the molecular nanomachines at the heart of cellular function and—with recent instrumental advances—it is now possible to image hundreds of cells per months, enabling collection of cellular data at a far faster rate than can currently be analyzed. Such large data sets offer insights into cellular variation in disease states as well as better insights into basic cellular function, opening new possibilities for studying the underpinnings of health and disease at the finest possible level, potentially leading to completely new diagnostics for cancer and other cell-altering diseases. This Program Project brings together an accomplished team of investigators to develop new strategies for effectively processing and interpreting this massive influx of data, developing a set of highly automated tools to permit us to interpret 5–10x as much data as is possible using existing methods, greatly expanding the types of cellular variations we can effectively study.",An automated pipeline for macromolecular structure discovery in cellular  electron cryo-tomography,9416022,P01GM121203,"['Address', 'Algorithms', 'Artificial Intelligence', 'Big Data', 'Biological', 'Biological Neural Networks', 'Biology', 'Cancer Diagnostics', 'Cell physiology', 'Cells', 'Classification', 'Collection', 'Complex', 'Computing Methodologies', 'Cryoelectron Microscopy', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Disease', 'Electron Microscopy', 'Electrons', 'Environment', 'Floods', 'Goals', 'Health', 'Heart', 'Human', 'Image', 'Imaging Device', 'Individual', 'Knowledge', 'Laboratories', 'Methodology', 'Methods', 'Molecular', 'Molecular Structure', 'Morphology', 'Pattern', 'Pharmaceutical Preparations', 'Phase', 'Preparation', 'Process', 'Production', 'Real-Time Systems', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Tomogram', 'Validation', 'Variant', 'Visualization software', 'base', 'computer framework', 'deep learning', 'electron tomography', 'experience', 'imaging detection', 'improved', 'insight', 'knowledge base', 'learning strategy', 'nanomachine', 'novel diagnostics', 'particle', 'programs', 'reconstruction', 'response', 'software development', 'statistics', 'tomography', 'tool', 'virtual']",NIGMS,SANFORD BURNHAM PREBYS MEDICAL DISCOVERY INSTITUTE,P01,2018,981617,-0.027233047244888904
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,9639469,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Research Infrastructure', 'Socioeconomic Status', 'Stream', 'Techniques', 'Testing', 'Time', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'Zika Virus', 'base', 'chikungunya', 'climate variability', 'computer infrastructure', 'digital', 'disease transmission', 'experience', 'flu', 'genomic data', 'improved', 'innovation', 'mathematical methods', 'novel', 'open data', 'open source', 'pathogen', 'predictive modeling', 'social', 'social media', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector', 'vector control']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2018,407175,-0.003015586397542168
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9570304,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Simulation', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Supervision', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in vivo', 'insight', 'intracranial artery', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'single photon emission computed tomography', 'spatiotemporal', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2018,528639,-0.005582933117973528
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9420621,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'human microbiota', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiome research', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2018,374683,0.0010379243160592254
"The Blackfynn Platform for Rapid Data Integration and Collaboration Summary One in seven people worldwide suffers from a brain disorder, e.g., epilepsy, Parkinson's, stroke, or dementia. Development of future treatments depends on improving our understanding of brain function and disease, and validating new treatments critically depends on identifying the underlying biomarkers associated with different conditions. Biomarker discovery requires volume, quality, richness, and diversity of data. This Direct-to-Phase II project extends Blackfynn's cloud data management platform for team science, in order to support interactive data curation and integration and to facilitate biomarker discovery. Our first technical aim develops tools to help select, curate, assess, and regularize datasets: we develop novel “live” query capabilities to ensure users discover relevant data, develop mechanisms for using data's provenance to decide on trustworthiness, and build tools for mapping fields to common data elements. These capabilities address the critical, under-served problem of selecting the data to analyze. Our second technical aim develops techniques for incorporating algorithms to link and co-register across multi-modal data and metadata. Using ranking and machine learning, we can incorporate and combine state-of-the-art algorithms for finding data relationships, and we can link to remote data sources. These capabilities enable scientists to analyze richer datasets with multiple data modalities and properties – thus enabling them to discover more complex correlations and biomarkers. In our third aim, Blackfynn's new technical capabilities will be applied to challenges faced by Blackfynn partners, including problems assessing trustworthiness of data annotations, conducting image analysis, modeling epileptic networks, and identifying biomarkers for neuro-oncology indications. As part of this validation we will also develop HIPAA-compliant mechanisms for working with protected and de-identified data together. Together, these three thrusts will ensure that development of the Blackfynn platform results in tools and technologies that meaningfully accelerate scientific understanding and discovery over rich and complex data, leading to improved treatments for neurologic disease.   Narrative This Direct-to-Phase II project extends the Blackfynn cloud data management platform to enable biomarker discovery for research and development of improved drugs, devices and clinical care for patients with neurologic disease: it develops tools for assembling, evaluating, and rating data, and linking it across modalities and to external systems. It also validates the techniques' effectiveness using real challenges faced by Blackfynn partners, in imaging, epilepsy, and brain tumor research.",The Blackfynn Platform for Rapid Data Integration and Collaboration,9468362,R44DA044929,"['Address', 'Algorithms', 'Benchmarking', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain Neoplasms', 'Case Study', 'Clinical Pharmacology', 'Collaborations', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Provenance', 'Data Science', 'Data Set', 'Data Sources', 'Dementia', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Ensure', 'Epilepsy', 'Funding', 'Future', 'Health', 'Health Insurance Portability and Accountability Act', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mental Depression', 'Metadata', 'Modality', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Neurosciences Research', 'Notification', 'Ontology', 'Output', 'Parkinson Disease', 'Patient Care', 'Pharmaceutical Preparations', 'Phase', 'Plug-in', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Science', 'Scientist', 'Semantics', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Standardization', 'Stroke', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Trust', 'Use Effectiveness', 'Validation', 'Work', 'base', 'biomarker discovery', 'clinical application', 'clinical care', 'cloud platform', 'computer science', 'data access', 'data integration', 'data management', 'improved', 'indexing', 'nervous system disorder', 'neuro-oncology', 'neuroimaging', 'novel', 'novel therapeutics', 'open source', 'research and development', 'tool']",NIDA,"BLACKFYNN, INC.",R44,2018,696602,-0.047453435164483714
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9420662,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2018,2000000,-0.029924869771353186
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),9527792,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug Addiction', 'Drug Receptors', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Epigenetic Process', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'algorithmic methodologies', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'cloud platform', 'computational chemistry', 'computer science', 'data mining', 'design', 'distinguished professor', 'dopamine transporter', 'drug abuse prevention', 'falls', 'genome-wide', 'genome-wide analysis', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'novel therapeutics', 'operation', 'prevent', 'professor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2018,1080816,-0.014989845877341989
"Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!) PROJECT SUMMARY/ABSTRACT Our society faces significant challenges in providing quality health care that is accessible by each person and is sensitive to each person's individual lifestyle and individual health needs. Due to recent advances in sensing technologies that have improved in accuracy, increased in throughput, and reduced in cost, it has become relatively easy to gather high resolution behavioral and individualized health data at scale. The resulting big datasets can be analyzed to understand the link between behavior and health and to design healthy behavior interventions. In this emerging area, however, very few courses are currently available for teaching researchers and practitioners about the foundational principles and best practices behind collecting, storing, analyzing, and using behavior- based sensor data. Teaching these skills can help the next generation of students thrive in the increasingly digital world.  The goal of this application is to design online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to WSU faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.  This contribution is significant because not only large research groups but even individual investigators can create large data sets that provide valuable, in-the-moment information about human behavior. They need to be able to handle the challenges that arise when working with sensor- based behavior data. Because students will receive hands-on training with actual sensor datasets and analysis tools, they will know how to get the best results from available tools and will be able to interpret the significance of analysis results.  Our proposed online course program, called AHA!, builds on the investigators' extensive experience and ongoing collaboration at Washington State University on the development of smart home and mobile health app design, activity recognition, scalable biological data mining, and the use of these technologies for clinical applications. Our approach will be to design online course modules to train individuals in the analysis of behavior-based sensor data using clinical case studies (Aim 1). We will design an educational program that involves students from diverse backgrounds and that is findable, accessible, interoperable, and reusable (Aim 2). Finally, we will conduct a thorough evaluation to monitor success and incrementally improve the program (Aim 3). All of the materials will be designed for continued use beyond the funding period of the program. PROJECT NARRATIVE  This program focuses on the development and dissemination of online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to Washington State University faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.",Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!),9482420,R25EB024327,"['Address', 'Aging', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological', 'Case Study', 'Charge', 'Chronic Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Development', 'Discipline', 'E-learning', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Evaluation', 'FAIR principles', 'Face', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'General Population', 'Goals', 'Health', 'Home environment', 'Human', 'Immersion Investigative Technique', 'Individual', 'Interdisciplinary Study', 'Life Style', 'Link', 'Longevity', 'Machine Learning', 'Methods', 'Mobile Health Application', 'Monitor', 'Performance', 'Persons', 'Precision Medicine Initiative', 'Pythons', 'Rehabilitation Nursing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Site', 'Societies', 'Structure', 'Students', 'Suggestion', 'Techniques', 'Technology', 'Training', 'Universities', 'Washington', 'Work', 'base', 'behavior influence', 'behavioral health', 'biocomputing', 'career networking', 'clinical application', 'cognitive rehabilitation', 'cost', 'course development', 'course module', 'data mining', 'design', 'digital', 'experience', 'health care quality', 'health data', 'improved', 'innovation', 'learning materials', 'learning strategy', 'mHealth', 'next generation', 'online course', 'programs', 'recruit', 'responsible research conduct', 'scale up', 'sensor', 'sensor technology', 'skills', 'statistics', 'success', 'synergism', 'tool', 'web page']",NIBIB,WASHINGTON STATE UNIVERSITY,R25,2018,169419,-0.0027900914013791324
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases. PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.",Integration and Visualization of Diverse Biological Data,9415436,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Drug Targeting', 'Expert Systems', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic Diseases', 'Genetic study', 'Goals', 'Gold', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'Supervision', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'diagnostic biomarker', 'drug development', 'drug discovery', 'experimental study', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'novel therapeutics', 'public health relevance', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2018,445349,-0.025817557548633848
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9487840,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2018,141048,-0.015752223679611717
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9495704,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2018,396544,0.002358813970665672
"Successful Clinical Response In Pneumonia Therapy (SCRIPT) Systems Biology Center Overall Project Summary: This innovative integrated systems biology application seeks to delineate the complex host/pathogen interactions occurring at the alveolar level that lead to unsuccessful response to therapy in serious pneumonia. To achieve this objective, we will leverage our unique access to alveolar fluid collected as part of routine clinical care in mechanically ventilated patients with suspected pneumonia in our medical intensive care unit. Bronchoalveolar lavage fluid will be obtained serially from well characterized mechanically ventilated patients with Pseudomonas or Acinetobacter pneumonia. Both of these CDC- designated serious hazard level pathogens have clinical failure rates as high as 50%. A robust clinical definition will allow comparison of both host and pathogen signatures associated with failure of therapy vs. success. These clinical specimens and extensive patient phenomics will anchor two mutually supportive and iterative research projects. Project One will deploy robust tools for flow sorting macrophage and lymphocyte subset populations, isolating RNA from these populations, and performing transcriptomic and epigenomic analysis to compare successful and unsuccessful host responses to infection. Project Two will focus on both specific pathogen genomic profiles associated with unsuccessful outcome. Changes in microbiome communities will be comprehensively assessed by shotgun deep sequencing to detect bacteriophage, other virus, and fungal DNA, in addition to bacterial. The Technology Core will perform cell sorting of NBBAL macrophage and lymphocyte subsets, RNA sequencing, and whole genome methylation, and perform parallel studies in a unique humanized alveolar macrophage mouse model. A Data Management and Bioinformatics Core will develop tools to reduce the dimensionality of these large comprehensive datasets, including the clinical phenomics, and provide them to the Modeling Core. The Modeling Core will then use innovative modeling approaches including a model of the alveolus during pneumonia as an ecosystem out of balance combined with unique machine learning tools and neural networks to generate biomarkers of host, pathogen and/or microbiome patterns predictive of successful pneumonia outcome. Predictive biomarkers developed in the Modeling Core will then be validated in a prospective confirmatory cohort of patients in whom analogous data will be generated. The Administrative Core will perform the outward-facing role of education and outreach to the community and sponsor, as well as regularly exchanging datasets, analytic tools, and specimens with NIH-sponsored/approved repository sites. Project Narrative: The Successful Clinical Response In Pneumonia Treatment (SCRIPT) systems biology center seeks to delineate the complex host/pathogen interactions occurring at the alveolar level that lead to unsuccessful response to therapy in serious pneumonia. We will leverage our unique access to alveolar fluid collected as part of routine clinical care in mechanically ventilated patients to generate clinical phenomic, transcriptomic, epigenomic and metagenomic data that describe the host response, pathogen characteristics and microbiome of the alveolar space during pneumonia. We will then integrate this comprehensive phenotypic data into an ecosystem-based model to generate predictive biomarkers of pneumonia outcome for subsequent validation in a second cohort and tested for causality in a humanized alveolar macrophage mouse model.",Successful Clinical Response In Pneumonia Therapy (SCRIPT) Systems Biology Center,9454818,U19AI135964,"['Acinetobacter', 'Address', 'Alveolar', 'Alveolar Macrophages', 'Alveolus', 'Animal Model', 'Antibiotic Therapy', 'Bacteriophages', 'Bioinformatics', 'Biological Markers', 'Biological Neural Networks', 'Bronchoalveolar Lavage', 'Bronchoalveolar Lavage Fluid', 'Cause of Death', 'Cell Separation', 'Cells', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Microbiology', 'Clinical Trials', 'Collection', 'Communities', 'Complex', 'DNA Viruses', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Education and Outreach', 'Equilibrium', 'Etiology', 'Failure', 'Fungal DNA', 'Generations', 'Genes', 'Genetic', 'Goals', 'Human', 'Immune response', 'Infection', 'Intensive Care Units', 'Lead', 'Link', 'Liquid substance', 'Lymphocyte Subset', 'Lymphoid Cell', 'Machine Learning', 'Mechanics', 'Medical', 'Metagenomics', 'Methylation', 'Modeling', 'Myeloid Cells', 'Nosocomial Infections', 'Nosocomial pneumonia', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Pneumonia', 'Population', 'Preparation', 'Prospective cohort', 'Protocols documentation', 'Pseudomonas', 'Pseudomonas aeruginosa', 'RNA', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Secondary to', 'Shotguns', 'Site', 'Sorting - Cell Movement', 'Specimen', 'Standardization', 'Stream', 'System', 'Systems Biology', 'Technology', 'Testing', 'Treatment Failure', 'United States National Institutes of Health', 'Validation', 'Virulence', 'analytical tool', 'base', 'clinical care', 'cohort', 'data management', 'deep sequencing', 'epigenomics', 'experience', 'genomic data', 'genomic profiles', 'hazard', 'humanized mouse', 'improved', 'innovation', 'macrophage', 'microbiome', 'microbiome composition', 'mouse model', 'multiple omics', 'new therapeutic target', 'novel', 'pathogen', 'phenomics', 'phenotypic data', 'pneumonia model', 'predictive marker', 'predictive tools', 'prospective', 'repository', 'response', 'success', 'tool', 'transcriptome sequencing', 'transcriptomics', 'viral DNA', 'whole genome']",NIAID,NORTHWESTERN UNIVERSITY AT CHICAGO,U19,2018,2400000,0.0025797422099569285
"Fluomics: The Next Generation Influenza A virus is a major human respiratory pathogen, and available vaccines and antivirals are of limited efficacy. In order to identify novel targets for therapeutic intervention during influenza virus infection, we have assembled an interdisciplinary team that uses a highly integrated systems level approach to identify and validate key genes/networks involved in virus pathogenesis. The overarching theme of our multidisciplinary proposal “FluOMICS: The NEXT Generation” is to obtain multiple OMICS-based systems level measurements and integrate them using modeling approaches and machine learning algorithms to identify and validate 1) host-virus networks that modulate influenza A virus disease severity, 2) biomarkers in blood that reflect the activation states of these networks and 3) novel host targets for therapeutic interventions. Our underlying main hypothesis is that host networks involved in viral replication and early host responses regulate disease outcomes and represent targets for therapeutic intervention. The proposed studies leverage on our previous collaborations that generated global datasets and models that predict severity of disease caused by three influenza virus strains with different levels of virulence. While our previous studies gave many novel insights in influenza pathogenesis, they likely provide a narrow window on the determinants of disease severity in humans. Thus, it is necessary expand beyond the specific virus strains that were used to study pathogenesis, and explore a broader context of viral and host perturbations linked to clinical outcomes. In order to identify clinically relevant networks involved in influenza virus pathogenesis we propose to integrate into predictive and comprehensive models global responses during influenza virus infection in three systems 1) human blood from a human cohort of patients with documented influenza virus infection and diverse clinical outcomes (Project 1); 2) mouse blood and tissues from experimentally infected animals under a variety of conditions and perturbations resulting in diverse disease outcomes (Project 1) and 3) relevant primary human cells (Project 2). Samples will be processed and send to the Technology Core for global transcriptomics, proteomics and metabolomics analysis. OMICS data sets will be integrated and compared by the Modeling Core to generate network models of disease, uncover blood biomarkers and identify key drivers as targets for therapeutic intervention. Predicted network regulators will be used as a source for subsequent iterative rounds of perturbations to refine existing and to identify new network disease models. Data and models will be managed and disseminated by the Data Management and Bioinformatics Core. We expect that these studies will uncover and validate novel pathogenic networks, blood biomarkers associated with them, and specific therapeutic targets to revert pathogenic networks. In summary, our modeling approaches will find correlates and associations between diverse experimental systems that will help us define human blood biomarkers, and link them to in vivo and ex vivo signatures for both companion diagnostics and personalized therapies. We propose a systematic approach (FluOMICS) to generate predictive models of influenza virus pathogenesis which will a) allow us to identify biomarkers for predicting disease outcome, and b) provide avenues to explore for new, host-directed, therapeutic interventions.",Fluomics: The Next Generation,9455006,U19AI135972,"['Algorithms', 'Animals', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Biological Markers', 'Blood', 'CRISPR screen', 'Cells', 'Clinical', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Complementary DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Disease', 'Disease Outcome', 'Disease model', 'Environmental Risk Factor', 'Experimental Models', 'Genes', 'Genetic Polymorphism', 'Human', 'Immune response', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A virus', 'Integration Host Factors', 'Intervention', 'Link', 'Lung', 'Lung diseases', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Profiling', 'Mus', 'Network-based', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Post-Translational Protein Processing', 'Predisposition', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Sampling', 'Severities', 'Severity of illness', 'Small Interfering RNA', 'Source', 'System', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic Intervention', 'Tissues', 'Vaccines', 'Viral', 'Virulence', 'Virus', 'Virus Diseases', 'Virus Replication', 'base', 'clinically relevant', 'cohort', 'companion diagnostics', 'data integration', 'data management', 'early detection biomarkers', 'epigenome', 'epigenomics', 'functional genomics', 'in vivo', 'influenza virus strain', 'influenzavirus', 'insight', 'metabolome', 'metabolomics', 'mouse model', 'multidisciplinary', 'multiple omics', 'network models', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'personalized medicine', 'predictive marker', 'predictive modeling', 'programs', 'protein protein interaction', 'resilience', 'respiratory', 'response', 'specific biomarkers', 'targeted treatment', 'therapeutic target', 'transcriptome', 'transcriptomics', 'virus pathogenesis']",NIAID,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U19,2018,2799687,0.007618075548388424
"Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity   Project Summary    For something as complex and multifaceted as bacterial antibiotic resistance (AR), our drug evaluation  paradigm is strikingly narrow and homogenous: MIC/MBC testing in standardized bacteriologic media. We  have shown that this drug evaluation paradigm is inadequate, even misleading, as changes in the media  conditions of the procedure lead to dramatically different results. A more holistic definition of antibiotic therapy  that centers on understanding antibiotic activity in synergy with host innate immune factors such as cationic  antimicrobial peptides (AMPs), serum and phagocytic cells (e.g. neutrophils) reveals therapeutic options  unrecognized in standard testing. The proposed U01 program represents a groundbreaking approach to use  systems biology approaches and inform more effective antibiotic utilization in the context of host innate  immunity. We propose to: 1) build an iterative systems biology workflow that integrates multiple experimental  and computational approaches to give a comprehensive assessment of AR; and 2) apply this workflow to high  priority pathogens to systematically elucidate AR mechanisms and their condition­dependency. The iterative  workflow includes: (i) omics and physiological data generation.  Clinically isolated strains of the selected  pathogens will be grown under conventional testing (bacteriologic media) and more physiologic conditions  (tissue culture media, serum, and in presence of AMPs and neutrophils) to probe for advantageous gain of  activity.  The omics data types collected are: DNA resequencing, RNAseq, and metabolomics.  (ii)  Bioinformatics and data modeling analysis involves three approaches: big data analysis for data set  dimensionality and coarse grained variable dependencies assessment, genome­scale modeling for  mechanistic elucidation and analysis, and machine learning that uses AR­related metadata to classify the  overall biological functions. This analysis will lead to understanding of AR mechanisms.  (iii) Multi­scale  validation from animal models, to laboratory evolution, to cytology, to gene expression alteration, to structural  protein analysis of putative targets. The validation thus ranges from host behavior to atomistic detail of  ligand­target interactions. The iterative loop then closes, comparing computational prediction to experimental  outcomes. False­negative and false­positive predictions are then algorithmically analyzed by a hypothesis  generating family of algorithms that then makes suggestions about what conditions to use in the next iteration  of the loop.  The pathogens that we will focus on are methicillin­resistant ​Staphylococcus aureus ​(MRSA), the  carbapenem­resistant Enterobacteriaceae (CRE) Klebsiella ​pneumoniae ​and ​Acinetobacter baumannii,​ and  Pseudomonas aeruginosa​. The team of investigators has made the foundational observations and led the  development of the technologies on which the iterative workflow is based. A multi­ and genome­scale methods  of systems biology fulfills requirements of RFA­AI­14­064 to which it responds.              Narrative    The current evaluation of antibiotic drug candidates in drug discovery and in clinical medicine is conducted in  laboratory media that ignores the actual physiologic conditions in the host and the host immune system.  We  have discovered potent antimicrobial activities of existing antibiotics against highly “drug­resistant superbugs”  that are currently ignored but revealed in synergy with the human immune system. This program proposes a  holistic and comprehensive systems biology approach to systematically discover novel treatment opportunities  and underlying mechanisms using a novel iterative data generation, analysis, and modeling workflow.       ",Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity,9440969,U01AI124316,"['Acinetobacter baumannii', 'Algorithmic Analysis', 'Algorithms', 'Animal Model', 'Animals', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Antimicrobial Cationic Peptides', 'Bacterial Antibiotic Resistance', 'Bacteriology', 'Behavior', 'Big Data', 'Bioinformatics', 'Biological Assay', 'Biological Process', 'Biological Products', 'Biology', 'Clinical', 'Clinical Medicine', 'Collection', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Culture Media', 'Cytology', 'DNA Resequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Drug Evaluation', 'Drug resistance', 'Effectiveness', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Future', 'Gene Expression Alteration', 'Generations', 'Genome', 'Grain', 'Growth', 'Human', 'Immune', 'Immune system', 'Immunity', 'Immunologic Factors', 'Infection', 'Integration Host Factors', 'Klebsiella pneumonia bacterium', 'Knowledge', 'Laboratories', 'Lead', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular Biology', 'Molecular Profiling', 'Mus', 'Natural Immunity', 'Network-based', 'Organism', 'Outcome', 'Output', 'Participant', 'Phagocytes', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Physiological', 'Predisposition', 'Procedures', 'Process', 'Protein Analysis', 'Pseudomonas aeruginosa', 'Research Personnel', 'Resistance', 'Resistance development', 'Risk Assessment', 'Series', 'Serum', 'Standardization', 'Statistical Data Interpretation', 'Structural Protein', 'Suggestion', 'Superbug', 'Systems Biology', 'Testing', 'Therapeutic', 'Update', 'Validation', 'antimicrobial', 'antimicrobial peptide', 'bacterial genetics', 'base', 'carbapenem-resistant Enterobacteriaceae', 'data modeling', 'design', 'drug candidate', 'drug discovery', 'experience', 'genome-wide', 'human tissue', 'improved', 'in vivo', 'macromolecule', 'metabolomics', 'methicillin resistant Staphylococcus aureus', 'microbial', 'microbial host', 'multi-drug resistant pathogen', 'multidrug-resistant Pseudomonas aeruginosa', 'multiple omics', 'neutrophil', 'novel', 'pathogen', 'product development', 'programs', 'reconstruction', 'resistance mechanism', 'response', 'screening', 'synergism', 'technology development', 'tissue culture', 'tool', 'transcriptome sequencing', 'transcriptomics', 'treatment response']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2018,2206539,-0.00880788892583872
"Development of mosaic mouse models of HCC for genetic interspecies inference DESCRIPTION (provided by applicant): Hepatocellular carcinoma (HCC) remains a menace for human health for the lack of any effective treatment. HCC is usually the end result of chronic liver diseases associated with diverse risk factors. Furthermore, non- alcoholic steatohepatitis (NASH) induced HCC, which is projected to be the leading cause of new cases, remains poorly characterized. Although many mouse models of HCC have been developed, it is unclear how well they represent different subgroups of human HCCs. This research plan proposes to transform HCC animal modeling by establishing a novel in vivo platform to accurately replicate the somatic molecular profiles of human HCC in mice. To do this, three independent HCC mouse models will be exhaustively characterized through exome sequencing and gene expression profiling. Using bioinformatics techniques including machine- learning and network analysis, these datasets will be compared with human HCC datasets from TCGA and the ICGC to identify subgroups of patients with similar somatic molecular profiles to the HCC mouse models as well as refined minimum sets of characteristic genetic aberrations. A derived transposon system will be used to generate mosaic mouse models replicating human HCC genetic subgroups faithfully. These models will enable 1) experimental dissection of the molecular mechanisms underlying distinct etiologies of HCC, 2) systematic assessment of candidate HCC therapies and 3) investigation of therapeutic resistances. This K99/R00 career development award proposal describes a two-year mentored and three-year independent research program essential for the development of Dr. Font-Burgada as an independent investigator. Dr. Font-Burgada received his PhD at University of Barcelona, Spain, for the work he performed to investigate basic chromatin regulatory and epigenetic mechanisms. He then moved to University of California, San Diego where he joined Dr. Michael Karin's laboratory to train in mouse models of cancer and signal transduction. For the accomplishment of this research proposal, Dr. Font-Burgada has designed a strong training and career development plan consisting of: 1- the continued mentorship of Dr. Michael Karin to gain additional expertise in mouse models of HCC and signal transduction, 2- Training in bioinformatics, specifically in methods to identify cancer driver genetic aberrations and network-based approaches for comparative genomic analysis of mouse and human HCCs, to be overseen by co-mentor Dr. Hannah Carter, an Assistant Professor of Medicine, at UCSD. 3- Training in application of emerging transposon vector technologies to generate mosaic mouse models for in vivo analysis of oncogenic pathways. 4- Career development courses and seminars in a supportive academic environment in the Department of Pharmacology at UCSD to complement other aspects of the training program. This training plan will be overseen by an advisory committee comprising 4 members, mentor, co-mentor, and additional experts in mouse models of cancer and bioinformatics, Inder Verma and Trey Ideker, providing key scientific insights and essential guidance in critical steps in Dr. Font-Burgada transition to independence. PUBLIC HEALTH RELEVANCE: Hepatocellular carcinoma (HCC) has a 5-year survival rate of less than 10% and leads to 700000 deaths globally each year. The genetic causes of this disease are poorly understood and the only available targeted therapy extends life expectancy by a mere 3 months. In order to improve these dismal statistics, I am developing a novel class of mouse models capable of reproducing the spectrum of mutations observed in a given human tumor, to enable study of HCC biology as well as systematic testing of therapeutic combinations.",Development of mosaic mouse models of HCC for genetic interspecies inference,9441722,R00CA191152,"['Advisory Committees', 'Animal Model', 'BRAF gene', 'Beauty', 'Bioinformatics', 'Biology', 'California', 'Cancer Etiology', 'Cancer Model', 'Catalogs', 'Cell Culture Techniques', 'Cells', 'Cessation of life', 'Characteristics', 'Chromatin', 'Clinical', 'Comparative Genomic Analysis', 'Complement', 'Complex', 'Data', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Dissection', 'Doctor of Philosophy', 'Drowsiness', 'Drug Targeting', 'Ensure', 'Environment', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Expression Profiling', 'Gene Expression Profiling', 'Genetic', 'Geography', 'Health', 'Hepatocyte', 'Heterogeneity', 'High Fat Diet', 'Human', 'Human Pathology', 'Investigation', 'K-Series Research Career Programs', 'Knowledge', 'Label', 'Laboratories', 'Life Expectancy', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mosaicism', 'Mouse Protein', 'Mus', 'Mutate', 'Mutation', 'Network-based', 'Oncogenic', 'Orthologous Gene', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Phase', 'Phenotype', 'Physiological', 'Population', 'Pre-Clinical Model', 'Primary carcinoma of the liver cells', 'Reproducibility', 'Research', 'Research Personnel', 'Research Proposals', 'Resistance', 'Risk', 'Risk Factors', 'Signal Transduction', 'Somatic Mutation', 'Spain', 'Subgroup', 'Survival Rate', 'System', 'Techniques', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Therapeutic', 'Training', 'Training Programs', 'Transgenes', 'Translating', 'Universities', 'Viral Vector', 'Work', 'Xenograft Model', 'accurate diagnosis', 'base', 'career development', 'chronic liver disease', 'combinatorial', 'comparative', 'course development', 'design', 'effective therapy', 'exhaustion', 'exome sequencing', 'human disease', 'improved', 'in vivo', 'in vivo Model', 'insight', 'member', 'mouse model', 'neoplastic cell', 'nonalcoholic steatohepatitis', 'novel', 'novel therapeutics', 'patient subsets', 'pre-clinical', 'professor', 'programs', 'public health relevance', 'response', 'statistics', 'success', 'targeted treatment', 'therapeutic evaluation', 'therapy resistant', 'tool', 'tumor', 'tumor progression', 'vector']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R00,2018,249000,-0.01471170853979563
"Acceleration techniques for SimSET SPECT simulations Abstract The Simulation System for Emission Tomography (SimSET) is one of the foundational tools for emission tomography research, used by hundreds of researchers worldwide for both positron emission tomography (PET) and single photon emission computed tomography (SPECT). It has proven to be accurate and efficient for both PET and low energy SPECT studies; because SimSET uses a geometric model for its SPECT collimation, it is less accurate for high energy isotopes. This application proposes to address this with the use of angular response functions (ARFs), a technique that has proven to accurately model SPECT collimation and detection for high-energy isotopes more efficiently than full photon-tracking simulations. In addition, we propose a novel ARF-based importance sampling method that will speed these simulations by a factor of >50. The generation of ARF tables is another consideration: it is extremely compute intensive and has caused ARF to be used only when a large number of simulations are needed using the same isotope/collimator/detector combination. For this reason, we also propose application of importance sampling to speed the generation of ARF tables by a factor 5, and the creation of a library of angular response functions for popular isotope/collimator/detector combinations. The former will lessen the computational cost of generating the tables, the latter will, for many users/uses, eliminate the need to generate ARF tables at all. This will greatly expand the potential applications of ARF-based simulations. Our first aim is to accelerate SimSET SPECT simulations without sacrificing accuracy. This will be accomplished by synergistically utilizing two tools: variance reduction and angular response function (ARF) tables. Variance reduction includes importance sampling and forced detection. We hypothesis that these techniques combined with information from our angular response function tables will improve SimSET simulation efficiency by >50 times of SPECT simulations of specific radioisotopes (e.g., I-123, Y-90, etc.). Our second aim is to accelerate ARF table generation. This will be accomplished by using importance sampling methods in the generation of ARFs. We further propose to use an adaptive stratification scheme that will simulate photons for a given table position only as long as required to determine its value to a user-specified precision. Our third aim is to create a library of pre-calculated ARF tables for popular vendor isotope/collimator/detector configurations. These ARF tables will then be made publically available for download through the SimSET website. With a registered user base of >500, we believe that these enhancements to SimSET will have far reaching impact on research projects throughout the world. Narrative The overall goal of this work is to develop methods to speed up the SimSET Monte Carlo-based simulation software for single photon computed tomography (SPECT) imaging systems by greater than 50-fold. This type of speed up with enable new research that was previously impractical due to the computation time required for simulation. In addition, all software tools and tables developed within this project will be made available via a web-based host.",Acceleration techniques for SimSET SPECT simulations,9583854,R03EB026800,"['90Y', 'Acceleration', 'Address', 'Algorithms', 'Collimator', 'Communities', 'Crystallization', 'Data', 'Detection', 'Foundations', 'Future', 'Generations', 'Goals', 'Industrialization', 'Institution', 'Isotopes', 'Libraries', 'Location', 'Machine Learning', 'Medical Research', 'Methods', 'Modeling', 'Online Systems', 'Photons', 'Positioning Attribute', 'Positron-Emission Tomography', 'Probability', 'Radioisotopes', 'Research', 'Research Personnel', 'Research Project Grants', 'Running', 'Sampling', 'Scheme', 'Software Tools', 'Specific qualifier value', 'Speed', 'Stratification', 'System', 'Techniques', 'Testing', 'Thick', 'Time', 'Training', 'Vendor', 'Weight', 'Work', 'X-Ray Computed Tomography', 'base', 'cost', 'detector', 'imaging system', 'improved', 'interest', 'novel', 'response', 'simulation', 'simulation software', 'single photon emission computed tomography', 'synergism', 'thallium-doped sodium iodide', 'tomography', 'tool', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R03,2018,74515,-0.006219890559378438
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9545035,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2018,67704,-0.010306998741517122
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),9571275,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Privacy', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Research Infrastructure', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data management', 'data portal', 'data sharing', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'social', 'success', 'theories', 'therapy design', 'tool']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2018,220751,-0.0039192263057880065
"COINSTAC: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community be- comes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2). The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates a dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informat- ics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an inde- pendent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented stor- age vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and pri- vacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix fac- torization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. 4 Project Narrative  Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don’t have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this ‘missing data’ and allow for pooling of both open and ‘closed’ repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed compu- tational solution for a large toolkit of widely used algorithms. 3","COINSTAC: decentralized, scalable analysis of loosely coupled data",9717051,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,282320,-0.0028889863494764227
"A Systems Biology Approach to Investigate the Structure Changes of Biological Network Project Summary/Abstract Networks have been widely used to describe many biological processes. Understanding the structure of biological network, especially regulatory network, will provide a key to discovering the mechanisms underlying important biological processes and pathogenesis of diseases. One of the most challenging tasks in systems biology is how to correctly reconstruct the networks from the high-dimensional data generated by modern genomic technology. Most network inference methods assume the network structure is time-invariant. Some recent studies revealed the structures of some biological networks are non-stationary or time-varying. For example, the neural information flow networks of brains are changing during learning process. Importantly, cancer studies found the native T cells would be converted into senescent T cells due to the structure changes of genetic network during tumorigenesis. The stationary network inference methods can't be used to reconstruct the time-varying network. Non-stationary network inference methods are urgently needed to investigate the time-varying networks at different stages. Some researchers have attempted to develop some time-varying network inference methods. However, the inferred networks using existing methods are only correlation or causality graphs, not regulatory networks which require activation & inhibition information. This project aims to develop novel non-stationary network inference methods to reconstruct time-varying regulatory networks from time series data. Since the networks are highly complex, it is not realistic to manually verify large networks as being used by the traditional methods. We will develop a powerful Model Checker, which is a Turing Award winning technique for hardware system verification, to intelligently verify the inferred time-varying networks. Our long-term goal is to integrate the statistical inference and model checking techniques in a unified platform to automatically reconstruct and verify time-varying networks. This integrative systems biology approach will make the large-network inference and verification automatic, intelligent and efficient. Recent cancer studies show that, restoring senescent T cells represents a promising strategy for cancer treatment. In collaboration with cancer immunologist, we will apply computational-experimental approaches to investigate what structure changes of the genetic network and how they induce T-cell's functional changes and influence its fate decision making from naive T-cells to senescent T-cells. Answering these questions will significantly improve our understanding of the mechanisms underlying the T cell differentiation during tumorigenesis. Public Health Relevance/Narrative This project aims to develop a novel systems biology approach to reconstruct the time-varying biological networks from high-dimensional data in collaboration with the cancer immunologist. The proposed research has relevance to public health, because it seeks to investigate what and how the structure changes of genetic network induce the T-cell's functional changes during tumorigenesis, which will ultimately improve our understanding of the mechanisms underlying the T cell differentiation and cancer.",A Systems Biology Approach to Investigate the Structure Changes of Biological Network,9655801,R15GM129696,"['Algorithms', 'Attention', 'Award', 'Bayesian Modeling', 'Biological', 'Biological Process', 'Brain', 'Cancer Immunology Science', 'Cells', 'Code', 'Collaborations', 'Complex', 'Data', 'Databases', 'Decision Making', 'Development', 'Disease', 'Drosophila genus', 'Etiology', 'Evolution', 'Gene Structure', 'Genetic', 'Genomics', 'Goals', 'Graph', 'Immunologist', 'Knowledge', 'Laboratories', 'Learning', 'Life Cycle Stages', 'Location', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Muscle Development', 'Mutation', 'Pathogenesis', 'Process', 'Public Health', 'Regulator Genes', 'Regulatory T-Lymphocyte', 'Research', 'Research Personnel', 'Series', 'Structure', 'System', 'Systems Biology', 'T cell differentiation', 'T-Lymphocyte', 'Techniques', 'Technology', 'Time', 'Work', 'base', 'cancer therapy', 'computer studies', 'exhaust', 'experimental study', 'high dimensionality', 'improved', 'neoplastic cell', 'next generation sequencing', 'novel', 'public health relevance', 'reconstruction', 'relating to nervous system', 'senescence', 'tool', 'tumor microenvironment', 'tumorigenesis']",NIGMS,SAINT LOUIS UNIVERSITY,R15,2018,454500,-0.023696420648642032
"Precision Medicine in Sarcoidosis ABSTRACT Sarcoidosis is a systemic inflammatory disease of unknown etiology characterized by non-caseating granulomas in affected organs, primarily in the lungs. Approximately 30% of patients with sarcoidosis progress to debilitating disease; however, the drivers of susceptibility or resilience to disease remain poorly understood. An inflammatory response to an undefined antigen is postulated as the etiology of granuloma formation, and the pathogenesis has been suggested to involve gene-pathogen interaction, yet analysis of single genes or microbes has not proven applicable to diagnosis of all forms of sarcoidosis. Indeed, rather than a single organism, the disease may represent an interaction between the community of organisms that comprise the lung microbiome (community of organisms that live in and on us) and the host immune response. We propose that understanding the microbiome/host interaction will suggest strategies for precision medicine approaches to sarcoidosis. This proposal addresses this significant gap by investigating interactions between the lung microbiome, host immune and clinical responses in sarcoidosis using multiomics approaches – a critically innovative strategy. Our preliminary data support our novel hypotheses. First, we identified distinct lung microbiomes that differentiated patients with sarcoidosis versus controls. Second, our results identified biomarkers of disease severity that were associated with decreased lung function. Third, a recurrent analytic theme that emerged, regardless of the type of -omic analysis, was that sarcoidosis is characterized by pathways related to apoptosis and autophagy, which is consistent with our observation of decreased abundance of peripheral lymphocytes and functional immune anergy. These data led us to our Overall Hypothesis: Lung microbiome and host immune interactions characterized by apoptosis and autophagy pathways influence sarcoidosis clinical course. This hypothesis will be tested by an observational prospective and validation study of sarcoidosis patients at 5 time points to facilitate time series analyses. Aims 1 and 2 focus on lung microbiome or host immune responses, respectively, in relation to clinical course of sarcoidosis. Using these data in Aim 3, predictive models will be constructed based on integrated data of metagenomic and host-immune interactions. The novelty and significance of our multiomics strategy is to construct models for precision medicine therapies to harness bioinformatic strategies into focused, patient-specific approaches. The long-term significance of this study is to define pathways for sarcoidosis progression or resolution, and to develop database of these findings to further develop more precise, testable, models. PROJECT NARRATIVE Sarcoidosis is a disease of unknown etiology that predominately affects the lung and may affect other organs. We propose to construct a model to predict sarcoidosis progression or resolution by identifying microbial and immune interactions. We postulate that these models will be helpful in designing therapeutic options.",Precision Medicine in Sarcoidosis,9530040,R01HL138628,"['Address', 'Affect', 'Algorithms', 'Antigens', 'Apoptosis', 'Apoptotic', 'Autophagocytosis', 'Bioinformatics', 'Biological Markers', 'Blood', 'Bronchoalveolar Lavage', 'Clinical', 'Communities', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Etiology', 'Feces', 'Genes', 'Granuloma', 'Immune', 'Immune response', 'Immunity', 'Inflammatory', 'Inflammatory Response', 'Lung', 'Lymphocyte', 'Machine Learning', 'Messenger RNA', 'Metagenomics', 'Methods', 'MicroRNAs', 'Microbe', 'Modeling', 'Organ', 'Organism', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Peripheral', 'Phenotype', 'Predisposition', 'Prospective Studies', 'Recurrence', 'Resolution', 'Respiratory physiology', 'Sarcoidosis', 'Severity of illness', 'Taxonomy', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'Tissue-Specific Gene Expression', 'anergy', 'base', 'clinical predictors', 'clinically relevant', 'cytokine', 'deep sequencing', 'design', 'host microbiome', 'indexing', 'inflammatory marker', 'innovation', 'lung microbiome', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbiome', 'novel', 'novel strategies', 'outcome forecast', 'pathogen', 'precision medicine', 'predictive modeling', 'resilience', 'response', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'validation studies']",NHLBI,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,680760,-0.020226994388651734
"Precision Medicine in Sarcoidosis ABSTRACT Sarcoidosis is a systemic inflammatory disease of unknown etiology characterized by non-caseating granulomas in affected organs, primarily in the lungs. Approximately 30% of patients with sarcoidosis progress to debilitating disease; however, the drivers of susceptibility or resilience to disease remain poorly understood. An inflammatory response to an undefined antigen is postulated as the etiology of granuloma formation, and the pathogenesis has been suggested to involve gene-pathogen interaction, yet analysis of single genes or microbes has not proven applicable to diagnosis of all forms of sarcoidosis. Indeed, rather than a single organism, the disease may represent an interaction between the community of organisms that comprise the lung microbiome (community of organisms that live in and on us) and the host immune response. We propose that understanding the microbiome/host interaction will suggest strategies for precision medicine approaches to sarcoidosis. This proposal addresses this significant gap by investigating interactions between the lung microbiome, host immune and clinical responses in sarcoidosis using multiomics approaches – a critically innovative strategy. Our preliminary data support our novel hypotheses. First, we identified distinct lung microbiomes that differentiated patients with sarcoidosis versus controls. Second, our results identified biomarkers of disease severity that were associated with decreased lung function. Third, a recurrent analytic theme that emerged, regardless of the type of -omic analysis, was that sarcoidosis is characterized by pathways related to apoptosis and autophagy, which is consistent with our observation of decreased abundance of peripheral lymphocytes and functional immune anergy. These data led us to our Overall Hypothesis: Lung microbiome and host immune interactions characterized by apoptosis and autophagy pathways influence sarcoidosis clinical course. This hypothesis will be tested by an observational prospective and validation study of sarcoidosis patients at 5 time points to facilitate time series analyses. Aims 1 and 2 focus on lung microbiome or host immune responses, respectively, in relation to clinical course of sarcoidosis. Using these data in Aim 3, predictive models will be constructed based on integrated data of metagenomic and host-immune interactions. The novelty and significance of our multiomics strategy is to construct models for precision medicine therapies to harness bioinformatic strategies into focused, patient-specific approaches. The long-term significance of this study is to define pathways for sarcoidosis progression or resolution, and to develop database of these findings to further develop more precise, testable, models. PROJECT NARRATIVE Sarcoidosis is a disease of unknown etiology that predominately affects the lung and may affect other organs. We propose to construct a model to predict sarcoidosis progression or resolution by identifying microbial and immune interactions. We postulate that these models will be helpful in designing therapeutic options.",Precision Medicine in Sarcoidosis,9708373,R01HL138628,"['Address', 'Affect', 'Algorithms', 'Antigens', 'Apoptosis', 'Apoptotic', 'Autophagocytosis', 'Bioinformatics', 'Biological Markers', 'Blood', 'Bronchoalveolar Lavage', 'Clinical', 'Communities', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Etiology', 'Feces', 'Genes', 'Granuloma', 'Immune', 'Immune response', 'Immunity', 'Inflammatory', 'Inflammatory Response', 'Lung', 'Lymphocyte', 'Machine Learning', 'Messenger RNA', 'Metagenomics', 'Methods', 'MicroRNAs', 'Microbe', 'Modeling', 'Organ', 'Organism', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Peripheral', 'Phenotype', 'Predisposition', 'Prospective Studies', 'Recurrence', 'Resolution', 'Respiratory physiology', 'Sarcoidosis', 'Severity of illness', 'Taxonomy', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'Tissue-Specific Gene Expression', 'anergy', 'base', 'clinical predictors', 'clinically relevant', 'cytokine', 'deep sequencing', 'design', 'host microbiome', 'indexing', 'inflammatory marker', 'innovation', 'lung microbiome', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbiome', 'novel', 'novel strategies', 'outcome forecast', 'pathogen', 'precision medicine', 'predictive modeling', 'resilience', 'response', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'validation studies']",NHLBI,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2018,143095,-0.020226994388651734
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9473021,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,649098,-0.0010096845818889015
"Joint Analysis of Microbiome and Other Genomic Data Types PROJECT SUMMARY In the same way that the human genome project created invaluable genomic maps, the objective of this project is to develop methods for eventual construction of comprehensive genetic and metabolomic by microbome relationship maps. Such maps would be an invaluable resource for improving our understanding as to the underlying mechanisms by which microbes and –omics features influence human diseases and conditions, potentially leading to identification of novel therapeutic targets. To these ends, this proposal seeks to develop statistical and computational tools for mapping associations and interactions between microbes and other – omic features and for further utilizing other –omics to improve microbiome based prediction models. Specifically, motivated by studies examining the role of the vaginal microbiome and other –omics in birth outcomes and menopause, we aim to develop statistical methodology for (1) mapping genetic variants that influence microbiome composition so as to understand the innate component of the microbiome as well as learn mechanisms by which genetics influence outcomes; (2) creating global metabolic maps integrating both microbes and metabolites which will enable understanding of how perturbations might influence the system and identify key pathways for therapeutic target; (3) exploiting other –omics in constructing more accurate microbiome based prediction models for preterm birth; (4) developing, distributing and supporting software packages for the proposed methods. The methods are based on frameworks in which we have considerable experience, but novel technical contributions are made to accommodate features of the data such as population stratification and relatedness in genetics, phylogenetic structure, and compositionality, as well as practical considerations such as availability of samples and other –omics data. Consequently, these new methods have the potential for accelerating mechanistic and translational microbiome studies, developing vital resources for enabling systematic achievement of many biological, clinical, and public health problems that have eluded researchers for decades. PROJECT NARRATIVE The methods developed in this proposal will enable improved understanding of the interactions between microbes and other –omics, thus aiding in elucidation of the mechanisms by which microbes and –omic features influence health outcomes and aiding in identification of potential molecular targets. Further emphasis is placed on utilization of other –omics to develop microbiome based prediction models in pregnancy outcomes, improving early detection of women who are at risk of preterm delivery.",Joint Analysis of Microbiome and Other Genomic Data Types,9577818,R01GM129512,"['Achievement', 'Address', 'Area', 'Automobile Driving', 'Biological', 'Birth', 'Chromosome Mapping', 'Clinical', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Early Diagnosis', 'Future', 'Genes', 'Genetic', 'Genomics', 'Grouping', 'Health', 'Human Genome Project', 'Joints', 'Lasso', 'Learning', 'Machine Learning', 'Maintenance', 'Maps', 'Medical', 'Menopause', 'Metabolic', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Modification', 'Molecular Target', 'Network-based', 'Outcome', 'Pathway interactions', 'Performance', 'Phylogenetic Analysis', 'Pregnancy Outcome', 'Premature Birth', 'Procedures', 'Public Health', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'Sampling', 'Scientific Advances and Accomplishments', 'Structure', 'System', 'Taxonomy', 'Training', 'Woman', 'Work', 'base', 'biological systems', 'computerized tools', 'experience', 'genetic variant', 'genomic data', 'human disease', 'improved', 'interest', 'loss of function', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiome analysis', 'microbiome components', 'microbiome composition', 'microbiome research', 'new therapeutic target', 'novel', 'open source', 'population stratification', 'predictive modeling', 'simulation', 'software development', 'therapeutic target', 'tool', 'translational study', 'user friendly software', 'vaginal microbiome']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2018,396000,-0.012579079785698019
"Integrating Bioinformatics and Clustering Analysis for Disease Surveillance ﻿    DESCRIPTION (provided by applicant):  There has been a tremendous focus in bioinformatics on translation of data from the bench into information and knowledge for clinical decision-making. This includes analysis of human genetics for personalized medicine and treatment. However, there has been much less attention on translational bioinformatics for public health practice such as surveillance of emerging/re-emerging viruses. This involves data acquisition, integration, and analyses of viral genetics to infer origin, spread, and evolution suc as the emergence of new strains. The relevant scientific fields for this practice include certain aspects of molecular epidemiology and phylogeography. Recent attention has focused on viruses of zoonotic origin, which are defined as pathogens that are transmittable between animals and humans. In addition to seasonal influenza and West Nile virus, this classification of pathogens includes novel viruses such as Middle Eastern Respiratory Syndrome and influenza A H7N9. Despite the successes highlighted in the literature, there has been little utilization of bioinformatics resources and tools among state public health, agriculture, and wildlife agencies for zoonotic surveillance. Previously this type of resource has been restricted primarily to those in academia.       While bioinformatics has been sparsely used for surveillance of zoonotic viruses, other applications such as Geospatial Information Systems (GIS) have been employed by state health agencies to analyze spatial patterns of infection. This includes software to produce disease maps using an array of data types such as clinical, geographical, or human mobility data for tasks such as, geocoding, clustering, or outbreak detection. In addition, advances in geospatial statistics have enabled health agencies to perform more powerful space-time analyses to infer spatiotemporal patterns. However, these GIS consider only traditional epidemiological data such as location and timing of reported cases and not the genetics of the virus that causes the disease. This prevents health agencies from understanding how changes in the genome of the virus and the associated environment in which it disseminates impacts disease risk.      The long-term goal of this proposal is to enhance the identification of geospatial hotspots of zoonotic viruses by applying bioinformatics principles to access, integrate, and analyze viral genetics and spatiotemporal reportable disease data. This project will include approaches from bioinformatics, genetics, spatial statistics, GIS, and epidemiology. To do this, I will first measue the utilization of bioinformatics resources and tools as well as the current approaches and limitations identified by state agencies of public health, agriculture, and wildlife for detecting nd predicting hotspots (clusters) of zoonotic viruses (Aim 1). I will then use this feedback to develo a spatial decision support system for detecting and predicting zoonotic hotspots that applies bioinformatics principles to access, integrate, and analyze viral genetics, environmental, and spatiotemporal reportable disease data (Aim 2). In Aim 3, I will then evaluate my system for cluster detection and prediction against a system that does not consider viral genetics and relies on traditional spatiotemporal data, and perform validation of the predictive capability. Additional evaluation of the user's satisfaction and system usability will be evaluated. Project Narrative I will develop and evaluate a spatial decision support system to support surveillance of zoonotic viruses in both human and animal populations. I will use approaches from bioinformatics and public health to integrate genetic sequence data from the virus with data from cases of reported infectious diseases and associated environmental data. A surveillance system that considers the genetics and environment of the virus along with public health data will assist public health officials in making informed decisions regarding risk of infectious diseases.",Integrating Bioinformatics and Clustering Analysis for Disease Surveillance,9406513,F31LM012176,"['Academia', 'Address', 'Agriculture', 'Algorithm Design', 'Algorithms', 'Animals', 'Area', 'Attention', 'Biodiversity', 'Bioinformatics', 'Case Study', 'Clinical', 'Cluster Analysis', 'Communicable Diseases', 'Computer software', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Disease', 'Disease Notification', 'Disease Outbreaks', 'Disease Surveillance', 'Ecology', 'Environment', 'Epidemiology', 'Evaluation', 'Evolution', 'Feedback', 'Future', 'Genbank', 'Genetic', 'Genetic Diseases', 'Geographic Information Systems', 'Geography', 'Goals', 'Health', 'Human', 'Human Genetics', 'Infection', 'Influenza', 'Influenza A Virus, H7N9 Subtype', 'Influenza A virus', 'Knowledge', 'Literature', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Metadata', 'Modeling', 'Molecular Epidemiology', 'Molecular Evolution', 'Pattern', 'Population', 'Public Health', 'Public Health Practice', 'Questionnaire Designs', 'Questionnaires', 'Research', 'Resources', 'Retrospective Studies', 'Risk', 'Scanning', 'Sequence Alignment', 'Syndrome', 'System', 'Time', 'Translations', 'Validation', 'Validity and Reliability', 'Viral', 'Viral Genome', 'Virus', 'Virus Integration', 'West Nile virus', 'Work', 'Zoonoses', 'bioinformatics resource', 'clinical decision-making', 'data acquisition', 'data warehouse', 'disorder risk', 'epidemiologic data', 'health data', 'high risk', 'novel virus', 'pathogen', 'personalized medicine', 'prevent', 'respiratory', 'satisfaction', 'seasonal influenza', 'spatiotemporal', 'statistics', 'success', 'tool', 'usability', 'virus classification', 'virus genetics']",NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,F31,2018,40590,-0.002438417130668557
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9502903,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2018,879004,-0.007978375344881931
"Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs PROJECT ABSTRACT Above-knee amputees often struggle to perform the varying activities of daily life with conventional prostheses. Emerging powered knee-ankle prostheses have motors that can restore normative biomechanics, but these devices are limited to a small set of pre-defined activities that must be tuned to the user by technical experts over several hours. The overall goal of this project is to model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning. The universal use of different task-specific controllers in current powered legs is a direct consequence of the prevailing paradigm for viewing human locomotion as a discrete set of activities. There is a fundamental gap in knowledge about how to analyze, model, and control continuously varying locomotion, which greatly limits the adaptability and agility of powered prostheses. The central hypothesis of this project is that continuously varying activities can be represented by a single mathematical model based on measureable physical quantities called task variables. The proposed project will be scientifically significant to understanding how humans continuously adapt to varying activities and environments, technologically significant to the design of agile, user-synchronized powered prosthetic legs, and clinically significant to the adoption of powered knee-ankle prostheses for improved community ambulation. The proposed model of human locomotion will enable new prosthetic strategies for controlling and adapting to the environment, which aligns with the missions of the NICHD/NCMRR Devices and Technology Development program area and the NIBIB Mathematical Modeling, Simulation, and Analysis program. The innovation of this work is encompassed in 1) a continuous paradigm for variable locomotor activities that challenges the existing discrete paradigm, 2) a unified task control methodology that drastically improves the agility of powered prosthetic legs, and 3) a partially automated tuning process that significantly reduces the time and technical expertise required to configure powered knee- ankle prostheses. This continuous task paradigm will provide new methods and models for studying human locomotion across tasks and task transitions. This innovation will address a key roadblock in control technology that currently restricts powered legs to a small set of activities that do not generalize well across users. The adaptability of the proposed control paradigm across users and activities will transform the prosthetics field with a new generation of “plug-and-play” powered legs for community ambulation. PROJECT NARRATIVE The proposed research is relevant to public health because the clinical application of variable-activity powered prosthetic legs can significantly improve community mobility and therefore quality of life for nearly a million American amputees. Recently developed powered knee-ankle prostheses are limited to a small set of pre- defined activities that require several hours of expert tuning for each user. This project will model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning, which aligns with the missions of the Devices and Technology Development program area of the NICHD National Center for Medical Rehabilitation Research and the Mathematical Modeling, Simulation, and Analysis program of the NIBIB.",Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs,9596236,R01HD094772,"['Address', 'Adoption', 'American', 'Amputees', 'Ankle', 'Area', 'Artificial Leg', 'Biomechanics', 'Clinical', 'Communities', 'Computer Simulation', 'Data', 'Degree program', 'Device or Instrument Development', 'Devices', 'Doctor of Philosophy', 'Electrical Engineering', 'Environment', 'Gait', 'Gait speed', 'Generations', 'Goals', 'Gray unit of radiation dose', 'Hand', 'Home environment', 'Hour', 'Human', 'Human body', 'Joints', 'Knee', 'Knowledge', 'Lead', 'Leg', 'Life', 'Locomotion', 'Lower Extremity', 'Machine Learning', 'Measurable', 'Measures', 'Mechanics', 'Medical center', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motor', 'Motor Activity', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'Orthotic Devices', 'Outcome', 'Phase', 'Play', 'Process', 'Program Development', 'Prosthesis', 'Public Health', 'Quality of life', 'Rehabilitation Research', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Speed', 'Spinal cord injury', 'Stroke', 'Study models', 'System', 'Technical Expertise', 'Technology', 'Time', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'clinical application', 'clinically significant', 'design', 'exoskeleton', 'experience', 'human data', 'human model', 'improved', 'innovation', 'kinematics', 'mathematical model', 'multidisciplinary', 'orthotics', 'powered prosthesis', 'programs', 'prosthesis control', 'robot control', 'sensor', 'success', 'technology development', 'temporal measurement', 'trend']",NICHD,UNIVERSITY OF TEXAS DALLAS,R01,2018,474602,-0.013265080302053707
"Dopaminergic encoding of counterfactual information in human striatum ﻿    DESCRIPTION (provided by applicant): Diseases and disorders directly affected by dopamine systems (e.g., drug addiction and Parkinson's disease) highlight the importance of these systems in motivated human behavior and cognition. The dopamine system is known to be a critical component of normal learning, reward processing, and decision-making (reviewed in Montague et al., 2004). Unfortunately, our present knowledge of dopamine systems in human brains is relatively sparse compared to the wealth of experimentation and computational modeling on these systems in rodents and non-human primates. Previously, technological constraints have limited direct experimentation in human brains. This proposal capitalizes on our group's recent technological innovation, which was supported by the NIH R21 mechanism - CEBRA: R21DA024140 - and resulted in the successful completion of the first sub-second measurements of dopamine release in a human brain. Furthermore, these measurements took place during an active decision-making task that was framed by computational models of learning and reward processing (Kishida et al., 2011 and Kishida et al., under review). We propose to pursue three specific aims, which combine our technological advance with active learning tasks designed to probe the role of dopamine in human behavior. Our aims incorporate three learning signals, where actual and counterfactual experience will each be examined in human striatal responses. The proposed work will inform on the controversial role for dopamine in reward/movement interactions. The experiments proposed will yield unprecedented insight into the function of the dopamine system in the humans afflicted with Parkinson's disease and Essential Tremor. With the support of the NIH (R21DA024140), our team successfully developed a complete prototype system for making electrochemical measurements of dopamine delivery in the human brain. Feasibility has been demonstrated by obtaining the first dopamine measurements in the striata of subjects with Parkinson's during a decision-making task. This substantial preliminary work is now ready for a larger scale with specific hypothesis testing about the role of dopamine systems in Parkinson's disease, Essential tremor, and human decision-making and behavior. PUBLIC HEALTH RELEVANCE: Experiments and computational models (primarily investigated in rodents and non-human primates) suggest dopamine systems in the brain are essential for normal learning, reward-processing, and ongoing decision- making. This proposal builds upon our group's innovative work, which - for the first time - recorded sub- second measurements of dopamine in human subjects and amalgamated these measurements with computational models of dopamine function. Herein we propose to use this newly developed technology to generate unprecedented insight into the function of dopamine systems directly in humans.",Dopaminergic encoding of counterfactual information in human striatum,9536159,R01NS092701,"['Active Learning', 'Acute', 'Affect', 'Animal Model', 'Behavior', 'Brain', 'Brain Diseases', 'Cells', 'Cessation of life', 'Cognition', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Coupling', 'Data', 'Decision Making', 'Disease', 'Dopamine', 'Drug Addiction', 'Electrodes', 'Essential Tremor', 'Exploratory/Developmental Grant', 'Funding', 'Human', 'Knowledge', 'Learning', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement', 'Movement Disorders', 'Operative Surgical Procedures', 'Parkinson Disease', 'Participant', 'Patients', 'Periodicity', 'Phase', 'Play', 'Replacement Therapy', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Sample Size', 'Scanning', 'Signal Transduction', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'blood oxygenation level dependent response', 'cognitive function', 'design', 'dopamine system', 'dopaminergic neuron', 'experience', 'experimental study', 'extracellular', 'human subject', 'implantation', 'innovation', 'insight', 'nonhuman primate', 'prototype', 'public health relevance', 'putamen', 'reduce symptoms', 'relating to nervous system', 'response', 'reward processing', 'social', 'symptomatic improvement', 'technological innovation', 'temporal measurement']",NINDS,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2018,492125,-0.0040301608457890626
"Dopaminergic encoding of counterfactual information in human striatum ﻿    DESCRIPTION (provided by applicant): Diseases and disorders directly affected by dopamine systems (e.g., drug addiction and Parkinson's disease) highlight the importance of these systems in motivated human behavior and cognition. The dopamine system is known to be a critical component of normal learning, reward processing, and decision-making (reviewed in Montague et al., 2004). Unfortunately, our present knowledge of dopamine systems in human brains is relatively sparse compared to the wealth of experimentation and computational modeling on these systems in rodents and non-human primates. Previously, technological constraints have limited direct experimentation in human brains. This proposal capitalizes on our group's recent technological innovation, which was supported by the NIH R21 mechanism - CEBRA: R21DA024140 - and resulted in the successful completion of the first sub-second measurements of dopamine release in a human brain. Furthermore, these measurements took place during an active decision-making task that was framed by computational models of learning and reward processing (Kishida et al., 2011 and Kishida et al., under review). We propose to pursue three specific aims, which combine our technological advance with active learning tasks designed to probe the role of dopamine in human behavior. Our aims incorporate three learning signals, where actual and counterfactual experience will each be examined in human striatal responses. The proposed work will inform on the controversial role for dopamine in reward/movement interactions. The experiments proposed will yield unprecedented insight into the function of the dopamine system in the humans afflicted with Parkinson's disease and Essential Tremor. With the support of the NIH (R21DA024140), our team successfully developed a complete prototype system for making electrochemical measurements of dopamine delivery in the human brain. Feasibility has been demonstrated by obtaining the first dopamine measurements in the striata of subjects with Parkinson's during a decision-making task. This substantial preliminary work is now ready for a larger scale with specific hypothesis testing about the role of dopamine systems in Parkinson's disease, Essential tremor, and human decision-making and behavior. PUBLIC HEALTH RELEVANCE: Experiments and computational models (primarily investigated in rodents and non-human primates) suggest dopamine systems in the brain are essential for normal learning, reward-processing, and ongoing decision- making. This proposal builds upon our group's innovative work, which - for the first time - recorded sub- second measurements of dopamine in human subjects and amalgamated these measurements with computational models of dopamine function. Herein we propose to use this newly developed technology to generate unprecedented insight into the function of dopamine systems directly in humans.",Dopaminergic encoding of counterfactual information in human striatum,9744955,R01NS092701,"['Active Learning', 'Acute', 'Affect', 'Animal Model', 'Behavior', 'Brain', 'Brain Diseases', 'Cells', 'Cessation of life', 'Cognition', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Coupling', 'Data', 'Decision Making', 'Disease', 'Dopamine', 'Drug Addiction', 'Electrodes', 'Essential Tremor', 'Exploratory/Developmental Grant', 'Funding', 'Human', 'Knowledge', 'Learning', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement', 'Movement Disorders', 'Operative Surgical Procedures', 'Parkinson Disease', 'Participant', 'Patients', 'Periodicity', 'Phase', 'Play', 'Replacement Therapy', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Sample Size', 'Scanning', 'Signal Transduction', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'blood oxygenation level dependent response', 'cognitive function', 'design', 'dopamine system', 'dopaminergic neuron', 'experience', 'experimental study', 'extracellular', 'human subject', 'implantation', 'innovation', 'insight', 'nonhuman primate', 'prototype', 'public health relevance', 'putamen', 'reduce symptoms', 'relating to nervous system', 'response', 'reward processing', 'social', 'symptomatic improvement', 'technological innovation', 'temporal measurement']",NINDS,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2018,72839,-0.0040301608457890626
"Improving safety and efficacy of platelet transfusion through systems biology Project Summary Platelet transfusion is critical for severely bleeding patients and nearly 6 million units are transfused in the United States and Europe annually. In the United States, platelets are typically stored for 5 days resulting in a waste of 20% of their supply. Short storage duration is a consequence of bacterial contamination and platelet quality considerations. Though many methods have been developed for bacterial testing and pathogen inactivation, fewer have been developed for improving quality of stored platelets. Platelet additive solutions have the possibility to increase storage quality and duration, reduce plasma-related allergic reactions, impact the efficacy of pathogen reduction techniques, and save plasma which can then be used as an additional transfusion product. While the benefits are well known, there has been little progress in developing new platelet additive solutions for increasing quality and safety of platelet transfusion because there is a lack of broad understanding of biochemical and signaling changes during storage. There has been interest to utilize high-throughput metabolite profiling for global understanding of platelet metabolic decline but data analysis of complex datasets has been a daunting challenge. In Phase I of this program, we developed the first, robust computational platform involving statistical analysis and systems biology of metabolic and signaling networks to interpret and analyze PLT metabolomic and proteomic profiles in a complete network context. Using time- course global, quantitative metabolite profiling, we determined that PLTs undergo a non-linear decay process and computationally identified key metabolic enzymes and cellular process that drive this decay. Based on the computational results, we have devised two novel additive solution strategies to mitigate the decay process and improve the length of PLT units. In this Phase II proposal, we will validate the computationally determined additive solutions for efficacy in alleviating the non-linear decay process through 1) metabolomics experiments, and 2) non-metabolic PLT physiology experiments including cell activation and hemostatic effectiveness. A successful additive solution will be progressed to media refinement and preclinical testing. Project Narrative Platelet transfusion units are typically stored for five days in the United States leading to a waste of 20% of units and potential quality concerns. The field is open for innovation as most storage media technologies are derived from work from the early 1990s. This proposal will develop novel computational methods to comprehensively understand the degradation of platelets under storage conditions and experimentally validate new additive solutions for increasing platelet quality and extending shelf life, an area that accounts for $2.5 billion of hospital costs.",Improving safety and efficacy of platelet transfusion through systems biology,9506810,R44HL127843,"['Accounting', 'Agreement', 'Algorithms', 'Allergic Reaction', 'Area', 'Biochemical', 'Biological', 'Blood', 'Blood Component Removal', 'Blood Platelets', 'Cell physiology', 'Cells', 'Complex', 'Computational Technique', 'Computational algorithm', 'Computer Simulation', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Effectiveness', 'Enzymes', 'Equipment and supply inventories', 'Europe', 'Formulation', 'Glutathione', 'Hemorrhage', 'Hemostatic Agents', 'Hospital Costs', 'In Vitro', 'Intervention', 'Length', 'Lesion', 'Life', 'Literature', 'Machine Learning', 'Mathematics', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Methods', 'Modeling', 'Pathway interactions', 'Patients', 'Phase', 'Physiology', 'Plasma', 'Platelet Transfusion', 'Preclinical Testing', 'Process', 'Production', 'Proteomics', 'Reaction', 'Recovery', 'Resources', 'Risk', 'Safety', 'Signal Pathway', 'Signal Transduction', 'State Hospitals', 'Statistical Data Interpretation', 'Supplementation', 'Surveys', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Transfusion', 'United States', 'Validation', 'Work', 'base', 'care costs', 'cost', 'design', 'experimental study', 'human subject', 'improved', 'insight', 'interest', 'metabolic profile', 'metabolomics', 'model design', 'novel', 'open innovation', 'oxidative damage', 'pathogen', 'predictive modeling', 'preservation', 'programs', 'statistics', 'success', 'time use', 'wasting']",NHLBI,"SINOPIA BIOSCIENCES, INC.",R44,2018,428727,-0.001184442925534342
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9559432,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2018,325325,-0.014877428980211953
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9699855,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,75000,-0.005138159122516431
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9537614,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'deep learning', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2018,329257,-0.005138159122516431
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,9661636,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola virus', 'Effectiveness', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2018,263913,0.0066395078112832595
"Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens ﻿    DESCRIPTION (provided by applicant): Infections caused by antibiotic-resistant bacterial pathogens are exceedingly common in immunocompromised hosts. Patients undergoing allogeneic hematopoietic stem cell transplantation (allo-HSCT) are particularly susceptible to these infections and are the patient population our studies will focus upon. Our goal is to extend and further develop systems biology approaches that our group has pioneered to identify mechanisms by which the intestinal microbiota confers resistance to infection by Vancomycin-resistant Enterococcus (VRE), antibiotic-resistant Klebsiella pneumoniae (arKp) and Clostridium difficile (C. diff). Aim 1 of our project is to establish a clinical database from the hospital recrds of allo-HSCT patients during their initial hospitalization that will include all laboratory values,vital signs, pharmacy data, dietary data, symptoms and physical exam findings. Aim 2 will expand our fecal bank by collecting fecal samples from approximately 160 allo-HSCT patients per year and determining the presence/absence of VRE, arKp and C. diff by culture and PCR. We will use NGS of 16S rRNA genes to determine microbiota composition on each sample, will perform metagenomic and RNA sequencing to determine the bacterial transcriptome and perform metabolomic analyses on a selected subset of fecal samples. Aim 3 is to extend our mathematical modeling to identify specific members of the microbiota, metabolic pathways and metabolic products that correlate with resistance to VRE or arKp expansion in the GI tract or are associated with resistance to C. diff infection. The clinical database will be used to establish correlations between clinical treatments or events and changes in the intestinal microbiota or the expression of bacterial metabolic pathways. Ultimately, the computational platforms developed in aim 3 will identify bacterial species or consortia that are associated with resistance to infection and Aim 4 will test these associations in germ-free mouse models. We will culture bacterial species associated with resistance, colonize mice with these protective bacteria and test for resistance against VRE, arKp and C. diff. Samples obtained from these experimental studies will be subjected to metagenomic and metabolomic analyses to further refine, in an iterative fashion, computational models developed in aim 3. Our proposed studies will develop new and extend existing computational models to identify bacterial species and molecular mechanisms that confer resistance to antibiotic-resistant bacterial infections. PUBLIC HEALTH RELEVANCE: The normal bacteria inhabiting the human intestine provide a high level of resistance against antibiotic-resistant bacterial pathogens. We are investigating the intestinal flora of hospitalized patients and using mathematical modeling to identify bacterial species and their metabolic products that reduce the risk of infection by three prevalent antibiotic-resistant bacteria. These studies may lead to the development of new approaches to treat and prevent antibiotic-resistant infections.",Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens,9444335,U01AI124275,"['16S ribosomal RNA sequencing', 'Allogenic', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bacterial Infections', 'Categories', 'Clinical', 'Clinical Treatment', 'Clostridium difficile', 'Collection', 'Computer Simulation', 'Data', 'Databases', 'Deposition', 'Development', 'Diagnostic radiologic examination', 'Diet', 'Dietary intake', 'Enrollment', 'Event', 'Feces', 'Gastrointestinal tract structure', 'Germ-Free', 'Gnotobiotic', 'Goals', 'Growth', 'Hematopoietic Stem Cell Transplantation', 'Hospitalization', 'Hospitals', 'Human', 'Immune', 'Immunocompromised Host', 'Infection', 'Integration Host Factors', 'Intestinal Content', 'Intestines', 'Klebsiella pneumonia bacterium', 'Laboratories', 'Lead', 'Machine Learning', 'Measurable', 'Mediating', 'Medical Records', 'Memorial Sloan-Kettering Cancer Center', 'Metabolic', 'Metabolic Pathway', 'Metagenomics', 'Modeling', 'Modification', 'Molecular', 'Mus', 'Patient risk', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Population', 'Predisposition', 'Resistance', 'Resistance to infection', 'Risk', 'Sampling', 'Symptoms', 'Systems Biology', 'Testing', 'Toxic effect', 'Transplantation', 'Uncertainty', 'Vancomycin Resistance', 'Vancomycin resistant enterococcus', 'antimicrobial drug', 'clinically relevant', 'colonization resistance', 'commensal bacteria', 'commensal microbes', 'computer studies', 'data warehouse', 'design', 'experimental study', 'falls', 'gut microbiota', 'host microbiome', 'immune activation', 'mathematical model', 'member', 'metabolome', 'metabolomics', 'metagenome', 'metagenomic sequencing', 'microbiome', 'microbiota', 'microorganism interaction', 'mouse model', 'network models', 'novel', 'novel strategies', 'parallel computer', 'pathogen', 'patient population', 'prevent', 'public health relevance', 'rRNA Genes', 'resilience', 'resistance mechanism', 'text searching', 'transcriptome', 'transcriptome sequencing', 'transcriptomics']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,U01,2018,1997054,-0.008919107381441535
"Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression Project Summary/Abstract This project will develop a new technological approach for the comprehensive analysis of adaptive immune responses, which holds the potential to catalyze new strategies to prevent and treat disease. Here we will apply immune profiling techniques recently invented by the PI to investigate the mechanisms of Epstein-Barr virus (EBV) adaptive immune control in clinical cohorts of infected patients. EBV is a highly prevalent pathogen infecting >90% of the world’s population. Primary EBV infection often causes infectious mononucleosis (IM) and long-term sequelae include numerous malignancies, lymphoproliferative disorders, and a strong association with multiple sclerosis. No EBV vaccine is approved to date, and the molecular mechanisms of immune protection from EBV-associated diseases are unclear. Unfortunately, prior technical barriers in high- throughput immune profiling methods have prevented a comprehensive understanding of adaptive immune protection against EBV diseases. A technological approach that identifies the critical features of EBV immune protection will advance new solutions for vaccine and therapeutic development. Therefore, we developed an experimental pipeline to enable rapid and cost-effective analysis of B- and T-cell responses to EBV that is scalable to dozens of human patients per experiment. We hypothesize that a comprehensive B- and T-cell analysis of carefully selected patient cohorts that either can or cannot suppress symptomatic infection will reveal function-based correlates of EBV control. To test this hypothesis, we will apply quantitative immune profiling technologies to analyze cryopreserved longitudinal samples from recently completed prospective clinical studies of IM. Patient samples in our cohort span pre- and post-infection through convalescence and encompass the full range of clinical IM severity scores (from 0, asymptomatic primary infection, to 6, essentially bedridden with IM). Immune profile data will be used to establish adaptive immune correlates of IM disease severity. In addition, we will analyze immune responses in apparently immunocompetent patients with chronic active EBV (CAEBV) disease, or patients who do not adequately suppress EBV infection, to gain insight regarding adaptive immune function and dysfunction in CAEBV. Finally, we will develop a new computational toolkit to rapidly identify immune correlates from high-throughput datasets. Successful completion of this project will constitute the first comprehensive functional B- and T-cell receptor analysis in a human clinical cohort. Our efforts will provide a repertoire-scale, mechanistic understanding of adaptive immunity to EBV and suggest new strategies for treatment and prevention of EBV-associated diseases. Our long-term goal is to develop human immune profiling techniques as a platform approach to accelerate the rational design of vaccines and therapeutics against pathogens of high public health importance, beginning with EBV. Project Narrative This project will apply new high-throughput immune profiling technologies to elucidate the features of effective Epstein-Barr virus (EBV) immune control. EBV causes a range of human diseases including infectious mononucleosis and several forms of cancer; however, limited EBV treatment options are available and no approved preventive EBV vaccines exist. Our long-term objective is to apply enhanced understanding of adaptive immunity to accelerate the rational development of new vaccines and therapeutics.",Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression,9561938,DP5OD023118,"['Address', 'Antibodies', 'Antibody titer measurement', 'B cell repertoire', 'B-Lymphocytes', 'Burkitt Lymphoma', 'CD8-Positive T-Lymphocytes', 'Cells', 'Chronic', 'Clinical', 'Clinical Research', 'Convalescence', 'Cost Effectiveness Analysis', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'EBV-associated disease', 'Ensure', 'Epstein-Barr Virus Infections', 'Exhibits', 'Fatigue', 'Future', 'Goals', 'Herpesviridae', 'Herpesviridae Infections', 'Hodgkin Disease', 'Human', 'Human Herpesvirus 4', 'Immune', 'Immune System Diseases', 'Immune response', 'Immunocompetent', 'Immunologic Receptors', 'Incidence', 'Individual', 'Infection', 'Infectious Mononucleosis', 'Intervention', 'Knowledge', 'Lymphoproliferative Disorders', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Multiple Sclerosis', 'Nasopharynx Carcinoma', 'Oncogenic', 'Patients', 'Phase', 'Population', 'Prevention', 'Preventive', 'Primary Infection', 'Public Health', 'Receptors, Antigen, B-Cell', 'Recovery', 'Research Personnel', 'Risk', 'Sampling', 'Serum', 'Severities', 'Severity of illness', 'Stomach Carcinoma', 'Symptoms', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Techniques', 'Technology', 'Teenagers', 'Testing', 'Therapeutic', 'Therapeutic antibodies', 'Vaccine Design', 'Vaccines', 'Viral', 'Virus Diseases', 'Work', 'adaptive immune response', 'adaptive immunity', 'base', 'career', 'cohort', 'cost', 'design', 'disorder control', 'experimental study', 'high dimensionality', 'human disease', 'immune function', 'insight', 'lead candidate', 'multiple sclerosis patient', 'neutralizing monoclonal antibodies', 'next generation', 'novel', 'novel therapeutics', 'novel vaccines', 'pathogen', 'pressure', 'prevent', 'prospective', 'response', 'therapeutic development', 'treatment strategy', 'vaccine development', 'vaccine trial', 'virology', 'young adult']",OD,UNIVERSITY OF KANSAS LAWRENCE,DP5,2018,363940,-0.008479642874375939
"Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology PROJECT SUMMARY/ABSTRACT Candidate After the completion of my Ph.D. in Bioinformatics, I joined the Center for Statistical Genetics at the University of Michigan (U-M) as research fellow and acquired extensive training in analysis on high- dimensional biological data. I uncovered a strong interest in studying the genetics and genomics of psoriasis when working with Dr. James Elder at the U-M, and I developed a fascination in understanding the functional roles of long non-coding RNAs (lncRNAs) in cutaneous diseases. I joined the Department of Dermatology at the U-M as a faculty in summer of 2015, with secondary appointments in the Department of Computational Medicine & Bioinformatics and the Department of Biostatistics. In addition, I direct the new Center for Cutaneous Bioinformatics within the Department of Dermatology, and serve to supervise and implement an analysis pipeline for studies investigating the immunological mechanisms for different skin diseases. Career Development Plan I aim to become a future leader in combining in silico discovery and bench experiments to advance biomedical research in autoimmune skin disorders. My objective in seeking a Mentored Research Scientist Development Award is to acquire the additional knowledge, training, and experience necessary for me to become an independent scholar in developing novel systems biology approaches to decipher the pathology and mechanisms of cutaneous diseases. The five year training proposed will provide knowledge and experience in aspects that are critical to my success, and they are: i) To develop knowledge and experimental skills in cutaneous biology --- achieved by guidance from Dr. Elder (investigative dermatology), intense research meetings/conferences, and practical laboratory experience in cutaneous research; ii) To develop knowledge and skills to study immunological systems of autoimmune skin diseases --- achieved by supervision from Dr. Johann Gudjonsson (skin immunology), attending formal Immunology courses and seminars, and earning laboratory experience from immunology experiments; iii) To advance skills in developing statistical and computational approaches --- accomplished by mentoring from Dr. Goncalo Abecasis (computational biologist), research meetings, and conducting research projects requiring advanced skills and knowledge in quantitative science; iv) To cultivate my professional development through enhancing scientific connections, grantsmanship skills, and educator portfolio --- achieved by establishing connections with colleagues during meetings, visiting King’s College London as scholar, attending a grantsmanship workshop and bootcamp, and learning mentoring skills through teaching formal classes and mentoring research students. Through the intensive and comprehensive training, I will be well grounded in conducting basic science experiments and also be able to capitalize my advanced knowledge in quantitative science to model mechanisms in cutaneous diseases. Research Project The research project will use psoriasis as a disease model to study the roles of lncRNAs in complex cutaneous disorders. I will test the hypotheses that (i): some lncRNAs are key causal elements and potentiate pathogenic inflammatory reactions in psoriasis development and (ii) by combining in silico predictions and in vitro validations we are able to provide comprehensive characterization of skin-expressing lncRNAs in keratinocytes and lymphocytes to infer their pathological implications for psoriasis. This work will demonstrate how we can take advantages of the genomic data to develop an integrative biology framework to provide novel biological insights and understand pathological roles of lncRNAs. Significance Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture. It is estimated that over 4 million Americans and 100 million people worldwide suffer from this disease. While genetic association studies have revealed the disease loci are highly enriched in non-coding regions, it is very challenging to translate genetic signals to biologic effects. In fact, most of the causal genes have not yet been identified. Our preliminary results showed that lncRNA is a class of gene that has largely been understudied for their roles in psoriasis, and both genetic and transcriptomic data suggested they can play important functions in psoriasis pathogenesis. By combining in silico analysis and in vitro validation we can expand our knowledge of lncRNAs in skin biology, and generate important hypotheses for future experiments. The results of this project can also identify novel biomarkers, and ultimately assist in the therapeutic drug discovery. PROJECT NARRATIVE Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture, and affects over 4 million Americans and 100 million people worldwide. Long non-coding RNAs (lncRNAs) is a class of gene that has largely been understudied, and recent studies suggested their potential roles in autoimmune diseases. This project aims to expand our knowledge of lncRNAs in skin biology, and advance identification of lncRNAs that play functional roles in psoriasis pathogenesis.",Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology,9550910,K01AR072129,"['Affect', 'American', 'Appointment', 'Autoimmune Diseases', 'Autoimmune Process', 'Basic Science', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Research', 'Biometry', 'Catalogs', 'Cellular Structures', 'Chromatin', 'Chronic', 'Comorbidity', 'Complex', 'Computer Simulation', 'Coronary Arteriosclerosis', 'Cutaneous', 'Data', 'Dermatology', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Disease susceptibility', 'Doctor of Philosophy', 'Economic Burden', 'Educational process of instructing', 'Educational workshop', 'Elderly', 'Elements', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Epigenetic Process', 'Expression Profiling', 'Faculty', 'Flow Cytometry', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genetic study', 'Genomics', 'Genotype', 'Homing', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunologics', 'Immunology', 'In Vitro', 'Inflammatory', 'Inflammatory Response', 'Knowledge', 'Learning', 'London', 'Lymphocyte', 'Machine Learning', 'Mediating', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Michigan', 'Modeling', 'Molecular', 'Pathogenesis', 'Pathogenicity', 'Pathologic', 'Pathology', 'Patients', 'Play', 'Process', 'Production', 'Proteins', 'Psoriasis', 'Public Health', 'Quality of life', 'Quantitative Reverse Transcriptase PCR', 'Reaction', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Signal Transduction', 'Skin', 'Societies', 'Statistical Methods', 'Students', 'Supervision', 'Susceptibility Gene', 'Systems Biology', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Transcript', 'Translating', 'United States', 'Universities', 'Untranslated RNA', 'Validation', 'Visit', 'Work', 'base', 'candidate identification', 'career development', 'cell type', 'cohort', 'college', 'cytokine', 'drug discovery', 'experience', 'experimental study', 'fascinate', 'genetic architecture', 'genetic association', 'genomic data', 'high dimensionality', 'human disease', 'human tissue', 'immunoregulation', 'insight', 'interest', 'keratinocyte', 'knock-down', 'laboratory experience', 'learning strategy', 'meetings', 'novel', 'novel marker', 'recruit', 'scaffold', 'skills', 'skin disorder', 'small hairpin RNA', 'statistical center', 'success', 'symposium', 'tool', 'transcriptome sequencing', 'transcriptomics']",NIAMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,K01,2018,98848,-0.01829290657322149
"New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis ﻿    DESCRIPTION (provided by applicant)    Maha R Farhat, MD is an Instructor of Medicine at Harvard Medical School on the tenure track and a staff physician in the Department of Pulmonary and Critical Care Medicine at Massachusetts General Hospital. She is completing a masters of biostatistics at the Harvard School of Public Health in 5/2015. She has spent the last 4.5 years acquiring skills in Mycobacterium tuberculosis biology, epidemiology, bioinformatics and biostatistics. She has experience in the analysis of whole genome sequence data, drug resistance data and patient clinical outcome data with the focus of identifying Mycobacterium tuberculosis genetic determinants of drug resistance. She has also developed new methods in this area. Dr. Farhat has 11 publications 5 of which are first author including high impact and highly cited work in the journals Nature Genetics, Genome Medicine and the International Journal of Tuberculosis and Lung Disease. The short term goals of this K01 award are to provide training for Dr. Farhat in critical aspects of data science, computational and evolutionary biology, advanced biostatistics and network science. Dr. Farhat's long term goal is to become a leader in the field of Big Data analysis for infectious diseases. The proposed research as well as the training activities outlined in the proposal will successfully position Dr. Farhat for her first R01 and an independent career as a physician scientist. Environment: Dr. Farhat will perform the interdisciplinary work outlined in this proposal at the distinguished Harvard Departments of Global Health Social Medicine, Biostatistics, Evolutionary biology and the Institute for Quantitative Social Sciences. Dr. Farhat' mentorship team will include two world renowned leaders in the fields of infectious diseases and Big Data, Dr. Megan Murray and Dr. Gary King; and two rising stars in the fields of network Science and evolutionary Biology, Dr. JP Onnela and Dr. Michael Desai. Dr. Murray, the principal mentor on this proposal has mentored over 38 trainees, 9 of which have went on to have independent research careers, and 6 competed successfully for K awards. She is also PI on two recently awarded NIH/NIAID grants a CETR U19 and a TBRU U19 and has over 350 peer reviewed publications. To complement the expertise of her mentors Dr. Farhat will be advised by Dr. Christiani a practicing pulmonary and critical care physician and world renowned researcher in the field of lung and environmental genetics. She will also collaborate and consult with Dr. Merce Crosas, a data scientist, and Dr. Pardis Sabeti, a computational biologist. She will rotate through Dr. Soumya Raychaudhuri's bioinformatics laboratory to diversify her exposure to biomedical Big Data. In addition, she will receive formal training in evolutionary biology, Bayesian and mixed-model biostatistics, computer science, leadership skills and grant writing. The collaborative opportunities, intellectual environment and resources available to Dr. Farhat are outstanding. Research: Infectious diseases continue to be a major cause of morbidity and mortality. Despite the availability of effective antimicrobials, pathogens are successfully evolving new disease phenotypes that allow them to resist killing by these drugs or in other instances cause more severe disease manifestations or wider chains of transmission. Drug resistance (DR) is now common and some bacteria have even become resistant to multiple types or classes of antibiotics6. A key strategy in the fight against emerging pathogen phenotypes in infectious diseases is surveillance, and early personalized therapy to prevent transmission and propagation of these strains. The timely initiation of antibiotic therapy to which the pathogen is sensitive has been shown to be the key factor influencing treatment outcome for a diverse array of infections. Molecular tests that rely on the detection of microbial genetic mutations are particularly promising for surveillance and diagnosis of these pathogen phenotypes but rely on a comprehensive understanding of how mutations associate with these pathogen phenotypes. Currently there is an explosion of data on pathogen whole genome sequences (WGS) that is increasingly generated from clinical laboratories. Data on disease phenotype may also be available, but methods for the analysis and interpretation of these Big Data are lagging. Here I propose tools to aid in this analysis leveraging Big Data sets from Mycobacterium tuberculosis (MTB) and my prior work. Specifically I propose to (1) develop a web-based public interface to several analysis tools, including a statistical learning model that can predict the MTB DR phenotype from its genomic sequence, (2) to develop and study an MTB gene-gene network, based on WGS data, to improve our understanding of the effect of mutation-mutation interactions on the DR phenotype, and (3) study the performance of methods in current use for the association of genotype and phenotype in pathogens, and develop a generalizable power calculator for the best performing method. PUBLIC HEALTH RELEVANCE    Infectious agents of disease are successfully evolving drug resistance and other adaptations that threaten human health. Understanding the genetic mutations that underlie these disease phenotypes can inform surveillance and diagnostic strategies to combat this threat. Here I propose to develop accessible tools for pathogen genomic analysis that will help identify which genetic mutations are relevant to disease.",New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis,9530648,K01ES026835,"['Affect', 'Antibiotic Therapy', 'Antibiotics', 'Area', 'Award', 'Bacteria', 'Big Data', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Cessation of life', 'Clinical', 'Collaborations', 'Communicable Diseases', 'Complement', 'Consult', 'Critical Care', 'DNA Sequence Alteration', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug resistance', 'Drug resistance in tuberculosis', 'Drug-sensitive', 'Environment', 'Epidemiology', 'Explosion', 'Exposure to', 'Future', 'Gene Structure', 'General Hospitals', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Epistasis', 'Genome', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Grant', 'Health', 'Heritability', 'Human', 'Infection', 'Infectious Agent', 'Infectious Diseases Research', 'Institutes', 'International', 'Journals', 'K-Series Research Career Programs', 'Laboratories', 'Leadership', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Massachusetts', 'Measures', 'Medical Genetics', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Mentorship', 'Methods', 'Microbial Genetics', 'Modeling', 'Molecular', 'Monitor', 'Morbidity - disease rate', 'Mutation', 'Mycobacterium tuberculosis', 'National Institute of Allergy and Infectious Disease', 'Nature', 'Network-based', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Phylogeny', 'Physicians', 'Population', 'Positioning Attribute', 'Public Health', 'Public Health Schools', 'Publications', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Resources', 'Science', 'Scientist', 'Severity of illness', 'Social Medicine', 'Social Sciences', 'Standardization', 'Statistical Data Interpretation', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Tuberculosis', 'United States National Institutes of Health', 'Variant', 'Virulent', 'Work', 'Writing', 'analytical tool', 'antimicrobial', 'base', 'big biomedical data', 'burden of illness', 'career', 'combat', 'computer science', 'design', 'disease phenotype', 'experience', 'fight against', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'global health', 'improved', 'instructor', 'medical schools', 'microbial', 'microbial genome', 'mortality', 'pathogen', 'pathogen genome', 'personalized medicine', 'prevent', 'prospective', 'public health relevance', 'simulation', 'skills', 'tenure track', 'tool', 'transmission process', 'whole genome']",NIEHS,HARVARD MEDICAL SCHOOL,K01,2018,220530,-0.013874960631570248
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9535429,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,315184,-0.023372206811977856
"A Systems Biology Approach to Mapping Aging in the lung PROJECT SUMMARY This proposal for an NRSA Individual Fellowship is motivated by two overarching goals: 1) to provide a rich environment for learning and growth to support the candidate's development into an effective physician- scientist, and 2) to generate a molecular map of aging in the lung by applying a systems biology approach to the analysis of transcriptomic data obtained over the lifespan of the laboratory mouse. The candidate and his mentors have designed a specific training plan that includes a rigorous research component and lays the foundation for a successful career. Aging is a key risk factor for morbidity and mortality related to diseases that affect the lung, however, the biological mechanisms contributing to this are not understood. The candidate will employ a systems biology approach to analyze already generated datasets to identify molecular pathways of aging in the lung. A better understanding of aging in the lung may ultimately lead to the development of novel therapies for lung diseases in older adults. Gene expression profiling using RNA-Seq has been used to generate “maps” of tissues by depicting tissues and purified constituent cellular populations on a molecular level. Transcriptional profiles from individuals with disease have been compared to those from healthy individuals in an attempt to develop “signatures” for a given disease. These signatures can be useful for disease diagnosis as well as for identifying targets for therapy. This proposal focuses on the contributions of two abundant cell populations in the lung, alveolar macrophages and alveolar type II cells (AT2), to the transcriptional signatures of aging in the lung. The candidate has preliminary data demonstrating evidence of an aging signature in alveolar macrophages and AT2 cells purified from mice at 18 months compared with 3 months. This signature persists during an influenza A virus infection challenge. The purpose of this proposal is to employ a systems biology approach to analyze transcriptomic data already obtained by investigators in the candidate's laboratory that includes whole lung tissue, alveolar macrophages, and AT2 cells harvested from mice over the lifespan (2 weeks, and 6,12,18 and 24 months of age). In Specific Aim 1, the candidate will generate a transcriptional map of aging in murine whole lung and purified cellular populations of alveolar macrophages and AT2 cells. In Specific Aim 2, the candidate will determine whether the transcriptional signature of aging in alveolar macrophages is cell autonomous or is driven by changes in the alveolar local microenvironment. Over the course of this award, the candidate will learn new systems biology approaches to analyzing large time-series genomic datasets and will develop a comprehensive understanding of the age-related changes in the innate immune system of the lung. The skills developed as part of this project will serve as a guide for the analysis of similar genomic data harvested from aging humans with lung disease. PROJECT NARRATIVE Aging is a key risk factor for morbidity and mortality related to diseases that affect the lung, including lung cancer, chronic obstructive pulmonary disease, lung fibrosis, and lung infections. The mechanisms contributing to the increased susceptibility of older adults to these diseases are unknown. We hope to identify novel mechanisms of aging in the lung that ultimately lead to new therapeutic options for diseases of the lung in older adults.",A Systems Biology Approach to Mapping Aging in the lung,9598302,F32HL136111,"['Address', 'Adoptive Transfer', 'Affect', 'Age', 'Age-Months', 'Aging', 'Algorithms', 'Alveolar', 'Alveolar Macrophages', 'Animals', 'Applications Grants', 'Award', 'Biochemical Genetics', 'Biological', 'C57BL/6 Mouse', 'Cells', 'Chronic Obstructive Airway Disease', 'Chronology', 'Coupled', 'Data', 'Data Set', 'Development', 'Disease', 'Elderly', 'Epithelial Cells', 'Fellowship', 'Flow Cytometry', 'Fluorescence-Activated Cell Sorting', 'Foundations', 'Future', 'Gene Expression Profile', 'Gene Expression Profiling', 'Genes', 'Genetic Transcription', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Growth', 'Harvest', 'Homeostasis', 'Human', 'Immune', 'Individual', 'Infection', 'Influenza A virus', 'Innate Immune System', 'Laboratories', 'Laboratory mice', 'Lead', 'Learning', 'Linear Regressions', 'Longevity', 'Lung', 'Lung diseases', 'Lung infections', 'Malignant neoplasm of lung', 'Maps', 'Mentors', 'Modeling', 'Molecular', 'Morbidity - disease rate', 'Mus', 'National Research Service Awards', 'Pathway interactions', 'Physicians', 'Play', 'Population', 'Predisposition', 'Principal Component Analysis', 'Proteomics', 'Publishing', 'Pulmonary Fibrosis', 'Rattus', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Role', 'Sampling', 'Scientist', 'Series', 'Structure of parenchyma of lung', 'Supervision', 'Surface', 'Systems Biology', 'Therapeutic Intervention', 'Time', 'Tissues', 'Training', 'Virus Diseases', 'age related', 'aged', 'alveolar type II cell', 'base', 'career', 'cell type', 'cohort', 'design', 'disease diagnosis', 'educational atmosphere', 'forest', 'genomic data', 'insight', 'juvenile animal', 'lung development', 'mortality', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'predictive modeling', 'prospective', 'skills', 'surfactant', 'targeted treatment', 'transcriptome sequencing', 'transcriptomics', 'unsupervised learning']",NHLBI,NORTHWESTERN UNIVERSITY AT CHICAGO,F32,2018,58282,-0.048819548016924896
"CRCNS: Representational foundations of adaptive behavior in natural and artificial  ﻿    DESCRIPTION (provided by applicant): Overview: Among the most celebrated success stories in computational neuroscience is the discovery that many aspects of decision-making can be understood in terms of the formal framework of reinforcement learning (RL). Ideas drawn from RL have shed light on many behavioral phenomena in learning and action selection, on the functional anatomy and neural processes underlying reward-driven behavior, and on fundamental aspects of neuromodulatory function. However, for all these successes, RL-based work is haunted by an inconvenient truth: Standard RL algorithms scale poorly to large, complex problems. If human learning and decision-making are driven by RL-like mechanisms, how is it that we cope with the kinds of rich, large-scale tasks that are typical of everyday life? Existing research in both psychology and neuroscience hints at one answer to this question: Complex problems can be conquered if the decision-maker is equipped with compact, intelligently formatted representations of the task. This principle is seen in studies of expert play in chess, which show that chess masters leverage highly integrative internal representations of board configurations; in studies of frontal and parietal lobe function, which have revealed receptive fields strongly shaped by task contingencies; and studies on the hippocampus, which point to the role of this structure in supporting a hierarchically organized 'cognitive map,' of task space. Not coincidentally, the critical role of representation has come increasingly to the fore in RL-based research in machine learning and robotics, with growing interest in techniques for dimensionality reduction, hierarchy and deep learning.            The present project aims toward a systematic, empirically validated account of the role of representation in supporting RL and goal-directed behavior at large. The project brings together three investigators with complementary expertise in cognitive and computational neuroscience (Botvinick, Gershman) and machine learning and robotics (Konidaris). Together, we propose an integrative, interdisciplinary program of research, applying behavioral and neuroimaging work with human subjects, computational modeling of neurophysiological and behavioral data, formal mathematical work and simulations with artificial agents. The proposed studies are diverse in theme and method, but work together toward a theory that is both formally grounded and empirically constrained. At a more concrete level, our research focuses on four specific classes of representation, considering the computational impact of each for RL, as well as the relevance of each to neuroscience and human behavior. As detailed in our Project Description, these include (1) metric embedding, (2) spectral decomposition, (3) hierarchical representation and (4) symbolic representation. In addition to investigating the implications of each of these four forms of representation individually, we hypothesize that they fit together into a tiered system, which works as a whole to support the sometimes competing demands of learning and action control.         Intellectual Merit (provided by applicant): Understanding how representational structure impacts learning and decision making is a core challenge in cognitive science, behavioral neuroscience and, artificial intelligence. Success in establishing a computationally explicit, empirically validated theory in this area, with a specific focus on the role of representation in R, would represent an important achievement with wide repercussions. The strategy of leveraging conceptual tools from machine learning to investigate human behavior and brain function can offer considerable scientific leverage, as our own previous research illustrates. The proposed work is motivated by and builds upon established lines of research, bringing these together in order to capitalize on opportunities for synergy. In addition to answering specific empirical and computational questions, the proposed work aims to open up new avenues for future research in an important area of inquiry.         Broader Impact (provided by applicant): The proposed work lies at the crossroads of neuroscience, psychology, artificial intelligence and machine learning, and promises to advance the growing exchange among these fields. The project brings together investigators with contrasting disciplinary affiliations, with the explicit goal of bridging between intellectual cultres. The proposed work is likely to find a wide scientific audience, given its relevance to cognitive and developmental psychology, behavioral, cognitive and systems neuroscience, and behavioral economics. However, the work is likely to be of equal interest within artificial intelligence, machine learning, and robotics, where a current challenge is precisely to understand how representation learning can allow RL to scale up to large problems. Representational approaches to RL are already of intense interest within industry, where the present investigators have a record of active engagement. The topic of the proposed work has applicability in other areas as well, including education and training, and military and medical decision support. The plan for the project has a robust training component at both graduate and postdoctoral levels, with a commitment to fostering involvement of underrepresented minorities, as well as international engagement. n/a",CRCNS: Representational foundations of adaptive behavior in natural and artificial ,9292377,R01MH109177,"['Achievement', 'Adaptive Behaviors', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Cognitive Science', 'Complex', 'Computer Simulation', 'Data', 'Decision Making', 'Dimensions', 'Fostering', 'Foundations', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Industry', 'International', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Mathematics', 'Medical', 'Methods', 'Military Personnel', 'Neurosciences', 'Parietal Lobe', 'Play', 'Process', 'Psychological reinforcement', 'Psychology', 'Research', 'Research Personnel', 'Rewards', 'Robotics', 'Role', 'Structure', 'System', 'Techniques', 'Training', 'Training and Education', 'Underrepresented Minority', 'Work', 'base', 'behavioral economics', 'cognitive neuroscience', 'cognitive system', 'computational neuroscience', 'developmental psychology', 'frontal lobe', 'human subject', 'interest', 'neuroimaging', 'neurophysiology', 'neuroregulation', 'programs', 'receptive field', 'relating to nervous system', 'research based learning', 'scale up', 'simulation', 'success', 'synergism', 'theories', 'tool']",NIMH,PRINCETON UNIVERSITY,R01,2017,373529,-0.01209927178109561
"Training a new generation of computational neuroscientists bridging neurobiology The Training Program in Computational Neuroscience (TPCN) will support integrated undergraduate and graduate training in computational neuroscience at New York University. The program will be hosted by the Center for Neural Science (CNS), with participation of faculty in the Departments of Psychology, Mathematics, and Computer Science, and the Institute of Neuroscience at the School of Medicine. The TPCN will fit well with NYU’s unique strengths and recent developments: (1) NYU is one of a few universities with a critical mass of computational neuroscientists. NYU has had a Sloan-Swartz Center for Theoretical Neuroscience since 1994. In the past three years alone, NYU has hired three computational neuroscientists. (2) CNS established an undergraduate major in neuroscience as early as 1992, and thus has a long track record in undergraduate education, it now has 136 students in the current academic year. (3) Recent faculty hiring in CNS, Psychology, and the School of Medicine has greatly expanded our teaching and research capabilities in the neuroscience of cognitive functions and their impairments associated with mental disorders. (3) As NYU is undertaking a merge of two historically separated neuroscience graduate programs (at CNS and the School of Medicine), this training grant will ensure that computational modeling, which has become indispensible in neuroscience, will be front-and-center in the integrated graduate program. (4) NYU is a major center of Artificial Intelligence and Data Science, with close links to Facebook’s AI Center and the Simons Center for Data Analysis. Our training faculty together with these connections will give our students ample opportunities to acquire machine learning techniques for data analysis and learn about brain-like AI algorithms. The proposed training program will support coherent undergraduate and graduate training in computational neuroscience at NYU. It will have several unique features: (1) Innovative mentorship methods: For example, (a) graduate trainees will mentor undergraduate trainees, (b) faculty will explicitly discuss human factors in academic practice; (c) there will be post-mortems after seminars by outside speakers. (2) Computational psychiatry: We propose new courses and research opportunities that are designed specifically to link cognitive function and the neurobiology of neural circuits. We propose innovative education in the nascent field of Computational Psychiatry, to bring theory and circuit modeling to clinical research in mental health. (3) Broad preparation: We aim to prepare trainees for jobs not only in academia, but also in medical and industry research. To achieve this, we will utilize our strength in machine learning and data science to broaden computational neuroscience training. The Program Directors have complementary strengths and will have complementary roles in the program. Wang will supervise graduate trainees and focus on training in mechanistic/circuit-level side of computational neuroscience as well as computational psychiatry. Ma will supervise undergraduate trainees and focus on the computational/behavioral side. This grant will support training of a new generation of graduate and undergraduate students in  computational neuroscience, which has become increasingly important to meet the challenges of  making discoveries with new data analysis tools and of understanding highly nonlinear complex  neural circuits. A salient component of our program is training in the nascent field  of   Computational  Psychiatry,  bridging  basic  neuroscience  and clinical research on mental  disorders. Therefore, the proposed program has the potential of making  a significant impact on  mental health.",Training a new generation of computational neuroscientists bridging neurobiology,9346652,R90DA043849,"['Academia', 'Algorithms', 'Artificial Intelligence', 'Behavioral', 'Brain', 'Clinical Research', 'Computer Simulation', 'Data Analyses', 'Data Science', 'Development', 'Education', 'Educational process of instructing', 'Ensure', 'Faculty', 'Generations', 'Grant', 'Human', 'Impairment', 'Industry', 'Institutes', 'Learning', 'Link', 'Machine Learning', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Neurobiology', 'Neurosciences', 'New York', 'Occupations', 'Preparation', 'Psychiatry', 'Psychology', 'Research', 'Role', 'Science', 'Side', 'Students', 'Supervision', 'Teacher Professional Development', 'Techniques', 'Training', 'Training Programs', 'Universities', 'cognitive function', 'cognitive neuroscience', 'computational neuroscience', 'computer science', 'design', 'innovation', 'mathematical sciences', 'medical schools', 'neural circuit', 'programs', 'relating to nervous system', 'theories', 'undergraduate education', 'undergraduate student']",NIDA,NEW YORK UNIVERSITY,R90,2017,204981,-0.0012055586093287541
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,9205487,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Immunology procedure', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Ships', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'Surveillance Program', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'experimental study', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza surveillance', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2017,372603,-0.01089269448222495
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9365558,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Cereals', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Data', 'Data Reporting', 'Data Set', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Graph', 'Image', 'Informatics', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Models', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'learning strategy', 'molecular modeling', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,264299,-0.016683818160472035
"A high-throughput imaging and classification system for fruit flies PROJECT SUMMARY / ABSTRACT In this Phase I SBIR application, FlySorter proposes to development a high throughput imaging and classification system to aid research with fruit flies, a widely-used model organism relevant to both basic science as well as studies in human health. The use of animal model systems is essential for research in almost all aspects of biology: genetics, development, neuroscience, disease, physiology, and beyond. The fruit fly – Drosophila melanogaster – is small and easy to care for, but is complex enough an organism to provide a wealth of information that directly relates to human biology and health. Over 75% of human diseases with a genetic basis (including depression, alcoholism, certain forms of cancer, and many more) are either present or have an analog in Drosophila. Modern genetic tools, such as CRISPR/cas9, allow the creation of transgenic flies that provide the opportunity to study diseases, pathways and systems that don’t exist naturally in Drosophila. With these advances, fruit flies are becoming more frequently subjects for drugs screens. For all the advances in the biological tools and techniques applicable to flies, however, the limiting factor in many experiments is the manual labor involved in a few common tasks: moving flies from vial to vial or other lab equipment; classifying and sorting flies by sex, eye color and other phenotypes; and collecting virgin female flies before they mate so that they can be used in controlled crosses, etc. FlySorter’s patent-pending fly dispensing mechanism can reliably deliver a single organism from a vial containing hundreds of awake flies, and our novel FlyPlate system allows storage of individual flies in custom 96 well plates. FlySorter’s robotic fly handling system, co-developed with the de Bivort Lab at Harvard, is capable of manipulating and transporting those individual flies between vial, 96 well plate, and experimental apparatus. The next piece of the automation puzzle to solve is high throughput imaging and classification. To accomplish this goal, FlySorter will: 1) complete a prototype automated image capture hardware system; 2) adapt state-of-the-art computer vision and machine learning algorithms for use on Drosophila; and 3) build a module that can physically sort the classified flies into different vials. Once integrated into the existing FlySorter product ecosystem, this imaging and classification module will greatly expand the kinds of experiments and screens that can be automated, allowing for the study of larger populations or a wider variety of flies, reducing the impact of human error, and freeing up valuable time for researchers. PROJECT NARRATIVE Fruit flies – Drosophila melanogaster – are one of the most widely used model organisms in biology, for research in genetics, development, neuroscience, disease, and much more. One of the most common tasks in Drosophila labs is sorting flies by various markers and phenotypes using a microscope and paintbrush. FlySorter aims to build an automated system for sorting flies using high resolution digital cameras and modern computer vision algorithms, which will obviate the need for such tedious manual labor.",A high-throughput imaging and classification system for fruit flies,9408980,R43OD023302,"['Air', 'Alcoholism', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Animal Model', 'Animals', 'Appearance', 'Automation', 'Basic Science', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Neural Networks', 'Biology', 'CRISPR/Cas technology', 'Caring', 'Classification', 'Code', 'Complex', 'Computer Vision Systems', 'Custom', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Pathway', 'Dorsal', 'Drosophila genus', 'Drosophila melanogaster', 'Ecosystem', 'Ensure', 'Eye', 'Eye Color', 'Female', 'Floor', 'Genes', 'Genetic', 'Genetic Screening', 'Genetic study', 'Genotype', 'Goals', 'Grant', 'Head', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Legal patent', 'Lighting', 'Longevity', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Mechanics', 'Mental Depression', 'Methodology', 'Microscope', 'Modernization', 'Motor', 'Mutation', 'Names', 'Neurosciences', 'Obesity', 'Optics', 'Organism', 'Partner in relationship', 'Phase', 'Phenotype', 'Physiology', 'Population', 'Preclinical Drug Evaluation', 'Pump', 'Research', 'Research Personnel', 'Resolution', 'Robot', 'Robotics', 'Sampling', 'Sclera', 'Shapes', 'Small Business Innovation Research Grant', 'Sorting - Cell Movement', 'Standardization', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transgenic Organisms', 'Universities', 'Vial device', 'Walking', 'Work', 'analog', 'awake', 'base', 'depression model', 'digital', 'digital imaging', 'experimental study', 'fly', 'genetic strain', 'human disease', 'improved', 'interest', 'laboratory equipment', 'male', 'meter', 'novel', 'phenotypic biomarker', 'prevent', 'prototype', 'sex', 'tool', 'virtual']",OD,"FLYSORTER, LLC",R43,2017,225000,-0.017067453006540688
"Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach Project Summary Biology is full of stunning examples of emergent behaviors – behaviors that arise from, but cannot be reduced to, the interactions of the constituent parts that make up the system under consideration. These behaviors span the full spectrum of length scales, from the emergence of distinct cell fates (e.g. neurons, muscle, etc.) due to the interactions of genes within cells, to the formation of complex ecological communities arising from the interactions of thousands of species. The overarching goal of my research is to develop new conceptual, theoretical, and computational tools to model such emergent, system-level behaviors in biology. To do so, we utilize an interdisciplinary approach that is grounded in Biological Physics, but draws heavily from Machine Learning, Information Theory, and Theoretical Ecology. Our work is unified and distinguished by our deep commitment to integrating theory with the vast amount of biological data now being generated by modern DNA sequencing-based techniques and quantitative microscopy. An important goal of the proposed research is to find common concepts and tools that transcend traditional biological sub-disciplines and models systems. The proposed research pursues four distinct but conceptually interrelated research directions: (1) understanding how distinct cell fates emerge from bimolecular interactions within mammalian cells (2) investigating how bimolecular networks within cells exploit energy consumption to improve computations, with applications to Synthetic Biology; (3) identifying the ecological principles governing community assembly in microbial communities and developing techniques for synthetically engineering ecological communities; and (4) developing new machine learning algorithms and techniques for biological data analysis. In addition to developing physics-based models for diverse biological phenomena, the proposed research will yield a series of practical important tools and algorithms which we will make publically available including: (1) a new linear-algebra based algorithm for assessing the fidelity of directed differentiation and cellular reprogramming protocols and visualizing reprogramming/differentiation dynamics and (2) improved algorithms for inferring microbial interactions in the human microbiome from high-throughput sequence data. These computational tools will allow scientists to realize the immense therapeutic potential of cellular reprogramming and microbial ecology-based techniques for studying and treating human disease. Project Narrative This project will develop new theoretical and computational tools that will allow scientists to realize the immense therapeutic potential of cellular reprogramming (the conversion of one cell type into another cell type) and microbial ecology-based techniques for studying and treating human diseases.",Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach,9317502,R35GM119461,"['Algorithms', 'Behavior', 'Biological', 'Biological Models', 'Biological Phenomena', 'Biology', 'Cells', 'Communities', 'Complex', 'Consumption', 'DNA sequencing', 'Data', 'Data Analyses', 'Discipline', 'Ecology', 'Engineering', 'Goals', 'Human Microbiome', 'Information Theory', 'Length', 'Linear Algebra', 'Machine Learning', 'Mammalian Cell', 'Modeling', 'Modernization', 'Muscle', 'Neurons', 'Physics', 'Protocols documentation', 'Quantitative Microscopy', 'Research', 'Scientist', 'Series', 'System', 'Systems Biology', 'Techniques', 'Therapeutic', 'Transcend', 'Work', 'base', 'cell type', 'computerized tools', 'gene interaction', 'human disease', 'improved', 'interdisciplinary approach', 'microbial', 'microbial community', 'microorganism interaction', 'synthetic biology', 'theories', 'tool']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R35,2017,309030,-0.031055592645089218
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9355693,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Disease model', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Mathematics', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'experimental study', 'improved', 'infancy', 'insight', 'network models', 'novel', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2017,406785,0.0027895398476973964
"Adapting the Berkeley Big Data Analytics Stack to Genomics and Health Project Summary We propose building a computational platform based on the high performance Berkeley Big Data Analytics Stack (BDAS) to support a new ecosystem of Clinical Decision Support (CDS) applications. This platform will make it faster, easier, and less expensive to develop molecular Clinical Decision Support Systems. These systems require real-time queries of globally distributed data, efficient machine learning on large genomic datasets, and must be secure, fault-tolerant and scalable. BDAS and associated technologies are designed to help us meet these challenges and are therefore ideal building blocks to help us create our computational platform. To encourage the adoption of standards for the querying and sharing of large genomic datasets, we will adapt the BDAS stack to support the standards of the Global Alliance for Genomics and Health (GA4GH). Project Narrative Funding this work will help establish a production quality FOSS implementation of the important Global Alliance for Genomics and Health standards. Without such open-source implementations, a fragmented and proprietary platform ecosystem would slow down innovation as well as divert resources away from the practice of medicine.",Adapting the Berkeley Big Data Analytics Stack to Genomics and Health,9466681,R44GM119858,"['Adoption', 'Algorithms', 'Apache', 'Big Data', 'Big Data to Knowledge', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Cloud Computing', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Contractor', 'Data', 'Data Analytics', 'Data Set', 'Distributed Systems', 'Ecosystem', 'Ensure', 'Feedback', 'Funding', 'Genome', 'Genomics', 'Health', 'Individual', 'Industrialization', 'Industry', 'Ingestion', 'Institutes', 'International', 'Leadership', 'Letters', 'Machine Learning', 'Maintenance', 'Measures', 'Medicine', 'Molecular', 'Performance', 'Phase', 'Phenotype', 'Policies', 'Production', 'Provider', 'Publications', 'Resources', 'Running', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Technology', 'Time', 'Training', 'Variant', 'Work', 'base', 'cloud platform', 'cluster computing', 'commercialization', 'design', 'distributed data', 'genomic data', 'health care delivery', 'individual patient', 'innovation', 'open source', 'operation', 'petabyte', 'precision medicine', 'symposium', 'web services', 'whole genome']",NIGMS,"CUROVERSE INNOVATIONS, INC.",R44,2017,1086725,-0.01674030862199678
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9335405,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modeling', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2017,372122,-0.020803768594724898
"INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE Project Summary  To understand the many disorders of the brain it is necessary to grapple with its complexity.  Increasingly large and complicated data sets are being collected, but the tools for analyzing and  modeling the data are not yet available. More researchers trained in computational neuroscience are  desperately needed. This project supports graduate and undergraduate training programs in  computational neuroscience (TPCN) at both Carnegie Mellon University (CMU) and the University of  Pittsburgh (Pitt), and a summer school in computational neuroscience for undergraduates, which are  available to students coming from colleges and universities throughout the United States.  The CMU-Pitt TPCN has 16 training faculty in computational neuroscience, 22 training faculty whose  laboratories are primarily experimental, and 20 training faculty whose laboratories are both  computational and experimental. At the graduate level the TPCN offers a PhD program in Neural  Computation (PNC) and joint PhD programs with CMU’s Department of Statistics (PNC-Stat) and its  Machine Learning Department (PNC- MLD), all set within a highly collegial, cross-disciplinary  environment of our Center for the Neural Basis of Cognition (CNBC), which is operated jointly by  CMU and Pitt. The CNBC was established in 1994 to foster interdisciplinary research on the neural  mechanisms of brain function, and now comprises 145 faculty having appointments in 22 departments.  At the undergraduate level a substantial pool of local students is supplemented during the summer  by a cohort of students from across the country. During this renewal funding period the project is  strengthening the role of statistics and machine learning throughout the training programs; (2) revising the summer undergraduate program by creating a didactic two-week “boot camp” at the  beginning, which includes a 20-lecture overview of computational neuroscience; (3) creating online  materials, in conjunction with the boot camp, that will serve not only our own students but also  the greater world of training in computational neuroscience; and (4) enhancing our minority  recruitment by (a) taking advantage of the boot camp and online materials, as well as making  promotional visits to targeted campuses, and (b) creating and running a one-year “bridge” program  to better prepare under-represented minorities for PhD programs.  TPCN trainees work in vertically integrated, cross-disciplinary research teams. Graduate students  take a year- long course in computational neuroscience that bridges modeling and modern statistical  machine learning approaches to neuroscience. To ensure their competency in core neuroscience  principles they also take courses in cognitive neuroscience, neurophysiology, and systems  neuroscience. They then pursue depth in a relevant quantitative discipline, such as computer  science, engineering, mathematics, or statistics. Graduate students have extended experience in at  least one experimental laboratory, and they take part in journal clubs and seminars within the  large Pittsburgh neuroscience community. Year-long undergraduates take courses in mathematics,  computer programming, statistics, and neuroscience; they take an additional course in neuroscience  or psychology and a course in computational neuroscience; and they complete a year-long research  project. In addition, they complete the TPCN summer program. Undergraduate trainees in the summer  program go through the boot camp on topics in computational neuroscience, including tutorials in  Matlab, statistical methods, fundamentals of differential equations, and ideas of neural coding;  they then complete a research project under careful guidance. All trainees will receive training in  responsible conduct of research. Across 5 years of funding, the TPCN supports 20 NRSA graduate  students, 10 non-NRSA graduate students, 30 undergraduate year-long fellows, and 60 undergraduate  summer fellows. Project Narrative  Research in neuroscience is crucial for attacking the causes of neurological and mental health  disorders. If the field of neuroscience is to continue its rapid advance, neuroscientists must use,  understand, and develop new technologies, acquire and analyze ever larger data sets, and grapple  more directly with the complexity of neurobiological systems. The primary goal of these training  programs will be to help train a new generation of interdisciplinary neuroscientists with strong  quantitative skills.",INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE,9349475,R90DA023426,"['Appointment', 'Brain', 'Brain Diseases', 'Code', 'Cognition', 'Communities', 'Competence', 'Country', 'Data Set', 'Differential Equation', 'Discipline', 'Doctor of Philosophy', 'Educational Status', 'Engineering', 'Ensure', 'Environment', 'Faculty', 'Fostering', 'Funding', 'Interdisciplinary Study', 'Joints', 'Journals', 'Laboratories', 'Machine Learning', 'Mathematics', 'Minority Recruitment', 'Modeling', 'Modernization', 'National Research Service Awards', 'Neurosciences', 'Psychology', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Running', 'Schools', 'Statistical Methods', 'Students', 'System', 'Teacher Professional Development', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States', 'Universities', 'Visit', 'Work', 'bridge program', 'cognitive neuroscience', 'cohort', 'college', 'computational neuroscience', 'computer program', 'computer science', 'data modeling', 'experience', 'graduate student', 'lectures', 'neuromechanism', 'neurophysiology', 'programs', 'relating to nervous system', 'responsible research conduct', 'statistics', 'summer program', 'tool', 'undergraduate student']",NIDA,CARNEGIE-MELLON UNIVERSITY,R90,2017,305372,-0.0020936381406555563
"A three-population three-scale social network model to assess disease dispersion DESCRIPTION (provided by applicant): Communicable diseases, such as influenza, are transmitted from individual to individual following a network of contacts in a population. Reports on recent outbreaks, such as SARS, the Bird flu, and the H1N1 flu, have repeatedly stressed the critical role of contact networks. We propose an innovative Three-population and Three-scale Social Network (3p3sNet) model to simulating the spatial and temporal dispersion of influenza in a metropolitan population in Buffalo, NY. The 3p3sNet aims to construct a realistic contact network by representing interacting and mobile behaviors of individuals at three scales and three types of places. These involve individual (microscopic) -> local network (mesoscopic) -> population (macroscopic) as nighttime population at homes, daytime population at workplaces, and pastime population at service places. Through this network, diseases disperse from infectious individuals to their local networks then to the population-wide network in a complex dynamic fashion. Modeling the disease dispersion through this network provides invaluable insights in who might be at risk, where and when this risk might occur, and with whom these at-risk individuals might be in contact. These insights lay the foundation of developing spatially and temporally sensitive intervention strategies targeted towards the most vulnerable individuals and social groups. Furthermore, the 3p3sNet can be applied in modeling the epidemiology of any disease where human contacts play a critical role.  In implementing 3p3sNet, we propose to use mobile phone data to extract the individual interaction and travel behaviors. We embrace recent developments in economics, geostatistics, econometrics, and machine learning to construct the network. We develop an innovative co-kriging approach to expanding local households to population-wide households and a novel distance-based GEV discrete choice model to link homes to workplaces and service places. It is anticipated that the assemblage of these advanced methods will enable new capabilities and bring transformative improvements in health-related studies in metropolitan areas. We will conduct a data-rich validation process for the three constructed populations, the links between them, and the simulated disease dispersion through the population. A comprehensive range of independent datasets will be used to support the proposed validation. These involve high-resolution population, workplace, and service place data, surveys of individual interaction and travel behavior, and reports on influenza infections.  The multidisciplinary team comprises world-renown leaders and scholars in epidemiology, agent-based and social network modeling, human mobility analysis, geographical information science, and machine learning. The proposed project represents emerging frontiers in the modeling of communicable diseases and will redefine the capabilities of epidemiological models. PUBLIC HEALTH RELEVANCE: This project addresses two issues of central importance to successfully capturing the complex, spatial and temporal dispersion process of a communicable disease through a population. They are: how to represent individuals as heterogeneous, mobile, and interacting and how to model the disease dispersion process from infectious individuals to their local networks then to the population-wide network. To address these issues, this project proposes to: (1) use mobile phone data to construct a three-population and three-scale social network (3p3sNet); (2) simulate the spatially and temporally dynamic dispersion of influenza through an urban population in Buffalo, NY; and (3) conduct an intensive, data-rich validation process on the simulated influenza dispersion.",A three-population three-scale social network model to assess disease dispersion,9228386,R01GM108731,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Behavior', 'Buffaloes', 'Car Phone', 'Communicable Diseases', 'Complex', 'Dangerousness', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Disease Outbreaks', 'Disease model', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Foundations', 'Geography', 'Health', 'Home environment', 'Household', 'Human', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Information Sciences', 'Intervention', 'Link', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Modeling', 'Outcome', 'Play', 'Population', 'Process', 'Records', 'Reporting', 'Resolution', 'Risk', 'Role', 'Sampling', 'Services', 'Severe Acute Respiratory Syndrome', 'Social Network', 'Stress', 'Surveys', 'Testing', 'Time', 'Travel', 'Urban Population', 'Validation', 'Workplace', 'base', 'disease transmission', 'epidemiological model', 'flu', 'frontier', 'fundamental research', 'human disease', 'innovation', 'insight', 'metropolitan', 'multidisciplinary', 'network models', 'novel', 'pandemic disease', 'public health relevance', 'social group', 'transmission process']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,483684,-0.012396095687758504
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9209155,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2017,1354554,-0.012678116277285309
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,229691,-0.008542411977654804
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9312083,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Marines', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'design', 'experimental study', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2017,395858,0.013572157472697763
"IGF::OT::IGF Base Award. Creation of an Accurate Model of the Topical Structure of PubMed and Associated Indicators. POP: 09/01/17 - 02/28/18. N43DA-17-1215. The Contractor will develop advanced and sophisticated analytical models, tools and metrics to enhance the professional evaluation and decision making in life sciences management and administration.  The intended result is a novel set of metrics that can be used by NGOs/disease foundations, advocacy groups, research funders, policy makers and by academic institutional bodies. n/a",IGF::OT::IGF Base Award. Creation of an Accurate Model of the Topical Structure of PubMed and Associated Indicators. POP: 09/01/17 - 02/28/18. N43DA-17-1215.,9583616,71201700041C,"['Advocacy', 'Award', 'Biological Sciences', 'Complement', 'Contractor', 'Data', 'Databases', 'Decision Making', 'Disease', 'Evaluation', 'Foundations', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Modeling', 'Policy Maker', 'PubMed', 'Quality Indicator', 'Records', 'Reproducibility', 'Research', 'Structure', 'Testing', 'Text', 'Translations', 'base', 'economic impact', 'novel', 'tool']",NIDA,"SCITECH STRATEGIES, INC.",N43,2017,225000,-0.008204789808123832
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9270498,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Logistics', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'System', 'Taxonomy', 'Technology', 'Testing', 'Time', 'Work', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'experimental study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2017,311153,-0.0023458727199927006
"Vaccine Beliefs and Decision Making Project Summary This project will use methods from quantitative anthropology to describe the social space of vaccine beliefs that circulate among the general public and to provide an initial assessment of how different belief variations influence decisions to vaccinate. The results will establish, for the first time, the patterns of co-variation in the wide variety of pro- and anti- vaccine beliefs, and which axes of this variation appear associated with decisions to vaccinate. Vaccination is a key public health defense against infectious disease, but the lay public largely does not fully appreciate scientific evidence when making decisions for or against vaccination. Understanding the inter-correlations of these beliefs, therefore, is imperative for designing effective educational interventions that can directly interface with the cultural beliefs that surround vaccination and influence the public's decision making on this issue. The project will leverage insights from two very different but complementary data sources: responses to a nationally representative survey (fielded on the RAND American Life Panel) and social media data from Twitter. Our analytic approach will begin with systematic coding techniques from mixed-methods research to classify vaccine beliefs into a comprehensive set of belief variants. Manual coding will be validated through inter-observer reliability checks and replicated at scale with machine-learning algorithms. Having systematically coded the data, we will then assess whether nationally representative survey data and data mined from Twitter produce similar results using Cultural Consensus Analysis, a technique from quantitative cultural anthropology. From the survey data we will test whether vaccine beliefs are correlated with decisions to vaccinate after controlling for demographic attributes. To ensure completion of this innovative and methodologically expansive project, the project team combines expertise from anthropology, decision science, clinical medicine, and biomathematics. The principal investigator brings to this project multiple years of both academic and industry experience in statistical modelling of cultural data. Project Narrative Vaccination is a key public health defense against infectious disease, but patients' vaccination decisions may be more influenced by broadly circulated cultural beliefs than they are influenced by scientific evidence. This proposed research will systematically map the diversity of the publics' vaccination beliefs, assess how these beliefs influence vaccination decisions, and advise policy makers how to interface more directly with these popular belief systems that are critical to effective vaccination efforts.",Vaccine Beliefs and Decision Making,9241259,R21HD087749,"['Achievement', 'Adolescent', 'Adopted', 'Adult', 'Algorithms', 'American', 'Anthropology', 'Autistic Disorder', 'Behavior', 'Belief', 'Belief System', 'Childhood', 'Clinical Medicine', 'Code', 'Cognitive', 'Communicable Diseases', 'Communities', 'Consensus', 'Cultural Anthropology', 'Data', 'Data Sources', 'Decision Making', 'Disease', 'Educational Intervention', 'Ensure', 'Environment', 'Fright', 'General Population', 'Health Communication', 'Health behavior', 'Immunization', 'Individual', 'Industry', 'International', 'Intervention', 'Lead', 'Life', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Maps', 'Measles', 'Mediating', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Parents', 'Patients', 'Pattern', 'Persons', 'Policies', 'Policy Maker', 'Positioning Attribute', 'Principal Component Analysis', 'Principal Investigator', 'Probability', 'Process', 'Public Health', 'Recommendation', 'Research', 'Research Methodology', 'Resistance', 'Role', 'Safety', 'Sampling', 'Science', 'Statistical Models', 'Structure', 'Survey Methodology', 'Surveys', 'System', 'Techniques', 'Testing', 'Time', 'United States', 'Vaccinated', 'Vaccination', 'Vaccines', 'Variant', 'Work', 'authority', 'biomathematics', 'cognitive process', 'design', 'efficacy testing', 'experience', 'innovation', 'insight', 'interest', 'prevent', 'response', 'social', 'social media', 'social space', 'theories', 'therapy design', 'vaccine safety']",NICHD,RAND CORPORATION,R21,2017,239723,-0.003996785644550197
"A platform for mining, visualization and design of microbial interaction networks Project Summary One of the burning questions in the study of the human microbiome is whether and how it is possible to design specific strategies for rebalancing the taxonomic and functional properties of human-associated microbial communities, triggering the transition from “disease states” to “healthy states”. While empirical studies provide strong support for the idea that we may be able to cure, or at least  treat, a number of diseases by simply transplanting microbiomes, or inducing changes through taxonomic or environmental perturbations, to date little mechanistic understanding exists on how microbial communities work, and on how to extend microbiome research from an empirical science to a systematic, quantitative field of biomedicine. We propose here to establish a computational platform--   a database (Aim 1) with fully integrated analytical software (Aims 2 and 3) --- developed for and with the cooperation of the scientific community. The resource goes beyond cataloguing microbial abundances under different condition; its aim is to enable an understanding of networks of interacting species and their condition-dependence, with the goal of eventually facilitating disease diagnosis and prognosis, and designing therapeutic strategies for microbiome intervention. Our project is centered around three key aims: 1.	The creation of a Microbial Interaction Network Database (MIND), a public resource that will collect data on inter-species interactions from metagenomic sequencing projects, computer simulations and direct experiments. This database will be accessed through a web-based platform complemented with tools for microbial interaction network analysis and visualization, akin to highly fruitful tools previously developed for the study of genetic networks; the database will also serve as the public repository of microbial networks associated with human diseases; 2.	The implementation of an integrated tool for simulation of interspecies interactions under different environments, based on genomic data and whole-cell models of metabolism; 3.	The implementation of new algorithms for microbial community analysis and engineering. These algorithms, including stoichiometric, machine-learning and statistical approaches will facilitate a “synthetic ecology” approach to help design strategies (e.g. microbial transplants or probiotic mixtures) for preventing and targeting microbiome-associated diseases. Our work will fill a major gap in current microbiome research, creating the first platform for global microbial interaction data integration, mining and computation. Project Narrative Among the major developments of the genomic revolution has been the ability to identify thousands of microbial species and strains living in communities in 5 major habitats in the human body, and the recognition that the relative abundances of these populations is strongly correlated with environment: disease state, diet, treatment protocol and so on. A major challenge in utilizing the deluge of health relevant data is structuring it into a database that facilitates understanding inter-microbial interactions in these communities. The aim of this proposal is to create a database and integrated computational platform, open to and contributed to by the research community, which will greatly accelerate the conversion of data into health related actionable knowledge.","A platform for mining, visualization and design of microbial interaction networks",9221662,R01GM121950,"['Affect', 'Algorithms', 'Cataloging', 'Catalogs', 'Cell model', 'Clinical', 'Communities', 'Complement', 'Complex', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dependence', 'Development', 'Diet', 'Discipline', 'Disease', 'Ecology', 'Ecosystem', 'Empirical Research', 'Engineering', 'Environment', 'Evolution', 'Future', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'Habitats', 'Health', 'Human', 'Human Biology', 'Human Microbiome', 'Human body', 'Imagery', 'Individual', 'Intervention', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurable', 'Mediating', 'Metabolic', 'Metabolism', 'Metadata', 'Methods', 'Microbe', 'Mining', 'Nature', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pattern', 'Population', 'Preventive Medicine', 'Probiotics', 'Property', 'Research', 'Resources', 'Science', 'Scientist', 'Structure', 'Taxonomy', 'Technology', 'Therapeutic', 'Time', 'Transplantation', 'Treatment Protocols', 'Work', 'base', 'computer framework', 'data integration', 'data to knowledge', 'design', 'disease diagnosis', 'experimental study', 'feeding', 'genome-wide', 'genomic data', 'human disease', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microbiota transplantation', 'microorganism interaction', 'novel diagnostics', 'novel therapeutics', 'open source', 'outcome forecast', 'prevent', 'repository', 'simulation', 'tool', 'user-friendly']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2017,377226,0.0010379243160592254
"The Blackfynn Platform for Rapid Data Integration and Collaboration Summary One in seven people worldwide suffers from a brain disorder, e.g., epilepsy, Parkinson's, stroke, or dementia. Development of future treatments depends on improving our understanding of brain function and disease, and validating new treatments critically depends on identifying the underlying biomarkers associated with different conditions. Biomarker discovery requires volume, quality, richness, and diversity of data. This Direct-to-Phase II project extends Blackfynn's cloud data management platform for team science, in order to support interactive data curation and integration and to facilitate biomarker discovery. Our first technical aim develops tools to help select, curate, assess, and regularize datasets: we develop novel “live” query capabilities to ensure users discover relevant data, develop mechanisms for using data's provenance to decide on trustworthiness, and build tools for mapping fields to common data elements. These capabilities address the critical, under-served problem of selecting the data to analyze. Our second technical aim develops techniques for incorporating algorithms to link and co-register across multi-modal data and metadata. Using ranking and machine learning, we can incorporate and combine state-of-the-art algorithms for finding data relationships, and we can link to remote data sources. These capabilities enable scientists to analyze richer datasets with multiple data modalities and properties – thus enabling them to discover more complex correlations and biomarkers. In our third aim, Blackfynn's new technical capabilities will be applied to challenges faced by Blackfynn partners, including problems assessing trustworthiness of data annotations, conducting image analysis, modeling epileptic networks, and identifying biomarkers for neuro-oncology indications. As part of this validation we will also develop HIPAA-compliant mechanisms for working with protected and de-identified data together. Together, these three thrusts will ensure that development of the Blackfynn platform results in tools and technologies that meaningfully accelerate scientific understanding and discovery over rich and complex data, leading to improved treatments for neurologic disease.   Narrative This Direct-to-Phase II project extends the Blackfynn cloud data management platform to enable biomarker discovery for research and development of improved drugs, devices and clinical care for patients with neurologic disease: it develops tools for assembling, evaluating, and rating data, and linking it across modalities and to external systems. It also validates the techniques' effectiveness using real challenges faced by Blackfynn partners, in imaging, epilepsy, and brain tumor research.",The Blackfynn Platform for Rapid Data Integration and Collaboration,9343385,R44DA044929,"['Address', 'Algorithms', 'Benchmarking', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain Neoplasms', 'Case Study', 'Clinical Pharmacology', 'Collaborations', 'Common Data Element', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Provenance', 'Data Science', 'Data Set', 'Data Sources', 'Dementia', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Ensure', 'Epilepsy', 'Funding', 'Future', 'Health', 'Health Insurance Portability and Accountability Act', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Machine Learning', 'Maps', 'Mental Depression', 'Metadata', 'Modality', 'Modeling', 'National Institute of Neurological Disorders and Stroke', 'Neurosciences', 'Neurosciences Research', 'Notification', 'Ontology', 'Output', 'Parkinson Disease', 'Patient Care', 'Pharmaceutical Preparations', 'Phase', 'Plug-in', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Science', 'Scientist', 'Semantics', 'Series', 'Small Business Innovation Research Grant', 'Source', 'Standardization', 'Stroke', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Trust', 'Use Effectiveness', 'Validation', 'Work', 'base', 'biomarker discovery', 'clinical application', 'clinical care', 'cloud platform', 'computer science', 'data access', 'data integration', 'data management', 'improved', 'indexing', 'nervous system disorder', 'neuro-oncology', 'neuroimaging', 'novel', 'novel therapeutics', 'open source', 'research and development', 'tool']",NIDA,"BLACKFYNN, INC.",R44,2017,652921,-0.047453435164483714
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9248178,U24HG009446,"['ATAC-seq', 'Alleles', 'Alpha Cell', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2017,2000000,-0.029924869771353186
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),9321115,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug Addiction', 'Drug Receptors', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Epigenetic Process', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Source', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'algorithmic methodologies', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'cloud platform', 'computational chemistry', 'computer science', 'data mining', 'design', 'distinguished professor', 'dopamine transporter', 'drug abuse prevention', 'falls', 'genome-wide', 'genome-wide analysis', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'novel therapeutics', 'operation', 'prevent', 'professor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2017,1080816,-0.014989845877341989
"Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!) PROJECT SUMMARY/ABSTRACT Our society faces significant challenges in providing quality health care that is accessible by each person and is sensitive to each person's individual lifestyle and individual health needs. Due to recent advances in sensing technologies that have improved in accuracy, increased in throughput, and reduced in cost, it has become relatively easy to gather high resolution behavioral and individualized health data at scale. The resulting big datasets can be analyzed to understand the link between behavior and health and to design healthy behavior interventions. In this emerging area, however, very few courses are currently available for teaching researchers and practitioners about the foundational principles and best practices behind collecting, storing, analyzing, and using behavior- based sensor data. Teaching these skills can help the next generation of students thrive in the increasingly digital world.  The goal of this application is to design online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to WSU faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.  This contribution is significant because not only large research groups but even individual investigators can create large data sets that provide valuable, in-the-moment information about human behavior. They need to be able to handle the challenges that arise when working with sensor- based behavior data. Because students will receive hands-on training with actual sensor datasets and analysis tools, they will know how to get the best results from available tools and will be able to interpret the significance of analysis results.  Our proposed online course program, called AHA!, builds on the investigators' extensive experience and ongoing collaboration at Washington State University on the development of smart home and mobile health app design, activity recognition, scalable biological data mining, and the use of these technologies for clinical applications. Our approach will be to design online course modules to train individuals in the analysis of behavior-based sensor data using clinical case studies (Aim 1). We will design an educational program that involves students from diverse backgrounds and that is findable, accessible, interoperable, and reusable (Aim 2). Finally, we will conduct a thorough evaluation to monitor success and incrementally improve the program (Aim 3). All of the materials will be designed for continued use beyond the funding period of the program. PROJECT NARRATIVE  This program focuses on the development and dissemination of online courses that train researchers and practitioners in sensor-based behavioral health. Specifically, we will offer training in responsible conduct, collection and understanding of behavioral sensor data, data exploration and statistical inference, scaling behavioral analysis to massive datasets, and introducing state of the art machine learning and activity learning techniques. The courses will be offered in person to Washington State University faculty and staff, offered with staff support through MOOCs, and available to the general public from our web page. Course material will be enhanced and driven by specific clinical case studies. Additionally, the courses will be supplemented with actual datasets that students can continue to use beyond the course.",Development of an Online Course Suite in Tools for Analysis of Sensor-Based Behavioral Health Data (AHA!),9313495,R25EB024327,"['Address', 'Aging', 'Area', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Big Data', 'Biological', 'Case Study', 'Charge', 'Chronic Disease', 'Clinical', 'Code', 'Collaborations', 'Collection', 'Data', 'Data Set', 'Development', 'Discipline', 'E-learning', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Evaluation', 'FAIR principles', 'Face', 'Faculty', 'Feedback', 'Foundations', 'Funding', 'General Population', 'Goals', 'Health', 'Home environment', 'Human', 'Immersion Investigative Technique', 'Individual', 'Interdisciplinary Study', 'Life Style', 'Link', 'Longevity', 'Machine Learning', 'Methods', 'Monitor', 'Performance', 'Persons', 'Precision Medicine Initiative', 'Pythons', 'Recruitment Activity', 'Rehabilitation Nursing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Site', 'Societies', 'Structure', 'Students', 'Suggestion', 'Techniques', 'Technology', 'Training', 'Universities', 'Washington', 'Work', 'base', 'behavior influence', 'behavioral health', 'biocomputing', 'career networking', 'clinical application', 'cognitive rehabilitation', 'cost', 'course development', 'course module', 'data mining', 'design', 'digital', 'experience', 'health care quality', 'health data', 'improved', 'innovation', 'learning materials', 'learning strategy', 'mHealth', 'next generation', 'online course', 'programs', 'responsible research conduct', 'scale up', 'sensor', 'skills', 'statistics', 'success', 'synergism', 'tool', 'web page']",NIBIB,WASHINGTON STATE UNIVERSITY,R25,2017,186245,-0.0027900914013791324
"Effects of Host Metabolic Variation on Antibiotic Susceptibility ﻿    DESCRIPTION (provided by applicant): Host variation affects the pathogenicity of, susceptibility to, and recovery from infectious diseases. Elucidating how the host environment alters antibiotic susceptibility is therefore a critical step towards the long-term goal of realizig precision medicine for the clinical management of infectious diseases. The overall objective of this project is to identify the metabolic pathways participating in antibiotic susceptibility and t determine how host metabolism may affect antibiotic treatments at the site of infection. The working hypothesis is that metabolic processes can function as bacterial control mechanisms for antibiotic susceptibility and that the host metabolome can act on these processes to affect the outcome of antibiotic treatment. This hypothesis will be tested in three specific aims: (1) identif metabolic pathways involved in antibiotic susceptibility (K99 phase); (2) characterize changes in the host metabolome elicited by antibiotic administration (K99 phase); (3) evaluate effects of the host metabolome on antibiotic killing (R00 phase). During the mentored K99 phase, bactericidal antibiotics will be counterscreened with various metabolites to identify metabolic perturbations that can affect antibiotic susceptibility. Metabolic pathways contributing to antibiotic lethality ill be identified by combining this data with metabolic modeling and machine learning. Additionally, plasma and peritoneal fluid will be sampled and metabolomically profiled from a mouse peritoneal infection model, with and without antibiotic treatment. These profiles will be used to determine if antibiotics can alter the host metabolism in ways that may affect antibiotic susceptibility at the site of infection. During the independent R00 phase, the effects of host metabolic variation on antibiotic killing will be systematically tested by quantifying antibiotic susceptibility in synthesized media defined by metabolomic profiles from published and measured human and mouse plasma samples. A better understanding of how the host metabolic environment participates in antibiotic treatment fits NIH's public health mission and has direct implications for the clinical management of infectious diseases. Work from the proposed studies will form a quantitative framework for directly evaluating how host metabolism may affect antibiotic treatment outcomes and guide improved antibiotic stewardship in clinical practice. Although the applicant has significant expertise in systems biology, this award will provide the applicant research training to gain new experimental skills and an opportunity for continued career training and mentorship from an advisory committee comprised of international leaders in systems biology, metabolomics, chemical biology and infectious diseases. The support and training provided by this award and by the advisory committee will provide the applicant tools and expertise critical to his future independent research program. PUBLIC HEALTH RELEVANCE (provided by applicant): Host variation alters the clinical response to antibiotic treatment for infectious disease, but the pathways underlying differences in patient outcome have not yet all been elucidated. The proposed research is relevant to public health because it is the first systematic investigation of how host metabolites may act on bacterial pathogen metabolism and alter antibiotic susceptibility. This work is relevant to NIH's public health mission by providing a quantitative framework for establishing precision medicine for the treatment of infectious diseases.",Effects of Host Metabolic Variation on Antibiotic Susceptibility,9355203,K99GM118907,"['Adjuvant', 'Advisory Committees', 'Affect', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotic susceptibility', 'Antibiotics', 'Award', 'Bacteria', 'Biochemical Pathway', 'Biology', 'C57BL/6 Mouse', 'Carbon', 'Cells', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Management', 'Clinical Treatment', 'Communicable Diseases', 'Culture Media', 'Data', 'Development', 'Dimensions', 'Disease Pathway', 'Disease Resistance', 'Environment', 'Escherichia coli', 'Formulation', 'Future', 'Germ-Free', 'Goals', 'Growth', 'Human', 'Immunity', 'In Vitro', 'Infection', 'Innovative Therapy', 'International', 'Investigation', 'Knock-out', 'Knowledge', 'Light', 'Liquid substance', 'Machine Learning', 'Measures', 'Mentors', 'Mentorship', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Mission', 'Modeling', 'Mus', 'Nitrogen', 'Outcome', 'Pathogenicity', 'Pathway interactions', 'Patient-Focused Outcomes', 'Peritoneal', 'Peritoneal Fluid', 'Peritoneal lavage', 'Phase', 'Phosphorus', 'Plasma', 'Predisposition', 'Process', 'Public Health', 'Publishing', 'Recovery', 'Research', 'Research Training', 'Sampling', 'Serum', 'Site', 'Source', 'Sulfur', 'Supplementation', 'Symbiosis', 'System', 'Systems Biology', 'Testing', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Variant', 'Virulence', 'Work', 'bacterial metabolism', 'bactericide', 'base', 'biological adaptation to stress', 'career', 'clinical practice', 'cohort', 'counterscreen', 'desensitization', 'genome-wide', 'improved', 'in vivo', 'infectious disease treatment', 'insight', 'killings', 'metabolome', 'metabolomics', 'microbial', 'microbiota', 'novel therapeutics', 'overexpression', 'pathogen', 'precision medicine', 'programs', 'public health relevance', 'respiratory', 'response', 'skills', 'tool', 'treatment strategy']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,K99,2017,87929,-0.00782387673091026
"All of Us, Wisconsin Our application for OT-PM-16-003, “One in a Million – Precision Medicine Initiative Wisconsin” represents the collaborative efforts of three fully integrated regional healthcare systems to form a virtual state-wide integrated delivery network. The application originates at the Marshfield Clinic Health System (MCHS), one of the most productive and earliest adopters of what has become Precision Medicine. The work of the MCHS has been cited by NIH Director Dr. Francis Collins as a model for the Precision Medicine Initiative (PMI). To ensure comprehensive nearly state-wide coverage of participant health care, MCHS has partnered with the other academic integrated delivery networks in Wisconsin, (University of Wisconsin (UW) and Medical College of Wisconsin (MCW)), that together include 173 clinics, 13 hospitals, insurer partners and 5 Federally Qualified Health Centers (FQHCs). The Blood Center of Wisconsin (BCW) also partners to operate state-wide mobile units of staff and equipment for blood collection that may be purposed for this effort. Through an independent agreement with Aurora Health, the electronic health records of participants recruited by academic and FQHC sites will be made available, ensuring nearly complete coverage of health care records across an entire state, with special emphasis on those populations that are traditionally the most underserved and understudied. The academic sites and FQHC partners have a long history of research cooperation and robust community engagement demonstrated by Clinical and Translational Science Awards (CTSA) ties, the Wisconsin Genome Initiative, reciprocal IRB arrangements, multiple joint grants and other collaborative studies. We have pioneered and implemented data sharing platforms and have demonstrated their usefulness with numerous high impact publications over many years. All three academic centers are leaders in the details of community engagement, recruitment, tracking, return of results and data sharing across various platforms in national and international consortia using large cohorts (e.g., MCHS’s 20,000 participant Personalized Medicine Research Project). A major innovation of this application is that it aligns the major healthcare systems of an entire state to more fully capture each participant’s data wherever participants get their health care. Indeed, 80% of all health care in Wisconsin is captured by our footprint. The State-wide catchment area also includes the oldest, sickest and poorest regions in the US and represent rural and urban areas, including African Americans, Hispanics and Native Americans, all represented as community champions. Through the FQHC partners, the recruiting institutions will achieve the 40% African American, Hispanic and Native American participant rate, with long standing community engagement and history of successful recruitment, including Wisconsin’s 12 Native American Tribes. Lending active support are Community Champions and Community Advisory Boards. A second innovative approach will employ Machine Learning (ML) techniques to optimize recruitment and retention processes. Our scientists have long collaborated on efforts to enhance the breadth and reliability of information extracted from the electronic health record (EHR), using a range of data to identify elements including family pedigrees or the occurrence of clinical events that are susceptible to unreliable coding. These efforts employed state-of-the-art methods including random forests, support vector machines and statistical relational learning, among others. These advanced computational methods will be used to predict those who are most likely to participate and which methods of approach are most effective. A third innovative approach is the planned use of mobile recruitment labs, previously successful with the Survey of the Health of Wisconsin (SHOW). SHOW, began in 2008 provides a novel infrastructure for population health research recruitment, enabling engagement across the state, longitudinal follow-up of participants and community-specific studies. Key assets of SHOW are two mobile units staffed by research and healthcare professionals to facilitate on-site health studies in randomly selected or community-specific participants. SHOW is predicated on community engagement and the response rate has been outstanding (in one study more than 90% of ~4,000 participants consented to follow-up, DNA testing, or blood work). This is true across all racial/ethnic groups, including non-Hispanic Whites, African Americans, Hispanics and Native Americans. The fourth innovative approach is to work with our FQHC partners to recruit traditionally underserved populations. MCHS, UW and MCW will work with FQHC partners (Marshfield Family Health Center, Access Community Health Center, Milwaukee Health Services, Progressive Health Center and 16th Street Community Health Center). Importantly, we are ready to start. We have already informed and engaged our communities through press releases and newsletters. Processes are in place. Volunteers have already asked us to contact them. All the required personnel are trained and SOPs are in place to begin recruitment. We have a productive history of working with partners already funded for PMI cohort recruitment. For several years, we have worked closely (and published) with our colleagues at Northwestern and Columbia through the eMERGE Network and with the University of Pittsburgh through CTSA. Project Director Murray Brilliant has recently served as an advisor to the University of Arizona’s Precision Medicine Program. Thus, we are team-players who are ready, willing and able to be valuable collaborative partners in the effort to build a national engine to transform healthcare under PMI. n/a","All of Us, Wisconsin",9512099,OT2OD025286,"['African American', 'Agreement', 'Arizona', 'Award', 'Blood', 'Catchment Area', 'Clinic', 'Clinical', 'Clinical Sciences', 'Code', 'Collection', 'Communities', 'Community Health Centers', 'Computing Methodologies', 'Consent', 'DNA', 'Data', 'Electronic Health Record', 'Elements', 'Ensure', 'Equipment', 'Ethnic group', 'Event', 'Family', 'Family health status', 'Federally Qualified Health Center', 'Funding', 'Genome', 'Grant', 'Health', 'Health Professional', 'Health Services', 'Health Surveys', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hispanic Americans', 'Hospitals', 'Human Resources', 'Institution', 'Insurance Carriers', 'International', 'Joints', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Native Americans', 'Newsletter', 'Not Hispanic or Latino', 'PMI cohort', 'Participant', 'Population', 'Precision Medicine Initiative', 'Press Releases', 'Process', 'Publications', 'Publishing', 'Recording of previous events', 'Records', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Scientist', 'Site', 'Techniques', 'Testing', 'Training', 'Translational Research', 'Tribes', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Wisconsin', 'Work', 'cohort', 'data sharing', 'follow-up', 'forest', 'genetic pedigree', 'innovation', 'medical schools', 'novel', 'personalized medicine', 'population health', 'precision medicine', 'programs', 'racial and ethnic', 'response', 'rural area', 'urban area', 'virtual', 'volunteer']",OD,MARSHFIELD CLINIC RESEARCH FOUNDATION,OT2,2017,5360833,-0.022645415324207552
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases. PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.",Integration and Visualization of Diverse Biological Data,9266422,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Drug Targeting', 'Expert Systems', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic study', 'Goals', 'Gold', 'Hereditary Disease', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'Supervision', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'diagnostic biomarker', 'drug development', 'drug discovery', 'experimental study', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'novel therapeutics', 'public health relevance', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2017,445349,-0.025817557548633848
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9275314,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk population', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'semiparametric', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2017,142177,-0.015752223679611717
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9279143,U01GM110721,"['Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Clinical Data', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronaviridae', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Immune system', 'Immunological Models', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methodology', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Research Methodology', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Syndrome', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'algorithmic methodologies', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiologic data', 'epidemiological model', 'forest', 'genetic evolution', 'high dimensionality', 'improved', 'infectious disease model', 'innovation', 'insight', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'predictive tools', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2017,202814,0.002358813970665672
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design A fundamental challenge to translate insights between biomedical researchers, who study biological mechanisms, and clinicians, who diagnose patient symptoms, is that many links between biological processes and disease pathophysiology are poorly understood. A comprehensive Biomedical Translator must enable chains of inference across objects as diverse as genetic mutations, molecular effects, tissue-specific expression patterns, cellular processes, organ phenotypes, disease states, patient symptoms, and drug responses, a challenge beyond the scope of any one organization. Fortunately, many individual links in this chain have been made by experiments yielding statistical connections between individual data types. High-throughput perturbation screens link chemical and genetic perturbations to cellular phenotypes such as gene-expression patterns, cell survival, or changes in phosphorylation. Genetic association studies link mutations to human disease or intermediate phenotypes and biomarkers. Electronic medical records (EMR) link diseases or human phenotypes to diagnostic or current procedural terminology (CPT) codes, and clinical trials link the impact of drugs and drug candidates on disease states. In principle, incorporating these links into chains of inference could translate results between the full set of data types within them. In practice, each link is maintained by experts with domain-specific experiments, semantic terminology, and methodological standards. While a key challenge faced by a global Biomedical Translator is to establish consistent standards across these existing data types, a more important goal is to develop a principled and robust framework to (a) model biological systems and experimental approaches to investigate them; (b) organize knowledge about biological mechanism and disease; and (c) incorporate diverse datasets that serve as windows into the underlying and unknown state of nature. We propose to implement a Biomedical Translator as a probabilistic graphical model, a paradigm from artificial intelligence (AI) research. Just as separate research communities form weakly coupled parts of the translation process, graphical models allow global inferences from weakly coupled “nodes”. These inferences require each node to publish only probability distributions, enabling interoperability without necessarily having global entity-resolution standards, and benefit from paradigms for quality control, fault tolerance, and relevance assessment common in AI research. We hypothesize that a limited number of APIs, implemented as probability computations by communities around the world, would yield a Biomedical Translator as an emergent property of weakly coupled knowledge sources. From basic properties of graphical models, such a Translator could probabilistically translate among any data types connected within it, allowing for relatively complex query concepts. For example: What cellular processes in which tissues are impacted in a patient-based EMR? What genetic mutations sensitize cells to small-molecule treatment effects? Which small molecules mimic genetic “experiments of nature” that protect against disease? To illustrate the value of these resources and our architectural paradigm, we propose a demonstration project to implement a Biomedical Translator supporting queries between small molecules, biological processes, genes, and disease. The demonstration project will provide a valuable first step to confront key data-integration and organizational challenges and will enable previously impossible queries, such as identifying small molecules that perturb the same biological processes implicated by human genetics in a disease context. In this capacity, such Translator could realistically identify existing drugs for known symptoms (i.e., repurposing), but could more broadly serve as an engine for hypothesis generation and biological discovery, suggesting pre-clinical small molecules to develop based on their observed biological activity, or providing heretofore novel links between cellular protein function and disease pathophysiology. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9540181,OT3TR002025,"['Architecture', 'Artificial Intelligence', 'Biological', 'Biological Models', 'Biological Process', 'Cell Survival', 'Cell physiology', 'Cells', 'Clinical Trials', 'Communities', 'Complex', 'Computerized Medical Record', 'Coupled', 'Current Procedural Terminology Codes', 'DNA Sequence Alteration', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Disease', 'Functional disorder', 'Gene Expression Profile', 'Generations', 'Genetic', 'Goals', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Link', 'Methodology', 'Modeling', 'Molecular', 'Mutation', 'Nature', 'Organ', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphorylation', 'Probability', 'Processed Genes', 'Property', 'Publishing', 'Quality Control', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Semantics', 'Source', 'Symptoms', 'Terminology', 'Tissues', 'Translating', 'Translation Process', 'base', 'biological systems', 'chemical genetics', 'data integration', 'design', 'drug candidate', 'experimental study', 'genetic association', 'human disease', 'insight', 'interoperability', 'novel', 'phenotypic biomarker', 'pre-clinical', 'protein function', 'response', 'small molecule', 'treatment effect']",NCATS,"BROAD INSTITUTE, INC.",OT3,2017,651119,-0.020792734544687417
"Development of mosaic mouse models of HCC for genetic interspecies inference DESCRIPTION (provided by applicant): Hepatocellular carcinoma (HCC) remains a menace for human health for the lack of any effective treatment. HCC is usually the end result of chronic liver diseases associated with diverse risk factors. Furthermore, non- alcoholic steatohepatitis (NASH) induced HCC, which is projected to be the leading cause of new cases, remains poorly characterized. Although many mouse models of HCC have been developed, it is unclear how well they represent different subgroups of human HCCs. This research plan proposes to transform HCC animal modeling by establishing a novel in vivo platform to accurately replicate the somatic molecular profiles of human HCC in mice. To do this, three independent HCC mouse models will be exhaustively characterized through exome sequencing and gene expression profiling. Using bioinformatics techniques including machine- learning and network analysis, these datasets will be compared with human HCC datasets from TCGA and the ICGC to identify subgroups of patients with similar somatic molecular profiles to the HCC mouse models as well as refined minimum sets of characteristic genetic aberrations. A derived transposon system will be used to generate mosaic mouse models replicating human HCC genetic subgroups faithfully. These models will enable 1) experimental dissection of the molecular mechanisms underlying distinct etiologies of HCC, 2) systematic assessment of candidate HCC therapies and 3) investigation of therapeutic resistances. This K99/R00 career development award proposal describes a two-year mentored and three-year independent research program essential for the development of Dr. Font-Burgada as an independent investigator. Dr. Font-Burgada received his PhD at University of Barcelona, Spain, for the work he performed to investigate basic chromatin regulatory and epigenetic mechanisms. He then moved to University of California, San Diego where he joined Dr. Michael Karin's laboratory to train in mouse models of cancer and signal transduction. For the accomplishment of this research proposal, Dr. Font-Burgada has designed a strong training and career development plan consisting of: 1- the continued mentorship of Dr. Michael Karin to gain additional expertise in mouse models of HCC and signal transduction, 2- Training in bioinformatics, specifically in methods to identify cancer driver genetic aberrations and network-based approaches for comparative genomic analysis of mouse and human HCCs, to be overseen by co-mentor Dr. Hannah Carter, an Assistant Professor of Medicine, at UCSD. 3- Training in application of emerging transposon vector technologies to generate mosaic mouse models for in vivo analysis of oncogenic pathways. 4- Career development courses and seminars in a supportive academic environment in the Department of Pharmacology at UCSD to complement other aspects of the training program. This training plan will be overseen by an advisory committee comprising 4 members, mentor, co-mentor, and additional experts in mouse models of cancer and bioinformatics, Inder Verma and Trey Ideker, providing key scientific insights and essential guidance in critical steps in Dr. Font-Burgada transition to independence. PUBLIC HEALTH RELEVANCE: Hepatocellular carcinoma (HCC) has a 5-year survival rate of less than 10% and leads to 700000 deaths globally each year. The genetic causes of this disease are poorly understood and the only available targeted therapy extends life expectancy by a mere 3 months. In order to improve these dismal statistics, I am developing a novel class of mouse models capable of reproducing the spectrum of mutations observed in a given human tumor, to enable study of HCC biology as well as systematic testing of therapeutic combinations.",Development of mosaic mouse models of HCC for genetic interspecies inference,9394884,R00CA191152,"['Advisory Committees', 'Animal Model', 'BRAF gene', 'Beauty', 'Bioinformatics', 'Biology', 'California', 'Cancer Etiology', 'Cancer Model', 'Catalogs', 'Cell Culture Techniques', 'Cells', 'Cessation of life', 'Characteristics', 'Chromatin', 'Clinical', 'Comparative Genomic Analysis', 'Complement', 'Complex', 'Data', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Dissection', 'Doctor of Philosophy', 'Drowsiness', 'Drug Targeting', 'Ensure', 'Environment', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Gene Expression Profiling', 'Genetic', 'Geography', 'Health', 'Hepatocyte', 'Heterogeneity', 'High Fat Diet', 'Human', 'Human Pathology', 'Investigation', 'K-Series Research Career Programs', 'Knowledge', 'Label', 'Laboratories', 'Life Expectancy', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mosaicism', 'Mouse Protein', 'Mus', 'Mutate', 'Mutation', 'Network-based', 'Oncogenic', 'Orthologous Gene', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Phase', 'Phenotype', 'Physiological', 'Population', 'Pre-Clinical Model', 'Primary carcinoma of the liver cells', 'Reproducibility', 'Research', 'Research Personnel', 'Research Proposals', 'Resistance', 'Risk', 'Risk Factors', 'Signal Transduction', 'Somatic Mutation', 'Spain', 'Subgroup', 'Survival Rate', 'System', 'Techniques', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Therapeutic', 'Training', 'Training Programs', 'Transgenes', 'Translating', 'Universities', 'Viral Vector', 'Work', 'Xenograft Model', 'accurate diagnosis', 'base', 'career development', 'chronic liver disease', 'combinatorial', 'comparative', 'course development', 'design', 'effective therapy', 'exhaustion', 'exome sequencing', 'human disease', 'improved', 'in vivo', 'in vivo Model', 'insight', 'member', 'mouse model', 'neoplastic cell', 'nonalcoholic steatohepatitis', 'novel', 'novel therapeutics', 'pre-clinical', 'professor', 'programs', 'public health relevance', 'response', 'statistics', 'success', 'targeted treatment', 'therapeutic evaluation', 'therapy resistant', 'tool', 'tumor', 'tumor progression', 'vector']",NCI,RESEARCH INST OF FOX CHASE CAN CTR,R00,2017,249000,-0.01471170853979563
"Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity   Project Summary    For something as complex and multifaceted as bacterial antibiotic resistance (AR), our drug evaluation  paradigm is strikingly narrow and homogenous: MIC/MBC testing in standardized bacteriologic media. We  have shown that this drug evaluation paradigm is inadequate, even misleading, as changes in the media  conditions of the procedure lead to dramatically different results. A more holistic definition of antibiotic therapy  that centers on understanding antibiotic activity in synergy with host innate immune factors such as cationic  antimicrobial peptides (AMPs), serum and phagocytic cells (e.g. neutrophils) reveals therapeutic options  unrecognized in standard testing. The proposed U01 program represents a groundbreaking approach to use  systems biology approaches and inform more effective antibiotic utilization in the context of host innate  immunity. We propose to: 1) build an iterative systems biology workflow that integrates multiple experimental  and computational approaches to give a comprehensive assessment of AR; and 2) apply this workflow to high  priority pathogens to systematically elucidate AR mechanisms and their condition­dependency. The iterative  workflow includes: (i) omics and physiological data generation.  Clinically isolated strains of the selected  pathogens will be grown under conventional testing (bacteriologic media) and more physiologic conditions  (tissue culture media, serum, and in presence of AMPs and neutrophils) to probe for advantageous gain of  activity.  The omics data types collected are: DNA resequencing, RNAseq, and metabolomics.  (ii)  Bioinformatics and data modeling analysis involves three approaches: big data analysis for data set  dimensionality and coarse grained variable dependencies assessment, genome­scale modeling for  mechanistic elucidation and analysis, and machine learning that uses AR­related metadata to classify the  overall biological functions. This analysis will lead to understanding of AR mechanisms.  (iii) Multi­scale  validation from animal models, to laboratory evolution, to cytology, to gene expression alteration, to structural  protein analysis of putative targets. The validation thus ranges from host behavior to atomistic detail of  ligand­target interactions. The iterative loop then closes, comparing computational prediction to experimental  outcomes. False­negative and false­positive predictions are then algorithmically analyzed by a hypothesis  generating family of algorithms that then makes suggestions about what conditions to use in the next iteration  of the loop.  The pathogens that we will focus on are methicillin­resistant ​Staphylococcus aureus ​(MRSA), the  carbapenem­resistant Enterobacteriaceae (CRE) Klebsiella ​pneumoniae ​and ​Acinetobacter baumannii,​ and  Pseudomonas aeruginosa​. The team of investigators has made the foundational observations and led the  development of the technologies on which the iterative workflow is based. A multi­ and genome­scale methods  of systems biology fulfills requirements of RFA­AI­14­064 to which it responds.              Narrative    The current evaluation of antibiotic drug candidates in drug discovery and in clinical medicine is conducted in  laboratory media that ignores the actual physiologic conditions in the host and the host immune system.  We  have discovered potent antimicrobial activities of existing antibiotics against highly “drug­resistant superbugs”  that are currently ignored but revealed in synergy with the human immune system. This program proposes a  holistic and comprehensive systems biology approach to systematically discover novel treatment opportunities  and underlying mechanisms using a novel iterative data generation, analysis, and modeling workflow.       ",Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity,9242604,U01AI124316,"['Acinetobacter baumannii', 'Algorithmic Analysis', 'Algorithms', 'Animals', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Antimicrobial Cationic Peptides', 'Bacterial Antibiotic Resistance', 'Bacteriology', 'Behavior', 'Big Data', 'Bioinformatics', 'Biological Assay', 'Biological Process', 'Biological Products', 'Biology', 'Cereals', 'Clinical', 'Clinical Medicine', 'Collection', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Culture Media', 'Cytology', 'DNA Resequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dependency', 'Dimensions', 'Drug Evaluation', 'Drug resistance', 'Effectiveness', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Future', 'Gene Expression Alteration', 'Generations', 'Genome', 'Growth', 'Human', 'Immune', 'Immune system', 'Immunity', 'Immunologic Factors', 'Infection', 'Integration Host Factors', 'Klebsiella pneumonia bacterium', 'Knowledge', 'Laboratories', 'Lead', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular Biology', 'Molecular Profiling', 'Mus', 'Natural Immunity', 'Network-based', 'Organism', 'Outcome', 'Output', 'Participant', 'Phagocytes', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Physiological', 'Predisposition', 'Procedures', 'Process', 'Protein Analysis', 'Pseudomonas aeruginosa', 'Research Personnel', 'Resistance', 'Resistance development', 'Risk Assessment', 'Series', 'Serum', 'Standardization', 'Statistical Data Interpretation', 'Structural Protein', 'Suggestion', 'Superbug', 'Systems Biology', 'Testing', 'Therapeutic', 'Update', 'Validation', 'antimicrobial', 'antimicrobial peptide', 'bacterial genetics', 'base', 'carbapenem-resistant Enterobacteriaceae', 'data modeling', 'design', 'drug candidate', 'drug discovery', 'experience', 'genome-wide', 'human tissue', 'improved', 'in vivo', 'macromolecule', 'metabolomics', 'methicillin resistant Staphylococcus aureus', 'microbial', 'microbial host', 'multi-drug resistant pathogen', 'multidrug-resistant Pseudomonas aeruginosa', 'neutrophil', 'novel', 'pathogen', 'product development', 'programs', 'reconstruction', 'resistance mechanism', 'response', 'screening', 'synergism', 'technology development', 'tissue culture', 'tool', 'transcriptome sequencing', 'transcriptomics', 'treatment response']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2017,2322437,-0.00880788892583872
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9357752,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2017,62304,-0.010306998741517122
"Sci-Score, a tool to support rigor and transparency guidelines Project Summary While  standards  in  reporting  of  scientific  methods  are  absolutely  critical  to  producing  reproducible  science,  meeting  such  standards  is  difficult.  Checklists  and  instructions  are  tough  to  follow  often  resulting  in  low  and inconsistent  compliance.  Scientific  journals  and  societies  as  well  as  the  National  Institutes  of  Health  are  now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods  (e.g.,  http://www.cell.com/star-­methods),  but  the  trickier  part  will  be  to  train  the  biomedical  community  to  use these standards to effectively improve how scientific methods are communicated. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency of  Key Biological Resources, we propose to build Sci-­Score a text mining based tool suite to help authors meet the  standard.  Sci-­Score  will  provide  an  automated  check  on  compliance  with  the  RRID  standard  already  implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. The innovation behind Sci-­ score is the provision of a score, which can be obtained by individual investigators, which reflects a numerical  validation of the quality of their methods reporting. We posit that the score will serve as a tool that investigators  and journals can use to compete with themselves and each other, or in the very least allow them to see how  close they are to the average in meeting quality requirements.   Recently, our group has developed a text mining algorithm that has now been successfully been used to detect software tools and databases from the SciCrunch Registry in published papers. Digital tools are one of four resource types that the RRID standard identifies. We propose to extend this approach to the other types of entities: antibodies, cell lines and model organisms. Resource identification along with other quality metrics twill be used to train an algorithm to score the overall quality of the methods document. If successful, the tool could be used by editors, reviewers, and investigators to improve the number of RRIDs, therefore the quality of descriptors of key biological resources in published papers. This SBIR project will build a set of algorithms similar to the resource finding pipeline and develop it into an industrial robust and reconfigurable software system. Our Phase I specific aims include to 1) creating gold sets of data for each resource type and training a set of algorithms for each resource type; 2) designing and evaluating the scoring system; 3) designing and evaluating a report generating system based on the previous aims. In Phase II, we will develop a scalable backend infrastructure to serve the needs of scientific publishers and research community. Standards for scientific methods reporting are absolutely critical to producing reproducible science, but meeting  such standards is difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent  compliance. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency, we propose to build Sci-Score text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. Sci-Score will provide a score rating the quality of  methods reporting in submitted articles, which provides feedback to authors, reviewers and editors on how to improvecompliance with RRIDs and other standards. ","Sci-Score, a tool to support rigor and transparency guidelines",9345707,R43OD024432,"['Address', 'Agreement', 'Algorithms', 'Animal Model', 'Antibodies', 'Area', 'Big Data', 'Biological', 'California', 'Cell Line', 'Cell model', 'Cells', 'Communities', 'Data Set', 'Databases', 'Descriptor', 'Elements', 'Ensure', 'Evaluation', 'Feedback', 'Funding', 'Glare', 'Gold', 'Guidelines', 'Habits', 'Human', 'Individual', 'Industrialization', 'Instruction', 'Journals', 'Learning', 'Literature', 'Machine Learning', 'Manuscripts', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neurosciences', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reading', 'Reagent', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sales', 'Science', 'Scoring Method', 'Services', 'Small Business Innovation Research Grant', 'Societies', 'Software Tools', 'System', 'Technology', 'Text', 'To specify', 'Training', 'United States National Institutes of Health', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'biological systems', 'computerized tools', 'design', 'digital', 'improved', 'innovation', 'interest', 'meetings', 'prototype', 'software systems', 'sound', 'text searching', 'tool', 'vigilance', 'web site']",OD,"SCICRUNCH, INC.",R43,2017,221865,-0.01787741567701609
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9250803,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'experimental study', 'genomic data', 'hazard', 'high dimensionality', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'predictive modeling', 'prevent', 'public health relevance', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2017,66026,-0.023047742273927963
"Predicting Resilience in the Human Microbiome DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience. PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.",Predicting Resilience in the Human Microbiome,9325416,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Modernization', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxonomy', 'Techniques', 'Testing', 'Time', 'Validation', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiome', 'gut microbiota', 'human subject', 'innovation', 'learning strategy', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'public health relevance', 'resilience', 'response', 'theories', 'tool', 'transcriptome', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2017,413065,0.00828655626121224
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9268713,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Data Sources', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2017,655080,-0.0010096845818889015
"Integrating Bioinformatics and Clustering Analysis for Disease Surveillance ﻿    DESCRIPTION (provided by applicant):  There has been a tremendous focus in bioinformatics on translation of data from the bench into information and knowledge for clinical decision-making. This includes analysis of human genetics for personalized medicine and treatment. However, there has been much less attention on translational bioinformatics for public health practice such as surveillance of emerging/re-emerging viruses. This involves data acquisition, integration, and analyses of viral genetics to infer origin, spread, and evolution suc as the emergence of new strains. The relevant scientific fields for this practice include certain aspects of molecular epidemiology and phylogeography. Recent attention has focused on viruses of zoonotic origin, which are defined as pathogens that are transmittable between animals and humans. In addition to seasonal influenza and West Nile virus, this classification of pathogens includes novel viruses such as Middle Eastern Respiratory Syndrome and influenza A H7N9. Despite the successes highlighted in the literature, there has been little utilization of bioinformatics resources and tools among state public health, agriculture, and wildlife agencies for zoonotic surveillance. Previously this type of resource has been restricted primarily to those in academia.       While bioinformatics has been sparsely used for surveillance of zoonotic viruses, other applications such as Geospatial Information Systems (GIS) have been employed by state health agencies to analyze spatial patterns of infection. This includes software to produce disease maps using an array of data types such as clinical, geographical, or human mobility data for tasks such as, geocoding, clustering, or outbreak detection. In addition, advances in geospatial statistics have enabled health agencies to perform more powerful space-time analyses to infer spatiotemporal patterns. However, these GIS consider only traditional epidemiological data such as location and timing of reported cases and not the genetics of the virus that causes the disease. This prevents health agencies from understanding how changes in the genome of the virus and the associated environment in which it disseminates impacts disease risk.      The long-term goal of this proposal is to enhance the identification of geospatial hotspots of zoonotic viruses by applying bioinformatics principles to access, integrate, and analyze viral genetics and spatiotemporal reportable disease data. This project will include approaches from bioinformatics, genetics, spatial statistics, GIS, and epidemiology. To do this, I will first measue the utilization of bioinformatics resources and tools as well as the current approaches and limitations identified by state agencies of public health, agriculture, and wildlife for detecting nd predicting hotspots (clusters) of zoonotic viruses (Aim 1). I will then use this feedback to develo a spatial decision support system for detecting and predicting zoonotic hotspots that applies bioinformatics principles to access, integrate, and analyze viral genetics, environmental, and spatiotemporal reportable disease data (Aim 2). In Aim 3, I will then evaluate my system for cluster detection and prediction against a system that does not consider viral genetics and relies on traditional spatiotemporal data, and perform validation of the predictive capability. Additional evaluation of the user's satisfaction and system usability will be evaluated.               Project Narrative I will develop and evaluate a spatial decision support system to support surveillance of zoonotic viruses in both human and animal populations. I will use approaches from bioinformatics and public health to integrate genetic sequence data from the virus with data from cases of reported infectious diseases and associated environmental data. A surveillance system that considers the genetics and environment of the virus along with public health data will assist public health officials in making informed decisions regarding risk of infectious diseases.",Integrating Bioinformatics and Clustering Analysis for Disease Surveillance,9189635,F31LM012176,"['Academia', 'Address', 'Agriculture', 'Algorithm Design', 'Algorithms', 'Animals', 'Area', 'Attention', 'Biodiversity', 'Bioinformatics', 'Case Study', 'Clinical', 'Cluster Analysis', 'Communicable Diseases', 'Computer software', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Disease', 'Disease Notification', 'Disease Outbreaks', 'Ecology', 'Environment', 'Epidemiology', 'Evaluation', 'Evolution', 'Feedback', 'Future', 'Genbank', 'Genetic', 'Geographic Information Systems', 'Geography', 'Goals', 'Health', 'Human', 'Human Genetics', 'Infection', 'Influenza', 'Influenza A Virus, H7N9 Subtype', 'Influenza A virus', 'Knowledge', 'Literature', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Metadata', 'Modeling', 'Molecular Epidemiology', 'Molecular Evolution', 'Pattern', 'Population', 'Public Health', 'Public Health Practice', 'Questionnaire Designs', 'Questionnaires', 'Research', 'Resources', 'Retrospective Studies', 'Risk', 'Scanning', 'Sequence Alignment', 'Syndrome', 'System', 'Time', 'Translations', 'Validation', 'Validity and Reliability', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Virus Integration', 'West Nile virus', 'Work', 'Zoonoses', 'clinical decision-making', 'data acquisition', 'disorder risk', 'epidemiologic data', 'health data', 'high risk', 'novel virus', 'pathogen', 'personalized medicine', 'prevent', 'respiratory', 'satisfaction', 'seasonal influenza', 'spatiotemporal', 'statistics', 'success', 'tool', 'usability', 'virus classification', 'virus genetics']",NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,F31,2017,38800,-0.002438417130668557
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9357870,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Protein Hybridization', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2017,720717,-0.007978375344881931
"Improving safety and efficacy of platelet transfusion through systems biology Project Summary Platelet transfusion is critical for severely bleeding patients and nearly 6 million units are transfused in the United States and Europe annually. In the United States, platelets are typically stored for 5 days resulting in a waste of 20% of their supply. Short storage duration is a consequence of bacterial contamination and platelet quality considerations. Though many methods have been developed for bacterial testing and pathogen inactivation, fewer have been developed for improving quality of stored platelets. Platelet additive solutions have the possibility to increase storage quality and duration, reduce plasma-related allergic reactions, impact the efficacy of pathogen reduction techniques, and save plasma which can then be used as an additional transfusion product. While the benefits are well known, there has been little progress in developing new platelet additive solutions for increasing quality and safety of platelet transfusion because there is a lack of broad understanding of biochemical and signaling changes during storage. There has been interest to utilize high-throughput metabolite profiling for global understanding of platelet metabolic decline but data analysis of complex datasets has been a daunting challenge. In Phase I of this program, we developed the first, robust computational platform involving statistical analysis and systems biology of metabolic and signaling networks to interpret and analyze PLT metabolomic and proteomic profiles in a complete network context. Using time- course global, quantitative metabolite profiling, we determined that PLTs undergo a non-linear decay process and computationally identified key metabolic enzymes and cellular process that drive this decay. Based on the computational results, we have devised two novel additive solution strategies to mitigate the decay process and improve the length of PLT units. In this Phase II proposal, we will validate the computationally determined additive solutions for efficacy in alleviating the non-linear decay process through 1) metabolomics experiments, and 2) non-metabolic PLT physiology experiments including cell activation and hemostatic effectiveness. A successful additive solution will be progressed to media refinement and preclinical testing. Project Narrative Platelet transfusion units are typically stored for five days in the United States leading to a waste of 20% of units and potential quality concerns. The field is open for innovation as most storage media technologies are derived from work from the early 1990s. This proposal will develop novel computational methods to comprehensively understand the degradation of platelets under storage conditions and experimentally validate new additive solutions for increasing platelet quality and extending shelf life, an area that accounts for $2.5 billion of hospital costs.",Improving safety and efficacy of platelet transfusion through systems biology,9347295,R44HL127843,"['Accounting', 'Agreement', 'Algorithms', 'Allergic Reaction', 'Area', 'Biochemical', 'Biological', 'Biological Preservation', 'Blood', 'Blood Component Removal', 'Blood Platelets', 'Caring', 'Cell physiology', 'Cells', 'Complex', 'Computational Technique', 'Computational algorithm', 'Computer Simulation', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Effectiveness', 'Enzymes', 'Equipment and supply inventories', 'Europe', 'Formulation', 'Glutathione', 'Hemorrhage', 'Hemostatic Agents', 'Hospital Costs', 'In Vitro', 'Intervention', 'Length', 'Lesion', 'Life', 'Literature', 'Machine Learning', 'Mathematics', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Methods', 'Modeling', 'Pathway interactions', 'Patients', 'Phase', 'Physiology', 'Plasma', 'Platelet Transfusion', 'Preclinical Testing', 'Process', 'Production', 'Proteomics', 'Reaction', 'Recovery', 'Resources', 'Risk', 'Safety', 'Signal Pathway', 'Signal Transduction', 'State Hospitals', 'Statistical Data Interpretation', 'Supplementation', 'Surveys', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Transfusion', 'United States', 'Validation', 'Work', 'base', 'cost', 'design', 'experimental study', 'human subject', 'improved', 'insight', 'interest', 'metabolic profile', 'metabolomics', 'model design', 'novel', 'open innovation', 'oxidative damage', 'pathogen', 'programs', 'statistics', 'success', 'time use', 'wasting']",NHLBI,"SINOPIA BIOSCIENCES, INC.",R44,2017,1099022,-0.001184442925534342
"Dopaminergic encoding of counterfactual information in human striatum ﻿    DESCRIPTION (provided by applicant): Diseases and disorders directly affected by dopamine systems (e.g., drug addiction and Parkinson's disease) highlight the importance of these systems in motivated human behavior and cognition. The dopamine system is known to be a critical component of normal learning, reward processing, and decision-making (reviewed in Montague et al., 2004). Unfortunately, our present knowledge of dopamine systems in human brains is relatively sparse compared to the wealth of experimentation and computational modeling on these systems in rodents and non-human primates. Previously, technological constraints have limited direct experimentation in human brains. This proposal capitalizes on our group's recent technological innovation, which was supported by the NIH R21 mechanism - CEBRA: R21DA024140 - and resulted in the successful completion of the first sub-second measurements of dopamine release in a human brain. Furthermore, these measurements took place during an active decision-making task that was framed by computational models of learning and reward processing (Kishida et al., 2011 and Kishida et al., under review). We propose to pursue three specific aims, which combine our technological advance with active learning tasks designed to probe the role of dopamine in human behavior. Our aims incorporate three learning signals, where actual and counterfactual experience will each be examined in human striatal responses. The proposed work will inform on the controversial role for dopamine in reward/movement interactions. The experiments proposed will yield unprecedented insight into the function of the dopamine system in the humans afflicted with Parkinson's disease and Essential Tremor. With the support of the NIH (R21DA024140), our team successfully developed a complete prototype system for making electrochemical measurements of dopamine delivery in the human brain. Feasibility has been demonstrated by obtaining the first dopamine measurements in the striata of subjects with Parkinson's during a decision-making task. This substantial preliminary work is now ready for a larger scale with specific hypothesis testing about the role of dopamine systems in Parkinson's disease, Essential tremor, and human decision-making and behavior. PUBLIC HEALTH RELEVANCE: Experiments and computational models (primarily investigated in rodents and non-human primates) suggest dopamine systems in the brain are essential for normal learning, reward-processing, and ongoing decision- making. This proposal builds upon our group's innovative work, which - for the first time - recorded sub- second measurements of dopamine in human subjects and amalgamated these measurements with computational models of dopamine function. Herein we propose to use this newly developed technology to generate unprecedented insight into the function of dopamine systems directly in humans.",Dopaminergic encoding of counterfactual information in human striatum,9308020,R01NS092701,"['Active Learning', 'Acute', 'Affect', 'Animal Model', 'Behavior', 'Brain', 'Brain Diseases', 'Cells', 'Cessation of life', 'Cognition', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Coupling', 'Data', 'Decision Making', 'Disease', 'Dopamine', 'Drug Addiction', 'Electrodes', 'Essential Tremor', 'Exploratory/Developmental Grant', 'Funding', 'Human', 'Knowledge', 'Learning', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement', 'Movement Disorders', 'Operative Surgical Procedures', 'Parkinson Disease', 'Participant', 'Patients', 'Periodicity', 'Phase', 'Play', 'Replacement Therapy', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Sample Size', 'Scanning', 'Signal Transduction', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'blood oxygenation level dependent response', 'cognitive function', 'design', 'dopamine system', 'dopaminergic neuron', 'experience', 'experimental study', 'extracellular', 'human subject', 'implantation', 'innovation', 'insight', 'nonhuman primate', 'prototype', 'public health relevance', 'putamen', 'reduce symptoms', 'relating to nervous system', 'response', 'reward processing', 'social', 'symptomatic improvement', 'technological innovation', 'temporal measurement']",NINDS,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2017,492125,-0.0040301608457890626
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation. n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,9310382,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Crystallization', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Genetic', 'Genetic Structures', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Imaging Device', 'Internships', 'Investigation', 'K-12 student', 'Knowledge', 'Link', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Mosaicism', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Statistical Data Interpretation', 'Statistical Models', 'Structure', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'experimental study', 'feeding', 'fluorescence imaging', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'neuroinformatics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'reconstruction', 'relating to nervous system', 'role model', 'simulation', 'spatiotemporal', 'synergism', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2017,323033,-0.016011617230946868
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9349367,DP5OD019820,"['Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Modernization', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'convict', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high dimensionality', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2017,326784,-0.014877428980211953
"Dietary modulation of gut microbiome and host gene expression across human evolution and the emergence of modern human disease Evidence suggests that lifestyle changes, concordant with the adoption of agriculture and industrialization, have impacted the emergence of the so-called diseases of modern civilization in humans (e.g. metabolic disorders, cardiovascular disease etc.). The incidence of these diseases in contemporary, industrialized populations is believed to be associated with a lack of adaptation of our genomes to the rapid dietary and lifestyle changes that occurred across human evolution. However the usefulness and resolution of this evolutionary model of disease are limited. Moreover, although the dietary and genetic markers of human evolution have been studied, we still lack understanding on how the microbiome, our second genome, has interacted with nutritional and host- genomic axes to confer increased disease risk in modern humans. Preliminary data by our group show that dietary shifts significantly modulate the gut microbiome and metabolome of wild primates, our closest evolutionary relatives. Additionally, we have identified gut microbiome markers only found in populations representing Paleolithic lifestyles (hunter-gatherers) and distinguishing them from traditional agriculturalists and industrialized populations. Thus, given 1) the potential role of diet in human evolution, 2) the critical impact of the gut microbiome on the nutritional and immune landscape of mammals, and 3) the existence of gut microbiome patterns exclusive of hunter-gatherers, we hypothesize that the emergence of metabolic disease in modern humans was significantly mediated by interactions between diet, the gut microbiome and the human genome across evolution. These issues are still unexplored. Thus, in Aim 1 of this proposal we will use a multi- OMIC approach (gut metabolomics, metagenomics and transcriptomics of the host colonic tissue) to identify metabolic and genetic markers that emerged and/or were lost when humans transitioned from hunter-gatherer to agricultural and industrialized lifestyles, and in humans affected by metabolic disease phenotypes. In Aim 2, we will use integrated meta-OMICs and network theory approaches to predict metabolic disease phenotypes, from hunter-gatherers to, populations in transition to agriculture to modern populations at risk. This system-level study will broaden our understanding of the extrinsic (environmental/nutritional) and intrinsic factors (genetic/metabolic) impacting the evolution of modern human disease. Additionally, the evolutionary approach proposed will shed light on potentially novel diet and microbe-based translational strategies to mitigate the incidence of metabolic disease in contemporary human populations. PROJECT NARRATIVE The high incidence of metabolic disorders (disorders of glucose, lipid and energy metabolism) is a significant public health threat in industrialized populations, affecting up to 25% of the adult population. Despite extensive work on characterizing the nutritional and genetic backgrounds of common metabolic disorders, we still have limited understanding as to how these factors interact with each other and with the gut microbiome, our second genome. This proposal interrogates the evolutionary baseline of modern human disease by exploring associations between nutritional, (host)genetic and microbiome markers in hunter-gatherers, traditional agriculturalists and industrialized human populations susceptible to metabolic disorders. The implementation of an evolutionary, system-level model improves our understanding of modern human disease, and, validates existing and novel dietary interventions to lessen their incidence in industrialized societies.",Dietary modulation of gut microbiome and host gene expression across human evolution and the emergence of modern human disease,9248094,R01DK112381,"['Adoption', 'Adult', 'Affect', 'African', 'Agriculture', 'Americas', 'C-reactive protein', 'Cardiovascular Diseases', 'Civilization', 'Data', 'Diet', 'Dietary Intervention', 'Disease', 'Disease model', 'Energy Metabolism', 'Evolution', 'Feces', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genome', 'Genomics', 'Glucose Metabolism Disorders', 'Glycosylated hemoglobin A', 'Goals', 'HDL-triglyceride', 'High Density Lipoproteins', 'Human', 'Human Genome', 'Immune', 'Incidence', 'Industrialization', 'Inflammation', 'Inflammatory', 'Intrinsic factor', 'Jamaica', 'Life Style', 'Light', 'Low-Density Lipoproteins', 'Machine Learning', 'Mammals', 'Measures', 'Mediating', 'Metabolic', 'Metabolic Diseases', 'Metabolic Marker', 'Metabolic syndrome', 'Metagenomics', 'Microbe', 'Modeling', 'Modernization', 'Molecular Profiling', 'Nutrient', 'Nutritional', 'Pattern', 'Population', 'Populations at Risk', 'Primates', 'Public Health', 'Resolution', 'Risk Factors', 'Role', 'Sampling', 'Societies', 'Structure', 'System', 'TNF gene', 'TNFRSF1A gene', 'Therapeutic Intervention', 'Tissue-Specific Gene Expression', 'Tissues', 'Trinidad', 'Variant', 'Work', 'base', 'cohort', 'density', 'disease phenotype', 'disorder risk', 'glycemic control', 'gut microbiome', 'human disease', 'improved', 'learning strategy', 'lipid metabolism', 'metabolome', 'metabolomics', 'metagenome', 'microbial', 'microbial host', 'microbiome', 'novel', 'nutrition related genetics', 'theories', 'transcriptome sequencing', 'transcriptomics', 'translational approach']",NIDDK,"J. CRAIG VENTER INSTITUTE, INC.",R01,2017,471957,-0.02157019210066562
"Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens ﻿    DESCRIPTION (provided by applicant): Infections caused by antibiotic-resistant bacterial pathogens are exceedingly common in immunocompromised hosts. Patients undergoing allogeneic hematopoietic stem cell transplantation (allo-HSCT) are particularly susceptible to these infections and are the patient population our studies will focus upon. Our goal is to extend and further develop systems biology approaches that our group has pioneered to identify mechanisms by which the intestinal microbiota confers resistance to infection by Vancomycin-resistant Enterococcus (VRE), antibiotic-resistant Klebsiella pneumoniae (arKp) and Clostridium difficile (C. diff). Aim 1 of our project is to establish a clinical database from the hospital recrds of allo-HSCT patients during their initial hospitalization that will include all laboratory values,vital signs, pharmacy data, dietary data, symptoms and physical exam findings. Aim 2 will expand our fecal bank by collecting fecal samples from approximately 160 allo-HSCT patients per year and determining the presence/absence of VRE, arKp and C. diff by culture and PCR. We will use NGS of 16S rRNA genes to determine microbiota composition on each sample, will perform metagenomic and RNA sequencing to determine the bacterial transcriptome and perform metabolomic analyses on a selected subset of fecal samples. Aim 3 is to extend our mathematical modeling to identify specific members of the microbiota, metabolic pathways and metabolic products that correlate with resistance to VRE or arKp expansion in the GI tract or are associated with resistance to C. diff infection. The clinical database will be used to establish correlations between clinical treatments or events and changes in the intestinal microbiota or the expression of bacterial metabolic pathways. Ultimately, the computational platforms developed in aim 3 will identify bacterial species or consortia that are associated with resistance to infection and Aim 4 will test these associations in germ-free mouse models. We will culture bacterial species associated with resistance, colonize mice with these protective bacteria and test for resistance against VRE, arKp and C. diff. Samples obtained from these experimental studies will be subjected to metagenomic and metabolomic analyses to further refine, in an iterative fashion, computational models developed in aim 3. Our proposed studies will develop new and extend existing computational models to identify bacterial species and molecular mechanisms that confer resistance to antibiotic-resistant bacterial infections. PUBLIC HEALTH RELEVANCE: The normal bacteria inhabiting the human intestine provide a high level of resistance against antibiotic-resistant bacterial pathogens. We are investigating the intestinal flora of hospitalized patients and using mathematical modeling to identify bacterial species and their metabolic products that reduce the risk of infection by three prevalent antibiotic-resistant bacteria. These studies may lead to the development of new approaches to treat and prevent antibiotic-resistant infections.",Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens,9234463,U01AI124275,"['Allogenic', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bacterial Infections', 'Categories', 'Clinical', 'Clinical Treatment', 'Clostridium difficile', 'Collection', 'Computer Simulation', 'Data', 'Databases', 'Deposition', 'Development', 'Diagnostic radiologic examination', 'Diet', 'Dietary intake', 'Enrollment', 'Event', 'Feces', 'Gastrointestinal tract structure', 'Germ-Free', 'Gnotobiotic', 'Goals', 'Growth', 'Hematopoietic Stem Cell Transplantation', 'Hospitalization', 'Hospitals', 'Human', 'Immune', 'Immunocompromised Host', 'Infection', 'Integration Host Factors', 'Intestinal Content', 'Intestines', 'Klebsiella pneumonia bacterium', 'Laboratories', 'Lead', 'Machine Learning', 'Measurable', 'Mediating', 'Medical Records', 'Memorial Sloan-Kettering Cancer Center', 'Metabolic', 'Metabolic Pathway', 'Metagenomics', 'Modeling', 'Modification', 'Molecular', 'Mus', 'Patient risk', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Population', 'Predisposition', 'Resistance', 'Resistance to infection', 'Risk', 'Sampling', 'Symbiosis', 'Symptoms', 'Systems Biology', 'Testing', 'Toxic effect', 'Transplantation', 'Uncertainty', 'Vancomycin Resistance', 'Vancomycin resistant enterococcus', 'antimicrobial drug', 'bacterial resistance', 'clinically relevant', 'commensal microbes', 'computer studies', 'design', 'experimental study', 'falls', 'gut microbiota', 'immune activation', 'mathematical model', 'member', 'metabolome', 'metabolomics', 'metagenome', 'metagenomic sequencing', 'microbiome', 'microbiota', 'microorganism interaction', 'mouse model', 'network models', 'novel', 'novel strategies', 'parallel computer', 'pathogen', 'patient population', 'prevent', 'public health relevance', 'rRNA Genes', 'resilience', 'resistance mechanism', 'text searching', 'transcriptome', 'transcriptome sequencing', 'transcriptomics']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,U01,2017,1697054,-0.008919107381441535
"Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression Project Summary/Abstract This project will develop a new technological approach for the comprehensive analysis of adaptive immune responses, which holds the potential to catalyze new strategies to prevent and treat disease. Here we will apply immune profiling techniques recently invented by the PI to investigate the mechanisms of Epstein-Barr virus (EBV) adaptive immune control in clinical cohorts of infected patients. EBV is a highly prevalent pathogen infecting >90% of the world’s population. Primary EBV infection often causes infectious mononucleosis (IM) and long-term sequelae include numerous malignancies, lymphoproliferative disorders, and a strong association with multiple sclerosis. No EBV vaccine is approved to date, and the molecular mechanisms of immune protection from EBV-associated diseases are unclear. Unfortunately, prior technical barriers in high- throughput immune profiling methods have prevented a comprehensive understanding of adaptive immune protection against EBV diseases. A technological approach that identifies the critical features of EBV immune protection will advance new solutions for vaccine and therapeutic development. Therefore, we developed an experimental pipeline to enable rapid and cost-effective analysis of B- and T-cell responses to EBV that is scalable to dozens of human patients per experiment. We hypothesize that a comprehensive B- and T-cell analysis of carefully selected patient cohorts that either can or cannot suppress symptomatic infection will reveal function-based correlates of EBV control. To test this hypothesis, we will apply quantitative immune profiling technologies to analyze cryopreserved longitudinal samples from recently completed prospective clinical studies of IM. Patient samples in our cohort span pre- and post-infection through convalescence and encompass the full range of clinical IM severity scores (from 0, asymptomatic primary infection, to 6, essentially bedridden with IM). Immune profile data will be used to establish adaptive immune correlates of IM disease severity. In addition, we will analyze immune responses in apparently immunocompetent patients with chronic active EBV (CAEBV) disease, or patients who do not adequately suppress EBV infection, to gain insight regarding adaptive immune function and dysfunction in CAEBV. Finally, we will develop a new computational toolkit to rapidly identify immune correlates from high-throughput datasets. Successful completion of this project will constitute the first comprehensive functional B- and T-cell receptor analysis in a human clinical cohort. Our efforts will provide a repertoire-scale, mechanistic understanding of adaptive immunity to EBV and suggest new strategies for treatment and prevention of EBV-associated diseases. Our long-term goal is to develop human immune profiling techniques as a platform approach to accelerate the rational design of vaccines and therapeutics against pathogens of high public health importance, beginning with EBV. Project Narrative This project will apply new high-throughput immune profiling technologies to elucidate the features of effective Epstein-Barr virus (EBV) immune control. EBV causes a range of human diseases including infectious mononucleosis and several forms of cancer; however, limited EBV treatment options are available and no approved preventive EBV vaccines exist. Our long-term objective is to apply enhanced understanding of adaptive immunity to accelerate the rational development of new vaccines and therapeutics.",Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression,9354523,DP5OD023118,"['Address', 'Antibodies', 'Antibody titer measurement', 'B cell repertoire', 'B-Lymphocytes', 'Burkitt Lymphoma', 'CD8-Positive T-Lymphocytes', 'Cells', 'Chronic', 'Clinical', 'Clinical Research', 'Convalescence', 'Cost Effectiveness Analysis', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'EBV-associated disease', 'Ensure', 'Epstein-Barr Virus Infections', 'Exhibits', 'Fatigue', 'Future', 'Goals', 'Herpesviridae', 'Herpesviridae Infections', 'Hodgkin Disease', 'Human', 'Human Herpesvirus 4', 'Immune', 'Immune System Diseases', 'Immune System and Related Disorders', 'Immune response', 'Immunocompetent', 'Immunologic Receptors', 'Incidence', 'Individual', 'Infection', 'Infectious Mononucleosis', 'Intervention', 'Knowledge', 'Large-Cell Immunoblastic Lymphoma', 'Lead', 'Lymphoproliferative Disorders', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Multiple Sclerosis', 'Nasopharynx Carcinoma', 'Oncogenic', 'Patients', 'Phase', 'Population', 'Prevention', 'Preventive', 'Primary Infection', 'Public Health', 'Receptor Cell', 'Receptors, Antigen, B-Cell', 'Recovery', 'Research Personnel', 'Risk', 'Sampling', 'Serum', 'Severities', 'Severity of illness', 'Stomach Carcinoma', 'Symptoms', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'Techniques', 'Technology', 'Teenagers', 'Testing', 'Therapeutic', 'Therapeutic antibodies', 'Vaccine Design', 'Vaccines', 'Viral', 'Virus Diseases', 'Work', 'adaptive immune response', 'adaptive immunity', 'base', 'career', 'cohort', 'cost', 'design', 'disorder control', 'experimental study', 'high dimensionality', 'human disease', 'immune function', 'insight', 'multiple sclerosis patient', 'neutralizing monoclonal antibodies', 'next generation', 'novel', 'novel therapeutics', 'novel vaccines', 'pathogen', 'pressure', 'prevent', 'prospective', 'response', 'therapeutic development', 'treatment strategy', 'vaccine development', 'vaccine trial', 'virology', 'young adult']",OD,UNIVERSITY OF KANSAS LAWRENCE,DP5,2017,369644,-0.008479642874375939
"Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology PROJECT SUMMARY/ABSTRACT Candidate After the completion of my Ph.D. in Bioinformatics, I joined the Center for Statistical Genetics at the University of Michigan (U-M) as research fellow and acquired extensive training in analysis on high- dimensional biological data. I uncovered a strong interest in studying the genetics and genomics of psoriasis when working with Dr. James Elder at the U-M, and I developed a fascination in understanding the functional roles of long non-coding RNAs (lncRNAs) in cutaneous diseases. I joined the Department of Dermatology at the U-M as a faculty in summer of 2015, with secondary appointments in the Department of Computational Medicine & Bioinformatics and the Department of Biostatistics. In addition, I direct the new Center for Cutaneous Bioinformatics within the Department of Dermatology, and serve to supervise and implement an analysis pipeline for studies investigating the immunological mechanisms for different skin diseases. Career Development Plan I aim to become a future leader in combining in silico discovery and bench experiments to advance biomedical research in autoimmune skin disorders. My objective in seeking a Mentored Research Scientist Development Award is to acquire the additional knowledge, training, and experience necessary for me to become an independent scholar in developing novel systems biology approaches to decipher the pathology and mechanisms of cutaneous diseases. The five year training proposed will provide knowledge and experience in aspects that are critical to my success, and they are: i) To develop knowledge and experimental skills in cutaneous biology --- achieved by guidance from Dr. Elder (investigative dermatology), intense research meetings/conferences, and practical laboratory experience in cutaneous research; ii) To develop knowledge and skills to study immunological systems of autoimmune skin diseases --- achieved by supervision from Dr. Johann Gudjonsson (skin immunology), attending formal Immunology courses and seminars, and earning laboratory experience from immunology experiments; iii) To advance skills in developing statistical and computational approaches --- accomplished by mentoring from Dr. Goncalo Abecasis (computational biologist), research meetings, and conducting research projects requiring advanced skills and knowledge in quantitative science; iv) To cultivate my professional development through enhancing scientific connections, grantsmanship skills, and educator portfolio --- achieved by establishing connections with colleagues during meetings, visiting King’s College London as scholar, attending a grantsmanship workshop and bootcamp, and learning mentoring skills through teaching formal classes and mentoring research students. Through the intensive and comprehensive training, I will be well grounded in conducting basic science experiments and also be able to capitalize my advanced knowledge in quantitative science to model mechanisms in cutaneous diseases. Research Project The research project will use psoriasis as a disease model to study the roles of lncRNAs in complex cutaneous disorders. I will test the hypotheses that (i): some lncRNAs are key causal elements and potentiate pathogenic inflammatory reactions in psoriasis development and (ii) by combining in silico predictions and in vitro validations we are able to provide comprehensive characterization of skin-expressing lncRNAs in keratinocytes and lymphocytes to infer their pathological implications for psoriasis. This work will demonstrate how we can take advantages of the genomic data to develop an integrative biology framework to provide novel biological insights and understand pathological roles of lncRNAs. Significance Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture. It is estimated that over 4 million Americans and 100 million people worldwide suffer from this disease. While genetic association studies have revealed the disease loci are highly enriched in non-coding regions, it is very challenging to translate genetic signals to biologic effects. In fact, most of the causal genes have not yet been identified. Our preliminary results showed that lncRNA is a class of gene that has largely been understudied for their roles in psoriasis, and both genetic and transcriptomic data suggested they can play important functions in psoriasis pathogenesis. By combining in silico analysis and in vitro validation we can expand our knowledge of lncRNAs in skin biology, and generate important hypotheses for future experiments. The results of this project can also identify novel biomarkers, and ultimately assist in the therapeutic drug discovery. PROJECT NARRATIVE Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture, and affects over 4 million Americans and 100 million people worldwide. Long non-coding RNAs (lncRNAs) is a class of gene that has largely been understudied, and recent studies suggested their potential roles in autoimmune diseases. This project aims to expand our knowledge of lncRNAs in skin biology, and advance identification of lncRNAs that play functional roles in psoriasis pathogenesis.",Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology,9371181,K01AR072129,"['Affect', 'American', 'Appointment', 'Architecture', 'Autoimmune Diseases', 'Autoimmune Process', 'Basic Science', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Research', 'Biometry', 'Catalogs', 'Cellular Structures', 'Chromatin', 'Chronic', 'Comorbidity', 'Complex', 'Computer Simulation', 'Coronary artery', 'Cutaneous', 'Data', 'Dermatologic', 'Dermatology', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Disease susceptibility', 'Doctor of Philosophy', 'Economic Burden', 'Educational process of instructing', 'Educational workshop', 'Elderly', 'Elements', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Epigenetic Process', 'Faculty', 'Flow Cytometry', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genetic study', 'Genomics', 'Genotype', 'Homing', 'Human', 'Immune', 'Immune system', 'Immunologics', 'Immunology', 'In Vitro', 'Inflammatory', 'Inflammatory Response', 'Knowledge', 'Learning', 'London', 'Lymphocyte', 'Machine Learning', 'Mediating', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Michigan', 'Modeling', 'Molecular', 'Molecular Profiling', 'Pathogenesis', 'Pathogenicity', 'Pathologic', 'Pathology', 'Patients', 'Play', 'Process', 'Production', 'Proteins', 'Psoriasis', 'Public Health', 'Quality of life', 'Quantitative Reverse Transcriptase PCR', 'Reaction', 'Recruitment Activity', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Signal Transduction', 'Skin', 'Societies', 'Statistical Methods', 'Students', 'Supervision', 'Susceptibility Gene', 'Systems Biology', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Transcript', 'Translating', 'United States', 'Universities', 'Untranslated RNA', 'Validation', 'Visit', 'Work', 'base', 'candidate identification', 'career development', 'cell type', 'cohort', 'college', 'cytokine', 'drug discovery', 'experience', 'experimental study', 'fascinate', 'genetic association', 'genomic data', 'high dimensionality', 'human disease', 'human tissue', 'immunoregulation', 'insight', 'interest', 'keratinocyte', 'knock-down', 'laboratory experience', 'learning strategy', 'meetings', 'novel', 'novel marker', 'scaffold', 'skills', 'skin disorder', 'small hairpin RNA', 'statistical center', 'success', 'symposium', 'tool', 'transcriptome sequencing', 'transcriptomics']",NIAMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,K01,2017,104175,-0.01829290657322149
"New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis ﻿    DESCRIPTION (provided by applicant)    Maha R Farhat, MD is an Instructor of Medicine at Harvard Medical School on the tenure track and a staff physician in the Department of Pulmonary and Critical Care Medicine at Massachusetts General Hospital. She is completing a masters of biostatistics at the Harvard School of Public Health in 5/2015. She has spent the last 4.5 years acquiring skills in Mycobacterium tuberculosis biology, epidemiology, bioinformatics and biostatistics. She has experience in the analysis of whole genome sequence data, drug resistance data and patient clinical outcome data with the focus of identifying Mycobacterium tuberculosis genetic determinants of drug resistance. She has also developed new methods in this area. Dr. Farhat has 11 publications 5 of which are first author including high impact and highly cited work in the journals Nature Genetics, Genome Medicine and the International Journal of Tuberculosis and Lung Disease. The short term goals of this K01 award are to provide training for Dr. Farhat in critical aspects of data science, computational and evolutionary biology, advanced biostatistics and network science. Dr. Farhat's long term goal is to become a leader in the field of Big Data analysis for infectious diseases. The proposed research as well as the training activities outlined in the proposal will successfully position Dr. Farhat for her first R01 and an independent career as a physician scientist. Environment: Dr. Farhat will perform the interdisciplinary work outlined in this proposal at the distinguished Harvard Departments of Global Health Social Medicine, Biostatistics, Evolutionary biology and the Institute for Quantitative Social Sciences. Dr. Farhat' mentorship team will include two world renowned leaders in the fields of infectious diseases and Big Data, Dr. Megan Murray and Dr. Gary King; and two rising stars in the fields of network Science and evolutionary Biology, Dr. JP Onnela and Dr. Michael Desai. Dr. Murray, the principal mentor on this proposal has mentored over 38 trainees, 9 of which have went on to have independent research careers, and 6 competed successfully for K awards. She is also PI on two recently awarded NIH/NIAID grants a CETR U19 and a TBRU U19 and has over 350 peer reviewed publications. To complement the expertise of her mentors Dr. Farhat will be advised by Dr. Christiani a practicing pulmonary and critical care physician and world renowned researcher in the field of lung and environmental genetics. She will also collaborate and consult with Dr. Merce Crosas, a data scientist, and Dr. Pardis Sabeti, a computational biologist. She will rotate through Dr. Soumya Raychaudhuri's bioinformatics laboratory to diversify her exposure to biomedical Big Data. In addition, she will receive formal training in evolutionary biology, Bayesian and mixed-model biostatistics, computer science, leadership skills and grant writing. The collaborative opportunities, intellectual environment and resources available to Dr. Farhat are outstanding. Research: Infectious diseases continue to be a major cause of morbidity and mortality. Despite the availability of effective antimicrobials, pathogens are successfully evolving new disease phenotypes that allow them to resist killing by these drugs or in other instances cause more severe disease manifestations or wider chains of transmission. Drug resistance (DR) is now common and some bacteria have even become resistant to multiple types or classes of antibiotics6. A key strategy in the fight against emerging pathogen phenotypes in infectious diseases is surveillance, and early personalized therapy to prevent transmission and propagation of these strains. The timely initiation of antibiotic therapy to which the pathogen is sensitive has been shown to be the key factor influencing treatment outcome for a diverse array of infections. Molecular tests that rely on the detection of microbial genetic mutations are particularly promising for surveillance and diagnosis of these pathogen phenotypes but rely on a comprehensive understanding of how mutations associate with these pathogen phenotypes. Currently there is an explosion of data on pathogen whole genome sequences (WGS) that is increasingly generated from clinical laboratories. Data on disease phenotype may also be available, but methods for the analysis and interpretation of these Big Data are lagging. Here I propose tools to aid in this analysis leveraging Big Data sets from Mycobacterium tuberculosis (MTB) and my prior work. Specifically I propose to (1) develop a web-based public interface to several analysis tools, including a statistical learning model that can predict the MTB DR phenotype from its genomic sequence, (2) to develop and study an MTB gene-gene network, based on WGS data, to improve our understanding of the effect of mutation-mutation interactions on the DR phenotype, and (3) study the performance of methods in current use for the association of genotype and phenotype in pathogens, and develop a generalizable power calculator for the best performing method. PUBLIC HEALTH RELEVANCE    Infectious agents of disease are successfully evolving drug resistance and other adaptations that threaten human health. Understanding the genetic mutations that underlie these disease phenotypes can inform surveillance and diagnostic strategies to combat this threat. Here I propose to develop accessible tools for pathogen genomic analysis that will help identify which genetic mutations are relevant to disease.",New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis,9326841,K01ES026835,"['Affect', 'Antibiotic Therapy', 'Antibiotics', 'Area', 'Award', 'Bacteria', 'Big Data', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Cessation of life', 'Clinical', 'Collaborations', 'Communicable Diseases', 'Complement', 'Consult', 'Critical Care', 'DNA Sequence Alteration', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug resistance', 'Drug resistance in tuberculosis', 'Drug-sensitive', 'Environment', 'Epidemiology', 'Explosion', 'Exposure to', 'Future', 'Gene Structure', 'General Hospitals', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Epistasis', 'Genome', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Grant', 'Health', 'Heritability', 'Human', 'Infection', 'Infectious Agent', 'Infectious Diseases Research', 'Institutes', 'International', 'Journals', 'K-Series Research Career Programs', 'Laboratories', 'Leadership', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Massachusetts', 'Measures', 'Medical Genetics', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Mentorship', 'Methods', 'Microbial Genetics', 'Modeling', 'Molecular', 'Monitor', 'Morbidity - disease rate', 'Mutation', 'Mycobacterium tuberculosis', 'National Institute of Allergy and Infectious Disease', 'Nature', 'Network-based', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Phylogeny', 'Physicians', 'Population', 'Positioning Attribute', 'Public Health', 'Public Health Schools', 'Publications', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Resources', 'Science', 'Scientist', 'Severity of illness', 'Social Medicine', 'Social Sciences', 'Standardization', 'Statistical Data Interpretation', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Tuberculosis', 'United States National Institutes of Health', 'Variant', 'Virulent', 'Work', 'Writing', 'analytical tool', 'antimicrobial', 'base', 'big biomedical data', 'burden of illness', 'career', 'combat', 'computer science', 'design', 'disease phenotype', 'experience', 'fight against', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'global health', 'improved', 'instructor', 'killings', 'medical schools', 'microbial', 'microbial genome', 'mortality', 'pathogen', 'pathogen genome', 'personalized medicine', 'prevent', 'prospective', 'public health relevance', 'simulation', 'skills', 'tenure track', 'tool', 'transmission process', 'whole genome']",NIEHS,HARVARD MEDICAL SCHOOL,K01,2017,225549,-0.013874960631570248
"Bioinformatics Tools for Circadian Biology Circadian rhythms are fundamental for understanding biology: they date back to the origin of life, they are found in virtually every species from cyanobacteria to mammals, and they coordinate many important biological functions from the sleep-wake cycle, to metabolism, and to cognitive functions. Circadian rhythms are equally fundamental for health and medicine: modifications in diet have been linked to modification in circadian rhythms at the molecular level; disruptions of circadian rhythms have been linked to health problems ranging from depression, to learning disorders, to diabetes, to obesity, to cardiovascular disease, to cancer, and to premature ageing; finally, a large fraction of drug targets have been found to oscillate in a circadian manner in one or several tissues, suggesting that a better understanding of circadian oscillations at the molecular level could have direct applications to precision medicine, for instance by optimizing the time at which drugs are taken.  To better understand circadian oscillations at the molecular level, modern high-throughput technologies are being used to measure the concentrations of many molecular species, including transcripts, proteins, and metabolites along the circadian cycle in different organs and tissues, and under different conditions. However, the informatics tools for processing, analyzing, and integrating the growing wealth of molecular circadian data are not yet in place.  This effort will fill this fundamental gap by developing and disseminating informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine. Specifically, through a close collaborations between computational and experimental scientists, this effort will: (1) Bring the power of deep learning methods to bear on the analyses of omic time series to determine, for instance, which molecular species are oscillating, their characteristics (period, phase, amplitude), and to predict the time/phase associated with a measurement taken at a single time point; (2) Develop Cyber-TC, an extension of the widely used Cyber-T software, for the differential analysis of circadian omic time series and expand MotifMap, a widely used genome-wide map of regulatory sites to better understand circadian regulation; and (3) Develop Circadiomics, an integrated database and web portal as a one-stop shop for circadian data, annotations, and analyses. All data, software, and results will be freely available for academic research purposes and broadly disseminated through multiple channels to benefit both the circadian community and the broader bioinformatics community. Circadian rhythms are fundamental for biology and medicine. Modern high-throughput technologies are revealing how the concentrations of many molecular species, including transcripts, proteins, and metabolites oscillate with the day and night cycle in almost every species, tissue, and cell. In close collaboration with biologists, this project will develop the informatics tools that will enable the collection, integration, and analyses of this wealth of information and lead to novel and fundamental insights about the organization and regulation of circadian oscillations, their roles in health and disease, and their future applications to precision medicine.",Bioinformatics Tools for Circadian Biology,9325275,R01GM123558,"['Address', 'Ally', 'Architecture', 'Back', 'Biogenesis', 'Bioinformatics', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Cells', 'Characteristics', 'Circadian Rhythms', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Cyanobacterium', 'Data', 'Databases', 'Diabetes Mellitus', 'Diet', 'Disease', 'Drug Targeting', 'Feedback', 'Future', 'Gene Expression Regulation', 'Health', 'Homeostasis', 'Informatics', 'Laboratories', 'Lead', 'Learning', 'Learning Disorders', 'Life', 'Link', 'Malignant Neoplasms', 'Mammals', 'Maps', 'Measurement', 'Measures', 'Medicine', 'Mental Depression', 'Metabolism', 'Modernization', 'Modification', 'Molecular', 'Obesity', 'Organ', 'Periodicity', 'Pharmaceutical Preparations', 'Phase', 'Plasticizers', 'Premature aging syndrome', 'Proteomics', 'Regulation', 'Research', 'Role', 'Scientist', 'Series', 'Site', 'Sleep Wake Cycle', 'System', 'Testing', 'Time', 'Tissues', 'Transcript', 'Update', 'Ursidae Family', 'Vision', 'annotation  system', 'cognitive function', 'cognitive process', 'direct application', 'genome-wide', 'high throughput analysis', 'high throughput technology', 'insight', 'learning strategy', 'member', 'metabolomics', 'novel', 'precision medicine', 'protein metabolite', 'software development', 'tool', 'transcriptomics', 'virtual', 'web portal']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2017,324508,-0.005138159122516431
"CRCNS: Representational foundations of adaptive behavior in natural and artificial  ﻿    DESCRIPTION (provided by applicant): Overview: Among the most celebrated success stories in computational neuroscience is the discovery that many aspects of decision-making can be understood in terms of the formal framework of reinforcement learning (RL). Ideas drawn from RL have shed light on many behavioral phenomena in learning and action selection, on the functional anatomy and neural processes underlying reward-driven behavior, and on fundamental aspects of neuromodulatory function. However, for all these successes, RL-based work is haunted by an inconvenient truth: Standard RL algorithms scale poorly to large, complex problems. If human learning and decision-making are driven by RL-like mechanisms, how is it that we cope with the kinds of rich, large-scale tasks that are typical of everyday life? Existing research in both psychology and neuroscience hints at one answer to this question: Complex problems can be conquered if the decision-maker is equipped with compact, intelligently formatted representations of the task. This principle is seen in studies of expert play in chess, which show that chess masters leverage highly integrative internal representations of board configurations; in studies of frontal and parietal lobe function, which have revealed receptive fields strongly shaped by task contingencies; and studies on the hippocampus, which point to the role of this structure in supporting a hierarchically organized 'cognitive map,' of task space. Not coincidentally, the critical role of representation has come increasingly to the fore in RL-based research in machine learning and robotics, with growing interest in techniques for dimensionality reduction, hierarchy and deep learning.            The present project aims toward a systematic, empirically validated account of the role of representation in supporting RL and goal-directed behavior at large. The project brings together three investigators with complementary expertise in cognitive and computational neuroscience (Botvinick, Gershman) and machine learning and robotics (Konidaris). Together, we propose an integrative, interdisciplinary program of research, applying behavioral and neuroimaging work with human subjects, computational modeling of neurophysiological and behavioral data, formal mathematical work and simulations with artificial agents. The proposed studies are diverse in theme and method, but work together toward a theory that is both formally grounded and empirically constrained. At a more concrete level, our research focuses on four specific classes of representation, considering the computational impact of each for RL, as well as the relevance of each to neuroscience and human behavior. As detailed in our Project Description, these include (1) metric embedding, (2) spectral decomposition, (3) hierarchical representation and (4) symbolic representation. In addition to investigating the implications of each of these four forms of representation individually, we hypothesize that they fit together into a tiered system, which works as a whole to support the sometimes competing demands of learning and action control.         Intellectual Merit (provided by applicant): Understanding how representational structure impacts learning and decision making is a core challenge in cognitive science, behavioral neuroscience and, artificial intelligence. Success in establishing a computationally explicit, empirically validated theory in this area, with a specific focus on the role of representation in R, would represent an important achievement with wide repercussions. The strategy of leveraging conceptual tools from machine learning to investigate human behavior and brain function can offer considerable scientific leverage, as our own previous research illustrates. The proposed work is motivated by and builds upon established lines of research, bringing these together in order to capitalize on opportunities for synergy. In addition to answering specific empirical and computational questions, the proposed work aims to open up new avenues for future research in an important area of inquiry.         Broader Impact (provided by applicant): The proposed work lies at the crossroads of neuroscience, psychology, artificial intelligence and machine learning, and promises to advance the growing exchange among these fields. The project brings together investigators with contrasting disciplinary affiliations, with the explicit goal of bridging between intellectual cultres. The proposed work is likely to find a wide scientific audience, given its relevance to cognitive and developmental psychology, behavioral, cognitive and systems neuroscience, and behavioral economics. However, the work is likely to be of equal interest within artificial intelligence, machine learning, and robotics, where a current challenge is precisely to understand how representation learning can allow RL to scale up to large problems. Representational approaches to RL are already of intense interest within industry, where the present investigators have a record of active engagement. The topic of the proposed work has applicability in other areas as well, including education and training, and military and medical decision support. The plan for the project has a robust training component at both graduate and postdoctoral levels, with a commitment to fostering involvement of underrepresented minorities, as well as international engagement. n/a",CRCNS: Representational foundations of adaptive behavior in natural and artificial ,9126614,R01MH109177,"['Accounting', 'Achievement', 'Adaptive Behaviors', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Cognitive Science', 'Complex', 'Computer Simulation', 'Data', 'Decision Making', 'Fostering', 'Foundations', 'Goals', 'Hippocampus (Brain)', 'Human', 'Industry', 'International', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Military Personnel', 'Neurosciences', 'Parietal Lobe', 'Play', 'Process', 'Psychological reinforcement', 'Psychology', 'Research', 'Research Personnel', 'Rewards', 'Robotics', 'Role', 'Shapes', 'Structure', 'System', 'Techniques', 'Training', 'Training and Education', 'Underrepresented Minority', 'Work', 'base', 'behavioral economics', 'cognitive neuroscience', 'cognitive system', 'computational neuroscience', 'coping', 'developmental psychology', 'driving behavior', 'frontal lobe', 'human subject', 'interest', 'neuroimaging', 'neurophysiology', 'neuroregulation', 'programs', 'receptive field', 'relating to nervous system', 'research based learning', 'scale up', 'simulation', 'success', 'theories', 'tool']",NIMH,PRINCETON UNIVERSITY,R01,2016,384420,-0.01209927178109561
"Training a new generation of computational neuroscientists bridging neurobiology The  Training  Program  in  Computational  Neuroscience  (TPCN)  will  support  integrated   undergraduate  and graduate training in computational neuroscience at New York University. The program will be hosted  by the Center for Neural Science (CNS), with participation of faculty in the Departments of  Psychology, Mathematics, and Computer Science, and the Institute of Neuroscience at the School of  Medicine. The TPCN will fit well with NYU’s unique strengths and recent developments: (1) NYU is  one of a few universities with a critical mass of computational neuroscientists. NYU has had a  Sloan-Swartz Center for Theoretical Neuroscience since 1994. In the past three years alone, NYU has  hired three computational neuroscientists. (2) CNS  established an undergraduate major in  neuroscience as early as 1992, and thus has a long track record in undergraduate education, it now  has 136 students in the current academic year. (3) Recent faculty hiring in CNS, Psychology, and  the School of Medicine has greatly expanded our teaching and research capabilities in the  neuroscience of cognitive functions and their impairments associated with mental disorders. (3) As  NYU is undertaking a merge of two historically separated neuroscience graduate programs (at CNS   and  the  School  of  Medicine),  this training grant will ensure that computational modeling,  which has become indispensible in neuroscience, will be front-and-center in the integrated graduate  program. (4) NYU is a major center of Artificial Intelligence and Data Science, with close links to  Facebook’s AI Center and the Simons Center for Data Analysis. Our training faculty together with  these connections will give our students ample opportunities to acquire machine learning techniques  for data analysis and learn about brain-like AI algorithms. The proposed  training  program will support coherent undergraduate  and  graduate  training  in   computational neuroscience at NYU. It will have several unique features: (1) Innovative mentorship  methods: For example, (a) graduate trainees will mentor undergraduate trainees, (b) faculty will explicitly discuss human  factors in academic practice; (c) there will be post-mortems after seminars by outside speakers.  (2) Computational psychiatry: We propose new courses and research opportunities that are designed  specifically to link cognitive function and the neurobiology of neural circuits. We propose  innovative education in the nascent field of Computational Psychiatry, to bring theory and circuit  modeling to clinical research in mental health. (3) Broad preparation: We aim to prepare trainees  for jobs not only in academia, but  also  in  medical  and  industry research. To achieve this, we  will utilize our strength in machine learning and data science to broaden computational  neuroscience training. The Program Directors have complementary strengths and will have  complementary roles in the program. Wang will supervise graduate trainees and focus on training in  mechanistic/circuit-level side of computational neuroscience as well as computational psychiatry.  Ma will supervise undergraduate trainees and focus on the computational/behavioral side. This grant will support training of a new generation of graduate and undergraduate students in  computational neuroscience, which has become increasingly important to meet the challenges of  making discoveries with new data analysis tools and of understanding highly nonlinear complex  neural circuits. A salient component of our program is training in the nascent field  of   Computational  Psychiatry,  bridging  basic  neuroscience  and clinical research on mental  disorders. Therefore, the proposed program has the potential of making  a significant impact on  mental health.",Training a new generation of computational neuroscientists bridging neurobiology,9316750,R90DA043849,"['Academia', 'Algorithms', 'Artificial Intelligence', 'Behavioral', 'Brain', 'Clinical Research', 'Computer Simulation', 'Data Analyses', 'Data Science', 'Development', 'Education', 'Educational process of instructing', 'Ensure', 'Faculty', 'Generations', 'Grant', 'Human', 'Impairment', 'Industry', 'Institutes', 'Learning', 'Link', 'Machine Learning', 'Mathematics', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Neurobiology', 'Neurosciences', 'New York', 'Occupations', 'Preparation', 'Psychiatry', 'Psychology', 'Research', 'Role', 'Science', 'Side', 'Students', 'Teacher Professional Development', 'Techniques', 'Training', 'Training Programs', 'Universities', 'cognitive function', 'computational neuroscience', 'computer science', 'design', 'innovation', 'medical schools', 'neural circuit', 'programs', 'relating to nervous system', 'theories', 'undergraduate education']",NIDA,NEW YORK UNIVERSITY,R90,2016,204981,-0.0012055586093287541
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control. PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,8994718,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Health', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Peptide Sequence Determination', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'flu', 'genome sequencing', 'genomic data', 'genomic signature', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'learning strategy', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'receptor binding', 'research study', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2016,370329,-0.01089269448222495
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,9015770,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'genomic data', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2016,71329,-0.016310518656166416
"Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach Project Summary Biology is full of stunning examples of emergent behaviors – behaviors that arise from, but cannot be reduced to, the interactions of the constituent parts that make up the system under consideration. These behaviors span the full spectrum of length scales, from the emergence of distinct cell fates (e.g. neurons, muscle, etc.) due to the interactions of genes within cells, to the formation of complex ecological communities arising from the interactions of thousands of species. The overarching goal of my research is to develop new conceptual, theoretical, and computational tools to model such emergent, system-level behaviors in biology. To do so, we utilize an interdisciplinary approach that is grounded in Biological Physics, but draws heavily from Machine Learning, Information Theory, and Theoretical Ecology. Our work is unified and distinguished by our deep commitment to integrating theory with the vast amount of biological data now being generated by modern DNA sequencing-based techniques and quantitative microscopy. An important goal of the proposed research is to find common concepts and tools that transcend traditional biological sub-disciplines and models systems. The proposed research pursues four distinct but conceptually interrelated research directions: (1) understanding how distinct cell fates emerge from bimolecular interactions within mammalian cells (2) investigating how bimolecular networks within cells exploit energy consumption to improve computations, with applications to Synthetic Biology; (3) identifying the ecological principles governing community assembly in microbial communities and developing techniques for synthetically engineering ecological communities; and (4) developing new machine learning algorithms and techniques for biological data analysis. In addition to developing physics-based models for diverse biological phenomena, the proposed research will yield a series of practical important tools and algorithms which we will make publically available including: (1) a new linear-algebra based algorithm for assessing the fidelity of directed differentiation and cellular reprogramming protocols and visualizing reprogramming/differentiation dynamics and (2) improved algorithms for inferring microbial interactions in the human microbiome from high-throughput sequence data. These computational tools will allow scientists to realize the immense therapeutic potential of cellular reprogramming and microbial ecology-based techniques for studying and treating human disease. Project Narrative This project will develop new theoretical and computational tools that will allow scientists to realize the immense therapeutic potential of cellular reprogramming (the conversion of one cell type into another cell type) and microbial ecology-based techniques for studying and treating human diseases.",Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach,9137947,R35GM119461,"['Algorithms', 'Behavior', 'Biological', 'Biological Models', 'Biological Phenomena', 'Biology', 'Cells', 'Communities', 'Complex', 'Consumption', 'DNA Sequence', 'Data', 'Data Analyses', 'Discipline', 'Ecology', 'Engineering', 'Goals', 'High-Throughput Nucleotide Sequencing', 'Human Microbiome', 'Information Theory', 'Length', 'Linear Algebra', 'Machine Learning', 'Mammalian Cell', 'Modeling', 'Muscle', 'Neurons', 'Physics', 'Protocols documentation', 'Quantitative Microscopy', 'Research', 'Scientist', 'Series', 'System', 'Systems Biology', 'Techniques', 'Therapeutic', 'Transcend', 'Work', 'base', 'cell type', 'computerized tools', 'gene interaction', 'human disease', 'improved', 'interdisciplinary approach', 'microbial', 'microbial community', 'microorganism interaction', 'synthetic biology', 'theories', 'tool']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R35,2016,302664,-0.031055592645089218
"Transmission Networks in Trait-Based Communities The complexity of ecological communities creates challenges to understanding multi-host parasite transmission. Pronounced heterogeneity in transmission among individuals, species and across space is the rule rather than the exception. Community ecologists are beginning to make great strides in predicting multi-species interactions using a trait-based rather than taxonomic approach, identifying key functional attributes of organisms and environments that are important to understanding the system. At the same time, disease ecologists generally use network modeling to understand parasite transmission in complex communities. Yet the merging of a trait-based approach with network modeling to understand multi-host transmission across space and time is in its infancy. We will take advantage of a highly tractable system - diverse communities of bees that transmit parasites via networks of flowering plants - to merge trait-based theory with network modeling, introducing a novel theoretical framework for multi-host parasite transmission in complex communities. We will collect empirical contact pattern and trait data from plant-pollinator networks to identify aspects of network structure that contribute to disease spread. Through the collection of extensive data on bee traits, floral traits and parasite spread, we will use machine learning techniques to construct and parameterize trait-based models of disease transmission in order to make falsifiable predictions for further testing. We will then test model predictions via whole-community manipulations of bees, parasites and plants in mesocosms. Such whole-community manipulations will offer unparalleled insight into the specific network patterns and traits that shape transmission in multi-host communities. Pollinators serve a critical role in our native ecosystems as well as agricultural crops, providing billions of dollars in pollination services annually. Recently, parasites have been linked to declines of several pollinator species. Thus, a better understanding of parasite transmission among bees has important conservation and economic implications.",Transmission Networks in Trait-Based Communities,9241579,R01GM122062,"['Address', 'Agricultural Crops', 'Angiosperms', 'Bees', 'Collection', 'Communities', 'Complex', 'Coupling', 'Data', 'Disease', 'Disease Vectors', 'Ecosystem', 'Environment', 'Epidemiology', 'Flowers', 'Goals', 'Heterogeneity', 'Individual', 'Infection', 'Knowledge', 'Link', 'Machine Learning', 'Modeling', 'Observational Study', 'Organism', 'Parasites', 'Pattern', 'Plants', 'Population', 'Prevalence', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Services', 'Shapes', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'disease transmission', 'economic implication', 'improved', 'infancy', 'insight', 'model building', 'network models', 'novel', 'research study', 'theories', 'tool', 'trait', 'transmission process', 'vector']",NIGMS,CORNELL UNIVERSITY,R01,2016,409580,0.0027895398476973964
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,9142240,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modeling', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2016,380459,-0.020803768594724898
"INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE Project Summary  To understand the many disorders of the brain it is necessary to grapple with its complexity.  Increasingly large and complicated data sets are being collected, but the tools for analyzing and  modeling the data are not yet available. More researchers trained in computational neuroscience are  desperately needed. This project supports graduate and undergraduate training programs in  computational neuroscience (TPCN) at both Carnegie Mellon University (CMU) and the University of  Pittsburgh (Pitt), and a summer school in computational neuroscience for undergraduates, which are  available to students coming from colleges and universities throughout the United States.  The CMU-Pitt TPCN has 16 training faculty in computational neuroscience, 22 training faculty whose  laboratories are primarily experimental, and 20 training faculty whose laboratories are both  computational and experimental. At the graduate level the TPCN offers a PhD program in Neural  Computation (PNC) and joint PhD programs with CMU’s Department of Statistics (PNC-Stat) and its  Machine Learning Department (PNC- MLD), all set within a highly collegial, cross-disciplinary  environment of our Center for the Neural Basis of Cognition (CNBC), which is operated jointly by  CMU and Pitt. The CNBC was established in 1994 to foster interdisciplinary research on the neural  mechanisms of brain function, and now comprises 145 faculty having appointments in 22 departments.  At the undergraduate level a substantial pool of local students is supplemented during the summer  by a cohort of students from across the country. During this renewal funding period the project is  strengthening the role of statistics and machine learning throughout the training programs; (2) revising the summer undergraduate program by creating a didactic two-week “boot camp” at the  beginning, which includes a 20-lecture overview of computational neuroscience; (3) creating online  materials, in conjunction with the boot camp, that will serve not only our own students but also  the greater world of training in computational neuroscience; and (4) enhancing our minority  recruitment by (a) taking advantage of the boot camp and online materials, as well as making  promotional visits to targeted campuses, and (b) creating and running a one-year “bridge” program  to better prepare under-represented minorities for PhD programs.  TPCN trainees work in vertically integrated, cross-disciplinary research teams. Graduate students  take a year- long course in computational neuroscience that bridges modeling and modern statistical  machine learning approaches to neuroscience. To ensure their competency in core neuroscience  principles they also take courses in cognitive neuroscience, neurophysiology, and systems  neuroscience. They then pursue depth in a relevant quantitative discipline, such as computer  science, engineering, mathematics, or statistics. Graduate students have extended experience in at  least one experimental laboratory, and they take part in journal clubs and seminars within the  large Pittsburgh neuroscience community. Year-long undergraduates take courses in mathematics,  computer programming, statistics, and neuroscience; they take an additional course in neuroscience  or psychology and a course in computational neuroscience; and they complete a year-long research  project. In addition, they complete the TPCN summer program. Undergraduate trainees in the summer  program go through the boot camp on topics in computational neuroscience, including tutorials in  Matlab, statistical methods, fundamentals of differential equations, and ideas of neural coding;  they then complete a research project under careful guidance. All trainees will receive training in  responsible conduct of research. Across 5 years of funding, the TPCN supports 20 NRSA graduate  students, 10 non-NRSA graduate students, 30 undergraduate year-long fellows, and 60 undergraduate  summer fellows. Project Narrative  Research in neuroscience is crucial for attacking the causes of neurological and mental health  disorders. If the field of neuroscience is to continue its rapid advance, neuroscientists must use,  understand, and develop new technologies, acquire and analyze ever larger data sets, and grapple  more directly with the complexity of neurobiological systems. The primary goal of these training  programs will be to help train a new generation of interdisciplinary neuroscientists with strong  quantitative skills.",INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE,9322706,R90DA023426,"['Appointment', 'Brain', 'Brain Diseases', 'Camping', 'Code', 'Cognition', 'Communities', 'Country', 'Data Set', 'Differential Equation', 'Discipline', 'Doctor of Philosophy', 'Engineering', 'Ensure', 'Environment', 'Faculty', 'Fostering', 'Funding', 'Interdisciplinary Study', 'Joints', 'Journals', 'Laboratories', 'Machine Learning', 'Mathematics', 'Minority Recruitment', 'Modeling', 'National Research Service Awards', 'Neurosciences', 'Psychology', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Running', 'Schools', 'Statistical Methods', 'Students', 'System', 'Teacher Professional Development', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States', 'Universities', 'Visit', 'Work', 'base', 'bridge program', 'cognitive neuroscience', 'cohort', 'college', 'computational neuroscience', 'computer program', 'computer science', 'data modeling', 'experience', 'graduate student', 'lectures', 'neuromechanism', 'neurophysiology', 'programs', 'relating to nervous system', 'responsible research conduct', 'statistics', 'summer program', 'tool']",NIDA,CARNEGIE-MELLON UNIVERSITY,R90,2016,316840,-0.0020936381406555563
"A three-population three-scale social network model to assess disease dispersion DESCRIPTION (provided by applicant): Communicable diseases, such as influenza, are transmitted from individual to individual following a network of contacts in a population. Reports on recent outbreaks, such as SARS, the Bird flu, and the H1N1 flu, have repeatedly stressed the critical role of contact networks. We propose an innovative Three-population and Three-scale Social Network (3p3sNet) model to simulating the spatial and temporal dispersion of influenza in a metropolitan population in Buffalo, NY. The 3p3sNet aims to construct a realistic contact network by representing interacting and mobile behaviors of individuals at three scales and three types of places. These involve individual (microscopic) -> local network (mesoscopic) -> population (macroscopic) as nighttime population at homes, daytime population at workplaces, and pastime population at service places. Through this network, diseases disperse from infectious individuals to their local networks then to the population-wide network in a complex dynamic fashion. Modeling the disease dispersion through this network provides invaluable insights in who might be at risk, where and when this risk might occur, and with whom these at-risk individuals might be in contact. These insights lay the foundation of developing spatially and temporally sensitive intervention strategies targeted towards the most vulnerable individuals and social groups. Furthermore, the 3p3sNet can be applied in modeling the epidemiology of any disease where human contacts play a critical role.  In implementing 3p3sNet, we propose to use mobile phone data to extract the individual interaction and travel behaviors. We embrace recent developments in economics, geostatistics, econometrics, and machine learning to construct the network. We develop an innovative co-kriging approach to expanding local households to population-wide households and a novel distance-based GEV discrete choice model to link homes to workplaces and service places. It is anticipated that the assemblage of these advanced methods will enable new capabilities and bring transformative improvements in health-related studies in metropolitan areas. We will conduct a data-rich validation process for the three constructed populations, the links between them, and the simulated disease dispersion through the population. A comprehensive range of independent datasets will be used to support the proposed validation. These involve high-resolution population, workplace, and service place data, surveys of individual interaction and travel behavior, and reports on influenza infections.  The multidisciplinary team comprises world-renown leaders and scholars in epidemiology, agent-based and social network modeling, human mobility analysis, geographical information science, and machine learning. The proposed project represents emerging frontiers in the modeling of communicable diseases and will redefine the capabilities of epidemiological models. PUBLIC HEALTH RELEVANCE: This project addresses two issues of central importance to successfully capturing the complex, spatial and temporal dispersion process of a communicable disease through a population. They are: how to represent individuals as heterogeneous, mobile, and interacting and how to model the disease dispersion process from infectious individuals to their local networks then to the population-wide network. To address these issues, this project proposes to: (1) use mobile phone data to construct a three-population and three-scale social network (3p3sNet); (2) simulate the spatially and temporally dynamic dispersion of influenza through an urban population in Buffalo, NY; and (3) conduct an intensive, data-rich validation process on the simulated influenza dispersion.",A three-population three-scale social network model to assess disease dispersion,9031775,R01GM108731,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Behavior', 'Buffaloes', 'Car Phone', 'Communicable Diseases', 'Complex', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Disease Outbreaks', 'Disease model', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Foundations', 'Health', 'Home environment', 'Household', 'Human', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Information Sciences', 'Intervention', 'Lead', 'Link', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Modeling', 'Outcome', 'Play', 'Population', 'Process', 'Records', 'Reporting', 'Resolution', 'Risk', 'Role', 'Sampling', 'Services', 'Severe Acute Respiratory Syndrome', 'Social Network', 'Stress', 'Surveys', 'Testing', 'Time', 'Travel', 'Urban Population', 'Validation', 'Workplace', 'base', 'disease transmission', 'epidemiological model', 'flu', 'frontier', 'fundamental research', 'human disease', 'innovation', 'insight', 'metropolitan', 'multidisciplinary', 'network models', 'novel', 'pandemic disease', 'social group', 'transmission process']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,583077,-0.012396095687758504
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,-0.008542411977654804
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,-0.008542411977654804
"Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research Abstract We propose a three-year interdisciplinary research plan to address two key issues currently facing the metagenomics community. The first issue concerns accurate construction and annotation of OTU tables using  of millions of 16S rRNA sequences, which is one of the most important yet most difficult problems inmicrobiome data analysis. Currently, it lacks computational algorithms capable of handling extremely large sequence data and constructing biologically consistent OTU tables. We propose a novel method that performs OTU table construction and annotation simultaneously by utilizing input and reference sequences, reference annotations, and data clustering structure within one analytical framework. Dynamic data-driven cutoffs are derived to identify OTUs that are consistent not only with data clustering structure but also with reference annotations. When successfully implemented, our method will generally address the computational needs of processing hundreds of millions of 16S rRNA reads that are currently being generated by large-scale studies. The second issue concerns developing novel methods to extract pertinent information from massive sequence data, thereby facilitating the field shifting from descriptive research to mechanistic studies. We are particularly interested in microbial community dynamics analysis, which can provide a wealth of insight into disease development unattainable through a static experiment design, and lays a critical foundation for developing probiotic and antibiotic strategies to manipulate microbial communities. Traditionally, system dynamics is approached through time-course studies. However, due to economical and logistical constraints, time-course studies are generally limited by the number of samples examined and the time period followed. With the rapid development of sequencing technology, many thousands of samples are being collected in large-scale studies. This provides us with a unique opportunity to develop a novel analytical strategy to use static data, instead of time-course data, to study microbial community dynamics. To our knowledge, this is the first time that massive static data is used to study dynamic aspects of microbial communities. When successfully implemented, our approach can effectively overcome the sampling limitation of time-course studies, and opens a new avenue of research to study microbial dynamics underlying disease development without performing a resource-intensive time-course study. The proposed pipeline will be intensively tested on a large oral microbiome dataset consisting of ~2,600 subgingival samples (~330M reads). The analysis can significantly advance our understanding of dynamic behaviors of oral microbial communities possibly contributing to the development of periodontal disease. To our knowledge, no prior work has been performed on this scale to study oral microbial community dynamics. We have assembled a multidisciplinary team that covers expertise spanning the areas of machine learning, bioinformatics, and oral microbiology. The expected outcome of this work will be a set of computational tools of high utility for the microbiology community and beyond. The human microbiome plays essential roles in many important physiological processes. We propose an interdisciplinary research plan to address some major computational challenges in current microbiome research. If successfully implemented, this work could significantly expand the capacity of existing pipelines for large-scale data analysis and scientific discovery, resulting in a significant impact on the field.",Developing Advanced Algorithms to Address Major Computational Challenges in Current Microbiome Research,9158909,R01AI125982,"['Address', 'Algorithms', 'Antibiotics', 'Area', 'Big Data', 'Bioinformatics', 'Communities', 'Computational algorithm', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Epidemiology', 'Floods', 'Foundations', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Interdisciplinary Study', 'Knowledge', 'Machine Learning', 'Metagenomics', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral Microbiology', 'Outcome', 'Periodontal Diseases', 'Physiological Processes', 'Play', 'Probiotics', 'Process', 'Reading', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Structure', 'Technology', 'Testing', 'Time', 'Work', 'abstracting', 'base', 'cohort', 'computerized tools', 'design', 'dynamic system', 'epidemiology study', 'innovation', 'insight', 'interest', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'multidisciplinary', 'novel', 'open source', 'oral behavior', 'oral microbiome', 'research study', 'response', 'tumor progression', 'web app']",NIAID,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2016,311803,-0.0023458727199927006
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,9005867,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'mobile computing', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2016,1271107,-0.00144690446304343
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,9268117,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost effectiveness', 'cost efficient', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2016,1378926,-0.030264770604566983
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),9118144,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'cloud platform', 'computational chemistry', 'computer science', 'data mining', 'design', 'distinguished professor', 'dopamine transporter', 'drug abuse prevention', 'epigenetic marker', 'falls', 'genome-wide', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'operation', 'predictive modeling', 'prevent', 'professor', 'receptor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2016,1070007,-0.014989845877341989
"Effects of Host Metabolic Variation on Antibiotic Susceptibility ﻿    DESCRIPTION (provided by applicant): Host variation affects the pathogenicity of, susceptibility to, and recovery from infectious diseases. Elucidating how the host environment alters antibiotic susceptibility is therefore a critical step towards the long-term goal of realizig precision medicine for the clinical management of infectious diseases. The overall objective of this project is to identify the metabolic pathways participating in antibiotic susceptibility and t determine how host metabolism may affect antibiotic treatments at the site of infection. The working hypothesis is that metabolic processes can function as bacterial control mechanisms for antibiotic susceptibility and that the host metabolome can act on these processes to affect the outcome of antibiotic treatment. This hypothesis will be tested in three specific aims: (1) identif metabolic pathways involved in antibiotic susceptibility (K99 phase); (2) characterize changes in the host metabolome elicited by antibiotic administration (K99 phase); (3) evaluate effects of the host metabolome on antibiotic killing (R00 phase). During the mentored K99 phase, bactericidal antibiotics will be counterscreened with various metabolites to identify metabolic perturbations that can affect antibiotic susceptibility. Metabolic pathways contributing to antibiotic lethality ill be identified by combining this data with metabolic modeling and machine learning. Additionally, plasma and peritoneal fluid will be sampled and metabolomically profiled from a mouse peritoneal infection model, with and without antibiotic treatment. These profiles will be used to determine if antibiotics can alter the host metabolism in ways that may affect antibiotic susceptibility at the site of infection. During the independent R00 phase, the effects of host metabolic variation on antibiotic killing will be systematically tested by quantifying antibiotic susceptibility in synthesized media defined by metabolomic profiles from published and measured human and mouse plasma samples. A better understanding of how the host metabolic environment participates in antibiotic treatment fits NIH's public health mission and has direct implications for the clinical management of infectious diseases. Work from the proposed studies will form a quantitative framework for directly evaluating how host metabolism may affect antibiotic treatment outcomes and guide improved antibiotic stewardship in clinical practice. Although the applicant has significant expertise in systems biology, this award will provide the applicant research training to gain new experimental skills and an opportunity for continued career training and mentorship from an advisory committee comprised of international leaders in systems biology, metabolomics, chemical biology and infectious diseases. The support and training provided by this award and by the advisory committee will provide the applicant tools and expertise critical to his future independent research program.         PUBLIC HEALTH RELEVANCE (provided by applicant): Host variation alters the clinical response to antibiotic treatment for infectious disease, but the pathways underlying differences in patient outcome have not yet all been elucidated. The proposed research is relevant to public health because it is the first systematic investigation of how host metabolites may act on bacterial pathogen metabolism and alter antibiotic susceptibility. This work is relevant to NIH's public health mission by providing a quantitative framework for establishing precision medicine for the treatment of infectious diseases.        ",Effects of Host Metabolic Variation on Antibiotic Susceptibility,9089492,K99GM118907,"['Adjuvant', 'Advisory Committees', 'Affect', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotic susceptibility', 'Antibiotics', 'Award', 'Bacteria', 'Biology', 'C57BL/6 Mouse', 'Carbon', 'Cells', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Management', 'Clinical Treatment', 'Communicable Diseases', 'Culture Media', 'Data', 'Development', 'Dimensions', 'Disease Resistance', 'Environment', 'Escherichia coli', 'Formulation', 'Future', 'Germ-Free', 'Goals', 'Growth', 'Human', 'Immunity', 'In Vitro', 'Infection', 'Innovative Therapy', 'International', 'Investigation', 'Knock-out', 'Knowledge', 'Light', 'Liquid substance', 'Machine Learning', 'Measures', 'Mentors', 'Mentorship', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Mission', 'Modeling', 'Mus', 'Nitrogen', 'Outcome', 'Pathogenicity', 'Pathway interactions', 'Patient-Focused Outcomes', 'Peritoneal', 'Peritoneal Fluid', 'Peritoneal lavage', 'Phase', 'Phosphorus', 'Plasma', 'Predisposition', 'Process', 'Public Health', 'Publishing', 'Recovery', 'Research', 'Research Training', 'Sampling', 'Serum', 'Site', 'Source', 'Sulfur', 'Supplementation', 'System', 'Systems Biology', 'Testing', 'Training', 'Training Support', 'Treatment outcome', 'Variant', 'Virulence', 'Work', 'bacterial metabolism', 'bactericide', 'base', 'biological adaptation to stress', 'career', 'clinical practice', 'cohort', 'counterscreen', 'genome-wide', 'improved', 'in vivo', 'infectious disease treatment', 'insight', 'killings', 'metabolome', 'metabolomics', 'microbial', 'microbiota', 'novel therapeutics', 'overexpression', 'pathogen', 'precision medicine', 'programs', 'public health relevance', 'respiratory', 'response', 'skills', 'tool', 'treatment strategy']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,K99,2016,90000,-0.00782387673091026
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases. PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.",Integration and Visualization of Diverse Biological Data,9057057,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Drug Targeting', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic study', 'Goals', 'Gold', 'Health', 'Hereditary Disease', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'diagnostic biomarker', 'drug development', 'drug discovery', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'novel therapeutics', 'research study', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2016,445349,-0.025817557548633848
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs. PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.",New Serological Measures of Infectious Disease Transmission Intensity,9094553,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Effectiveness of Interventions', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Qualifying', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Staging', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk', 'improved', 'intervention effect', 'intervention program', 'low income country', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'research study', 'response', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2016,141588,-0.015752223679611717
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",9099895,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Influenza A virus', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2016,418572,0.002358813970665672
"Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing Abstract  Systematic reviews constitute the highest quality of evidence and form the cornerstone of evidence-based medicine (EBM). Such reviews now inform everything from national health policy guidelines to bedside care. However, systematic reviews are extremely laborious to produce; researchers can no longer keep pace with the massive amount of evidence now being published.  Semi-automation of systematic review production via machine learning (ML) has demonstrated the potential to substantially reduce reviewer workload while maintaining comprehensiveness. However, it is unlikely that machines will fully supplant human reviewers in the near future. Rather, human experts will probably remain in the loop, assisted by automated methods. Methods that exploit the intersection of human workers and ML models in the context of systematic reviews have not been explored at length. Furthermore, we believe there is substantial untapped potential in harnessing distributed crowd-workers to contribute to systematic reviews, and thus economize expert reviewer efforts. This novel avenue has largely been neglected as a means of increasing the efficiency of review production.  We propose addressing this gap by developing and evaluating novel, hybrid approaches to generating systematic reviews that jointly incorporate domain experts (systematic reviewers), layperson workers recruited via crowdworking platforms such as Amazon's Mechanical Turk and volunteer citizen scientists, while simultaneously capitalizing on ML models.  This innovative, hybrid approach will be the first in-depth exploration of intelligent ML/human systems that aim to reduce the workload in the production of biomedical systematic reviews. Our strong preliminary work demonstrates the promise of this general strategy.   We propose to develop hybrid approaches that combine crowdsourcing and machine learning methods to optimize the conduct of systematic reviews.  ",Hybrid Approaches to Optimizing Evidence Synthesis via Machine Learning and Crowdsourcing,9223968,R03HS025024,[' '],AHRQ,NORTHEASTERN UNIVERSITY,R03,2016,98635,-0.0126423097549892
"Development of mosaic mouse models of HCC for genetic interspecies inference DESCRIPTION (provided by applicant): Hepatocellular carcinoma (HCC) remains a menace for human health for the lack of any effective treatment. HCC is usually the end result of chronic liver diseases associated with diverse risk factors. Furthermore, non- alcoholic steatohepatitis (NASH) induced HCC, which is projected to be the leading cause of new cases, remains poorly characterized. Although many mouse models of HCC have been developed, it is unclear how well they represent different subgroups of human HCCs. This research plan proposes to transform HCC animal modeling by establishing a novel in vivo platform to accurately replicate the somatic molecular profiles of human HCC in mice. To do this, three independent HCC mouse models will be exhaustively characterized through exome sequencing and gene expression profiling. Using bioinformatics techniques including machine- learning and network analysis, these datasets will be compared with human HCC datasets from TCGA and the ICGC to identify subgroups of patients with similar somatic molecular profiles to the HCC mouse models as well as refined minimum sets of characteristic genetic aberrations. A derived transposon system will be used to generate mosaic mouse models replicating human HCC genetic subgroups faithfully. These models will enable 1) experimental dissection of the molecular mechanisms underlying distinct etiologies of HCC, 2) systematic assessment of candidate HCC therapies and 3) investigation of therapeutic resistances. This K99/R00 career development award proposal describes a two-year mentored and three-year independent research program essential for the development of Dr. Font-Burgada as an independent investigator. Dr. Font-Burgada received his PhD at University of Barcelona, Spain, for the work he performed to investigate basic chromatin regulatory and epigenetic mechanisms. He then moved to University of California, San Diego where he joined Dr. Michael Karin's laboratory to train in mouse models of cancer and signal transduction. For the accomplishment of this research proposal, Dr. Font-Burgada has designed a strong training and career development plan consisting of: 1- the continued mentorship of Dr. Michael Karin to gain additional expertise in mouse models of HCC and signal transduction, 2- Training in bioinformatics, specifically in methods to identify cancer driver genetic aberrations and network-based approaches for comparative genomic analysis of mouse and human HCCs, to be overseen by co-mentor Dr. Hannah Carter, an Assistant Professor of Medicine, at UCSD. 3- Training in application of emerging transposon vector technologies to generate mosaic mouse models for in vivo analysis of oncogenic pathways. 4- Career development courses and seminars in a supportive academic environment in the Department of Pharmacology at UCSD to complement other aspects of the training program. This training plan will be overseen by an advisory committee comprising 4 members, mentor, co-mentor, and additional experts in mouse models of cancer and bioinformatics, Inder Verma and Trey Ideker, providing key scientific insights and essential guidance in critical steps in Dr. Font-Burgada transition to independence. PUBLIC HEALTH RELEVANCE: Hepatocellular carcinoma (HCC) has a 5-year survival rate of less than 10% and leads to 700000 deaths globally each year. The genetic causes of this disease are poorly understood and the only available targeted therapy extends life expectancy by a mere 3 months. In order to improve these dismal statistics, I am developing a novel class of mouse models capable of reproducing the spectrum of mutations observed in a given human tumor, to enable study of HCC biology as well as systematic testing of therapeutic combinations.",Development of mosaic mouse models of HCC for genetic interspecies inference,9125796,K99CA191152,"['Advisory Committees', 'Animal Model', 'BRAF gene', 'Beauty', 'Bioinformatics', 'Biology', 'California', 'Cancer Etiology', 'Cataloging', 'Catalogs', 'Cell Culture Techniques', 'Cells', 'Cessation of life', 'Characteristics', 'Chromatin', 'Clinical', 'Comparative Genomic Analysis', 'Complement', 'Complex', 'Data', 'Data Set', 'Development', 'Development Plans', 'Disease', 'Dissection', 'Doctor of Philosophy', 'Drug Targeting', 'Ensure', 'Environment', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Gene Expression Profiling', 'Genetic', 'Health', 'Hepatocyte', 'Heterogeneity', 'High Fat Diet', 'Human', 'Human Pathology', 'Investigation', 'K-Series Research Career Programs', 'Knowledge', 'Label', 'Laboratories', 'Life Expectancy', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mouse Protein', 'Mus', 'Mutate', 'Mutation', 'Mutation Spectra', 'Network-based', 'Oncogenic', 'Orthologous Gene', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Phase', 'Physiological', 'Population', 'Pre-Clinical Model', 'Primary carcinoma of the liver cells', 'Research', 'Research Personnel', 'Research Proposals', 'Resistance', 'Risk', 'Risk Factors', 'Signal Transduction', 'Somatic Mutation', 'Spain', 'Subgroup', 'Survival Rate', 'System', 'Techniques', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Therapeutic', 'Training', 'Training Programs', 'Transgenes', 'Translating', 'Universities', 'Viral Vector', 'Work', 'Xenograft Model', 'accurate diagnosis', 'base', 'career development', 'chronic liver disease', 'combinatorial', 'comparative', 'design', 'effective therapy', 'exome sequencing', 'human disease', 'improved', 'in vivo', 'insight', 'learning network', 'member', 'mouse model', 'neoplastic cell', 'nonalcoholic steatohepatitis', 'novel', 'novel therapeutics', 'pre-clinical', 'professor', 'programs', 'response', 'statistics', 'success', 'targeted treatment', 'therapy resistant', 'tool', 'tumor', 'tumor progression', 'vector']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2016,109620,-0.01471170853979563
"Biomedical Data Translator Technical Feasibility Assessment and Architecture Design A fundamental challenge to translate insights between biomedical researchers, who study biological mechanisms, and clinicians, who diagnose patient symptoms, is that many links between biological processes and disease pathophysiology are poorly understood. A comprehensive Biomedical Translator must enable chains of inference across objects as diverse as genetic mutations, molecular effects, tissue-specific expression patterns, cellular processes, organ phenotypes, disease states, patient symptoms, and drug responses, a challenge beyond the scope of any one organization. Fortunately, many individual links in this chain have been made by experiments yielding statistical connections between individual data types. High-throughput perturbation screens link chemical and genetic perturbations to cellular phenotypes such as gene-expression patterns, cell survival, or changes in phosphorylation. Genetic association studies link mutations to human disease or intermediate phenotypes and biomarkers. Electronic medical records (EMR) link diseases or human phenotypes to diagnostic or current procedural terminology (CPT) codes, and clinical trials link the impact of drugs and drug candidates on disease states. In principle, incorporating these links into chains of inference could translate results between the full set of data types within them. In practice, each link is maintained by experts with domain-specific experiments, semantic terminology, and methodological standards. While a key challenge faced by a global Biomedical Translator is to establish consistent standards across these existing data types, a more important goal is to develop a principled and robust framework to (a) model biological systems and experimental approaches to investigate them; (b) organize knowledge about biological mechanism and disease; and (c) incorporate diverse datasets that serve as windows into the underlying and unknown state of nature. We propose to implement a Biomedical Translator as a probabilistic graphical model, a paradigm from artificial intelligence (AI) research. Just as separate research communities form weakly coupled parts of the translation process, graphical models allow global inferences from weakly coupled “nodes”. These inferences require each node to publish only probability distributions, enabling interoperability without necessarily having global entity-resolution standards, and benefit from paradigms for quality control, fault tolerance, and relevance assessment common in AI research. We hypothesize that a limited number of APIs, implemented as probability computations by communities around the world, would yield a Biomedical Translator as an emergent property of weakly coupled knowledge sources. From basic properties of graphical models, such a Translator could probabilistically translate among any data types connected within it, allowing for relatively complex query concepts. For example: What cellular processes in which tissues are impacted in a patient-based EMR? What genetic mutations sensitize cells to small-molecule treatment effects? Which small molecules mimic genetic “experiments of nature” that protect against disease? To illustrate the value of these resources and our architectural paradigm, we propose a demonstration project to implement a Biomedical Translator supporting queries between small molecules, biological processes, genes, and disease. The demonstration project will provide a valuable first step to confront key data-integration and organizational challenges and will enable previously impossible queries, such as identifying small molecules that perturb the same biological processes implicated by human genetics in a disease context. In this capacity, such Translator could realistically identify existing drugs for known symptoms (i.e., repurposing), but could more broadly serve as an engine for hypothesis generation and biological discovery, suggesting pre-clinical small molecules to develop based on their observed biological activity, or providing heretofore novel links between cellular protein function and disease pathophysiology. n/a",Biomedical Data Translator Technical Feasibility Assessment and Architecture Design,9337924,OT3TR002025,"['Architecture', 'Artificial Intelligence', 'Biological', 'Biological Markers', 'Biological Process', 'Cell Survival', 'Cell physiology', 'Cells', 'Clinical Trials', 'Communities', 'Complex', 'Computerized Medical Record', 'Coupled', 'Current Procedural Terminology Codes', 'DNA Sequence Alteration', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Disease', 'Functional disorder', 'Gene Expression Profile', 'Generations', 'Genetic', 'Goals', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Link', 'Modeling', 'Molecular', 'Mutation', 'Nature', 'Organ', 'Patients', 'Pattern', 'Pharmaceutical Preparations', 'Phenotype', 'Phosphorylation', 'Probability', 'Processed Genes', 'Property', 'Publishing', 'Quality Control', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Semantics', 'Source', 'Symptoms', 'Terminology', 'Tissues', 'Translating', 'Translation Process', 'base', 'biological systems', 'chemical genetics', 'data integration', 'design', 'disease phenotype', 'drug candidate', 'genetic association', 'human disease', 'insight', 'interoperability', 'novel', 'pre-clinical', 'protein function', 'research study', 'response', 'small molecule', 'treatment effect']",NCATS,"BROAD INSTITUTE, INC.",OT3,2016,399608,-0.020792734544687417
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases. PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,9123353,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency response', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'climate data', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'model building', 'news', 'novel', 'pandemic influenza', 'skills', 'social media', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,UNIVERSITY OF WASHINGTON,K01,2016,107469,0.002036301452338346
"Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity   Project Summary    For something as complex and multifaceted as bacterial antibiotic resistance (AR), our drug evaluation  paradigm is strikingly narrow and homogenous: MIC/MBC testing in standardized bacteriologic media. We  have shown that this drug evaluation paradigm is inadequate, even misleading, as changes in the media  conditions of the procedure lead to dramatically different results. A more holistic definition of antibiotic therapy  that centers on understanding antibiotic activity in synergy with host innate immune factors such as cationic  antimicrobial peptides (AMPs), serum and phagocytic cells (e.g. neutrophils) reveals therapeutic options  unrecognized in standard testing. The proposed U01 program represents a groundbreaking approach to use  systems biology approaches and inform more effective antibiotic utilization in the context of host innate  immunity. We propose to: 1) build an iterative systems biology workflow that integrates multiple experimental  and computational approaches to give a comprehensive assessment of AR; and 2) apply this workflow to high  priority pathogens to systematically elucidate AR mechanisms and their condition­dependency. The iterative  workflow includes: (i) omics and physiological data generation.  Clinically isolated strains of the selected  pathogens will be grown under conventional testing (bacteriologic media) and more physiologic conditions  (tissue culture media, serum, and in presence of AMPs and neutrophils) to probe for advantageous gain of  activity.  The omics data types collected are: DNA resequencing, RNAseq, and metabolomics.  (ii)  Bioinformatics and data modeling analysis involves three approaches: big data analysis for data set  dimensionality and coarse grained variable dependencies assessment, genome­scale modeling for  mechanistic elucidation and analysis, and machine learning that uses AR­related metadata to classify the  overall biological functions. This analysis will lead to understanding of AR mechanisms.  (iii) Multi­scale  validation from animal models, to laboratory evolution, to cytology, to gene expression alteration, to structural  protein analysis of putative targets. The validation thus ranges from host behavior to atomistic detail of  ligand­target interactions. The iterative loop then closes, comparing computational prediction to experimental  outcomes. False­negative and false­positive predictions are then algorithmically analyzed by a hypothesis  generating family of algorithms that then makes suggestions about what conditions to use in the next iteration  of the loop.  The pathogens that we will focus on are methicillin­resistant ​Staphylococcus aureus ​(MRSA), the  carbapenem­resistant Enterobacteriaceae (CRE) Klebsiella ​pneumoniae ​and ​Acinetobacter baumannii,​ and  Pseudomonas aeruginosa​. The team of investigators has made the foundational observations and led the  development of the technologies on which the iterative workflow is based. A multi­ and genome­scale methods  of systems biology fulfills requirements of RFA­AI­14­064 to which it responds.              Narrative    The current evaluation of antibiotic drug candidates in drug discovery and in clinical medicine is conducted in  laboratory media that ignores the actual physiologic conditions in the host and the host immune system.  We  have discovered potent antimicrobial activities of existing antibiotics against highly “drug­resistant superbugs”  that are currently ignored but revealed in synergy with the human immune system. This program proposes a  holistic and comprehensive systems biology approach to systematically discover novel treatment opportunities  and underlying mechanisms using a novel iterative data generation, analysis, and modeling workflow.       ",Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity,9108693,U01AI124316,"['Acinetobacter baumannii', 'Algorithms', 'Animal Model', 'Animals', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Antimicrobial Cationic Peptides', 'Bacterial Antibiotic Resistance', 'Behavior', 'Big Data', 'Bioinformatics', 'Biological Assay', 'Biological Process', 'Biological Products', 'Biology', 'Cereals', 'Clinical', 'Clinical Medicine', 'Collection', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Conditioned Culture Media', 'Culture Media', 'Cytology', 'DNA Resequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dependency', 'Drug Evaluation', 'Drug resistance', 'Effectiveness', 'Evaluation', 'Evolution', 'Family', 'Future', 'Gene Expression Alteration', 'Generations', 'Growth', 'Human', 'Immune', 'Immune system', 'Immunity', 'Infection', 'Integration Host Factors', 'Klebsiella pneumonia bacterium', 'Knowledge', 'Laboratories', 'Lead', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Maps', 'Marketing', 'Metadata', 'Methods', 'Modeling', 'Molecular Biology', 'Molecular Profiling', 'Mus', 'Natural Immunity', 'Network-based', 'Neutrophil Antimicrobial Peptides', 'Organism', 'Outcome', 'Output', 'Participant', 'Phagocytes', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Physiological', 'Predisposition', 'Procedures', 'Process', 'Protein Analysis', 'Pseudomonas aeruginosa', 'Research Personnel', 'Resistance', 'Resistance development', 'Risk Assessment', 'Series', 'Serum', 'Statistical Data Interpretation', 'Structural Genes', 'Suggestion', 'Superbug', 'Systems Biology', 'Testing', 'Therapeutic', 'Update', 'Validation', 'antimicrobial', 'bacterial genetics', 'base', 'carbapenem-resistant Enterobacteriaceae', 'data modeling', 'design', 'drug candidate', 'drug discovery', 'experience', 'genome sequencing', 'genome-wide', 'human tissue', 'improved', 'in vivo', 'macromolecule', 'metabolomics', 'methicillin resistant Staphylococcus aureus', 'microbial', 'microbial host', 'multi-drug resistant pathogen', 'multidrug-resistant Pseudomonas aeruginosa', 'neutrophil', 'novel', 'pathogen', 'product development', 'programs', 'reconstruction', 'resistance mechanism', 'response', 'screening', 'technology development', 'tissue culture', 'tool', 'transcriptome sequencing', 'transcriptomics', 'treatment response']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2016,1989374,-0.00880788892583872
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9041640,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'genomic data', 'hazard', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2016,255295,-0.023047742273927963
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9100683,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2016,680020,-0.0010096845818889015
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9097763,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2016,52816,-0.010295532070478805
"Predicting Resilience in the Human Microbiome DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience. PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.",Predicting Resilience in the Human Microbiome,9125723,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiome', 'gut microbiota', 'human subject', 'innovation', 'learning strategy', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'resilience', 'response', 'theories', 'tool', 'transcriptome', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2016,413065,0.00828655626121224
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation. n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,9097814,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Genetic', 'Genetic Structures', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Internships', 'K-12 student', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Staging', 'Statistical Data Interpretation', 'Statistical Models', 'Structure', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'fluorescence imaging', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'neuroinformatics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'quantitative imaging', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2016,323032,-0.016011617230946868
"Dopaminergic encoding of counterfactual information in human striatum ﻿    DESCRIPTION (provided by applicant): Diseases and disorders directly affected by dopamine systems (e.g., drug addiction and Parkinson's disease) highlight the importance of these systems in motivated human behavior and cognition. The dopamine system is known to be a critical component of normal learning, reward processing, and decision-making (reviewed in Montague et al., 2004). Unfortunately, our present knowledge of dopamine systems in human brains is relatively sparse compared to the wealth of experimentation and computational modeling on these systems in rodents and non-human primates. Previously, technological constraints have limited direct experimentation in human brains. This proposal capitalizes on our group's recent technological innovation, which was supported by the NIH R21 mechanism - CEBRA: R21DA024140 - and resulted in the successful completion of the first sub-second measurements of dopamine release in a human brain. Furthermore, these measurements took place during an active decision-making task that was framed by computational models of learning and reward processing (Kishida et al., 2011 and Kishida et al., under review). We propose to pursue three specific aims, which combine our technological advance with active learning tasks designed to probe the role of dopamine in human behavior. Our aims incorporate three learning signals, where actual and counterfactual experience will each be examined in human striatal responses. The proposed work will inform on the controversial role for dopamine in reward/movement interactions. The experiments proposed will yield unprecedented insight into the function of the dopamine system in the humans afflicted with Parkinson's disease and Essential Tremor. With the support of the NIH (R21DA024140), our team successfully developed a complete prototype system for making electrochemical measurements of dopamine delivery in the human brain. Feasibility has been demonstrated by obtaining the first dopamine measurements in the striata of subjects with Parkinson's during a decision-making task. This substantial preliminary work is now ready for a larger scale with specific hypothesis testing about the role of dopamine systems in Parkinson's disease, Essential tremor, and human decision-making and behavior. PUBLIC HEALTH RELEVANCE: Experiments and computational models (primarily investigated in rodents and non-human primates) suggest dopamine systems in the brain are essential for normal learning, reward-processing, and ongoing decision- making. This proposal builds upon our group's innovative work, which - for the first time - recorded sub- second measurements of dopamine in human subjects and amalgamated these measurements with computational models of dopamine function. Herein we propose to use this newly developed technology to generate unprecedented insight into the function of dopamine systems directly in humans.",Dopaminergic encoding of counterfactual information in human striatum,9131824,R01NS092701,"['Active Learning', 'Acute', 'Affect', 'Animal Model', 'Behavior', 'Brain', 'Cells', 'Cessation of life', 'Cognition', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Coupling', 'Data', 'Decision Making', 'Disease', 'Dopamine', 'Drug Addiction', 'Electrodes', 'Essential Tremor', 'Exploratory/Developmental Grant', 'Funding', 'Health', 'Human', 'Knowledge', 'Learning', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement', 'Movement Disorders', 'Operative Surgical Procedures', 'Parkinson Disease', 'Participant', 'Patients', 'Phase', 'Play', 'Replacement Therapy', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Sample Size', 'Scanning', 'Signal Transduction', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'blood oxygenation level dependent response', 'cognitive function', 'design', 'dopamine system', 'dopaminergic neuron', 'experience', 'extracellular', 'human subject', 'implantation', 'innovation', 'insight', 'nonhuman primate', 'prototype', 'putamen', 'reduce symptoms', 'relating to nervous system', 'research study', 'response', 'reward processing', 'social', 'symptomatic improvement', 'technological innovation', 'temporal measurement']",NINDS,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2016,492125,-0.0040301608457890626
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,9135552,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Enrollment', 'Equilibrium', 'Event', 'Formulation', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Polynomial Models', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'data to knowledge', 'flexibility', 'genetic makeup', 'genetic signature', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'patient subsets', 'personalized medicine', 'predictive marker', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2016,324169,-0.014877428980211953
"Impact of Gastrointestinal Microbiome on the Host Response to an Effector-Memory T-Cell AIDS Vaccine and Multiple Low-Dose Challenge ﻿    DESCRIPTION (provided by applicant): This computational research project will analyze gastrointestinal microbiome, host cellular immune response, and host transcriptional response datasets derived from a large-scale rhesus macaque AIDS vaccine study. The vaccine study includes 70 animals divided into five groups and is designed to test the efficacy of a cytomegalovirus (CMV) vector expressing the major simian immunodeficiency virus (SIV) proteins. In previous studies, RhCMV/SIV vaccines elicit a robust effector-memory T cell response. After challenge with highly pathogenic SIVmac239, approximately 50% of RhCMV/SIV-vaccinated monkeys exhibit a pattern of viral control that is characterized by a ""blip"" of viremia followed by control of plasma viremia to undetectable levels. One to three years later, RhCMV/SIV-protected monkeys show no sign of SIV infection, suggesting immune-mediated clearance of the virus. The mechanisms responsible for this 50% efficacy are not clear, but may reflect T cell responses at the site of SIV entry or sites of early replication. Thi project will investigate the role of the gastrointestinal microbiome on RhCMV/SIV-induced effector-memory T cell responses and protection against SIVmac239 challenge. The project includes two computationally based Specific Aims: 1) Define the composition of the gut microbiome prior to and after vaccination and during repeated limiting-dose SIVmac239 challenge; and 2) Determine how the composition of the gut microbiome correlates with protective immune cell responses and vaccine-induced host transcriptional responses. In Aim 1, 16S ribosomal sequence data (obtained from rectal swabs) will be used to determine operational taxonomic units. Variation in bacterial species over time and within individual animals will be determined using Mothur and Qiime metagenomic software.  Nonparametric statistical analyses and co-occurrence and co-exclusion networks will be used to identify bacterial species associated with progression or control of SIV infection after repeated limiting-dose intrarectal SIVmac239 challenge. In Aim 2, principle component analysis will be used to identify associations between microbiome composition and SIV-specific CD4+ and CD8+ T cell responses. Gene module-based and correlation network-based approaches will be used to determine associations between microbiome composition and host transcriptional responses (collected from whole blood samples). Together, these analyses will allow us to better understand microbiome-host interactions and their role in eliciting a protective vaccine-derived effector-memory T cell response. Such understanding will inform future vaccination strategies and attempts to modulate microbiome composition to affect vaccine efficacy. PUBLIC HEALTH RELEVANCE: The types and numbers of bacterial species that live in the gastrointestinal tract (together known as the microbiome) affect many aspects of human health and disease, including how individuals respond to vaccination and viral infection. In this study, we will define the gastrointestinal microbiome of rhesus monkeys before and after vaccination with an experimental AIDS vaccine and after infection with a monkey immunodeficiency virus. This study will allow us to determine how the microbiome impacts the effectiveness of the vaccine and may suggest whether altering the microbiome may improve vaccine performance.",Impact of Gastrointestinal Microbiome on the Host Response to an Effector-Memory T-Cell AIDS Vaccine and Multiple Low-Dose Challenge,9267858,R21AI120713,"['AIDS Vaccines', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Animals', 'Blood specimen', 'CD8B1 gene', 'Cells', 'Communities', 'Computer Analysis', 'Computer software', 'Confounding Factors (Epidemiology)', 'Cytomegalovirus', 'Data', 'Data Set', 'Databases', 'Diet', 'Disease', 'Dose', 'Dose-Limiting', 'Environmental Risk Factor', 'Exclusion', 'Exhibits', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Future', 'Gastrointestinal tract structure', 'Gene Expression', 'Generations', 'Genes', 'Goals', 'Head', 'Health', 'Health Sciences', 'Human', 'Immune', 'Immune response', 'Immunologic Deficiency Syndromes', 'Individual', 'Infection', 'Joints', 'Laboratories', 'Lead', 'Life', 'Life Style', 'Location', 'Macaca mulatta', 'Machine Learning', 'Mediating', 'Memory', 'Metagenomics', 'Microbiology', 'Modeling', 'Molecular Profiling', 'Monkeys', 'National Institute of Allergy and Infectious Disease', 'Network-based', 'Oregon', 'Pattern', 'Performance', 'Plasma', 'Process', 'Research', 'Research Assistant', 'Research Project Grants', 'Role', 'SIV', 'Sampling', 'Scientist', 'Site', 'Swab', 'T cell response', 'T memory cell', 'Testing', 'Time', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Variant', 'Viral', 'Viral Proteins', 'Viremia', 'Virus', 'Virus Diseases', 'Washington', 'Whole Blood', 'base', 'design', 'efficacy testing', 'functional genomics', 'gut microbiome', 'improved', 'microbiome', 'nonhuman primate', 'professor', 'rectal', 'response', 'vaccination strategy', 'vaccine development', 'vaccine effectiveness', 'vaccine efficacy', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity', 'vector']",NIAID,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R21,2016,227250,-0.007272756567280649
"Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens ﻿    DESCRIPTION (provided by applicant): Infections caused by antibiotic-resistant bacterial pathogens are exceedingly common in immunocompromised hosts. Patients undergoing allogeneic hematopoietic stem cell transplantation (allo-HSCT) are particularly susceptible to these infections and are the patient population our studies will focus upon. Our goal is to extend and further develop systems biology approaches that our group has pioneered to identify mechanisms by which the intestinal microbiota confers resistance to infection by Vancomycin-resistant Enterococcus (VRE), antibiotic-resistant Klebsiella pneumoniae (arKp) and Clostridium difficile (C. diff). Aim 1 of our project is to establish a clinical database from the hospital recrds of allo-HSCT patients during their initial hospitalization that will include all laboratory values,vital signs, pharmacy data, dietary data, symptoms and physical exam findings. Aim 2 will expand our fecal bank by collecting fecal samples from approximately 160 allo-HSCT patients per year and determining the presence/absence of VRE, arKp and C. diff by culture and PCR. We will use NGS of 16S rRNA genes to determine microbiota composition on each sample, will perform metagenomic and RNA sequencing to determine the bacterial transcriptome and perform metabolomic analyses on a selected subset of fecal samples. Aim 3 is to extend our mathematical modeling to identify specific members of the microbiota, metabolic pathways and metabolic products that correlate with resistance to VRE or arKp expansion in the GI tract or are associated with resistance to C. diff infection. The clinical database will be used to establish correlations between clinical treatments or events and changes in the intestinal microbiota or the expression of bacterial metabolic pathways. Ultimately, the computational platforms developed in aim 3 will identify bacterial species or consortia that are associated with resistance to infection and Aim 4 will test these associations in germ-free mouse models. We will culture bacterial species associated with resistance, colonize mice with these protective bacteria and test for resistance against VRE, arKp and C. diff. Samples obtained from these experimental studies will be subjected to metagenomic and metabolomic analyses to further refine, in an iterative fashion, computational models developed in aim 3. Our proposed studies will develop new and extend existing computational models to identify bacterial species and molecular mechanisms that confer resistance to antibiotic-resistant bacterial infections.         PUBLIC HEALTH RELEVANCE: The normal bacteria inhabiting the human intestine provide a high level of resistance against antibiotic-resistant bacterial pathogens. We are investigating the intestinal flora of hospitalized patients and using mathematical modeling to identify bacterial species and their metabolic products that reduce the risk of infection by three prevalent antibiotic-resistant bacteria. These studies may lead to the development of new approaches to treat and prevent antibiotic-resistant infections.        ",Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens,9108539,U01AI124275,"['Allogenic', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bacterial Infections', 'Categories', 'Clinical', 'Clinical Treatment', 'Clostridium difficile', 'Collection', 'Computer Simulation', 'Data', 'Databases', 'Deposition', 'Development', 'Diet', 'Dietary intake', 'Enrollment', 'Event', 'Feces', 'Gastrointestinal tract structure', 'Germ-Free', 'Gnotobiotic', 'Goals', 'Growth', 'Hematopoietic Stem Cell Transplantation', 'Hospitalization', 'Hospitals', 'Human', 'Immune', 'Immunocompromised Host', 'Infection', 'Integration Host Factors', 'Intestinal Content', 'Intestines', 'Klebsiella pneumonia bacterium', 'Laboratories', 'Lead', 'Machine Learning', 'Measurable', 'Mediating', 'Medical Records', 'Memorial Sloan-Kettering Cancer Center', 'Metabolic', 'Metabolic Pathway', 'Metagenomics', 'Modeling', 'Modification', 'Molecular', 'Mus', 'Patient risk', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Population', 'Predisposition', 'Resistance', 'Resistance to infection', 'Risk', 'Sampling', 'Systems Biology', 'Testing', 'Toxic effect', 'Transplantation', 'Uncertainty', 'Vancomycin Resistance', 'Vancomycin resistant enterococcus', 'antimicrobial drug', 'commensal microbes', 'computer studies', 'design', 'falls', 'gut microbiota', 'immune activation', 'mathematical model', 'member', 'metabolome', 'metabolomics', 'metagenome', 'metagenomic sequencing', 'microbiome', 'microbiota', 'microorganism interaction', 'mouse model', 'network models', 'novel', 'novel strategies', 'pathogen', 'patient population', 'physical symptom', 'prevent', 'public health relevance', 'rRNA Genes', 'research study', 'resilience', 'resistance mechanism', 'text searching', 'transcriptome', 'transcriptome sequencing', 'transcriptomics']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,U01,2016,1722908,-0.008919107381441535
"The q-bio Summer School DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required. PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.",The q-bio Summer School,9002062,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Career Mobility', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Health', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Professional Competence', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'Underrepresented Groups', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'learning materials', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2016,168780,-6.362309526271044e-05
"Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression Project Summary/Abstract This project will develop a new technological approach for the comprehensive analysis of adaptive immune responses, which holds the potential to catalyze new strategies to prevent and treat disease. Here we will apply immune profiling techniques recently invented by the PI to investigate the mechanisms of Epstein-Barr virus (EBV) adaptive immune control in clinical cohorts of infected patients. EBV is a highly prevalent pathogen infecting >90% of the world’s population. Primary EBV infection often causes infectious mononucleosis (IM) and long-term sequelae include numerous malignancies, lymphoproliferative disorders, and a strong association with multiple sclerosis. No EBV vaccine is approved to date, and the molecular mechanisms of immune protection from EBV-associated diseases are unclear. Unfortunately, prior technical barriers in high- throughput immune profiling methods have prevented a comprehensive understanding of adaptive immune protection against EBV diseases. A technological approach that identifies the critical features of EBV immune protection will advance new solutions for vaccine and therapeutic development. Therefore, we developed an experimental pipeline to enable rapid and cost-effective analysis of B- and T-cell responses to EBV that is scalable to dozens of human patients per experiment. We hypothesize that a comprehensive B- and T-cell analysis of carefully selected patient cohorts that either can or cannot suppress symptomatic infection will reveal function-based correlates of EBV control. To test this hypothesis, we will apply quantitative immune profiling technologies to analyze cryopreserved longitudinal samples from recently completed prospective clinical studies of IM. Patient samples in our cohort span pre- and post-infection through convalescence and encompass the full range of clinical IM severity scores (from 0, asymptomatic primary infection, to 6, essentially bedridden with IM). Immune profile data will be used to establish adaptive immune correlates of IM disease severity. In addition, we will analyze immune responses in apparently immunocompetent patients with chronic active EBV (CAEBV) disease, or patients who do not adequately suppress EBV infection, to gain insight regarding adaptive immune function and dysfunction in CAEBV. Finally, we will develop a new computational toolkit to rapidly identify immune correlates from high-throughput datasets. Successful completion of this project will constitute the first comprehensive functional B- and T-cell receptor analysis in a human clinical cohort. Our efforts will provide a repertoire-scale, mechanistic understanding of adaptive immunity to EBV and suggest new strategies for treatment and prevention of EBV-associated diseases. Our long-term goal is to develop human immune profiling techniques as a platform approach to accelerate the rational design of vaccines and therapeutics against pathogens of high public health importance, beginning with EBV. Project Narrative This project will apply new high-throughput immune profiling technologies to elucidate the features of effective Epstein-Barr virus (EBV) immune control. EBV causes a range of human diseases including infectious mononucleosis and several forms of cancer; however, limited EBV treatment options are available and no approved preventive EBV vaccines exist. Our long-term objective is to apply enhanced understanding of adaptive immunity to accelerate the rational development of new vaccines and therapeutics.",Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression,9212615,DP5OD023118,"['Address', 'Antibodies', 'B cell repertoire', 'B-Lymphocytes', 'Burkitt Lymphoma', 'CD8B1 gene', 'Cells', 'Chronic', 'Clinical', 'Clinical Research', 'Convalescence', 'Cost Effectiveness Analysis', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'EBV-associated disease', 'Ensure', 'Epstein-Barr Virus Infections', 'Exhibits', 'Fatigue', 'Future', 'Goals', 'Herpesviridae', 'Herpesviridae Infections', 'Hodgkin Disease', 'Human', 'Human Herpesvirus 4', 'Immune', 'Immune System Diseases', 'Immune response', 'Immunocompetent', 'Immunologic Receptors', 'Incidence', 'Individual', 'Infection', 'Infectious Mononucleosis', 'Intervention', 'Knowledge', 'Large-Cell Immunoblastic Lymphoma', 'Lead', 'Lymphoproliferative Disorders', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Multiple Sclerosis', 'Nasopharynx Carcinoma', 'Oncogenic', 'Patients', 'Phase', 'Population', 'Prevention', 'Preventive', 'Primary Infection', 'Public Health', 'Questioning individuals', 'Receptors, Antigen, B-Cell', 'Recovery', 'Research Personnel', 'Risk', 'Sampling', 'Serum', 'Severities', 'Severity of illness', 'Stomach Carcinoma', 'Symptoms', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'Techniques', 'Technology', 'Teenagers', 'Testing', 'Therapeutic', 'Vaccine Design', 'Vaccines', 'Viral', 'Virus Diseases', 'Work', 'abstracting', 'adaptive immunity', 'base', 'career', 'cohort', 'cost', 'design', 'disorder control', 'human disease', 'immune function', 'insight', 'multiple sclerosis patient', 'neutralizing monoclonal antibodies', 'next generation', 'novel', 'novel therapeutics', 'novel vaccines', 'pathogen', 'pressure', 'prevent', 'prospective', 'research study', 'response', 'therapeutic development', 'therapeutic vaccine', 'treatment strategy', 'vaccine development', 'vaccine trial', 'virology', 'young adult']",OD,UNIVERSITY OF KANSAS LAWRENCE,DP5,2016,369806,-0.008479642874375939
"New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis ﻿    DESCRIPTION (provided by applicant)    Maha R Farhat, MD is an Instructor of Medicine at Harvard Medical School on the tenure track and a staff physician in the Department of Pulmonary and Critical Care Medicine at Massachusetts General Hospital. She is completing a masters of biostatistics at the Harvard School of Public Health in 5/2015. She has spent the last 4.5 years acquiring skills in Mycobacterium tuberculosis biology, epidemiology, bioinformatics and biostatistics. She has experience in the analysis of whole genome sequence data, drug resistance data and patient clinical outcome data with the focus of identifying Mycobacterium tuberculosis genetic determinants of drug resistance. She has also developed new methods in this area. Dr. Farhat has 11 publications 5 of which are first author including high impact and highly cited work in the journals Nature Genetics, Genome Medicine and the International Journal of Tuberculosis and Lung Disease. The short term goals of this K01 award are to provide training for Dr. Farhat in critical aspects of data science, computational and evolutionary biology, advanced biostatistics and network science. Dr. Farhat's long term goal is to become a leader in the field of Big Data analysis for infectious diseases. The proposed research as well as the training activities outlined in the proposal will successfully position Dr. Farhat for her first R01 and an independent career as a physician scientist. Environment: Dr. Farhat will perform the interdisciplinary work outlined in this proposal at the distinguished Harvard Departments of Global Health Social Medicine, Biostatistics, Evolutionary biology and the Institute for Quantitative Social Sciences. Dr. Farhat' mentorship team will include two world renowned leaders in the fields of infectious diseases and Big Data, Dr. Megan Murray and Dr. Gary King; and two rising stars in the fields of network Science and evolutionary Biology, Dr. JP Onnela and Dr. Michael Desai. Dr. Murray, the principal mentor on this proposal has mentored over 38 trainees, 9 of which have went on to have independent research careers, and 6 competed successfully for K awards. She is also PI on two recently awarded NIH/NIAID grants a CETR U19 and a TBRU U19 and has over 350 peer reviewed publications. To complement the expertise of her mentors Dr. Farhat will be advised by Dr. Christiani a practicing pulmonary and critical care physician and world renowned researcher in the field of lung and environmental genetics. She will also collaborate and consult with Dr. Merce Crosas, a data scientist, and Dr. Pardis Sabeti, a computational biologist. She will rotate through Dr. Soumya Raychaudhuri's bioinformatics laboratory to diversify her exposure to biomedical Big Data. In addition, she will receive formal training in evolutionary biology, Bayesian and mixed-model biostatistics, computer science, leadership skills and grant writing. The collaborative opportunities, intellectual environment and resources available to Dr. Farhat are outstanding. Research: Infectious diseases continue to be a major cause of morbidity and mortality. Despite the availability of effective antimicrobials, pathogens are successfully evolving new disease phenotypes that allow them to resist killing by these drugs or in other instances cause more severe disease manifestations or wider chains of transmission. Drug resistance (DR) is now common and some bacteria have even become resistant to multiple types or classes of antibiotics6. A key strategy in the fight against emerging pathogen phenotypes in infectious diseases is surveillance, and early personalized therapy to prevent transmission and propagation of these strains. The timely initiation of antibiotic therapy to which the pathogen is sensitive has been shown to be the key factor influencing treatment outcome for a diverse array of infections. Molecular tests that rely on the detection of microbial genetic mutations are particularly promising for surveillance and diagnosis of these pathogen phenotypes but rely on a comprehensive understanding of how mutations associate with these pathogen phenotypes. Currently there is an explosion of data on pathogen whole genome sequences (WGS) that is increasingly generated from clinical laboratories. Data on disease phenotype may also be available, but methods for the analysis and interpretation of these Big Data are lagging. Here I propose tools to aid in this analysis leveraging Big Data sets from Mycobacterium tuberculosis (MTB) and my prior work. Specifically I propose to (1) develop a web-based public interface to several analysis tools, including a statistical learning model that can predict the MTB DR phenotype from its genomic sequence, (2) to develop and study an MTB gene-gene network, based on WGS data, to improve our understanding of the effect of mutation-mutation interactions on the DR phenotype, and (3) study the performance of methods in current use for the association of genotype and phenotype in pathogens, and develop a generalizable power calculator for the best performing method. PUBLIC HEALTH RELEVANCE    Infectious agents of disease are successfully evolving drug resistance and other adaptations that threaten human health. Understanding the genetic mutations that underlie these disease phenotypes can inform surveillance and diagnostic strategies to combat this threat. Here I propose to develop accessible tools for pathogen genomic analysis that will help identify which genetic mutations are relevant to disease.",New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis,9147601,K01ES026835,"['Affect', 'Antibiotic Therapy', 'Antibiotics', 'Area', 'Award', 'Bacteria', 'Big Data', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Cessation of life', 'Clinical', 'Collaborations', 'Communicable Diseases', 'Complement', 'Consult', 'Critical Care', 'DNA Sequence Alteration', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug resistance', 'Drug resistance in tuberculosis', 'Drug-sensitive', 'Environment', 'Epidemiology', 'Explosion', 'Exposure to', 'Future', 'Gene Structure', 'General Hospitals', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Epistasis', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Health', 'Human', 'Infection', 'Infectious Agent', 'Infectious Diseases Research', 'Institutes', 'International', 'Journals', 'K-Series Research Career Programs', 'Laboratories', 'Leadership', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Maps', 'Massachusetts', 'Measures', 'Medical Genetics', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Mentorship', 'Methods', 'Microbial Genetics', 'Modeling', 'Molecular', 'Monitor', 'Morbidity - disease rate', 'Mutation', 'Mycobacterium tuberculosis', 'National Institute of Allergy and Infectious Disease', 'Nature', 'Network-based', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Phylogeny', 'Physicians', 'Population', 'Positioning Attribute', 'Public Health', 'Public Health Schools', 'Publications', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Resources', 'Science', 'Scientist', 'Severity of illness', 'Social Medicine', 'Social Sciences', 'Statistical Data Interpretation', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Tuberculosis', 'United States National Institutes of Health', 'Variant', 'Virulent', 'Work', 'Writing', 'analytical tool', 'antimicrobial', 'base', 'big biomedical data', 'burden of illness', 'career', 'combat', 'computer science', 'design', 'disease phenotype', 'experience', 'fight against', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'global health', 'improved', 'instructor', 'killings', 'medical schools', 'microbial', 'microbial genome', 'mortality', 'pathogen', 'pathogen genome', 'personalized medicine', 'power analysis', 'prevent', 'prospective', 'simulation', 'skills', 'tenure track', 'tool', 'transmission process', 'whole genome']",NIEHS,MASSACHUSETTS GENERAL HOSPITAL,K01,2016,49355,-0.013874960631570248
"Supporting Systematic Review Production with Article Similarity Network Visualization PROJECT SUMMARY Systematic reviews (SRs), or systematic reviews of literature, summarize evidence drawn from high quality studies, and are often the preferred source of evidence-based practice (EBP). However, conducting an SR is labor-intensive and time consuming, typically requiring several months to complete. It has been reported that more than ten thousands of SRs are needed to synthesize existing medical knowledge. An Article screening process is one of the most intensive and time consuming steps, which requires SR researchers to screen a large amount of references, ranging from hundreds to more than 10,000 articles, depending on the size of a SR. In the past 10 years, machine learning model training approaches24-29 were developed to accelerate the article selection process through automation. However, they are not widely used due to diffusion challenges.7,14 Major obstacles include 1) a training sample is required to generate the automation algorithm. If the training sample is biased, the article selection process will systematically fail; 2) the automation approach is not made available for non-computer science specialists, therefore SR researchers will not be able to “fine-tune” the automation algorithm for particular conditions in various SR topics; 3) As there is no global automation algorithm, the generalizability is significantly limited; 4) It is difficult to assess the actual workload saved, while finding every relevant article is required in SR. We propose a new approach to provide views of article relationships in an article network. This is different from other bibliometric networks constructing citation, co-author, or co-occurrence networks. Article network is a simple and logical concept: visualizing article relationships and distribution based on articles' similarities in titles, abstracts, keywords, publication types, etc. SR researchers can also alter the article distribution by adjusting the similarities. This approach does not aim to suggest an end-point of the screening process. Rather, it provides a view of distribution for included, excluded, and undecided articles. In the proposed research, we will integrate advanced techniques to sparsify article networks with mixed sparsification methods, and improve the quality and efficiency of large network visualization layouts by constructing a multi-level network structure and advanced force model. We aim to provide approaches to sparsify and visualize article networks with more than 10,000 articles. Our approach is highly generalizable that it can be used for any health science topics. By viewing the article distribution, SR researchers will be able to screen a large amount of literature more efficiently. This approach can be integrated into current SR technologies and used directly by SR researchers. The success of this project can support SR production on any health science topics, and thus streamline their ultimate application in EBP paradigms. PROJECT NARRATIVE Systematic reviews (SRs) provide the highest quality of research evidence for patient care. To accelerate the production of SRs, we will implement advanced visualization techniques to view article relationships and distribution with article networks and in a timely and human readable manner. The success of the project will support SR production and thus streamline their ultimate application to evidence-based practice.",Supporting Systematic Review Production with Article Similarity Network Visualization,9227858,R03HS025047,[' '],AHRQ,OHIO STATE UNIVERSITY,R03,2016,100000,-0.0077655196375842886
"Understanding host-virus interactions at the single cell level No abstract available PUBLIC HEALTH RELEVANCE: Bacteriophages mediate horizontal gene transfer (HGT), the exchange of novel genetic materials between bacteria, transforming otherwise benign bacteria into human pathogens and driving the rapid emergence of pathogens resistant to existing treatments. Accordingly, there is a critical need to elucidate when, where, and how phage-mediated HGT occurs. Our proposal focuses on using novel technologies to answer these questions for bacteriophage lambda and its host, E. coli.        ",Understanding host-virus interactions at the single cell level,9123197,F32GM119319,"['Algorithms', 'Automobile Driving', 'Bacteria', 'Bacterial Physiology', 'Bacteriophage lambda', 'Bacteriophages', 'Benign', 'Biochemical', 'Biological Models', 'Biology', 'Carbon', 'Cell Size', 'Cells', 'Computer Vision Systems', 'Consensus', 'Coupled', 'Cytolysis', 'DNA', 'Data', 'Data Analyses', 'Decision Making', 'Development', 'Disease', 'Enzymes', 'Escherichia coli', 'Genetic', 'Genetic Materials', 'Genome', 'Goals', 'Health', 'Horizontal Gene Transfer', 'Human', 'Image', 'Individual', 'Infection', 'Label', 'Lead', 'Learning', 'Libraries', 'Literature', 'Lysogeny', 'Lytic', 'Manuals', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Modern Medicine', 'Molecular', 'Molecular Target', 'Outcome', 'Peptide Hydrolases', 'Phosphorus', 'Physiological', 'Physiology', 'Plasmids', 'Play', 'Probability', 'Process', 'Proteins', 'Regulatory Element', 'Reporter', 'Reporting', 'Resistance', 'Role', 'Source', 'System', 'Technology', 'Temperature', 'Testing', 'Viral Physiology', 'Viral Proteins', 'Viral Regulatory Proteins', 'Virus', 'Weight', 'Work', 'base', 'cellular imaging', 'genetic regulatory protein', 'imaging Segmentation', 'live cell imaging', 'microbial', 'new technology', 'novel', 'pathogen', 'protein function', 'public health relevance', 'research study', 'transcription factor', 'virus host interaction']",NIGMS,STANFORD UNIVERSITY,F32,2016,62010,-0.03202723310846988
"CRCNS: Representational foundations of adaptive behavior in natural and artificial  ﻿    DESCRIPTION (provided by applicant): Overview: Among the most celebrated success stories in computational neuroscience is the discovery that many aspects of decision-making can be understood in terms of the formal framework of reinforcement learning (RL). Ideas drawn from RL have shed light on many behavioral phenomena in learning and action selection, on the functional anatomy and neural processes underlying reward-driven behavior, and on fundamental aspects of neuromodulatory function. However, for all these successes, RL-based work is haunted by an inconvenient truth: Standard RL algorithms scale poorly to large, complex problems. If human learning and decision-making are driven by RL-like mechanisms, how is it that we cope with the kinds of rich, large-scale tasks that are typical of everyday life? Existing research in both psychology and neuroscience hints at one answer to this question: Complex problems can be conquered if the decision-maker is equipped with compact, intelligently formatted representations of the task. This principle is seen in studies of expert play in chess, which show that chess masters leverage highly integrative internal representations of board configurations; in studies of frontal and parietal lobe function, which have revealed receptive fields strongly shaped by task contingencies; and studies on the hippocampus, which point to the role of this structure in supporting a hierarchically organized 'cognitive map,' of task space. Not coincidentally, the critical role of representation has come increasingly to the fore in RL-based research in machine learning and robotics, with growing interest in techniques for dimensionality reduction, hierarchy and deep learning.            The present project aims toward a systematic, empirically validated account of the role of representation in supporting RL and goal-directed behavior at large. The project brings together three investigators with complementary expertise in cognitive and computational neuroscience (Botvinick, Gershman) and machine learning and robotics (Konidaris). Together, we propose an integrative, interdisciplinary program of research, applying behavioral and neuroimaging work with human subjects, computational modeling of neurophysiological and behavioral data, formal mathematical work and simulations with artificial agents. The proposed studies are diverse in theme and method, but work together toward a theory that is both formally grounded and empirically constrained. At a more concrete level, our research focuses on four specific classes of representation, considering the computational impact of each for RL, as well as the relevance of each to neuroscience and human behavior. As detailed in our Project Description, these include (1) metric embedding, (2) spectral decomposition, (3) hierarchical representation and (4) symbolic representation. In addition to investigating the implications of each of these four forms of representation individually, we hypothesize that they fit together into a tiered system, which works as a whole to support the sometimes competing demands of learning and action control.         Intellectual Merit (provided by applicant): Understanding how representational structure impacts learning and decision making is a core challenge in cognitive science, behavioral neuroscience and, artificial intelligence. Success in establishing a computationally explicit, empirically validated theory in this area, with a specific focus on the role of representation in R, would represent an important achievement with wide repercussions. The strategy of leveraging conceptual tools from machine learning to investigate human behavior and brain function can offer considerable scientific leverage, as our own previous research illustrates. The proposed work is motivated by and builds upon established lines of research, bringing these together in order to capitalize on opportunities for synergy. In addition to answering specific empirical and computational questions, the proposed work aims to open up new avenues for future research in an important area of inquiry.         Broader Impact (provided by applicant): The proposed work lies at the crossroads of neuroscience, psychology, artificial intelligence and machine learning, and promises to advance the growing exchange among these fields. The project brings together investigators with contrasting disciplinary affiliations, with the explicit goal of bridging between intellectual cultres. The proposed work is likely to find a wide scientific audience, given its relevance to cognitive and developmental psychology, behavioral, cognitive and systems neuroscience, and behavioral economics. However, the work is likely to be of equal interest within artificial intelligence, machine learning, and robotics, where a current challenge is precisely to understand how representation learning can allow RL to scale up to large problems. Representational approaches to RL are already of intense interest within industry, where the present investigators have a record of active engagement. The topic of the proposed work has applicability in other areas as well, including education and training, and military and medical decision support. The plan for the project has a robust training component at both graduate and postdoctoral levels, with a commitment to fostering involvement of underrepresented minorities, as well as international engagement.                 n/a",CRCNS: Representational foundations of adaptive behavior in natural and artificial ,9052441,R01MH109177,"['Accounting', 'Achievement', 'Adaptive Behaviors', 'Algorithms', 'Anatomy', 'Area', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Cognitive Science', 'Complex', 'Computer Simulation', 'Data', 'Decision Making', 'Fostering', 'Foundations', 'Goals', 'Hippocampus (Brain)', 'Human', 'Industry', 'International', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Military Personnel', 'Neurosciences', 'Parietal Lobe', 'Play', 'Process', 'Psychological reinforcement', 'Psychology', 'Research', 'Research Personnel', 'Rewards', 'Robotics', 'Role', 'Shapes', 'Structure', 'System', 'Techniques', 'Training', 'Training and Education', 'Underrepresented Minority', 'Work', 'base', 'behavioral economics', 'cognitive neuroscience', 'cognitive system', 'computational neuroscience', 'coping', 'developmental psychology', 'driving behavior', 'frontal lobe', 'human subject', 'interest', 'neuroimaging', 'neurophysiology', 'neuroregulation', 'programs', 'receptive field', 'relating to nervous system', 'scale up', 'simulation', 'success', 'theories', 'tool']",NIMH,PRINCETON UNIVERSITY,R01,2015,425468,-0.01209927178109561
"Genome Based Influenza Vaccine Strain Selection  using Machine Learning ﻿    DESCRIPTION (provided by applicant):     Influenza A virus causes both pandemic and seasonal outbreaks, leading to loss of from thousands to millions of human lives within a short time period. Vaccination is the best option to prevent and minimize the effects of influenza outbreaks. Rapid selection of a well-matched influenza vaccine strain is the key to developing an effective vaccination program. However, this is a non-trivial task due to three major challenges in influenza vaccine strain selection: labor an time intensive virus isolation and serology-based antigenic characterization, poor growth of selected strains in chicken embryonic eggs during production, and biased sampling in influenza surveillance. Each year, many scientists worldwide, including thousands from the United States, are working altogether to select an optimal vaccine strain. However, incorrect vaccine strains have still been frequently chosen in the past decades.  Recent advances in genomic sequencing allow us to rapidly and economically sequence influenza genomes from the isolates and from the clinical samples. Sequencing influenza genomes has become a routine and important component in influenza surveillance. The objectives of this project are to develop a sequence-based strategy for influenza antigenic variant identification and to optimize vaccine strain selection using genomic data. To achieve these aims, we will develop machine learning based computational methods to estimate antigenic distances among influenza viruses by directly using their genome sequences. We will then identify the key residues and mutations in influenza genomes affecting influenza antigenic drift events. Such information will allow us to select most promising virus strains as candidates for vaccine production. Since economical virus production requires the selected virus strains to grow easily in chicken embryonic eggs, we also propose the development of a machine learning based method that can predict the growth ability of a virus strain based on its sequence information. This integrated genome based influenza vaccine strain selection system will be developed for detecting antigenic variants for influenza A viruses.  This project will help us provide fundamental technology that employs genomic signatures determining influenza antigenicity and growth ability in chicken embryonic eggs, which are the two key issues for efficient and effective influenza vaccine strain development. The resulting genome based vaccine strain selection strategy will significantly reduce the human labor needed for serological characterization, decrease the time required to select an effective strain that will grow well in eggs, and increase the likelihood of correct influenza vaccine candidate selection. Thus, this project will lead to significant technological advances in influenza prevention and control.         PUBLIC HEALTH RELEVANCE:     This study is to develop and validate a genome based strategy for influenza vaccine strain selection, and it will lead to significant technological advances in influenza prevention and control.                ",Genome Based Influenza Vaccine Strain Selection  using Machine Learning,8859887,R01AI116744,"['Affect', 'Africa', 'Algorithms', 'Amino Acid Sequence', 'Area', 'Base Sequence', 'Binding Sites', 'Biological Assay', 'Chickens', 'Clinical', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Databases', 'Development', 'Disease Outbreaks', 'Effectiveness', 'Embryo', 'Epidemic', 'Event', 'Future', 'Genes', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Hemagglutination', 'Hemagglutinin', 'Human', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Mutagenesis', 'Mutation', 'Peptide Sequence Determination', 'Phenotype', 'Procedures', 'Process', 'Production', 'Proteins', 'Public Health', 'Publishing', 'Research Infrastructure', 'Resources', 'Sampling', 'Sampling Biases', 'Scientist', 'Seasons', 'Serologic tests', 'Serological', 'Site', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Technology', 'Testing', 'Time', 'Training', 'United States', 'Vaccination', 'Vaccine Production', 'Vaccines', 'Variant', 'Viral', 'Virus', 'Work', 'base', 'candidate selection', 'egg', 'flu', 'genome sequencing', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'multitask', 'new technology', 'novel', 'pandemic disease', 'prevent', 'programs', 'public health relevance', 'receptor binding', 'research study', 'vaccine candidate']",NIAID,MISSISSIPPI STATE UNIVERSITY,R01,2015,381704,-0.01089269448222495
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8825472,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis', 'transcriptome sequencing']",NCI,BROWN UNIVERSITY,R01,2015,71329,-0.016310518656166416
"Reproducibility Assessment for Multivariate Assays DESCRIPTION (provided by applicant): This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of features, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combination of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penalization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools availble for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.",Reproducibility Assessment for Multivariate Assays,8828718,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'Health', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'research study']",NIGMS,INSILICOS,R43,2015,64005,-0.013058592203921785
"A three-population three-scale social network model to assess disease dispersion     DESCRIPTION (provided by applicant): Communicable diseases, such as influenza, are transmitted from individual to individual following a network of contacts in a population. Reports on recent outbreaks, such as SARS, the Bird flu, and the H1N1 flu, have repeatedly stressed the critical role of contact networks. We propose an innovative Three-population and Three-scale Social Network (3p3sNet) model to simulating the spatial and temporal dispersion of influenza in a metropolitan population in Buffalo, NY. The 3p3sNet aims to construct a realistic contact network by representing interacting and mobile behaviors of individuals at three scales and three types of places. These involve individual (microscopic) -> local network (mesoscopic) -> population (macroscopic) as nighttime population at homes, daytime population at workplaces, and pastime population at service places. Through this network, diseases disperse from infectious individuals to their local networks then to the population-wide network in a complex dynamic fashion. Modeling the disease dispersion through this network provides invaluable insights in who might be at risk, where and when this risk might occur, and with whom these at-risk individuals might be in contact. These insights lay the foundation of developing spatially and temporally sensitive intervention strategies targeted towards the most vulnerable individuals and social groups. Furthermore, the 3p3sNet can be applied in modeling the epidemiology of any disease where human contacts play a critical role.  In implementing 3p3sNet, we propose to use mobile phone data to extract the individual interaction and travel behaviors. We embrace recent developments in economics, geostatistics, econometrics, and machine learning to construct the network. We develop an innovative co-kriging approach to expanding local households to population-wide households and a novel distance-based GEV discrete choice model to link homes to workplaces and service places. It is anticipated that the assemblage of these advanced methods will enable new capabilities and bring transformative improvements in health-related studies in metropolitan areas. We will conduct a data-rich validation process for the three constructed populations, the links between them, and the simulated disease dispersion through the population. A comprehensive range of independent datasets will be used to support the proposed validation. These involve high-resolution population, workplace, and service place data, surveys of individual interaction and travel behavior, and reports on influenza infections.  The multidisciplinary team comprises world-renown leaders and scholars in epidemiology, agent-based and social network modeling, human mobility analysis, geographical information science, and machine learning. The proposed project represents emerging frontiers in the modeling of communicable diseases and will redefine the capabilities of epidemiological models.         PUBLIC HEALTH RELEVANCE: This project addresses two issues of central importance to successfully capturing the complex, spatial and temporal dispersion process of a communicable disease through a population. They are: how to represent individuals as heterogeneous, mobile, and interacting and how to model the disease dispersion process from infectious individuals to their local networks then to the population-wide network. To address these issues, this project proposes to: (1) use mobile phone data to construct a three-population and three-scale social network (3p3sNet); (2) simulate the spatially and temporally dynamic dispersion of influenza through an urban population in Buffalo, NY; and (3) conduct an intensive, data-rich validation process on the simulated influenza dispersion.            ",A three-population three-scale social network model to assess disease dispersion,8818541,R01GM108731,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Behavior', 'Buffaloes', 'Car Phone', 'Communicable Diseases', 'Complex', 'Data', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Disease Outbreaks', 'Disease model', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Foundations', 'Health', 'Home environment', 'Household', 'Human', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Information Sciences', 'Intervention', 'Lead', 'Link', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Microscopic', 'Modeling', 'Outcome', 'Play', 'Population', 'Process', 'Records', 'Reporting', 'Resolution', 'Risk', 'Role', 'Sampling', 'Services', 'Severe Acute Respiratory Syndrome', 'Social Network', 'Stress', 'Surveys', 'Testing', 'Time', 'Travel', 'Urban Population', 'Validation', 'Workplace', 'base', 'disease transmission', 'epidemiological model', 'flu', 'frontier', 'fundamental research', 'human disease', 'innovation', 'insight', 'metropolitan', 'multidisciplinary', 'network models', 'novel', 'pandemic disease', 'public health relevance', 'social group', 'transmission process']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,R01,2015,621703,-0.012396095687758504
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,-0.008542411977654804
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,8840984,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2015,1243799,-0.00144690446304343
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8889700,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2015,2005492,-0.030264770604566983
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),8896676,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'computational chemistry', 'computer science', 'data mining', 'design', 'dopamine transporter', 'drug abuse prevention', 'epigenetic marker', 'falls', 'genome-wide', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'operation', 'predictive modeling', 'prevent', 'professor', 'receptor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2015,1064603,-0.014989845877341989
"Methods for high-dimensional data in HIV/CVD research DESCRIPTION (provided by applicant): Recent technological advances have yielded vast quantities of molecular and cellular data, affording unprecedented opportunities to understand complex disease etiologies and to inform clinical management strategies. However, in order to derive information from these rich stores of data we need to develop sound and appropriate analytic techniques. This need is especially relevant in studies at the intersection of human immunodeficiency virus (HIV) and cardiovascular disease (CVD), which are characterized by an elaborate set of interactions among viral and host factors. These factors include viral and host genetic profiles, as well as markers of caloric metabolism, immune activation and inflammation, which work together to determine response to therapy and overall disease progression. A comprehensive assessment of these markers presents several analytical challenges owing to the large number of potentially informative variables and the largely uncharacterized relationship among them. We propose a multi-faceted strategy that focuses on the development and application of integrative statistical approaches. Such approaches will allow us to explore and characterize novel hypotheses relating to the complex relationships among multiple genetic, environmental, demographic, and clinical factors and measures of disease progression. Specifically, this continuation application focuses on advancing and applying statistical methods in two settings: first, we consider population-based genetic association studies of innate-immunity, adipokine, drug metabolism and drug transport genes and markers of immune reconstitution, inflammation and risk of CVD in HIV-infected individuals; and second, we address investigations of metabolic and immunologic profiles that associate with immune recovery, inflammation and risk of CVD. The Specific Aims of the proposed research are to develop and evaluate: (1) Latent class and mixture modeling paradigms for (a) discovering and characterizing multi-locus genotype-trait associations and (b) evaluating unobservable haplotype-trait associations in candidate-gene investigations; and (2) Hierarchical mixture models and machine learning approaches for (a) monitoring quantitative biomarkers in resource-limited settings and (b) characterizing high- dimensional predictors of immune reconstitution and inflammation. IMPACT: This research will lead to the creation of appropriate and carefully evaluated analytic tools to derive information from the rich array of molecular and cellular data now available. Ultimately, this research will advance our ability to translate molecular and cellular level data for clinical decision making, serving at the cornerstone of personalized medicine. PUBLIC HEALTH RELEVANCE: The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.",Methods for high-dimensional data in HIV/CVD research,8995000,R01HL107196,"['Address', 'Award', 'Biological Markers', 'Biological Models', 'Biometry', 'Candidate Disease Gene', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Clinical Research', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Development', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug Transport', 'Dyslipidemias', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genotype', 'Goals', 'HIV', 'Haplotypes', 'Health', 'Immune', 'Immunologic Markers', 'Immunologics', 'Immunology', 'Individual', 'Inflammation', 'Integration Host Factors', 'Investigation', 'Laboratories', 'Lead', 'Lipids', 'Machine Learning', 'Measures', 'Medicine', 'Metabolic', 'Metabolic Marker', 'Metabolism', 'Methods', 'Microbiology', 'Modeling', 'Molecular', 'Monitor', 'Natural Immunity', 'Peer Review', 'Pharmacology', 'Publications', 'Recovery', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Testing', 'Textbooks', 'Time', 'Translating', 'Translational Research', 'Viral', 'Work', 'adaptive immunity', 'cardiovascular disorder risk', 'clinical decision-making', 'drug metabolism', 'genetic association', 'genetic profiling', 'genome wide association study', 'immune activation', 'insight', 'molecular array', 'novel', 'open source', 'personalized medicine', 'population based', 'professor', 'reconstitution', 'response', 'sound', 'statistics', 'success', 'tool', 'trait', 'translational medicine']",NHLBI,MOUNT HOLYOKE COLLEGE,R01,2015,385769,-0.05749850413599663
"Integration and Visualization of Diverse Biological Data ﻿    DESCRIPTION (provided by applicant): The onset of most human disease involves multiple, molecular-level changes to the complex system of interacting genes and pathways that function differently in specific cell-lineage, pathway and treatment contexts. While this system has been probed by the thousands of functional genomics and quantitative genetic studies, careful extraction of signals relevant to these specific contexts is a challenging problem. General integration of these heterogeneous data was an important first step in detecting signals that be used to build networks to generate experimentally-testable hypotheses. However, only by dealing with the fact that disease happens at the intersection of multiple contexts and by integrating functional genomics with quantitative genetics will we be able to move toward a molecular-level understanding of human pathophysiology, which will pave the way to new therapy and drug development.  The long-term goal of this project is to enable such discoveries through the development of innovative bioinformatics frameworks for integrative analysis of diverse functional genomic data. In the previous funding periods, we developed accurate data integration and visualization methodologies for most common model organisms and human, created methods for tissue-specific data analysis, and applied these methods to make novel insights about important biological processes. We further enabled experimental biological discovery by implementing these methods in publicly accessible interactive systems that are popular with experimental biologists.  Leveraging our prior work, we now will directly address the challenge of enabling data-driven study of molecular mechanisms underlying human disease by developing novel semi-supervised and multi-task machine learning approaches and implementing them in a real-time integration system capable of predicting genome-scale functional and mechanism-specific networks focused on any biological context of interest. This will allow any biomedical researcher to quickly make data-driven hypotheses about function, interactions, and regulation of genes involved in hypertension in the kidney glomerulus or to predict new regulatory interactions relevant to Parkinson's disease that affect the ubiquitination pathway in Substantia nigra. Furthermore, we will develop methods for disease gene discovery that leverage these highly specific networks for functional analysis of quantitative genetics data. Our deliverable will be a general, robust, user-friendly, and automatically updated system for user-driven functional genomic data integration and functional analysis of quantitative genetics data. Throughout this work, we (with our close experimental and clinical collaborators) will also apply our methods to chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders both as case studies for the iterative improvement of our methods and to make direct contribution to better understanding of these diseases.         PUBLIC HEALTH RELEVANCE: We will create a web-accessible system that will enable biologists and clinicians to generate hypotheses regarding disease-linked genes, molecular mechanisms underlying genetic disorders, and drug discovery for more targeted treatment. Underlying this user-friendly web interface will be novel algorithms for on-the-fly integration of  vast amount of functional genomics and quantitative genetics data based on the context(s) defined by the biologist or clinician. As applied to the three case study areas, chronic kidney disease, cardiovascular disease/hypertension, and autism spectrum disorders, our system has the potential to identify novel disease genes and pathways and to enable development of better diagnostic biomarkers, drug targets, and, in the longer term, treatments.            ",Integration and Visualization of Diverse Biological Data,8886554,R01GM071966,"['Address', 'Affect', 'Algorithms', 'Animal Model', 'Area', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Chronic Kidney Failure', 'Clinical', 'Collaborations', 'Complex', 'Computer Systems', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Drug Targeting', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression Regulation', 'Generations', 'Genes', 'Genetic Databases', 'Genetic study', 'Goals', 'Gold', 'Hereditary Disease', 'Human', 'Hypertension', 'Imagery', 'Kidney Glomerulus', 'Knowledge', 'Label', 'Laboratories', 'Learning', 'Letters', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nephrology', 'Network-based', 'Parkinson Disease', 'Pathway interactions', 'Quantitative Genetics', 'Real-Time Systems', 'Research', 'Research Personnel', 'Scientist', 'Signal Transduction', 'Substantia nigra structure', 'System', 'Systems Integration', 'Time', 'Tissues', 'Training', 'Ubiquitination', 'Update', 'Work', 'autism spectrum disorder', 'base', 'clinical investigation', 'data integration', 'data visualization', 'drug development', 'drug discovery', 'functional genomics', 'gene discovery', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'innovation', 'insight', 'interest', 'multitask', 'novel', 'public health relevance', 'research study', 'targeted treatment', 'therapy development', 'user-friendly', 'web interface', 'web-accessible']",NIGMS,PRINCETON UNIVERSITY,R01,2015,473642,-0.025817557548633848
"New Serological Measures of Infectious Disease Transmission Intensity ﻿    DESCRIPTION (provided by applicant):    Candidate: Benjamin Arnold    I am an epidemiologist at the University of California, Berkeley. I completed my MA in Biostatistics and a PhD in Epidemiology from UC Berkeley in 2009. Since then, I have worked as an epidemiologist in Professor Jack Colford's group. The opportunity to work as the coordinating epidemiologist for a touchstone, multi-country cluster randomized trial - combined with the addition of two children to my family - led me to delay my academic career. I am now ready to restart my career progress toward independent investigator status.     My long-term career goal is to become a leader in the application of novel statistical methods to target and evaluate interventions that reduce the burden of enteric infections and neglected tropical diseases (NTDs) in low-income countries. This research focus and career objective build from my experience and from a growing collaboration with Dr. Patrick Lammie at the US Centers for Disease Control (CDC) that started in 2013 and has introduced me to seroepidemiologic research. My background in epidemiologic methods, biostatistics, and international field research makes me uniquely qualified to make significant contributions to infectious disease epidemiology at the interface between recent advances in statistical methodology and serological assays.    Environment: University of California, Berkeley    To achieve my career goal, I have developed a training and mentoring plan that focuses on recent advances in statistics (semi-parametric estimation theory and machine learning) and on infectious disease immunology. These are two areas where additional training will open up significant and unique opportunities for me to make meaningful contributions to seroepidemiologic research, and will enable me to launch an independent career as a productive faculty member at UC Berkeley.    I have assembled a multidisciplinary mentoring team of senior investigators in biostatistics and immunology to support my training, research, and career objectives. Mark van der Laan (primary mentor, biostatistics) will guide my training in semi-parametric methods and machine learning. Alan Hubbard (co-mentor, biostatistics) will guide my translation of the methodology to applications for enteric pathogens and NTDs. Patrick Lammie (co-mentor at CDC, immunology) will guide my immunology training and research with his expertise in the immunology of enteric pathogens and NTDs    Research: New Serological Measures of Infectious Disease Transmission    Background: Recent advances in multiplex antigen assays have led to the development of low-cost and sensitive methods to measure enteric pathogens and neglected tropical diseases (NTDs). There have not been commensurate advances in the statistical methods used to derive measures of transmission intensity from antibody response. Translating antibody response into metrics of transmission intensity is a key step from a public health perspective because it enables us to target intervention programs to the populations most in need and then measure the effectiveness of those programs.     Aims and Methods: The overarching goal of this research is to develop a methodologic framework to translate antibody response measured in cross-sectional surveys into measures of transmission intensity for enteric pathogens (7 included in the study, e.g., Cryptosporidium parvum, enterotoxigenic E. coli) and neglected tropical diseases (principal focus: lymphatic filariasis). We approach this goal from two novel perspectives. In Aim 1, we draw on the ""peak shift"" phenomenon for infectious diseases, and hypothesize that changes in transmission will be detectable in the age-specific antibody response curve. At lower transmission, antibody levels should decline across all ages due to fewer and less frequent active infections, leading to an overall shift in the age-specific response curve. We will evaluate the approach by comparing antibody response curves for young children with different exposures (improved vs. unimproved drinking water for enteric pathogens; pre- versus post- mass drug administration for lymphatic filariasis) in large, well characterized cohorts in Kenya, Tanzania, and Haiti.     In Aim 2, we will develop semi-parametric methods to estimate the force of infection (seroconversion rate) from seroprevalence data for pathogens where seroreversion is possible, using lymphatic filariasis as an example. Our new approach marks a significant advance over previous work in this area by making few modeling assumptions and by allowing for the flexible control of confounding between comparison groups. We will evaluate the approach in Haiti by measuring the effect of mass drug administration on the force of infection for lymphatic filariasis For all of the methods, we will create user-friendly, open source software to accelerate translation to applied research.     The Future: This mentored training and research plan represents a natural next step for me on a productive and collaborative path to independence at UC Berkeley. It will set the stage for a broader R01-level research portfolio that applies the newly developed methods to primary research studies that evaluate the impact of interventions on enteric infections, and help target and monitor global elimination efforts for NTDs.         PUBLIC HEALTH RELEVANCE: Antibodies measured in blood provide a sensitive measure of infection for many infectious diseases. Statistical methods that enable us to measure disease transmission intensity at the population level from blood antibody levels are an important tool for public health efforts because they help identify populations in greatest need of intervention and help measure the effectiveness of interventions designed to reduce transmission. No statistical tools like this exist for enteric pathogens (those that cause diarrhea) and neglected tropical diseases, which together cause an immense health burden among the world's poorest people, and so we propose to develop new methods to measure population-level transmission intensity of these diseases based on antibodies measured in blood from children in Kenya, Tanzania, and Haiti.            ",New Serological Measures of Infectious Disease Transmission Intensity,8947064,K01AI119180,"['Age', 'Antibodies', 'Antibody Response', 'Antigens', 'Applied Research', 'Area', 'Biological Assay', 'Biometry', 'Blood', 'California', 'Campylobacter', 'Caregivers', 'Centers for Disease Control and Prevention (U.S.)', 'Child', 'Cluster randomized trial', 'Collaborations', 'Communicable Diseases', 'Computer software', 'Country', 'Cross-Sectional Studies', 'Cryptosporidium', 'Cryptosporidium parvum', 'Data', 'Development', 'Diagnostic tests', 'Diarrhea', 'Disease', 'Doctor of Philosophy', 'Effectiveness of Interventions', 'Entamoeba histolytica', 'Enteral', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Faculty', 'Family', 'Filarial Elephantiases', 'Future', 'Giardia', 'Goals', 'Haiti', 'Handwashing', 'Health', 'Immune response', 'Immunologist', 'Immunology', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Infectious Disease Immunology', 'Infectious Diseases Research', 'International', 'Intervention', 'Intervention Studies', 'Kenya', 'Literature', 'Low income', 'Machine Learning', 'Measles', 'Measurement', 'Measures', 'Mentors', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Mumps', 'Outcome', 'Pharmaceutical Preparations', 'Play', 'Population', 'Public Health', 'Qualifying', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Role', 'Rubella', 'Running', 'Salmonella', 'Sanitation', 'Serological', 'Seroprevalences', 'Source', 'Spottings', 'Staging', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Tanzania', 'Testing', 'Time', 'Training', 'Translating', 'Translations', 'Universities', 'Vibrio cholerae', 'Viral', 'Water', 'Work', 'base', 'career', 'cohort', 'comparison group', 'cost', 'disease transmission', 'drinking water', 'effectiveness measure', 'enteric pathogen', 'enterotoxigenic Escherichia coli', 'experience', 'flexibility', 'high risk', 'improved', 'intervention effect', 'intervention program', 'member', 'multidisciplinary', 'neglected tropical diseases', 'novel', 'novel strategies', 'open source', 'pathogen', 'professor', 'programs', 'public health intervention', 'public health relevance', 'research study', 'response', 'seroconversion', 'seropositive', 'skills', 'statistics', 'theories', 'therapy design', 'tool', 'transmission process', 'user-friendly']",NIAID,UNIVERSITY OF CALIFORNIA BERKELEY,K01,2015,142069,-0.015752223679611717
"Models for synthesising molecular, clinical and epidemiological data, and transla DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)? PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.","Models for synthesising molecular, clinical and epidemiological data, and transla",8927659,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Health', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2015,434391,0.002358813970665672
"Development of mosaic mouse models of HCC for genetic interspecies inference DESCRIPTION (provided by applicant): Hepatocellular carcinoma (HCC) remains a menace for human health for the lack of any effective treatment. HCC is usually the end result of chronic liver diseases associated with diverse risk factors. Furthermore, non- alcoholic steatohepatitis (NASH) induced HCC, which is projected to be the leading cause of new cases, remains poorly characterized. Although many mouse models of HCC have been developed, it is unclear how well they represent different subgroups of human HCCs. This research plan proposes to transform HCC animal modeling by establishing a novel in vivo platform to accurately replicate the somatic molecular profiles of human HCC in mice. To do this, three independent HCC mouse models will be exhaustively characterized through exome sequencing and gene expression profiling. Using bioinformatics techniques including machine- learning and network analysis, these datasets will be compared with human HCC datasets from TCGA and the ICGC to identify subgroups of patients with similar somatic molecular profiles to the HCC mouse models as well as refined minimum sets of characteristic genetic aberrations. A derived transposon system will be used to generate mosaic mouse models replicating human HCC genetic subgroups faithfully. These models will enable 1) experimental dissection of the molecular mechanisms underlying distinct etiologies of HCC, 2) systematic assessment of candidate HCC therapies and 3) investigation of therapeutic resistances. This K99/R00 career development award proposal describes a two-year mentored and three-year independent research program essential for the development of Dr. Font-Burgada as an independent investigator. Dr. Font-Burgada received his PhD at University of Barcelona, Spain, for the work he performed to investigate basic chromatin regulatory and epigenetic mechanisms. He then moved to University of California, San Diego where he joined Dr. Michael Karin's laboratory to train in mouse models of cancer and signal transduction. For the accomplishment of this research proposal, Dr. Font-Burgada has designed a strong training and career development plan consisting of: 1- the continued mentorship of Dr. Michael Karin to gain additional expertise in mouse models of HCC and signal transduction, 2- Training in bioinformatics, specifically in methods to identify cancer driver genetic aberrations and network-based approaches for comparative genomic analysis of mouse and human HCCs, to be overseen by co-mentor Dr. Hannah Carter, an Assistant Professor of Medicine, at UCSD. 3- Training in application of emerging transposon vector technologies to generate mosaic mouse models for in vivo analysis of oncogenic pathways. 4- Career development courses and seminars in a supportive academic environment in the Department of Pharmacology at UCSD to complement other aspects of the training program. This training plan will be overseen by an advisory committee comprising 4 members, mentor, co-mentor, and additional experts in mouse models of cancer and bioinformatics, Inder Verma and Trey Ideker, providing key scientific insights and essential guidance in critical steps in Dr. Font-Burgada transition to independence. PUBLIC HEALTH RELEVANCE: Hepatocellular carcinoma (HCC) has a 5-year survival rate of less than 10% and leads to 700000 deaths globally each year. The genetic causes of this disease are poorly understood and the only available targeted therapy extends life expectancy by a mere 3 months. In order to improve these dismal statistics, I am developing a novel class of mouse models capable of reproducing the spectrum of mutations observed in a given human tumor, to enable study of HCC biology as well as systematic testing of therapeutic combinations.",Development of mosaic mouse models of HCC for genetic interspecies inference,8805692,K99CA191152,"['Advisory Committees', 'Animal Model', 'BRAF gene', 'Beauty', 'Bioinformatics', 'Biology', 'California', 'Cancer Etiology', 'Cataloging', 'Catalogs', 'Cell Culture Techniques', 'Cells', 'Cessation of life', 'Characteristics', 'Chromatin', 'Clinical', 'Comparative Genomic Analysis', 'Complement', 'Complex', 'Data', 'Data Set', 'Development', 'Development Plans', 'Diet', 'Disease', 'Dissection', 'Doctor of Philosophy', 'Drug Targeting', 'Ensure', 'Environment', 'Environmental Exposure', 'Epigenetic Process', 'Etiology', 'Fatty acid glycerol esters', 'Gene Expression Profiling', 'Genetic', 'Health', 'Hepatocyte', 'Heterogeneity', 'Human', 'Human Pathology', 'Investigation', 'K-Series Research Career Programs', 'Knowledge', 'Label', 'Laboratories', 'Life Expectancy', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Monitor', 'Mouse Protein', 'Mus', 'Mutate', 'Mutation', 'Mutation Spectra', 'Network-based', 'Oncogenic', 'Orthologous Gene', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pharmacology', 'Phase', 'Physiological', 'Population', 'Pre-Clinical Model', 'Primary carcinoma of the liver cells', 'Research', 'Research Personnel', 'Research Proposals', 'Resistance', 'Risk', 'Risk Factors', 'Signal Transduction', 'Somatic Mutation', 'Spain', 'Subgroup', 'Survival Rate', 'System', 'Techniques', 'Technology', 'Testing', 'The Cancer Genome Atlas', 'Therapeutic', 'Training', 'Training Programs', 'Transgenes', 'Translating', 'Universities', 'Viral Vector', 'Work', 'Xenograft Model', 'accurate diagnosis', 'base', 'career development', 'chronic liver disease', 'combinatorial', 'comparative', 'design', 'effective therapy', 'exome sequencing', 'human disease', 'improved', 'in vivo', 'insight', 'member', 'mouse model', 'neoplastic cell', 'nonalcoholic steatohepatitis', 'novel', 'pre-clinical', 'professor', 'programs', 'response', 'statistics', 'success', 'targeted treatment', 'therapy resistant', 'tool', 'tumor', 'tumor progression', 'vector']",NCI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2015,109620,-0.01471170853979563
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases     DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases.         PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.            ",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,8935819,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency response', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East Respiratory Syndrome Coronavirus', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Report (account)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Science', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'climate data', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'model building', 'news', 'novel', 'pandemic influenza', 'public health relevance', 'skills', 'social', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,UNIVERSITY OF WASHINGTON,K01,2015,107469,0.002036301452338346
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,8858662,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2015,248912,-0.023047742273927963
"Predicting Resilience in the Human Microbiome DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience. PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.",Predicting Resilience in the Human Microbiome,8904596,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiota', 'human subject', 'innovation', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'resilience', 'response', 'theories', 'tool', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2015,413065,0.00828655626121224
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities.         PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.                ","COINSTAC: decentralized, scalable analysis of loosely coupled data",8975906,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Solutions', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2015,727692,-0.0010096845818889015
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,8898177,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2015,52816,-0.010295532070478805
"Integrating Bioinformatics and Clustering Analysis for Disease Surveillance ﻿    DESCRIPTION (provided by applicant):  There has been a tremendous focus in bioinformatics on translation of data from the bench into information and knowledge for clinical decision-making. This includes analysis of human genetics for personalized medicine and treatment. However, there has been much less attention on translational bioinformatics for public health practice such as surveillance of emerging/re-emerging viruses. This involves data acquisition, integration, and analyses of viral genetics to infer origin, spread, and evolution suc as the emergence of new strains. The relevant scientific fields for this practice include certain aspects of molecular epidemiology and phylogeography. Recent attention has focused on viruses of zoonotic origin, which are defined as pathogens that are transmittable between animals and humans. In addition to seasonal influenza and West Nile virus, this classification of pathogens includes novel viruses such as Middle Eastern Respiratory Syndrome and influenza A H7N9. Despite the successes highlighted in the literature, there has been little utilization of bioinformatics resources and tools among state public health, agriculture, and wildlife agencies for zoonotic surveillance. Previously this type of resource has been restricted primarily to those in academia.       While bioinformatics has been sparsely used for surveillance of zoonotic viruses, other applications such as Geospatial Information Systems (GIS) have been employed by state health agencies to analyze spatial patterns of infection. This includes software to produce disease maps using an array of data types such as clinical, geographical, or human mobility data for tasks such as, geocoding, clustering, or outbreak detection. In addition, advances in geospatial statistics have enabled health agencies to perform more powerful space-time analyses to infer spatiotemporal patterns. However, these GIS consider only traditional epidemiological data such as location and timing of reported cases and not the genetics of the virus that causes the disease. This prevents health agencies from understanding how changes in the genome of the virus and the associated environment in which it disseminates impacts disease risk.      The long-term goal of this proposal is to enhance the identification of geospatial hotspots of zoonotic viruses by applying bioinformatics principles to access, integrate, and analyze viral genetics and spatiotemporal reportable disease data. This project will include approaches from bioinformatics, genetics, spatial statistics, GIS, and epidemiology. To do this, I will first measue the utilization of bioinformatics resources and tools as well as the current approaches and limitations identified by state agencies of public health, agriculture, and wildlife for detecting nd predicting hotspots (clusters) of zoonotic viruses (Aim 1). I will then use this feedback to develo a spatial decision support system for detecting and predicting zoonotic hotspots that applies bioinformatics principles to access, integrate, and analyze viral genetics, environmental, and spatiotemporal reportable disease data (Aim 2). In Aim 3, I will then evaluate my system for cluster detection and prediction against a system that does not consider viral genetics and relies on traditional spatiotemporal data, and perform validation of the predictive capability. Additional evaluation of the user's satisfaction and system usability will be evaluated.               Project Narrative I will develop and evaluate a spatial decision support system to support surveillance of zoonotic viruses in both human and animal populations. I will use approaches from bioinformatics and public health to integrate genetic sequence data from the virus with data from cases of reported infectious diseases and associated environmental data. A surveillance system that considers the genetics and environment of the virus along with public health data will assist public health officials in making informed decisions regarding risk of infectious diseases.",Integrating Bioinformatics and Clustering Analysis for Disease Surveillance,9050106,F31LM012176,"['Academia', 'Address', 'Agriculture', 'Algorithm Design', 'Algorithms', 'Animals', 'Area', 'Attention', 'Biodiversity', 'Bioinformatics', 'Case Study', 'Clinical', 'Cluster Analysis', 'Communicable Diseases', 'Computer software', 'Data', 'Databases', 'Decision Support Systems', 'Detection', 'Disease', 'Disease Outbreaks', 'Ecology', 'Environment', 'Epidemiology', 'Evaluation', 'Evolution', 'Feedback', 'Future', 'Genbank', 'Genetic', 'Geographic Information Systems', 'Goals', 'Health', 'Human', 'Human Genetics', 'Infection', 'Influenza', 'Influenza A Virus, H7N9 Subtype', 'Influenza A virus', 'Knowledge', 'Literature', 'Location', 'Machine Learning', 'Maps', 'Measures', 'Metadata', 'Modeling', 'Molecular Epidemiology', 'Molecular Evolution', 'Pattern', 'Population', 'Public Health', 'Public Health Practice', 'Questionnaire Designs', 'Questionnaires', 'Research', 'Resources', 'Retrospective Studies', 'Risk', 'Satellite Viruses', 'Scanning', 'Sequence Alignment', 'Syndrome', 'System', 'Time', 'Translations', 'Validation', 'Validity and Reliability', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'West Nile virus', 'Work', 'clinical decision-making', 'data acquisition', 'disorder risk', 'health data', 'high risk', 'novel virus', 'pathogen', 'personalized medicine', 'prevent', 'respiratory', 'satisfaction', 'seasonal influenza', 'spatiotemporal', 'statistics', 'success', 'tool', 'usability', 'virus classification', 'virus genetics']",NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,F31,2015,36613,-0.002438417130668557
"Dopaminergic encoding of counterfactual information in human striatum ﻿    DESCRIPTION (provided by applicant): Diseases and disorders directly affected by dopamine systems (e.g., drug addiction and Parkinson's disease) highlight the importance of these systems in motivated human behavior and cognition. The dopamine system is known to be a critical component of normal learning, reward processing, and decision-making (reviewed in Montague et al., 2004). Unfortunately, our present knowledge of dopamine systems in human brains is relatively sparse compared to the wealth of experimentation and computational modeling on these systems in rodents and non-human primates. Previously, technological constraints have limited direct experimentation in human brains. This proposal capitalizes on our group's recent technological innovation, which was supported by the NIH R21 mechanism - CEBRA: R21DA024140 - and resulted in the successful completion of the first sub-second measurements of dopamine release in a human brain. Furthermore, these measurements took place during an active decision-making task that was framed by computational models of learning and reward processing (Kishida et al., 2011 and Kishida et al., under review). We propose to pursue three specific aims, which combine our technological advance with active learning tasks designed to probe the role of dopamine in human behavior. Our aims incorporate three learning signals, where actual and counterfactual experience will each be examined in human striatal responses. The proposed work will inform on the controversial role for dopamine in reward/movement interactions. The experiments proposed will yield unprecedented insight into the function of the dopamine system in the humans afflicted with Parkinson's disease and Essential Tremor. With the support of the NIH (R21DA024140), our team successfully developed a complete prototype system for making electrochemical measurements of dopamine delivery in the human brain. Feasibility has been demonstrated by obtaining the first dopamine measurements in the striata of subjects with Parkinson's during a decision-making task. This substantial preliminary work is now ready for a larger scale with specific hypothesis testing about the role of dopamine systems in Parkinson's disease, Essential tremor, and human decision-making and behavior.   PUBLIC HEALTH RELEVANCE: Experiments and computational models (primarily investigated in rodents and non-human primates) suggest dopamine systems in the brain are essential for normal learning, reward-processing, and ongoing decision- making. This proposal builds upon our group's innovative work, which - for the first time - recorded sub- second measurements of dopamine in human subjects and amalgamated these measurements with computational models of dopamine function. Herein we propose to use this newly developed technology to generate unprecedented insight into the function of dopamine systems directly in humans.          ",Dopaminergic encoding of counterfactual information in human striatum,9029452,R01NS092701,"['Active Learning', 'Acute', 'Affect', 'Animal Model', 'Behavior', 'Brain', 'Cells', 'Cessation of life', 'Cognition', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Coupling', 'Data', 'Decision Making', 'Disease', 'Dopamine', 'Drug Addiction', 'Electrodes', 'Essential Tremor', 'Exploratory/Developmental Grant', 'Funding', 'Health', 'Human', 'Knowledge', 'Learning', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Monitor', 'Motor', 'Movement', 'Movement Disorders', 'Operative Surgical Procedures', 'Parkinson Disease', 'Participant', 'Patients', 'Phase', 'Play', 'Replacement Therapy', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Sample Size', 'Scanning', 'Signal Transduction', 'Symptoms', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'Work', 'base', 'blood oxygenation level dependent response', 'cognitive function', 'design', 'dopamine system', 'dopaminergic neuron', 'experience', 'extracellular', 'human subject', 'implantation', 'innovation', 'insight', 'nonhuman primate', 'prototype', 'putamen', 'relating to nervous system', 'research study', 'response', 'reward processing', 'social', 'technological innovation', 'temporal measurement']",NINDS,VIRGINIA POLYTECHNIC INST AND ST UNIV,R01,2015,507375,-0.0040301608457890626
"Data-Driven Statistical Learning with Applications to Genomics DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software. PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.",Data-Driven Statistical Learning with Applications to Genomics,8929328,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Health', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'genetic makeup', 'high throughput analysis', 'individualized medicine', 'interest', 'novel', 'patient population', 'personalized medicine', 'predictive modeling', 'relating to nervous system', 'response', 'statistics', 'targeted treatment', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2015,329422,-0.014877428980211953
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation. n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,8920676,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Staging', 'Statistical Models', 'Structure', 'Students', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'quantitative imaging', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2015,323033,-0.016011617230946868
"The q-bio Summer School DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required. PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.",The q-bio Summer School,8802880,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Health', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2015,169997,-6.362309526271044e-05
"Impact of Gastrointestinal Microbiome on the Host Response to an Effector-Memory T-Cell AIDS Vaccine and Multiple Low-Dose Challenge ﻿    DESCRIPTION (provided by applicant): This computational research project will analyze gastrointestinal microbiome, host cellular immune response, and host transcriptional response datasets derived from a large-scale rhesus macaque AIDS vaccine study. The vaccine study includes 70 animals divided into five groups and is designed to test the efficacy of a cytomegalovirus (CMV) vector expressing the major simian immunodeficiency virus (SIV) proteins. In previous studies, RhCMV/SIV vaccines elicit a robust effector-memory T cell response. After challenge with highly pathogenic SIVmac239, approximately 50% of RhCMV/SIV-vaccinated monkeys exhibit a pattern of viral control that is characterized by a ""blip"" of viremia followed by control of plasma viremia to undetectable levels. One to three years later, RhCMV/SIV-protected monkeys show no sign of SIV infection, suggesting immune-mediated clearance of the virus. The mechanisms responsible for this 50% efficacy are not clear, but may reflect T cell responses at the site of SIV entry or sites of early replication. Thi project will investigate the role of the gastrointestinal microbiome on RhCMV/SIV-induced effector-memory T cell responses and protection against SIVmac239 challenge. The project includes two computationally based Specific Aims: 1) Define the composition of the gut microbiome prior to and after vaccination and during repeated limiting-dose SIVmac239 challenge; and 2) Determine how the composition of the gut microbiome correlates with protective immune cell responses and vaccine-induced host transcriptional responses. In Aim 1, 16S ribosomal sequence data (obtained from rectal swabs) will be used to determine operational taxonomic units. Variation in bacterial species over time and within individual animals will be determined using Mothur and Qiime metagenomic software.  Nonparametric statistical analyses and co-occurrence and co-exclusion networks will be used to identify bacterial species associated with progression or control of SIV infection after repeated limiting-dose intrarectal SIVmac239 challenge. In Aim 2, principle component analysis will be used to identify associations between microbiome composition and SIV-specific CD4+ and CD8+ T cell responses. Gene module-based and correlation network-based approaches will be used to determine associations between microbiome composition and host transcriptional responses (collected from whole blood samples). Together, these analyses will allow us to better understand microbiome-host interactions and their role in eliciting a protective vaccine-derived effector-memory T cell response. Such understanding will inform future vaccination strategies and attempts to modulate microbiome composition to affect vaccine efficacy.         PUBLIC HEALTH RELEVANCE: The types and numbers of bacterial species that live in the gastrointestinal tract (together known as the microbiome) affect many aspects of human health and disease, including how individuals respond to vaccination and viral infection. In this study, we will define the gastrointestinal microbiome of rhesus monkeys before and after vaccination with an experimental AIDS vaccine and after infection with a monkey immunodeficiency virus. This study will allow us to determine how the microbiome impacts the effectiveness of the vaccine and may suggest whether altering the microbiome may improve vaccine performance.            ",Impact of Gastrointestinal Microbiome on the Host Response to an Effector-Memory T-Cell AIDS Vaccine and Multiple Low-Dose Challenge,8992226,R21AI120713,"['AIDS Vaccines', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Animals', 'Blood specimen', 'CD8B1 gene', 'Cells', 'Communities', 'Computer software', 'Confounding Factors (Epidemiology)', 'Cytomegalovirus', 'Data', 'Data Set', 'Databases', 'Diet', 'Disease', 'Dose', 'Dose-Limiting', 'Environmental Risk Factor', 'Exclusion', 'Exhibits', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Future', 'Gastrointestinal tract structure', 'Gene Expression', 'Generations', 'Genes', 'Goals', 'Head', 'Health', 'Health Sciences', 'Human', 'Immune', 'Immune response', 'Immunologic Deficiency Syndromes', 'Individual', 'Infection', 'Joints', 'Laboratories', 'Lead', 'Life', 'Life Style', 'Location', 'Macaca mulatta', 'Machine Learning', 'Mediating', 'Memory', 'Metagenomics', 'Microbiology', 'Modeling', 'Molecular Profiling', 'Monkeys', 'National Institute of Allergy and Infectious Disease', 'Network-based', 'Oregon', 'Pattern', 'Performance', 'Plasma', 'Process', 'Research', 'Research Project Grants', 'Role', 'SIV', 'Sampling', 'Scientist', 'Site', 'Swab', 'T cell response', 'T memory cell', 'Testing', 'Time', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Variant', 'Viral', 'Viral Proteins', 'Viremia', 'Virus', 'Virus Diseases', 'Washington', 'Whole Blood', 'base', 'design', 'efficacy testing', 'functional genomics', 'gastrointestinal', 'improved', 'microbiome', 'nonhuman primate', 'professor', 'public health relevance', 'rectal', 'response', 'vaccination strategy', 'vaccine development', 'vaccine effectiveness', 'vaccine efficacy', 'vaccine response', 'vaccine trial', 'vaccine-induced immunity', 'vector']",NIAID,UNIVERSITY OF WASHINGTON,R21,2015,217500,-0.007272756567280649
"New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis ﻿    DESCRIPTION (provided by applicant)    Maha R Farhat, MD is an Instructor of Medicine at Harvard Medical School on the tenure track and a staff physician in the Department of Pulmonary and Critical Care Medicine at Massachusetts General Hospital. She is completing a masters of biostatistics at the Harvard School of Public Health in 5/2015. She has spent the last 4.5 years acquiring skills in Mycobacterium tuberculosis biology, epidemiology, bioinformatics and biostatistics. She has experience in the analysis of whole genome sequence data, drug resistance data and patient clinical outcome data with the focus of identifying Mycobacterium tuberculosis genetic determinants of drug resistance. She has also developed new methods in this area. Dr. Farhat has 11 publications 5 of which are first author including high impact and highly cited work in the journals Nature Genetics, Genome Medicine and the International Journal of Tuberculosis and Lung Disease. The short term goals of this K01 award are to provide training for Dr. Farhat in critical aspects of data science, computational and evolutionary biology, advanced biostatistics and network science. Dr. Farhat's long term goal is to become a leader in the field of Big Data analysis for infectious diseases. The proposed research as well as the training activities outlined in the proposal will successfully position Dr. Farhat for her first R01 and an independent career as a physician scientist. Environment: Dr. Farhat will perform the interdisciplinary work outlined in this proposal at the distinguished Harvard Departments of Global Health Social Medicine, Biostatistics, Evolutionary biology and the Institute for Quantitative Social Sciences. Dr. Farhat' mentorship team will include two world renowned leaders in the fields of infectious diseases and Big Data, Dr. Megan Murray and Dr. Gary King; and two rising stars in the fields of network Science and evolutionary Biology, Dr. JP Onnela and Dr. Michael Desai. Dr. Murray, the principal mentor on this proposal has mentored over 38 trainees, 9 of which have went on to have independent research careers, and 6 competed successfully for K awards. She is also PI on two recently awarded NIH/NIAID grants a CETR U19 and a TBRU U19 and has over 350 peer reviewed publications. To complement the expertise of her mentors Dr. Farhat will be advised by Dr. Christiani a practicing pulmonary and critical care physician and world renowned researcher in the field of lung and environmental genetics. She will also collaborate and consult with Dr. Merce Crosas, a data scientist, and Dr. Pardis Sabeti, a computational biologist. She will rotate through Dr. Soumya Raychaudhuri's bioinformatics laboratory to diversify her exposure to biomedical Big Data. In addition, she will receive formal training in evolutionary biology, Bayesian and mixed-model biostatistics, computer science, leadership skills and grant writing. The collaborative opportunities, intellectual environment and resources available to Dr. Farhat are outstanding. Research: Infectious diseases continue to be a major cause of morbidity and mortality. Despite the availability of effective antimicrobials, pathogens are successfully evolving new disease phenotypes that allow them to resist killing by these drugs or in other instances cause more severe disease manifestations or wider chains of transmission. Drug resistance (DR) is now common and some bacteria have even become resistant to multiple types or classes of antibiotics6. A key strategy in the fight against emerging pathogen phenotypes in infectious diseases is surveillance, and early personalized therapy to prevent transmission and propagation of these strains. The timely initiation of antibiotic therapy to which the pathogen is sensitive has been shown to be the key factor influencing treatment outcome for a diverse array of infections. Molecular tests that rely on the detection of microbial genetic mutations are particularly promising for surveillance and diagnosis of these pathogen phenotypes but rely on a comprehensive understanding of how mutations associate with these pathogen phenotypes. Currently there is an explosion of data on pathogen whole genome sequences (WGS) that is increasingly generated from clinical laboratories. Data on disease phenotype may also be available, but methods for the analysis and interpretation of these Big Data are lagging. Here I propose tools to aid in this analysis leveraging Big Data sets from Mycobacterium tuberculosis (MTB) and my prior work. Specifically I propose to (1) develop a web-based public interface to several analysis tools, including a statistical learning model that can predict the MTB DR phenotype from its genomic sequence, (2) to develop and study an MTB gene-gene network, based on WGS data, to improve our understanding of the effect of mutation-mutation interactions on the DR phenotype, and (3) study the performance of methods in current use for the association of genotype and phenotype in pathogens, and develop a generalizable power calculator for the best performing method.         PUBLIC HEALTH RELEVANCE    Infectious agents of disease are successfully evolving drug resistance and other adaptations that threaten human health. Understanding the genetic mutations that underlie these disease phenotypes can inform surveillance and diagnostic strategies to combat this threat. Here I propose to develop accessible tools for pathogen genomic analysis that will help identify which genetic mutations are relevant to disease.                ",New Tools for the interpretation of Pathogen Genomic Data with a focus on Mycobacterium tuberculosis,9044227,K01ES026835,"['Affect', 'Antibiotic Therapy', 'Antibiotics', 'Area', 'Award', 'Bacteria', 'Big Data', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Cessation of life', 'Clinical', 'Collaborations', 'Communicable Diseases', 'Complement', 'Computational Science', 'Consult', 'Critical Care', 'DNA Sequence Alteration', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug resistance', 'Drug resistance in tuberculosis', 'Drug-sensitive', 'Environment', 'Epidemiology', 'Explosion', 'Exposure to', 'Future', 'Gene Structure', 'General Hospitals', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Epistasis', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Grant', 'Health', 'Human', 'Infection', 'Infectious Agent', 'Infectious Diseases Research', 'Institutes', 'International', 'Journals', 'K-Series Research Career Programs', 'Laboratories', 'Leadership', 'Low income', 'Lung', 'Lung diseases', 'Machine Learning', 'Maps', 'Massachusetts', 'Measures', 'Medical Genetics', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Mentorship', 'Methods', 'Microbial Genetics', 'Modeling', 'Molecular', 'Monitor', 'Morbidity - disease rate', 'Mutation', 'Mycobacterium tuberculosis', 'National Institute of Allergy and Infectious Disease', 'Nature', 'Network-based', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Phylogeny', 'Physicians', 'Population', 'Positioning Attribute', 'Public Health', 'Public Health Schools', 'Publications', 'Research', 'Research Personnel', 'Resistance', 'Resolution', 'Resources', 'Science', 'Scientist', 'Severity of illness', 'Social Medicine', 'Social Sciences', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Training Activity', 'Treatment outcome', 'Tuberculosis', 'United States National Institutes of Health', 'Variant', 'Virulent', 'Work', 'Writing', 'analytical tool', 'antimicrobial', 'base', 'burden of illness', 'career', 'combat', 'computer science', 'design', 'disease phenotype', 'experience', 'fight against', 'gene interaction', 'genome sequencing', 'genome-wide', 'global health', 'improved', 'instructor', 'killings', 'medical schools', 'microbial', 'microbial genome', 'mortality', 'pathogen', 'personalized medicine', 'prevent', 'prospective', 'public health relevance', 'simulation', 'skills', 'tool', 'transmission process']",NIEHS,MASSACHUSETTS GENERAL HOSPITAL,K01,2015,230806,-0.013874960631570248
"Informatic tools for predicting an ordinal response for high-dimensional data DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale. Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8900334,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Breast Cancer Patient', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2015,121902,-0.016790531903960848
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,-0.018101162152221417
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,-0.018101162152221417
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,-0.018101162152221417
"Translating from Rats to Humans: A Human Foraging Model of Decision-Making No abstract available PUBLIC HEALTH RELEVANCE:  Animal models of addiction are some of the most highly regarded models of psychopathology; however, there remains a disconnection between these pre-clinical models and treatment outcomes.  This division may be, in part, due to untested assumptions that different species recruit the same cognitive systems in the compared tasks.  The novel research outlined in this proposal will help improve the translation between human and nonhuman animal models, and in turn, improve treatment efficacy for these disorders.                ",Translating from Rats to Humans: A Human Foraging Model of Decision-Making,8977868,F31DA040335,"['Accounting', 'Advanced Development', 'Aggressive behavior', 'Alcohol or Other Drugs use', 'Animal Model', 'Animals', 'Back', 'Basic Science', 'Behavior', 'Behavioral', 'Behavioral Mechanisms', 'Categories', 'Chronic', 'Clinical', 'Clinical Psychology', 'Clinical Research', 'Clinical Sciences', 'Clip', 'Cognitive', 'Data', 'Decision Making', 'Development', 'Disease', 'Disinhibition', 'Economics', 'Emotional', 'Evaluation', 'Event', 'Exhibits', 'Faculty', 'Food', 'Human', 'Impulsivity', 'Individual Differences', 'Internet', 'Intervention', 'Investigation', 'Linear Models', 'Literature', 'Machine Learning', 'Magnetic Resonance', 'Maps', 'Measures', 'Mentorship', 'Methods', 'Minnesota', 'Modeling', 'Neurosciences', 'Participant', 'Patient Self-Report', 'Pattern', 'Pre-Clinical Model', 'Process', 'Psychology', 'Psychopathology', 'Rattus', 'Recruitment Activity', 'Regrets', 'Research', 'Research Personnel', 'Resources', 'Restaurants', 'Rewards', 'Sampling', 'Series', 'Staging', 'Stimulus', 'Students', 'Substance abuse problem', 'System', 'Task Performances', 'Testing', 'Time', 'Translating', 'Translations', 'Travel', 'Treatment Efficacy', 'Treatment outcome', 'Universities', 'Variant', 'Ventral Striatum', 'Work', 'addiction', 'analog', 'base', 'behavior test', 'cognitive process', 'cognitive system', 'design', 'effective therapy', 'experience', 'graduate student', 'human ethology', 'human subject', 'improved', 'neuroimaging', 'neurophysiology', 'novel', 'pleasure', 'pre-clinical', 'preference', 'public health relevance', 'relating to nervous system', 'research study', 'stem', 'trait', 'trait impulsivity', 'willingness']",NIDA,UNIVERSITY OF MINNESOTA,F31,2015,38875,-0.0015900806012814374
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8685211,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2014,69189,-0.01407434427552578
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.        This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8669000,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'inquiry-based learning', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2014,293329,-0.01268599345958007
"GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY     DESCRIPTION:  Many complex diseases such as cancer, cardiovascular disorders, and schizophrenia may be understood as failures in the functioning of nested hierarchies of biomolecular and cellular networks. These nested hierarchies control a range of processes including the differentiation and migration of cells, remodeling of extracellular matrices and tissues, and information encoding in neuronal subsystems. Washington University has established expertise in cutting edge imaging, molecular biology and genomic technologies synergistic with computational approaches such as machine learning and unraveling the principles of hierarchical organization and dynamics of complex systems. This collective expertise is being leveraged to develop new drugs, improve our ability to interpret sophisticated imaging data, understand how populations of neurons act collectively to accomplish complex tasks, and model the onset and progression of complex diseases as dynamical rewiring of hierarchical, multi-scale networks. Biological network analyses provide a rich set of tools for organizing and interpreting the vast quantities of data produced by state-of-the-art experimental protocols. The rapid advancement of computationally intensive research in these areas is outstripping the capabilities of CPU-based high performance computing (HPC) systems. This application would support the acquisition and integration of a large-scale IBM high performance cluster of Graphics Processor Units (GPUs) to be added as an upgrade to the existing IBM-designed Heterogeneous High Performance Computing environment to form a state-of-the-art hybrid computing capability. Such a resource is essential to match the growing need for high performance computing at Washington University and to support state of the art research software applications that are optimized for GPU computing. The acquisition and integration of a high performance GPU cluster will solve critical computing challenges that exist within Washington University's growing NIH research portfolio. The proposed state-of-the-art hybrid GPU/CPU computing capabilities will be deployed within the framework of a stable, productive and rapidly growing resource center. The addition of high-capacity GPU computing capabilities will allow critical calculation to be performed in hours instead of days and enable substantial increases in productivity for existing projects covering a broad range of application areas as well as enabling new research directions.             n/a",GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY,8640341,S10OD018091,"['Area', 'Biological', 'Biology', 'Cardiovascular Diseases', 'Complex', 'Computer Systems', 'Computer software', 'Data', 'Disease', 'Environment', 'Extracellular Matrix', 'Failure', 'Genomics', 'High Performance Computing', 'Hour', 'Hybrids', 'Image', 'Machine Learning', 'Malignant Neoplasms', 'Modeling', 'Molecular Biology', 'Neurons', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Productivity', 'Protocols documentation', 'Research', 'Resources', 'Schizophrenia', 'System', 'Technology', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'cell motility', 'computing resources', 'design', 'improved', 'innovation', 'tool']",OD,WASHINGTON UNIVERSITY,S10,2014,597700,-0.0029752254615794003
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8657051,R01GM053163,"['Address', 'Algorithms', 'Biochemical', 'Budgets', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Development', 'Drug Design', 'Evaluation', 'Funding', 'Geometry', 'Goals', 'Image', 'Machine Learning', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Output', 'Pattern', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Relative (related person)', 'Research', 'Shapes', 'Signal Transduction', 'Solutions', 'Specimen', 'Spottings', 'Structure', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2014,320096,-0.018904332713188574
"Reproducibility Assessment for Multivariate Assays  Project Summary. This Small Business Innovation Research project addresses the problem of assessing reproducibility in analyzing high-throughput data. In feature selection for data with large numbers of fea- tures, it is well known that some features will appear to affect an outcome by chance, and that subsequent predictions based on these features may not be as successful as initial results would seem to indicate. Similarly, there are often multiple stages, and many parameters, involved in the multivariate assays de- signed to analyze high-throughput profiles. For example, good results achieved with a particular combina- tion of settings for an instance of cross-validation may not generalize to other instances. The objective of this proposal is to extend new statistical methods for assessing reproducibility in replicate experiments to the context of machine learning, and demonstrate effectiveness in this application. The machine-learning methods to be investigated will include random forests, supervised principal components, lasso penal- ization and support vector machines. We will use simulated and real data from genomic applications to show the potential of this approach for providing reproducibility assessments that are not confounded with prespecified choices, for determining biologically relevant thresholds, for improving the accuracy of signal identification, and for identifying suboptimal results. Relevance. Although today's high-throughput technologies offer the possibility of revolutionizing clinical practice, the analytical tools available for extracting information from this amount of data are not yet sufficiently developed for targeted exploration of the underlying biology. This project directly addresses the need to make what the FDA terms IVDMIA (In-Vitro Diagnostic Multivariate Index Assays) transparent, interpretable, and reproducible, and is thus an opportunity to improve analysis products and services provided to companies that identify, characterize, and validate biomarkers for clinical diagnostics and drug development decision points. The long-term goal of the proposed project is to develop a platform for biomarker discovery and integrative genomic analysis, with reproducibility assessment incorporated into multivariate assays. This will enable evaluation and improvement of approaches to detecting the biological factors that affect a particular outcome, and lead to more efficient and more effective methods for disease diagnosis, treatment monitoring, and therapeutic drug development. PUBLIC HEALTH RELEVANCE: Statistical models play a key role in medical research in uncovering information from data that leads to new diagnostics and therapies. However, development of standards for reliability in biomedical data mining has not kept up with the rapid pace at which new data types and modeling approaches are being devised. This proposal is for new methods for quantifying reproducibility in biomedical data analyses that will have a far-reaching impact on public health by streamlining protocols, reducing costs and offering more effective clinical support systems.            ",Reproducibility Assessment for Multivariate Assays,8647816,R43GM109503,"['Address', 'Affect', 'Algorithms', 'Area', 'Bioinformatics', 'Biological Assay', 'Biological Factors', 'Biological Markers', 'Biology', 'ChIP-seq', 'Clinical', 'Cloud Computing', 'Data', 'Data Analyses', 'Decision Trees', 'Development', 'Diagnostic', 'Dimensions', 'Effectiveness', 'Evaluation', 'Evolution', 'Genomics', 'Goals', 'Guidelines', 'In Vitro', 'Investigation', 'Lasso', 'Lead', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Performance', 'Phase', 'Play', 'Protocols documentation', 'Public Health', 'Publishing', 'ROC Curve', 'Reproducibility', 'Research Project Grants', 'Scheme', 'Services', 'Signal Transduction', 'Simulate', 'Small Business Innovation Research Grant', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Methods', 'Statistical Models', 'Support System', 'Techniques', 'Technology', 'Therapeutic', 'Trees', 'Validation', 'analytical tool', 'base', 'clinical practice', 'cost', 'data mining', 'design', 'disease diagnosis', 'drug development', 'follow-up', 'forest', 'high throughput technology', 'improved', 'indexing', 'novel diagnostics', 'public health relevance', 'research study']",NIGMS,INSILICOS,R43,2014,131071,-0.013142963395423793
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,-0.008542411977654804
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8661774,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2014,1248956,-0.00144690446304343
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8653848,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,436094,-0.014425926966765276
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8725717,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2014,2015775,-0.030264770604566983
"NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR) DESCRIPTION (provided by applicant):  We propose to establish a NIDA Center of Excellence for Computational Drug Abuse Research (CDAR) between the University of Pittsburgh (Pitt) and (CMU), with the goal of advancing and ensuring the productive and broad usage of state-of-the-art computational technologies that will facilitate and enhance drug abuse (DA) research, both in the local (Pittsburgh) area and nationwide. To this end, we will develop/integrate tools for DA-domain-specific chemical-to-protein-to-genomics mapping using cheminformatics, computational biology and computational genomics methods by centralizing computational chemical genomics (or chemogenomics) resources while also making them available on a cloud server. The Center will foster collaboration and advance knowledge-based translational research and increase the effectiveness of ongoing funded research project (FRPs) via the following Research Support Cores: (1) The Computational Chemogenomics Core for DA (CC4DA) will help address polydrug addiction/polypharmacology by developing new chemogenomics tools and by compiling the data collected/generated, along with those from other Cores, into a DA knowledge-based chemogenomics (DA-KB) repository that will be made accessible to the DA community. (2) The Computational Biology Core (CB4DA) will focus on developing a resource for structure-based investigation of the interactions among substances of DA and their target proteins, in addition to assessing the drugability of receptors and transporters involved in DA and addiction. These activities will be complemented by quantitative systems pharmacology methods to enable a systems-level approach to DA research. (3) The Computational Genomics Core (CG4DA) will carry out genome-wide discovery of new DA targets, markers, and epigenetic influences using developed machine learning models and algorithms. (4) The Administrative Core will coordinate Center activities, provide management to oversee the CDAR activities in consultation with the Scientific Steering Committee (SSC) and an External Advisory Board (EAB), ensure the effective dissemination of software/data among the Cores and the FRPs, and establish mentoring mechanisms to train junior researchers. Overall, the Center will strive to achieve the long-term goal of translating advances in computational chemistry, biology and genomics toward the development of novel personalized DA therapeutics. We propose a Computational Drug Abuse Research (CDAR) Center, as a joint initiative between the  University of Pittsburgh and Carnegie Mellon University. The Center consist of three Cores (CC4DA, CB4DA  and CG4DA) that will leverage our expertise in computational chemogenomics, computational biology, and  computational genomics to facilitate basic and translational drug abuse and medication research.",NIDA Center of Excellence OF Computational Drug Abuse Research (CDAR),8743368,P30DA035778,"['Address', 'Algorithms', 'Area', 'Biological', 'Biology', 'Cannabinoids', 'Categories', 'Cells', 'Chemicals', 'Clinical Trials Network', 'Cloud Computing', 'Cocaine', 'Collaborations', 'Communities', 'Complement', 'Computational Biology', 'Computer software', 'Consultations', 'Data', 'Databases', 'Development', 'Doctor of Philosophy', 'Drug abuse', 'Effectiveness', 'Endocytosis', 'Ensure', 'Environmental Risk Factor', 'Feedback', 'Fostering', 'Funding', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human Genome', 'Intervention', 'Investigation', 'Joints', 'Leadership', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Modeling', 'Molecular', 'N-Methyl-D-Aspartate Receptors', 'National Institute of Drug Abuse', 'Neuropharmacology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Pharmacology', 'Phenotype', 'Proteins', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Support', 'Resources', 'Science', 'Signal Transduction', 'Software Tools', 'Structure', 'Substance Use Disorder', 'System', 'Systems Biology', 'Technology', 'Therapeutic', 'Training', 'Translating', 'Translational Research', 'Universities', 'addiction', 'base', 'biobehavior', 'cheminformatics', 'cloud based', 'computational chemistry', 'computer science', 'data mining', 'design', 'dopamine transporter', 'drug abuse prevention', 'epigenetic marker', 'falls', 'genome-wide', 'improved', 'insight', 'knowledge base', 'member', 'novel', 'operation', 'predictive modeling', 'prevent', 'professor', 'receptor', 'repository', 'tool']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P30,2014,1094316,-0.014989845877341989
"Methods for high-dimensional data in HIV/CVD research    DESCRIPTION (provided by applicant): Recent technological advances have yielded vast quantities of molecular and cellular data, affording unprecedented opportunities to understand complex disease etiologies and to inform clinical management strategies. However, in order to derive information from these rich stores of data we need to develop sound and appropriate analytic techniques. This need is especially relevant in studies at the intersection of human immunodeficiency virus (HIV) and cardiovascular disease (CVD), which are characterized by an elaborate set of interactions among viral and host factors. These factors include viral and host genetic profiles, as well as markers of caloric metabolism, immune activation and inflammation, which work together to determine response to therapy and overall disease progression. A comprehensive assessment of these markers presents several analytical challenges owing to the large number of potentially informative variables and the largely uncharacterized relationship among them. We propose a multi-faceted strategy that focuses on the development and application of integrative statistical approaches. Such approaches will allow us to explore and characterize novel hypotheses relating to the complex relationships among multiple genetic, environmental, demographic, and clinical factors and measures of disease progression. Specifically, this continuation application focuses on advancing and applying statistical methods in two settings: first, we consider population-based genetic association studies of innate-immunity, adipokine, drug metabolism and drug transport genes and markers of immune reconstitution, inflammation and risk of CVD in HIV-infected individuals; and second, we address investigations of metabolic and immunologic profiles that associate with immune recovery, inflammation and risk of CVD. The Specific Aims of the proposed research are to develop and evaluate: (1) Latent class and mixture modeling paradigms for (a) discovering and characterizing multi-locus genotype-trait associations and (b) evaluating unobservable haplotype-trait associations in candidate-gene investigations; and (2) Hierarchical mixture models and machine learning approaches for (a) monitoring quantitative biomarkers in resource-limited settings and (b) characterizing high- dimensional predictors of immune reconstitution and inflammation. IMPACT: This research will lead to the creation of appropriate and carefully evaluated analytic tools to derive information from the rich array of molecular and cellular data now available. Ultimately, this research will advance our ability to translate molecular and cellular level data for clinical decision making, serving at the cornerstone of personalized medicine.       PUBLIC HEALTH RELEVANCE: The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.         ",Methods for high-dimensional data in HIV/CVD research,8606493,R01HL107196,"['Address', 'Award', 'Biological Markers', 'Biological Models', 'Biometry', 'Candidate Disease Gene', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Clinical Research', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Development', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug Transport', 'Dyslipidemias', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genotype', 'Goals', 'HIV', 'Haplotypes', 'Immune', 'Immunologic Markers', 'Immunologics', 'Immunology', 'Individual', 'Inflammation', 'Integration Host Factors', 'Investigation', 'Laboratories', 'Lead', 'Lipids', 'Machine Learning', 'Measures', 'Medicine', 'Metabolic', 'Metabolic Marker', 'Metabolism', 'Methods', 'Microbiology', 'Modeling', 'Molecular', 'Monitor', 'Natural Immunity', 'Peer Review', 'Pharmacology', 'Publications', 'Recovery', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Testing', 'Textbooks', 'Time', 'Translating', 'Translational Research', 'Viral', 'Work', 'adaptive immunity', 'cardiovascular disorder risk', 'clinical decision-making', 'drug metabolism', 'genetic association', 'genetic profiling', 'genome wide association study', 'immune activation', 'insight', 'molecular array', 'novel', 'open source', 'population based', 'professor', 'public health relevance', 'reconstitution', 'response', 'sound', 'statistics', 'success', 'tool', 'trait', 'translational medicine']",NHLBI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2014,392192,-0.05749850413599663
"Models for synthesising molecular, clinical and epidemiological data, and transla     DESCRIPTION (provided by applicant): A mathematical or computational model of infectious disease transmission represents the process of how an infection spreads from one person to another. Such models have a long history within infectious disease epidemiology, and are useful tools for giving insight into the dynamics of epidemics and for evaluating the potential effect of control methods. The overall objective of this project is to substantially improve the methods by which models of infectious diseases transmission are calibrated against biological and disease surveillance data. This will both improve the utility of models as tools for analyzing data on infectious disease outbreaks (for instance to provide more rapid and reliable estimates of how transmissible and lethal a new virus is to public health agencies) and also improve the reliability of models as tools for predicting the likely effect of different interventions (such as vaccines or case isolation) to help policy makers make more informed decisions about control policies. As with many areas of biology and medicine, the data landscape for infectious diseases modeling is changing rapidly. Larger and more complex datasets are becoming available that cover many different aspects of the interaction between a pathogen and the human population: clinical episode data, genetic data about fast-evolving pathogens; animal-model transmission data and community-based representative serological data. The specific aims of our project are to: (a) develop new machine-learning based methods to discover interesting patterns in complex datasets related to the transmission of infectious disease, so as to better specify subsequent mechanistic mathematical or computational models; (b) derive new approaches for using more than one type of data simultaneously to calibrate transmission models and (c) derive new methods of parameter estimation for simulations which model the spatial spread of infection or model both the transmission and genetic evolution of a pathogen. We will achieve these aims in the applied context of research on three key infections: emerging infectious diseases (such as MERS-CoV - the novel coronavirus currently spreading in the Middle East), influenza and Streptococcus pneumonia (a major bacterial pathogen). Examples of the scientific questions we will address that cannot be answered with current methods are: (i) how many unobserved cases of MERS-CoV have occurred so far (to be answered using data on case clusters data, the spatial distribution of cases and viral genetic sequences)? (ii) how many people in different age groups are infected with influenza each year and how does their immune system respond to infection (to be answered using data on case incidence and serological testing of the population)? (iii) how much is vaccination coupled with prescribing practices influencing the emergence of resistant strains of pneumococcus (to be addressed with data on antibiotic and vaccine use, case incidence and bacterial strain frequency)?         PUBLIC HEALTH RELEVANCE: Mathematical and computational models of infectious disease spread can provide valuable information to aid policy-makers in the tough choices they face when trying to control infectious diseases, but models must be designed to make the best possible use of the often limited data available. As the digital footprints of our lives grow, so te datasets available for infectious disease models become larger and more complex. This project will develop new algorithms and methods to allow models to make better use of all available data and therefore better inform control policy planning for diseases such as: influenza, pneumococcal infection and novel viruses like MERS-CoV.            ","Models for synthesising molecular, clinical and epidemiological data, and transla",8703195,U01GM110721,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Animal Model', 'Antibiotics', 'Antigenic Variation', 'Area', 'Biological', 'Biology', 'Cells', 'Clinical', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Emerging Communicable Diseases', 'Epidemic', 'Epidemiology', 'Evolution', 'Face', 'Frequencies', 'Funding', 'Generations', 'Generic Drugs', 'Genetic', 'Genotype', 'Hospitalization', 'Human', 'Human Influenza A Virus', 'Immune', 'Immune system', 'Incidence', 'Individual', 'Infection', 'Infectious Disease Epidemiology', 'Influenza', 'Intervention', 'Joints', 'Knowledge', 'Location', 'Machine Learning', 'Maps', 'Medicine', 'Methods', 'Middle East', 'Modeling', 'Molecular', 'Monte Carlo Method', 'Movement', 'Natural History', 'Pattern', 'Persons', 'Phenotype', 'Pneumococcal Infections', 'Policies', 'Policy Maker', 'Population', 'Process', 'Public Health', 'Recording of previous events', 'Research', 'Serologic tests', 'Serological', 'Shapes', 'Site', 'Spatial Distribution', 'Specific qualifier value', 'Specificity', 'Stream', 'Streptococcus pneumoniae', 'Testing', 'Time', 'Vaccination', 'Vaccines', 'Variant', 'Virus', 'Work', 'age group', 'base', 'contextual factors', 'data exchange', 'data mining', 'design', 'digital', 'disease natural history', 'disease transmission', 'epidemiological model', 'forest', 'genetic evolution', 'improved', 'infectious disease model', 'innovation', 'insight', 'interest', 'mathematical model', 'meetings', 'mortality', 'novel', 'novel strategies', 'novel virus', 'pandemic influenza', 'pathogen', 'predictive modeling', 'public health relevance', 'resistant strain', 'seasonal influenza', 'simulation', 'social', 'surveillance data', 'tool', 'transmission process', 'virus genetics']",NIGMS,U OF L IMPERIAL COL OF SCI/TECHNLGY/MED,U01,2014,427668,0.002358813970665672
"Building an open-source cloud-based computational platform to improve data access   We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members.  One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. This proposal aims to fill this gap by developing a robust platform that integrates state-of-the-art open-source technologies for data storage, data access, data mining and analysis, annotation, visualization and reporting.  We previously developed a cloud-based BioDatomics platform for Next Generation Sequencing (NGS), BioDX, which has been successful and has been used commercially by several clients. This proposal aims to develop a new platform leveraging our experience with the BioDX platform that integrates: data storage and real-time data querying using Cloudera Impala; powerful and customizable analytics tools using R and its derivative Bioconductor suite of programs for bioinformatics; annotation integration and reporting which is an existing feature of BioDX; and a visual programming interface that will simplify and enhance the development and maintenance of reproducible analytics workflows. We believe this powerful integrated data platform, if successful, will enable real-time collaboration, dramatically reduce data repository costs, and increase the efficiency and efficacy of data analyses for translating experimental data into actionable research products.  We are committed to analyzing stakeholder needs and optimizing hardware, software and information technology systems to meet their demands. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools.  The initial phase of work will involve development of the platform, optimizing performance on the cloud and testing the integration of new technology. BioDatomics is committed to funding the next phase of work which will include usability testing and finalizing a commercial product, following which full commercialization will proceed. Preliminary commercialization plans have demonstrated that the project has the capacity to generate a million dollars in revenue during the first full year after commercial release.  The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This open source platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders. PUBLIC HEALTH RELEVANCE: One of the unmet infrastructural challenges of modern molecular research is the availability of computational platforms that allow the management of large databases, easy access to data, the availability of powerful customizable tools for data mining, analysis and visualization, and integration of different data sources to allow successful analysis of complex data problems. Such problems are commonplace in high- throughput molecular research. We propose to develop a novel, cost-effective, cloud-based data and analytics platform that will provide efficient data storage solutions and enhanced analytics, annotation and reporting capabilities for supporting and accelerating clinical and molecular research in the treatment of substance use disorders (SUD). This open source platform, which leverages existing BioDX technology, will provide a centralized, multi-user environment that enables and encourages collaborative research and information dissemination among team members. This platform will enhance stakeholder capabilities for developing, implementing and testing various models for substance addiction, risky behavior, discovery of molecular targets for treatment, genomic profiling of patients and other relevant scientific questions. Users will have access to modern statistical, machine learning, data mining and visualization tools. The ultimate beneficiaries of this platform will be government agencies, academic researchers and pharmaceutical companies pursuing collaborative projects to discover treatments for substance abuse disorders. This platform will enable significant savings to the end users in terms of data storage and analytic capabilities, and promises to have a major impact in increasing the success of molecular, clinical and translational research for substance abuse disorders.            ",Building an open-source cloud-based computational platform to improve data access,8647860,R43DA036970,"['Academia', 'Apache Indians', 'Bioconductor', 'Bioinformatics', 'Biological', 'Businesses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Clinical', 'Clinical Research', 'Collaborations', 'Commit', 'Complex', 'Computer software', 'Cost Savings', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Databases', 'Development', 'Disease', 'Distributed Databases', 'Drug Industry', 'Environment', 'Expenditure', 'Funding', 'Genomics', 'Government Agencies', 'Growth', 'Imagery', 'Industry', 'Information Dissemination', 'Information Systems', 'Java', 'Language', 'Licensing', 'Link', 'Machine Learning', 'Maintenance', 'Marketing', 'Messenger RNA', 'Modeling', 'Molecular', 'Molecular Target', 'Mutation', 'Online Systems', 'Patients', 'Performance', 'Pharmacologic Substance', 'Phase', 'Programming Languages', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk Behaviors', 'Savings', 'Services', 'Software Engineering', 'Software Tools', 'Solutions', 'Speed', 'Statistical Data Interpretation', 'Statistical Models', 'Substance Addiction', 'Substance Use Disorder', 'Substance abuse problem', 'System', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Translational Research', 'United States National Institutes of Health', 'Visual', 'Work', 'base', 'beneficiary', 'cloud based', 'commercialization', 'computer infrastructure', 'cost', 'cost effective', 'data mining', 'drug discovery', 'experience', 'improved', 'mathematical model', 'meetings', 'member', 'models and simulation', 'new technology', 'next generation sequencing', 'novel', 'open source', 'programs', 'public health relevance', 'substance abuse treatment', 'success', 'tool', 'usability', 'user-friendly']",NIDA,"BIODATOMICS, LLC",R43,2014,195584,-0.006851392343029263
"Heterogeneous and Robust Survival Analysis in Genomic Studies     DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed.         PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.            ",Heterogeneous and Robust Survival Analysis in Genomic Studies,8696520,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,255295,-0.023047742273927963
"A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases     DESCRIPTION (provided by applicant): I am trained as a computational biologist and statistician, and I am currently a postdoctoral fellow at Boston Children's Hospital, Harvard Medical School. My main career goal is to become an independent researcher at a major research institution. I plan to continue my current research pursuits in global health and infectious diseases. Specifically, I aim to continue developing mathematical and computational approaches for modeling to understand disease transmission, forecasting future dynamics and evaluating interventions for public policy decisions. As a postdoctoral research fellow, I have had the wonderful opportunity of working with data from multiple sources. Although several of these data streams could be labeled as ""Big Data"", I typically work with the data after it is already processed, filtered and aggregated to a daily or weekly resolution. While I have developed the necessary skills for modeling these already processed data, there are three important areas where I require additional training, mentoring, and experience: (1) advanced computational skills especially in the use of high performance computing and informatics tools, (2) techniques in computational machine learning and data mining necessary for data acquisition and processing, and (3) biostatistical methodology needed for the statistical design of studies involving big data. These three training and mentoring aims would enable me to develop the skills necessary to become an independent investigator in Big Data Science for biomedical research. Boston Children's School and Harvard Medical School are leading institutions in translational biomedical research, thereby making them the ideal environment to pursue the training and research aims in this proposal. The recent emergence of infectious diseases such as the avian influenza H7N9 in China, and re-emergence of diseases such as polio in Syria underscores the importance of strengthening immunization and emergency response programs for the prevention and control of infectious diseases. Researchers have developed computational and mathematical models to capture determinants of infectious disease dynamics and identify factors that support prediction of these dynamics, provide estimates of disease risk, and evaluate various intervention scenarios. While these studies have been extremely useful for the understanding of infectious disease transmission and control, most have been disease specific and solely used data from traditional disease surveillance systems. In contrast, there is a huge amount of internet-based data that have been extensively assessed and validated for public health surveillance in the last decade, but it has been scarcely used in conjunction with other data sources for modeling to predict disease spread. Using these novel digital event-based data sources in combination with climate and case data from traditional disease surveillance systems, we will establish a much needed framework for integrating these disparate data sources for modeling to estimate disease risk and forecasting temporal dynamics of infectious diseases. Our approach will be achieved through three aims. The first objective is to develop an automated process for acquiring, processing and filtering data for modeling (Aim 1). Once we gather this data, we will develop temporal models for the dynamical assessment of the relationship between the various data variables and infectious disease incidence (Aim 2). Finally, we will assess the utility of the modeling approaches developed under Aim 2 for forecasting temporal trends of infectious diseases (Aim 3). Through data acquisition, thorough processing, statistical and epidemiological modeling, and guided by advisers with expertise in biomedical informatics, computer science and statistics, we plan to achieve a comprehensive approach to integrating multiple data streams for modeling to forecast infectious diseases.         PUBLIC HEALTH RELEVANCE: Although there have been significant medical and technological advances towards infectious disease prevention, surveillance and control, infectious diseases still account for an estimated 15 million deaths each year worldwide. Reliable forecasts of infectious disease dynamics can influence decisions regarding prioritization of limited resources during outbreaks, optimization of disease interventions and implementation of rigorous surveillance processes for quicker case identification and control of emerging disease outbreaks. Our goal is therefore to develop a data mining/informatics framework that leverages the huge amount of digital event-based data sources in combination with climate data, and data from traditional disease surveillance systems for modeling and forecasting infectious diseases.            ",A Framework for Integrating Multiple Data Sources for Modeling and Forecasting of Infectious Diseases,8829434,K01ES025438,"['Accounting', 'Address', 'Area', 'Avian Influenza', 'Big Data', 'Biological Models', 'Biomedical Research', 'Boston', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Child', 'China', 'Climate', 'Communicable Diseases', 'Computer Simulation', 'Coronavirus', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Dengue', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease model', 'Emergency Situation', 'Emerging Communicable Diseases', 'Environment', 'Epidemic', 'Epidemiology', 'Event', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Human', 'Humidity', 'Immunization', 'Incidence', 'Individual', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A Virus, H7N9 Subtype', 'Informatics', 'Institution', 'International', 'Internet', 'Intervention', 'Label', 'Linear Models', 'Machine Learning', 'Medical', 'Mentors', 'Methodology', 'Middle East', 'Modeling', 'Monitor', 'Outcome', 'Pattern', 'Pediatric Hospitals', 'Poliomyelitis', 'Population Surveillance', 'Postdoctoral Fellow', 'Prevention program', 'Process', 'Public Health', 'Public Policy', 'Report (account)', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Research Proposals', 'Research Training', 'Resolution', 'Resources', 'Review Literature', 'Schools', 'Science', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Stream', 'Syndrome', 'Syria', 'System', 'Techniques', 'Temperature', 'Time', 'Training', 'Weight', 'Work', 'World Health Organization', 'base', 'biomedical informatics', 'career', 'computer science', 'computerized data processing', 'data acquisition', 'data integration', 'data mining', 'data modeling', 'digital', 'disease transmission', 'disorder control', 'disorder prevention', 'disorder risk', 'epidemiological model', 'experience', 'global health', 'improved', 'infectious disease model', 'mathematical model', 'medical schools', 'news', 'novel', 'pandemic influenza', 'public health relevance', 'respiratory', 'response', 'skills', 'social', 'statistics', 'tool', 'trend', 'web based interface']",NIEHS,BOSTON CHILDREN'S HOSPITAL,K01,2014,42469,0.002036301452338346
"Predicting Resilience in the Human Microbiome     DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience.         PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.                            ",Predicting Resilience in the Human Microbiome,8741929,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Statistical Methods', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiota', 'human subject', 'innovation', 'mathematical methods', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'public health relevance', 'resilience', 'response', 'theories', 'tool', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2014,413065,0.00828655626121224
"CSHL Computational and Comparative Genomics Course     DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions.         PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.                 ",CSHL Computational and Comparative Genomics Course,8737540,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'public health relevance', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2014,52816,-0.010295532070478805
"Data-Driven Statistical Learning with Applications to Genomics     DESCRIPTION (provided by applicant): This project involves the development of statistical and computational methods for the analysis of high throughput biological data. Effective methods for analyzing this data must balance two opposing ideals. They must be (a) flexible and sufficiently data-adaptive to deal with the data's complex structure, yet (b) sufficiently simpe and transparent to interpret their results and analyze their uncertainty (so as not to mislead with conviction). This is additionally challenging because these datasets are massive, so attacking these problems requires a marriage of statistical and computational ideas. This project develops frameworks for attacking several problems involving this biological data. These frameworks balance flexibility and simplicity and are computationally tractable even on massive datasets. This application has three specific aims. Aim 1: A flexible and computationally tractable framework for building predictive models. Commonly we are interested in modelling phenotypic traits of an individual using omics data. We would like to find a small subset of genetic features which are important in phenotype expression level. In this approach, I propose a method for flexibly modelling a response variable (e.g. phenotype) with a small, adaptively chosen subset of features, in a computationally scalable fashion. Aim 2: A framework for jointly identifying and testing regions which differ across conditions. For example, in the context of methylation data measured in normal and cancer tissue samples, one might expect that some regions are more methylated in one tissue type or the other. These regions might suggest targets for therapy. However, we do not have the background biological knowledge to pre-specify regions to test. I propose an approach which adaptively selects regions and then tests them in a principled way. This approach is based on a convex formulation to the problem, using shrinkage to achieve sparse differences. Aim 3: A principled framework for developing and evaluating predictive biomarkers during clinical trials. Modern treatments target specific genetic abnormalities that are generally present in only a subset of patients with a disease. A major current goal in medicine is to develop biomarkers that identify those patients likely to benefit from treatment. I propose a framework for developing and testing biomarkers during large-scale clinical trials. This framework simultaneously builds these biomarkers and applies them to restrict enrollment into the trial to only those likely to benefit from treatment. The statistical tools that result from th proposed research will be implemented in freely available software.         PUBLIC HEALTH RELEVANCE: Recent advances in high-throughput biotechnology have provided us with a wealth of new biological data, a large step towards unlocking the tantalizing promise of personalized medicine: the tailoring of treatment to the genetic makeup of each individual and disease. However, classical statistical and computational tools have proven unable to exploit the extensive information these new experimental technologies bring to bear. This project focuses on building new flexible, data-adaptive tools to translate this wealth of low level information into actionable discoveries, and actual biological understanding.            ",Data-Driven Statistical Learning with Applications to Genomics,8796068,DP5OD019820,"['Accounting', 'Address', 'Bayesian Modeling', 'Biological', 'Biological Markers', 'Biology', 'Biotechnology', 'Cancer Patient', 'Clinical Trials', 'Clinical Trials Design', 'Code', 'Complex', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Drug Formulations', 'Enrollment', 'Equilibrium', 'Event', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Individual', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Marriage', 'Measurement', 'Measures', 'Medicine', 'Memory', 'Methods', 'Methylation', 'Modeling', 'Molecular Abnormality', 'Outcome', 'Patients', 'Performance', 'Phenotype', 'Population', 'Proteomics', 'Reading', 'Research', 'Research Personnel', 'Science', 'Simulate', 'Single Nucleotide Polymorphism', 'Site', 'Somatic Mutation', 'Specific qualifier value', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Telomerase', 'Testing', 'Time', 'Tissue Sample', 'Translating', 'Uncertainty', 'Update', 'Ursidae Family', 'Variant', 'Work', 'base', 'computerized tools', 'flexibility', 'high throughput analysis', 'interest', 'novel', 'patient population', 'predictive modeling', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'tool', 'trait', 'transcriptome sequencing']",OD,UNIVERSITY OF WASHINGTON,DP5,2014,361063,-0.014877428980211953
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development     DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation.             n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,8697162,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Biophysical Process', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Simulate', 'Staging', 'Statistical Models', 'Structure', 'Students', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGIA STATE UNIVERSITY,R01,2014,333533,-0.016011617230946868
"The q-bio Summer School     DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required.         PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.            ",The q-bio Summer School,8643269,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'Underrepresented Minority', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'public health relevance', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2014,171217,-6.362309526271044e-05
"Integration and visualization of diverse biological data    DESCRIPTION (provided by applicant): Modern genome-scale experimental techniques enable for the first time in biological research the comprehensive monitoring of the entire molecular regulatory events leading to disease. Their integrative analyses hold the promise of generating specific, experimentally testable hypotheses, paving the way for a systems-level molecular view of complex disease. However, systems-level modeling of metazoan biology must address the challenges of: 1. biological complexity, including individual cell lineages and tissue types, 2. the increasingly large scale of data in higher organisms, and 3. the diversity of biomolecules and interaction mechanisms in the cell. The long-term goal of this research is to address these challenges through the development of bioinformatics frameworks for the study of gene function and regulation in complex biological systems thereby contributing to a greater understanding of human disease. In the initial funding period, we have developed accurate methods for integrating and visualizing diverse functional genomics data in S. cerevisiae and implemented them in interactive web-based systems for the biology community. Our methods have led to experimental discoveries of novel biology, are widely used by the yeast community, and are integrated with the SGD model organism database. We now propose to leverage our previous work to develop novel data integration and analysis methods and implement them in a public system for human data. In the proposed research period, we will create algorithms appropriate for integrating metazoan data in a tissue- and cell-lineage specific manner in health and disease. We will also develop novel hierarchical methods for predicting specific molecular interaction mechanisms and will extend our methods for integrating additional biomolecules. These methods will direct experiments focused on the glomerular kidney filter, a critical and complex component of the human vascular system whose dysfunction directly contributes to microvascular disease. Prediction of these cell-lineage specific functional networks will advance the understanding of the glomerulus function and its role in microvascular disease, leading to better clinical predictors, diagnoses, and treatments. From a technical perspective, application to glomerular biology will enable iterative improvement of the proposed methods based on experimental feedback. The end product of this research will be a general, robust, interactive, and automatically updated system for human data integration and analysis that will be freely available to the biomedical community. We will leverage parallel processing technologies (inspired by Google- type cloud computing solutions) to ensure interactive-analysis speed on the system. This system will allow biomedical researchers to synthesize, analyze, and visualize diverse data in human biology, enabling accurate predictions of biological networks and understanding their cell-lineage specificity and role in disease. Such integrative analyses will provide experimentally testable hypotheses, leading to a deeper understanding of complex disorders and paving the way to molecular-defined tissue targeted therapies and drug development.       PUBLIC HEALTH RELEVANCE: Our general system will enable integrative analysis of human functional genomics data in a cell-lineage and disease-focused manner, allowing biomedical researchers to identify potential clinical biomarkers and to formulate specific hypotheses elucidating the cause and development of a variety of complex disorders. Our application of this system to generate cell-lineage specific functional networks will lead to a better understanding of the glomerulus function and will directly benefit human health through the development of improved predictors, diagnoses, and treatments for microvascular disease.            ",Integration and visualization of diverse biological data,8601095,R01GM071966,"['Address', 'Algorithms', 'Bayesian Method', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Models', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Cells', 'Clinical', 'Cloud Computing', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Endothelium', 'Ensure', 'Event', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Biology', 'Imagery', 'Individual', 'Joints', 'Kidney', 'Kidney Diseases', 'Kidney Glomerulus', 'Lead', 'Machine Learning', 'Medicine', 'Methodology', 'Methods', 'Microvascular Dysfunction', 'Modeling', 'Molecular', 'Monitor', 'Mus', 'Online Systems', 'Organism', 'Participant', 'Plasma', 'Progress Reports', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Role', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Sampling', 'Solutions', 'Specificity', 'Speed', 'Structure', 'Structure of glomerular mesangium', 'System', 'Systems Biology', 'Systems Integration', 'Techniques', 'Technology', 'Time', 'Tissues', 'Update', 'Urine', 'Vascular System', 'Work', 'Yeasts', 'base', 'biological research', 'biological systems', 'cell type', 'complex biological systems', 'data integration', 'drug development', 'functional genomics', 'gene function', 'genome database', 'human data', 'human disease', 'human tissue', 'improved', 'model organisms databases', 'novel', 'parallel processing', 'podocyte', 'public health relevance', 'research study', 'therapy development', 'transcriptomics']",NIGMS,PRINCETON UNIVERSITY,R01,2014,391301,-0.025261921472653406
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8714054,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2014,227026,-0.016790531903960848
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,-0.018101162152221417
"MACHINE LEARNING TO FORECAST ZOONOTIC DISEASE EMERGENCE    DESCRIPTION (provided by applicant): As over 70% of emerging infectious diseases are caused by parasites or pathogens transmitted from animals to humans (leading to 'zoonotic' infections), a fundamental issues for public health is identifying the drivers leading to zoonotic diseases in humans. Cross-species transmission of infectious agents depends on numerous traits of hosts, their infectious agents, and environmental factors defining the external context of disease. Previous studies identifying predictors of cross-species transmission have been limited by a focus on single infectious diseases (e.g., rabies, Lyme disease) at restricted spatial scales, in part because large-scale analyses spanning numerous host species and infectious agents are precluded by the many complex interactions, autocorrelations, and sampling biases common in multivariate, high-dimensional data. The proposed research confronts these computational limitations through the innovative application of machine learning algorithms. Specifically, analyses will address three outstanding and interrelated questions in global health: (1) What characteristics signal a predisposition of mammalian host species to be reservoirs of zoonotic disease?; (2) What traits among infectious agents predict their potential to cause zoonotic infection?; (3) What are the most important environmental and anthropogenic predictors of zoonotic outbreaks globally? Analyses will apply a series of supervised, unsupervised and semi-supervised machine learning algorithms to new, global-scale databases containing biological, ecological, environmental, and anthropogenic data for three groups of mammalian hosts (primates, carnivores, and ungulates) and their zoonotic infectious agents. A long-term goal of this research is to empirically develop ""rules of thumb"" about zoonotic diseases by highlighting the key traits of mammalian hosts, infectious agents, and the environmental and human factors describing zoonotic outbreaks in recent history. Ultimately, research proposed herein will provide a basis for predicting the geographic locations, infectious agents, and animal reservoirs from which future zoonoses will emerge.        This project proposes to investigate the factors driving zoonotic disease outbreaks and cross-species transmission from wild mammals into humans through the innovative application of machine learning algorithms to newly published data describing hundreds of infectious agents, their mammalian host species, human populations, and the global environment. Ultimately, this project aims to predict the locations and species from which future diseases will emerge, and is therefore directly relevant for the improvement of human health.         ",MACHINE LEARNING TO FORECAST ZOONOTIC DISEASE EMERGENCE,8515458,F32GM087811,"['Address', 'Algorithms', 'Animals', 'Area', 'Automobile Driving', 'Award', 'Biological', 'Characteristics', 'Climate', 'Communicable Diseases', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Disease', 'Disease Outbreaks', 'Ecology', 'Emerging Communicable Diseases', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Future', 'Geographic Locations', 'Goals', 'Health', 'Human', 'Infection', 'Infectious Agent', 'Location', 'Lyme Disease', 'Machine Learning', 'Mammals', 'Methods', 'Nature', 'Output', 'Parasites', 'Parasitic Diseases', 'Pattern', 'Pattern Recognition', 'Population', 'Precipitation', 'Predisposition', 'Primates', 'Public Health', 'Publications', 'Publishing', 'Rabies', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Sampling Biases', 'Series', 'Signal Transduction', 'Source', 'Training', 'Ungulate', 'Vertebrates', 'West Nile virus', 'Zoonoses', 'Zoonotic Infection', 'anthropogenesis', 'base', 'career development', 'comparative', 'computer science', 'disease transmission', 'global environment', 'global health', 'innovation', 'land use', 'pathogen', 'trait', 'transmission process']",NIGMS,UNIVERSITY OF GEORGIA,F32,2013,55670,0.0053647442871486845
"Analytical Approaches to Massive Data Computation with Applications to Genomics     DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.              n/a",Analytical Approaches to Massive Data Computation with Applications to Genomics,8599823,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics']",NCI,BROWN UNIVERSITY,R01,2013,71329,-0.014087593961857648
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.        This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8501535,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2013,283518,-0.01268599345958007
"HyPhy: Molecular Evolutionary Analyses    DESCRIPTION (provided by applicant): Project Description HyPhy (http://www.hyphy.org) is a scriptable software platform designed to enable flexible and powerful analyses of DNA, RNA, codon, amino acid and other types of sequence data in an evolutionary context. Such analyses have become an indispensable component of most research studies that make use of comparative genomic data. Biologists and bioinformaticians increasingly recognize the benefits of molecular evolutionary analyses. Since its initial release in 2001, HyPhy has become a relatively stable and mature product, and has been downloaded by more than 4,500 unique users, integrated into several popular web-based genomic data analysis servers, cited in over 400 peer-reviewed publications and described in three book chapters, in spite of the fact that the development of the package has never been directly funded. This proposal seeks support to improve the quality, performance, reliability, modularity, documentation and feature sets of the HyPhy system. Specific aims can be divided into four major areas: 1. Software engineering, testing, and documentation of the HyPhy codebase. 2. The development of a high-performance engine for phylogenetic maximum likelihood model fitting and inference. 3. Extension of a newly-initiated toolbox for machine learning applications in molecular evolution. 4. Creation and maintenance of a wiki-themed documentation resource.      PUBLIC HEALTH RELEVANCE: Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.           Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.",HyPhy: Molecular Evolutionary Analyses,8542870,R01GM093939,"['Amino Acids', 'Animal Model', 'Area', 'Biomedical Research', 'Book Chapters', 'Classification', 'Code', 'Codon Nucleotides', 'Collection', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Development', 'Documentation', 'Drug resistance', 'Evolution', 'Funding', 'Gene Mutation', 'Genes', 'Genetic Programming', 'Genome', 'Genomics', 'Goals', 'Human', 'Immune response', 'Internet', 'Language', 'Libraries', 'Likelihood Functions', 'Machine Learning', 'Maintenance', 'Modeling', 'Molecular', 'Molecular Evolution', 'Molecular Sequence Data', 'Online Systems', 'Pathogenicity', 'Pattern', 'Peer Review', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Procedures', 'Publications', 'RNA', 'Research Personnel', 'Resources', 'Software Engineering', 'Study models', 'System', 'Testing', 'Viral', 'Work', 'acronyms', 'base', 'combinatorial', 'comparative genomics', 'design', 'flexibility', 'improved', 'pathogen', 'pressure', 'programs', 'protein structure function', 'public health relevance', 'research study', 'tool', 'user-friendly', 'wiki']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2013,265099,-0.02010502719300724
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8470172,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2013,309127,-0.018904332713188574
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8473164,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2013,1216983,-0.00144690446304343
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8461069,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,425454,-0.014425926966765276
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8722983,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,115680,-0.030264770604566983
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8548395,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,1871839,-0.030264770604566983
"Methods for high-dimensional data in HIV/CVD research    DESCRIPTION (provided by applicant): Recent technological advances have yielded vast quantities of molecular and cellular data, affording unprecedented opportunities to understand complex disease etiologies and to inform clinical management strategies. However, in order to derive information from these rich stores of data we need to develop sound and appropriate analytic techniques. This need is especially relevant in studies at the intersection of human immunodeficiency virus (HIV) and cardiovascular disease (CVD), which are characterized by an elaborate set of interactions among viral and host factors. These factors include viral and host genetic profiles, as well as markers of caloric metabolism, immune activation and inflammation, which work together to determine response to therapy and overall disease progression. A comprehensive assessment of these markers presents several analytical challenges owing to the large number of potentially informative variables and the largely uncharacterized relationship among them. We propose a multi-faceted strategy that focuses on the development and application of integrative statistical approaches. Such approaches will allow us to explore and characterize novel hypotheses relating to the complex relationships among multiple genetic, environmental, demographic, and clinical factors and measures of disease progression. Specifically, this continuation application focuses on advancing and applying statistical methods in two settings: first, we consider population-based genetic association studies of innate-immunity, adipokine, drug metabolism and drug transport genes and markers of immune reconstitution, inflammation and risk of CVD in HIV-infected individuals; and second, we address investigations of metabolic and immunologic profiles that associate with immune recovery, inflammation and risk of CVD. The Specific Aims of the proposed research are to develop and evaluate: (1) Latent class and mixture modeling paradigms for (a) discovering and characterizing multi-locus genotype-trait associations and (b) evaluating unobservable haplotype-trait associations in candidate-gene investigations; and (2) Hierarchical mixture models and machine learning approaches for (a) monitoring quantitative biomarkers in resource-limited settings and (b) characterizing high- dimensional predictors of immune reconstitution and inflammation. IMPACT: This research will lead to the creation of appropriate and carefully evaluated analytic tools to derive information from the rich array of molecular and cellular data now available. Ultimately, this research will advance our ability to translate molecular and cellular level data for clinical decision making, serving at the cornerstone of personalized medicine.       PUBLIC HEALTH RELEVANCE: The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.         ",Methods for high-dimensional data in HIV/CVD research,8423746,R01HL107196,"['Address', 'Award', 'Biological Markers', 'Biological Models', 'Biometry', 'Candidate Disease Gene', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Clinical Research', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Development', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug Transport', 'Dyslipidemias', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genotype', 'Goals', 'HIV', 'Haplotypes', 'Immune', 'Immunologic Markers', 'Immunologics', 'Immunology', 'Individual', 'Inflammation', 'Integration Host Factors', 'Investigation', 'Laboratories', 'Lead', 'Lipids', 'Machine Learning', 'Measures', 'Medicine', 'Metabolic', 'Metabolic Marker', 'Metabolism', 'Methods', 'Microbiology', 'Modeling', 'Molecular', 'Monitor', 'Natural Immunity', 'Peer Review', 'Pharmacology', 'Publications', 'Recovery', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Testing', 'Textbooks', 'Time', 'Translating', 'Translational Research', 'Viral', 'Work', 'adaptive immunity', 'cardiovascular disorder risk', 'clinical decision-making', 'drug metabolism', 'genetic association', 'genetic profiling', 'genome wide association study', 'immune activation', 'insight', 'molecular array', 'novel', 'open source', 'population based', 'professor', 'public health relevance', 'reconstitution', 'response', 'sound', 'statistics', 'success', 'tool', 'trait', 'translational medicine']",NHLBI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2013,380937,-0.05749850413599663
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8426190,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'stress disorder', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,315672,-0.0203995311475224
"Predicting Resilience in the Human Microbiome     DESCRIPTION (provided by applicant): Humans have co-evolved with complex, dynamic microbial communities that play essential roles in nutrition, metabolism, immunity, and numerous other aspects of human physiology. Hence, maintenance and recovery of key beneficial services by the microbiota in the face of disturbance is fundamental to health. Yet, stability and resilience vary in, and between individuals, and are poorly understood. Our goal is to identify features of the human microbiome that predict microbial community stability and resilience following disturbance. We propose an innovative large-scale clinical study design that will generate the necessary compositional and functional data from the most relevant ecosystem, i.e., humans!  We will develop novel statistical and mathematical methods for data integration (sparse, non-linear multi-table methods), and test existing ecological theories and apply statistical learning strategies to allow data-driven investigation of ecological and clinical properties that determine and predict stability and/or resilience. The breadth and magnitude of this project's impact are significant: We envision tests to predict microbial community responses to disturbance, and procedures to stabilize or restore beneficial microbial interactions as needed. A predictive understanding of the stability and resilience of the gut microbiota will advance the rational practice of medicine. There are three key innovative aspects to our approach: 1) sequential perturbations of different types in a large number of human subjects sampled over time; 2) multiple compositional and functional measurements made on the same samples; and 3) novel data integration methods that incorporate all of the information. Aim 1. Profile the human microbiome before, during and after multiple forms of disturbance. One hundred subjects will each be sampled at 40 time points over a 34 week study period that encompasses two types of perturbation in each subject (dietary shift, and bowel cleansing or antibiotic). From each sample, we will determine taxonomic composition, genomic content, meta-transcriptome, and metabolomic profiles. Aim 2. Discover resilience: Develop non-linear approaches for complex data integration using sparse, multiple-table methods. We will develop a novel sparse, multiple-table approach for data integration and simultaneous analysis of diverse types of complex data over time. Aim 3. Explain resilience: Use statistical learning approaches to find the predictive features that characterize resilience. Using the multiple table approach, we will compare routine unperturbed dynamics within a community to the varied responses to a perturbation, define stable states, and identify common network features characteristic of resilient communities subjected to different forms of disturbance. Finally, we wil use validation techniques to confirm these candidate predictors of community resilience.         PUBLIC HEALTH RELEVANCE: Humans rely on the microbial communities that colonize the gut for a wide variety of critical functions, including nutrition, immune system maturation, protection against infection by disease-causing microbes, and detoxification of environmental chemicals. Daily life is punctuated by events, such as exposure to antibiotics or other chemicals, or changes in diet, that sometimes disturb or destabilize our microbial communities with potentially severe and sustained negative impacts on health. We propose an ambitious study in which we will monitor the microbial communities of healthy humans before, during and after several types of planned disturbance, and discover community features that predict future stability or future recovery from disturbance, with the expectation that our findings will fundamentally change the practice of medicine.                            ",Predicting Resilience in the Human Microbiome,8549818,R01AI112401,"['Allergic Disease', 'Antibiotics', 'Attention', 'Characteristics', 'Chemicals', 'Chronic', 'Clinical', 'Clinical Research', 'Communities', 'Complex', 'Data', 'Data Set', 'Diet', 'Dimensions', 'Disease', 'Drug Metabolic Detoxication', 'Ecology', 'Ecosystem', 'Event', 'Exposure to', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genomics', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Immune system', 'Immunity', 'Individual', 'Infection', 'Inflammatory', 'Intervention', 'Intestines', 'Investigation', 'Life', 'Machine Learning', 'Maintenance', 'Measurement', 'Medicine', 'Metabolism', 'Methods', 'Microbe', 'Monitor', 'Multivariate Analysis', 'Obesity', 'Output', 'Physiology', 'Play', 'Predisposition', 'Procedures', 'Property', 'Recovery', 'Research Design', 'Role', 'Sampling', 'Services', 'Taxon', 'Techniques', 'Testing', 'Time', 'Validation', 'abstracting', 'analytical method', 'data integration', 'environmental chemical', 'expectation', 'gut microbiota', 'human subject', 'innovation', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'novel', 'nutrition', 'pathogen', 'public health relevance', 'resilience', 'response', 'theories', 'tool', 'urinary']",NIAID,PALO ALTO VETERANS INSTIT FOR RESEARCH,R01,2013,1235799,0.00828655626121224
"CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development     DESCRIPTION (provided by applicant): Background: Dendritic arbor shape and functional properties emerge from the interaction of many complex developmental processes. It is now accepted that multiple local-level interactions of cytoskeleton elements direct the growth and development of the dendrite arbor. However, the specific mechanisms that control developmental acquisition of final functional dendritic properties are largely unknown. Addressing this fundamental question requires novel data driven systems-biology tools to study developmental and biophysical mechanisms in the same neuronal model. A tightly-knit collaboration between molecular genetics, quantitative morphometry, and mathematical simulation can for the first time enable large-scale studies capable of achieving holistic understanding of the mechanisms underlying emergent features of the arbor. Project Goals: The main neuroscientific goal of this project is to understand how multiple local interactions of cytoskeleton components during differentiation define mature dendritic arbor shape and its functional integrative properties, using Drosophila sensory neurons as a model. The technological goal of this project is to develop a novel investigative approach that integrates and extends previously separate approaches from developmental biology & genetics, in vivo confocal imaging & electrophysiology, computer vision, and neuroanatomical modeling. Specific Aims: We propose 3 tightly integrated specific aims. Aim 1: use genetic manipulations and electrophysiological recordings to model the role of cytoskeletal organization and dynamics as a fundamental determinant of emergent dendrite arbor shape and function. Aim 2: Implement advanced 4D multi-parameter imaging protocols and automated algorithms to reconstruct the arbor, and quantify spatial and temporal associations among multiple sub-cellular components. Aim 3: using automated reconstructions & measurements from aim 2, statistically characterize the structural and cytoskeletal features of dendrite arbors, and stochastically simulate the growth and electrotonic properties of anatomically realistic virtual neuronal analogues. The data from aim 3 will feed back novel hypotheses to be tested by a subsequent repetition of the (aim 1 - aim 2 - aim 3) cycle. Approach: We will focus on a single model system - Drosophila dendritic arborization (da) sensory neurons. More specifically, we will investigate class I and class IV da neuron arborization based upon their radically distinct dendritic morphologies (simple vs. complex) and underlying cytoskeletal organizations. We will make fusion constructs of cytoskeleton components with spectrally distinct fluorescent proteins. These will be used in transgenic Drosophila in order to quantitatively measure the distribution of F-actin, microtubules, and microtubule polarity within the dendrite arbor throughout its development in vivo using confocal multi-fluor imaging. The resulting images will be processed by automated quantitative computer vision algorithms that will accurately extract the topology of the dendritic arbor, and it changes over time. We will use the resulting maps in neuroanatomical stochastic simulations to establish the links between the emergent morphometrics of the dendrite and specific cytoskeleton features at various developmental stages. Intellectual Merit: From a neurogenetics perspective, this project will pioneer the use of cytoskeletal features as putative fundamental determinants in statistical neuroanatomical models. These determinants will be linked to morphological determinants. From a computational perspective, this project will advance the state of the art in automated algorithms for delineating neuroanatomy (and its morphological dynamics) by deploying core technologies for large-scale multi-parameter studies, and result in an effective interfacing of automated reconstruction and simulation technologies. With this innovation, model predictions can be tested by molecular biological techniques, and findings of statistical models can be used to inform molecular models of dendrite arbor development. Educational Impact: This project will result in a cross-disciplinary training of post-doctoral fellows, graduate students, undergraduate students and high school interns. It will result in practical insight on ways to conduct cutting-edge systems-level scientific research overcoming disciplinary boundaries and using best-available collaborative tools. The trainees from this program will be uniquely positioned to develop the broader field of imaging-driven integrative systems neurobiology. It will expose minority and K-12 students to a new world of trans-disciplinary research that is indicative of the future. Broader Impacts: The combined body of molecular, imaging, and computational tools and datasets from this research will be disseminated widely, and made available to a broad class of investigators for adoption in the study of other major neuroscience problems. This project will serve as a new model for computationally enabled neuroscience research that achieves a long-desired synergy between the wet lab and computation.             n/a",CRCNS: Cytoskeletal Mechanisms of Dendrite Arbor Shape Development,8644396,R01NS086082,"['Address', 'Adoption', 'Affect', 'Afferent Neurons', 'Algorithms', 'Anatomic Models', 'Back', 'Biological', 'Biological Models', 'Characteristics', 'Coal', 'Collaborations', 'Collection', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Confocal Microscopy', 'Cytoskeletal Modeling', 'Cytoskeleton', 'Data', 'Data Set', 'Dendrites', 'Development', 'Developmental Biology', 'Developmental Process', 'Discipline', 'Drosophila genus', 'Drosophila melanogaster', 'Electrophysiology (science)', 'Elements', 'F-Actin', 'Future', 'Goals', 'Growth', 'Growth and Development function', 'Image', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Microtubules', 'Minority', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Models', 'Morphology', 'Nervous system structure', 'Neurites', 'Neuroanatomy', 'Neurobiology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Property', 'Proteins', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Role', 'Series', 'Shapes', 'Signal Transduction', 'Simulate', 'Staging', 'Statistical Models', 'Structure', 'Students', 'Synapses', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'Transgenic Organisms', 'Trees', 'analog', 'base', 'computerized tools', 'data sharing', 'developmental genetics', 'developmental neurobiology', 'digital', 'feeding', 'genetic manipulation', 'graduate student', 'high school', 'in vivo', 'innovation', 'insight', 'molecular imaging', 'molecular modeling', 'morphometry', 'neurogenetics', 'next generation', 'novel', 'open source', 'post-doctoral training', 'programs', 'reconstruction', 'relating to nervous system', 'research study', 'role model', 'simulation', 'tool', 'undergraduate student', 'virtual']",NINDS,GEORGE MASON UNIVERSITY,R01,2013,322109,-0.016011617230946868
"The q-bio Summer School     DESCRIPTION (provided by applicant): The purpose of the Annual q-bio Summer School, founded in 2007 in Los Alamos, NM, is to advance predictive modeling of cellular regulatory systems by providing trtaining in mathematical, statistical, and computational techniques that are important in systems and synthetic biology. A secondary goal is to advance the careers of researchers new to quantitative biology. The school consists of courses in six subjects: 1) stochastic gene regulation, 2) cell signaling, 3) biomolecular simulations, 4) viral dynamics, 5) synthetic biology, and 6) computational neuroscience. Demand for training in quantitative biology is increasing rapidly-the number of qualified summer school applicants increased from fewer than 40 in 2010 to over 170 in 2012. In response to this demand, we expanded the school in 2012 to accommodate more students. The school now takes place at two campuses, in Santa Fe, NM (courses 1-4) and in San Diego, CA (courses 5-6). Approximately 30 students attend at each campus and are diverse in terms of educational background (mathematics, engineering, physical sciences, and biology), career level (~75% are graduate students, ~20% are postdocs, and ~5% are more advanced), and demographics (gender, race, ethnicity, and worldwide geographical origin). Students attend all core lectures in the courses offered at their campus, as well as specialized course-specific lectures, student get-to-know-me talks, and other talks (e.g., talks focused on career skills), and participate in hands-on computer labs and mentored projects. After two intensive weeks, all students gather in Santa Fe for a 2-day q- bio Student Symposium, which features student projects reports, student poster presentations, and external invited speakers. All students then attend the 4-day q-bio Conference, an international conference attended by >200 researchers. All q-bio Summer School participants can expect the following: a) broad exposure to mathematical/statistical/computational tools used in quantitative biology, b) in-depth training in techniques of special interest (i.e., in one of the six course subjects) through course-specific lectures, computer labs, and mentored projects; c) multiple opportunities to practice scientific communication through talks and poster presentations; d) exposure to cutting-edge research, and e) extensive networking opportunities with peers and thought leaders. Lecturers and speakers include more than 50 different researchers active in quantitative biology, including very distinguished scientists. For example, in 2013, confirmed lecturers include six academicians. By the time the students attend the q-bio Conference, they are equipped with a powerful social network that facilitates interactions, idea exchange, and initiation of collaborative research. The long-term goal of the school is to change the way biological research is conducted, making biology a more quantitative field, like physics and chemistry. In this effort, the organizers are supported by significant goodwill from the international quantitative biology community and a number of local institutions, including two national centers for systems biology. However, to maintain and improve the school, additional financial support is required.         PUBLIC HEALTH RELEVANCE: Many future biomedical and biotechnological advances in synthetic and systems biology will require investigators who have the ability to carefully integrate quantitative experimentation with mathematical, statistical and computational modeling. The goal of the q-bio Summer School is to prepare a new generation of quantitative biologists who are adept at modeling and/or working with modelers to advance our predictive understanding of cellular regulatory systems. The complexity and importance of these systems, which govern cellular activities and fates, provides motivation for developing a scientific and engineering workforce equipped to deal with the complexity.            ",The q-bio Summer School,8475276,R25GM105608,"['Behavior', 'Bioinformatics', 'Biological', 'Biology', 'Career Choice', 'Cell model', 'Cell physiology', 'Cells', 'Chemistry', 'Communication', 'Communities', 'Complex', 'Computational Technique', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Cues', 'Data', 'Education', 'Educational Background', 'Educational Curriculum', 'Educational workshop', 'Engineering', 'Ethnic Origin', 'Event', 'Exposure to', 'Financial Support', 'Funding', 'Future', 'Gender', 'Gene Expression Regulation', 'Generations', 'Goals', 'Heterogeneity', 'Immune system', 'Individual', 'Institution', 'International', 'Internet', 'Machine Learning', 'Mathematics', 'Mentors', 'Minority-Serving Institution', 'Modeling', 'Molecular', 'Motivation', 'National Institute of General Medical Sciences', 'Neurons', 'Occupations', 'Participant', 'Physics', 'Population', 'Postdoctoral Fellow', 'Qualifying', 'Race', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Schools', 'Scientist', 'Series', 'Signal Transduction', 'Social Network', 'Statistical Data Interpretation', 'Statistical Models', 'Students', 'Synthetic Genes', 'System', 'Systems Biology', 'Techniques', 'Time', 'Training', 'United States National Institutes of Health', 'Viral', 'Woman', 'Work', 'biochemical model', 'biological research', 'career', 'career development', 'computational neuroscience', 'computerized tools', 'demographics', 'graduate student', 'improved', 'interest', 'lecture notes', 'lecturer', 'lectures', 'mathematical model', 'member', 'model design', 'multidisciplinary', 'peer', 'physical science', 'posters', 'predictive modeling', 'programs', 'public health relevance', 'research study', 'response', 'simulation', 'skills', 'statistics', 'symposium', 'synthetic biology', 'tool']",NIGMS,"NEW MEXICO CONSORTIUM, INC.",R25,2013,174626,-6.362309526271044e-05
"Integration and visualization of diverse biological data    DESCRIPTION (provided by applicant): Modern genome-scale experimental techniques enable for the first time in biological research the comprehensive monitoring of the entire molecular regulatory events leading to disease. Their integrative analyses hold the promise of generating specific, experimentally testable hypotheses, paving the way for a systems-level molecular view of complex disease. However, systems-level modeling of metazoan biology must address the challenges of: 1. biological complexity, including individual cell lineages and tissue types, 2. the increasingly large scale of data in higher organisms, and 3. the diversity of biomolecules and interaction mechanisms in the cell. The long-term goal of this research is to address these challenges through the development of bioinformatics frameworks for the study of gene function and regulation in complex biological systems thereby contributing to a greater understanding of human disease. In the initial funding period, we have developed accurate methods for integrating and visualizing diverse functional genomics data in S. cerevisiae and implemented them in interactive web-based systems for the biology community. Our methods have led to experimental discoveries of novel biology, are widely used by the yeast community, and are integrated with the SGD model organism database. We now propose to leverage our previous work to develop novel data integration and analysis methods and implement them in a public system for human data. In the proposed research period, we will create algorithms appropriate for integrating metazoan data in a tissue- and cell-lineage specific manner in health and disease. We will also develop novel hierarchical methods for predicting specific molecular interaction mechanisms and will extend our methods for integrating additional biomolecules. These methods will direct experiments focused on the glomerular kidney filter, a critical and complex component of the human vascular system whose dysfunction directly contributes to microvascular disease. Prediction of these cell-lineage specific functional networks will advance the understanding of the glomerulus function and its role in microvascular disease, leading to better clinical predictors, diagnoses, and treatments. From a technical perspective, application to glomerular biology will enable iterative improvement of the proposed methods based on experimental feedback. The end product of this research will be a general, robust, interactive, and automatically updated system for human data integration and analysis that will be freely available to the biomedical community. We will leverage parallel processing technologies (inspired by Google- type cloud computing solutions) to ensure interactive-analysis speed on the system. This system will allow biomedical researchers to synthesize, analyze, and visualize diverse data in human biology, enabling accurate predictions of biological networks and understanding their cell-lineage specificity and role in disease. Such integrative analyses will provide experimentally testable hypotheses, leading to a deeper understanding of complex disorders and paving the way to molecular-defined tissue targeted therapies and drug development.       PUBLIC HEALTH RELEVANCE: Our general system will enable integrative analysis of human functional genomics data in a cell-lineage and disease-focused manner, allowing biomedical researchers to identify potential clinical biomarkers and to formulate specific hypotheses elucidating the cause and development of a variety of complex disorders. Our application of this system to generate cell-lineage specific functional networks will lead to a better understanding of the glomerulus function and will directly benefit human health through the development of improved predictors, diagnoses, and treatments for microvascular disease.            ",Integration and visualization of diverse biological data,8403055,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Models', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Cells', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diabetic Angiopathies', 'Diagnosis', 'Disease', 'Endothelium', 'Ensure', 'Event', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Biology', 'Imagery', 'Individual', 'Joints', 'Kidney', 'Kidney Diseases', 'Kidney Glomerulus', 'Lead', 'Machine Learning', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Monitor', 'Mus', 'Online Systems', 'Organism', 'Participant', 'Plasma', 'Progress Reports', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Role', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Sampling', 'Solutions', 'Specificity', 'Speed', 'Structure', 'Structure of glomerular mesangium', 'System', 'Systems Biology', 'Systems Integration', 'Techniques', 'Technology', 'Time', 'Tissues', 'Update', 'Urine', 'Vascular System', 'Work', 'Yeasts', 'base', 'biological research', 'biological systems', 'cell type', 'complex biological systems', 'data integration', 'drug development', 'functional genomics', 'gene function', 'genome database', 'human data', 'human disease', 'human tissue', 'improved', 'model organisms databases', 'novel', 'parallel processing', 'podocyte', 'public health relevance', 'research study', 'therapy development', 'transcriptomics']",NIGMS,PRINCETON UNIVERSITY,R01,2013,378540,-0.025261921472653406
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8538496,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Chips', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2013,234332,-0.016790531903960848
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,-0.018101162152221417
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8545224,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,4024095,-0.006661708397445563
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known  as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network  (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5  years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases  Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and  procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on  32 current studies, contingent upon the successful re-competition of their associated clinical research  consortia, addition of new studies reflecting the growth of the network, accommodation of federated  databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and  registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per  year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical  trials. We will continue development of new technologies to support scalability and generalizability and tools  for cross-disease data mining. Our international clinical information network is secure providing coordinated  data management services for collection, storage and analysis of diverse data types from multiple diseases  and geographically disparate locations and a portal for the general public and larger community of clinical  investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8734648,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,304154,-0.006661708397445563
"MACHINE LEARNING TO FORECAST ZOONOTIC DISEASE EMERGENCE    DESCRIPTION (provided by applicant): As over 70% of emerging infectious diseases are caused by parasites or pathogens transmitted from animals to humans (leading to 'zoonotic' infections), a fundamental issues for public health is identifying the drivers leading to zoonotic diseases in humans. Cross-species transmission of infectious agents depends on numerous traits of hosts, their infectious agents, and environmental factors defining the external context of disease. Previous studies identifying predictors of cross-species transmission have been limited by a focus on single infectious diseases (e.g., rabies, Lyme disease) at restricted spatial scales, in part because large-scale analyses spanning numerous host species and infectious agents are precluded by the many complex interactions, autocorrelations, and sampling biases common in multivariate, high-dimensional data. The proposed research confronts these computational limitations through the innovative application of machine learning algorithms. Specifically, analyses will address three outstanding and interrelated questions in global health: (1) What characteristics signal a predisposition of mammalian host species to be reservoirs of zoonotic disease?; (2) What traits among infectious agents predict their potential to cause zoonotic infection?; (3) What are the most important environmental and anthropogenic predictors of zoonotic outbreaks globally? Analyses will apply a series of supervised, unsupervised and semi-supervised machine learning algorithms to new, global-scale databases containing biological, ecological, environmental, and anthropogenic data for three groups of mammalian hosts (primates, carnivores, and ungulates) and their zoonotic infectious agents. A long-term goal of this research is to empirically develop ""rules of thumb"" about zoonotic diseases by highlighting the key traits of mammalian hosts, infectious agents, and the environmental and human factors describing zoonotic outbreaks in recent history. Ultimately, research proposed herein will provide a basis for predicting the geographic locations, infectious agents, and animal reservoirs from which future zoonoses will emerge.        This project proposes to investigate the factors driving zoonotic disease outbreaks and cross-species transmission from wild mammals into humans through the innovative application of machine learning algorithms to newly published data describing hundreds of infectious agents, their mammalian host species, human populations, and the global environment. Ultimately, this project aims to predict the locations and species from which future diseases will emerge, and is therefore directly relevant for the improvement of human health.         ",MACHINE LEARNING TO FORECAST ZOONOTIC DISEASE EMERGENCE,8314607,F32GM087811,"['Address', 'Algorithms', 'Animals', 'Area', 'Automobile Driving', 'Award', 'Biological', 'Characteristics', 'Climate', 'Communicable Diseases', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Disease', 'Disease Outbreaks', 'Ecology', 'Emerging Communicable Diseases', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Future', 'Geographic Locations', 'Goals', 'Health', 'Human', 'Infection', 'Infectious Agent', 'Location', 'Lyme Disease', 'Machine Learning', 'Mammals', 'Methods', 'Nature', 'Output', 'Parasites', 'Parasitic Diseases', 'Pattern', 'Pattern Recognition', 'Population', 'Precipitation', 'Predisposition', 'Primates', 'Public Health', 'Publications', 'Publishing', 'Rabies', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Sampling Biases', 'Series', 'Signal Transduction', 'Source', 'Training', 'Ungulate', 'Vertebrates', 'West Nile virus', 'Zoonoses', 'Zoonotic Infection', 'anthropogenesis', 'base', 'career development', 'comparative', 'computer science', 'disease transmission', 'global environment', 'global health', 'innovation', 'land use', 'pathogen', 'trait', 'transmission process']",NIGMS,UNIVERSITY OF GEORGIA,F32,2012,53942,0.0053647442871486845
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.        This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8281439,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2012,294260,-0.01268599345958007
"ISMB 2012 Conference Support for Students & Young Scientists     DESCRIPTION (provided by applicant): The 2012 Intelligent Systems for Molecular Biology (ISMB) conference in will be held in Long Beach, California, with 1,500-1,700 attendees, including 33-38% students/post doctoral researchers. ISMB brings together graduate students, post doctoral researchers, faculty, research staff and senior scientists of many different nationalities, all of whom are studying or working in computer science, molecular biology, mathematics or statistics. The conference brings biologists and computational scientists together to focus on research centered on actual biological problems rather than simply theoretical calculations. The combined focus on ""intelligent systems"" and actual biological data makes ISMB a highly relevant meeting, and many years of producing the event has resulted in a professionally organized and respected annual conference. The ISMB conference presents the latest research methods and results developed through the application of computer programming to the study of biological sciences, including advances in sequencing genomes that may lead to a better understanding of how, for instance, cells interact for the treatment of diseases such as cancer. Presentations may describe methods and advances associated with the analysis of existing biological literature, including benchmarking experiments, to create a better public understanding of scientific research reports. Overall, ISMB serves to educate attendees on the latest developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to appy what they have learned as they advance their own research efforts or begin investigating new areas they were exposed to as a result of attending ISMB. The scientific program for each ISMB meeting includes parallel presentation tracks of original research papers, highlights of recently published papers, special sessions focused on emerging topics, technology demos, late breaking research and poster presentations, an art in science exhibition, tutorial workshops, special interest group meetings and a student symposium organized by and for students. For ISMB 2011, 258 original research papers were submitted and 48 selected for the Proceedings Track, while 88 previously published papers were submitted and 38 selected for the Highlights Track. In all, over 225 talks were presented during the course of the 2011 conference, and similar numbers are anticipated for 2012. In all cases, submissions are rigorously reviewed, typically by three members of each track's committee before approval by the track chair, insuring the highest possible quality of work is presented. The specific areas represented in the conference vary each year depending on the areas that researchers find most interesting and innovative, and therefore submit as papers and proposals. This proposal seeks funding to assist students and junior researchers in attending the conference, thus exposing them to the latest research of their own areas as well as areas that may be new to them.        PUBLIC HEALTH RELEVANCE: Bioinformatics is well established as an essential tool for understanding biological systems, largely driven by genomic sequence efforts due to the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field and exposing what's on the horizon of future discoveries, but is distinguished from many other events in computational biology or artificial intelligence by an insistence that the researchers work with real molecular biology data, not theoretical or toy examples. Although the cultures of computer science and biology are so disparate, ISMB bridges this cultural gap by providing a forum among biological conferences that features technical advances as they occur, which otherwise may be shunned until a firm experimental result is published.              Bioinformatics is well established as an essential tool for understanding biological systems, largely driven by genomic sequence efforts due to the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field and exposing what's on the horizon of future discoveries, but is distinguished from many other events in computational biology or artificial intelligence by an insistence that the researchers work with real molecular biology data, not theoretical or toy examples. Although the cultures of computer science and biology are so disparate, ISMB bridges this cultural gap by providing a forum among biological conferences that features technical advances as they occur, which otherwise may be shunned until a firm experimental result is published.            ",ISMB 2012 Conference Support for Students & Young Scientists,8317817,R13GM101868,"['Address', 'Algorithms', 'Area', 'Artificial Intelligence', 'Arts', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biology', 'California', 'Cells', 'Computational Biology', 'Computational Technique', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Educational workshop', 'Elements', 'Event', 'Evolution', 'Expert Systems', 'Faculty', 'Feedback', 'Financial Support', 'Funding', 'Future', 'Genomics', 'Graph', 'Group Meetings', 'Home environment', 'Human', 'International', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Limited Stage', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Metabolic Pathway', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Structure', 'Nationalities', 'Oral', 'Paper', 'Participant', 'Pattern Recognition', 'Peer Review', 'Phylogenetic Analysis', 'Postdoctoral Fellow', 'Published Comment', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Robotics', 'Science', 'Scientist', 'Senior Scientist', 'Sequence Analysis', 'Series', 'Specialist', 'Speed', 'Students', 'System', 'Technology', 'Time', 'Toy', 'Training', 'Validation', 'Work', 'biological systems', 'career', 'computer program', 'computer science', 'disorder prevention', 'exhibitions', 'experience', 'genome sequencing', 'graduate student', 'improved', 'information organization', 'innovation', 'interest', 'lectures', 'medical specialties', 'meetings', 'member', 'multidisciplinary', 'next generation', 'novel', 'parallel computing', 'posters', 'practical application', 'programs', 'research study', 'role model', 'satisfaction', 'skills', 'special interest group', 'statistics', 'symposium', 'tool']",NIGMS,INTERNATIONAL SOCIETY/COMP BIOLOGY,R13,2012,25000,-0.004010762084115454
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8494858,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2012,371054,-0.010557416930519476
"HyPhy: Molecular Evolutionary Analyses    DESCRIPTION (provided by applicant): Project Description HyPhy (http://www.hyphy.org) is a scriptable software platform designed to enable flexible and powerful analyses of DNA, RNA, codon, amino acid and other types of sequence data in an evolutionary context. Such analyses have become an indispensable component of most research studies that make use of comparative genomic data. Biologists and bioinformaticians increasingly recognize the benefits of molecular evolutionary analyses. Since its initial release in 2001, HyPhy has become a relatively stable and mature product, and has been downloaded by more than 4,500 unique users, integrated into several popular web-based genomic data analysis servers, cited in over 400 peer-reviewed publications and described in three book chapters, in spite of the fact that the development of the package has never been directly funded. This proposal seeks support to improve the quality, performance, reliability, modularity, documentation and feature sets of the HyPhy system. Specific aims can be divided into four major areas: 1. Software engineering, testing, and documentation of the HyPhy codebase. 2. The development of a high-performance engine for phylogenetic maximum likelihood model fitting and inference. 3. Extension of a newly-initiated toolbox for machine learning applications in molecular evolution. 4. Creation and maintenance of a wiki-themed documentation resource.      PUBLIC HEALTH RELEVANCE: Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.           Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.",HyPhy: Molecular Evolutionary Analyses,8322626,R01GM093939,"['Amino Acids', 'Animal Model', 'Area', 'Biomedical Research', 'Book Chapters', 'Classification', 'Code', 'Codon Nucleotides', 'Collection', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Development', 'Documentation', 'Drug resistance', 'Evolution', 'Funding', 'Gene Mutation', 'Genes', 'Genetic Programming', 'Genome', 'Genomics', 'Goals', 'Human', 'Immune response', 'Internet', 'Language', 'Libraries', 'Likelihood Functions', 'Machine Learning', 'Maintenance', 'Modeling', 'Molecular', 'Molecular Evolution', 'Molecular Sequence Data', 'Online Systems', 'Pathogenicity', 'Pattern', 'Peer Review', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Procedures', 'Publications', 'RNA', 'Research Personnel', 'Resources', 'Software Engineering', 'Study models', 'System', 'Testing', 'Viral', 'Work', 'acronyms', 'base', 'combinatorial', 'comparative genomics', 'design', 'flexibility', 'improved', 'pathogen', 'pressure', 'programs', 'protein structure function', 'public health relevance', 'research study', 'tool', 'user-friendly', 'wiki']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2012,274156,-0.02010502719300724
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.        The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8269876,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2012,323305,-0.018904332713188574
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.        PUBLIC HEALTH RELEVANCE: RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                  RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8268588,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2012,1300000,0.0026307864105932746
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8402447,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Set', 'Development', 'Disease', 'Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2012,2460045,-0.030264770604566983
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8259135,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'public health relevance', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,445075,-0.014425926966765276
"Methods for high-dimensional data in HIV/CVD research    DESCRIPTION (provided by applicant): Recent technological advances have yielded vast quantities of molecular and cellular data, affording unprecedented opportunities to understand complex disease etiologies and to inform clinical management strategies. However, in order to derive information from these rich stores of data we need to develop sound and appropriate analytic techniques. This need is especially relevant in studies at the intersection of human immunodeficiency virus (HIV) and cardiovascular disease (CVD), which are characterized by an elaborate set of interactions among viral and host factors. These factors include viral and host genetic profiles, as well as markers of caloric metabolism, immune activation and inflammation, which work together to determine response to therapy and overall disease progression. A comprehensive assessment of these markers presents several analytical challenges owing to the large number of potentially informative variables and the largely uncharacterized relationship among them. We propose a multi-faceted strategy that focuses on the development and application of integrative statistical approaches. Such approaches will allow us to explore and characterize novel hypotheses relating to the complex relationships among multiple genetic, environmental, demographic, and clinical factors and measures of disease progression. Specifically, this continuation application focuses on advancing and applying statistical methods in two settings: first, we consider population-based genetic association studies of innate-immunity, adipokine, drug metabolism and drug transport genes and markers of immune reconstitution, inflammation and risk of CVD in HIV-infected individuals; and second, we address investigations of metabolic and immunologic profiles that associate with immune recovery, inflammation and risk of CVD. The Specific Aims of the proposed research are to develop and evaluate: (1) Latent class and mixture modeling paradigms for (a) discovering and characterizing multi-locus genotype-trait associations and (b) evaluating unobservable haplotype-trait associations in candidate-gene investigations; and (2) Hierarchical mixture models and machine learning approaches for (a) monitoring quantitative biomarkers in resource-limited settings and (b) characterizing high- dimensional predictors of immune reconstitution and inflammation. IMPACT: This research will lead to the creation of appropriate and carefully evaluated analytic tools to derive information from the rich array of molecular and cellular data now available. Ultimately, this research will advance our ability to translate molecular and cellular level data for clinical decision making, serving at the cornerstone of personalized medicine.      PUBLIC HEALTH RELEVANCE: The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.           Public Health Relevance The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.",Methods for high-dimensional data in HIV/CVD research,8220786,R01HL107196,"['Address', 'Award', 'Biological Markers', 'Biological Models', 'Biometry', 'Candidate Disease Gene', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Clinical Research', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Development', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug Transport', 'Dyslipidemias', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genotype', 'Goals', 'HIV', 'Haplotypes', 'Immune', 'Immunologic Markers', 'Immunologics', 'Immunology', 'Individual', 'Inflammation', 'Integration Host Factors', 'Investigation', 'Laboratories', 'Lead', 'Lipids', 'Machine Learning', 'Measures', 'Medicine', 'Metabolic', 'Metabolic Marker', 'Metabolism', 'Methods', 'Microbiology', 'Modeling', 'Molecular', 'Monitor', 'Natural Immunity', 'Peer Review', 'Pharmacology', 'Publications', 'Recovery', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Testing', 'Textbooks', 'Time', 'Translating', 'Translational Research', 'Viral', 'Work', 'adaptive immunity', 'cardiovascular disorder risk', 'clinical decision-making', 'drug metabolism', 'genetic association', 'genetic profiling', 'genome wide association study', 'immune activation', 'insight', 'molecular array', 'novel', 'open source', 'population based', 'professor', 'public health relevance', 'reconstitution', 'response', 'sound', 'statistics', 'success', 'tool', 'trait', 'translational medicine']",NHLBI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2012,403591,-0.05577493993907798
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8238371,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,326844,-0.0203995311475224
"Ontology-based Information Network to Support Vaccine Research  Project Summary (Abstract):  Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.  Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8311060,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'abstracting', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2012,264994,-0.004519707649039073
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8310258,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2012,367520,-0.015356486227960413
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8537085,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2012,139853,-0.015356486227960413
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,8279285,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Running', 'Rural', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'high risk', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2012,105194,-0.03583217578330079
"Integration and visualization of diverse biological data    DESCRIPTION (provided by applicant): Modern genome-scale experimental techniques enable for the first time in biological research the comprehensive monitoring of the entire molecular regulatory events leading to disease. Their integrative analyses hold the promise of generating specific, experimentally testable hypotheses, paving the way for a systems-level molecular view of complex disease. However, systems-level modeling of metazoan biology must address the challenges of: 1. biological complexity, including individual cell lineages and tissue types, 2. the increasingly large scale of data in higher organisms, and 3. the diversity of biomolecules and interaction mechanisms in the cell. The long-term goal of this research is to address these challenges through the development of bioinformatics frameworks for the study of gene function and regulation in complex biological systems thereby contributing to a greater understanding of human disease. In the initial funding period, we have developed accurate methods for integrating and visualizing diverse functional genomics data in S. cerevisiae and implemented them in interactive web-based systems for the biology community. Our methods have led to experimental discoveries of novel biology, are widely used by the yeast community, and are integrated with the SGD model organism database. We now propose to leverage our previous work to develop novel data integration and analysis methods and implement them in a public system for human data. In the proposed research period, we will create algorithms appropriate for integrating metazoan data in a tissue- and cell-lineage specific manner in health and disease. We will also develop novel hierarchical methods for predicting specific molecular interaction mechanisms and will extend our methods for integrating additional biomolecules. These methods will direct experiments focused on the glomerular kidney filter, a critical and complex component of the human vascular system whose dysfunction directly contributes to microvascular disease. Prediction of these cell-lineage specific functional networks will advance the understanding of the glomerulus function and its role in microvascular disease, leading to better clinical predictors, diagnoses, and treatments. From a technical perspective, application to glomerular biology will enable iterative improvement of the proposed methods based on experimental feedback. The end product of this research will be a general, robust, interactive, and automatically updated system for human data integration and analysis that will be freely available to the biomedical community. We will leverage parallel processing technologies (inspired by Google- type cloud computing solutions) to ensure interactive-analysis speed on the system. This system will allow biomedical researchers to synthesize, analyze, and visualize diverse data in human biology, enabling accurate predictions of biological networks and understanding their cell-lineage specificity and role in disease. Such integrative analyses will provide experimentally testable hypotheses, leading to a deeper understanding of complex disorders and paving the way to molecular-defined tissue targeted therapies and drug development.      PUBLIC HEALTH RELEVANCE: Our general system will enable integrative analysis of human functional genomics data in a cell-lineage and disease-focused manner, allowing biomedical researchers to identify potential clinical biomarkers and to formulate specific hypotheses elucidating the cause and development of a variety of complex disorders. Our application of this system to generate cell-lineage specific functional networks will lead to a better understanding of the glomerulus function and will directly benefit human health through the development of improved predictors, diagnoses, and treatments for microvascular disease.              Our general system will enable integrative analysis of human functional genomics data in a cell-lineage and disease-focused manner, allowing biomedical researchers to identify potential clinical biomarkers and to formulate specific hypotheses elucidating the cause and development of a variety of complex disorders. Our application of this system to generate cell-lineage specific functional networks will lead to a better understanding of the glomerulus function and will directly benefit human health through the development of improved predictors, diagnoses, and treatments for microvascular disease.",Integration and visualization of diverse biological data,8209212,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Models', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Cells', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diabetic Angiopathies', 'Diagnosis', 'Disease', 'Endothelium', 'Ensure', 'Event', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Biology', 'Imagery', 'Individual', 'Joints', 'Kidney', 'Kidney Diseases', 'Kidney Glomerulus', 'Lead', 'Machine Learning', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Monitor', 'Mus', 'Online Systems', 'Organism', 'Participant', 'Plasma', 'Progress Reports', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Role', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Sampling', 'Solutions', 'Specificity', 'Speed', 'Structure', 'Structure of glomerular mesangium', 'System', 'Systems Biology', 'Systems Integration', 'Techniques', 'Technology', 'Time', 'Tissues', 'Update', 'Urine', 'Vascular System', 'Work', 'Yeasts', 'base', 'biological research', 'biological systems', 'cell type', 'complex biological systems', 'data integration', 'drug development', 'functional genomics', 'gene function', 'genome database', 'human data', 'human disease', 'human tissue', 'improved', 'model organisms databases', 'novel', 'parallel processing', 'podocyte', 'public health relevance', 'research study', 'therapy development', 'transcriptomics']",NIGMS,PRINCETON UNIVERSITY,R01,2012,393228,-0.029831473368669162
"Research Education in Statistical Genetics of Substance Abuse This application seeks support from the National Institute on Drug Abuse R25 mechanism for pre-doctoral and postdoctoral research positions at the Virginia Institute for Psychiatric and Behavioral Genetics (VIPBG) at Virginia Commonwealth University (VCU). The overall goal of this research education program is to provide an environment that encourages the development and application of statistical genetics at the highest levels. To accomplish this goal, a research education program in statistical genetics will been developed to foster interdisciplinary research, a key tenet of the NIDA mission, and which we believe is essential to conducting research in this field. Two areas of research focus are identified: Advanced Genetic Epidemiology and Statistical Molecular Genetics. These areas are synergistic and address a diverse variety of statistical methods, both those in current use and those under development. The research education program consists of pre-doctoral and postdoctoral components. Pre-doctoral participants will pursue degrees in either Human and Molecular Genetics or Biostatistics. This component is designed to recruit, at the earliest time in their careers, potential future investigators to research in statistical genetics focused on substance use, abuse and dependence (SUAD). The aim is to create a cohort of PhD graduates who are have been extensively exposed to, and begun to publish research in, this area at the outset of their research training. The postdoctoral component recognizes that many promising young researchers have training in areas relevant to, but not focused on, the statistical genetics of substance use and abuse. This flexible 2-3 year postdoctoral training component will help guide young investigators to this field of study and will provide them with integrated and focused training that will enable them to pursue careers in statistical genetics of SUAD. Research education is intended for individuals with training in mathematics, statistics, biostatistics, genetics, psychology or pharmacology and to those who have completed their clinical requirements for the MD degree. The goal of the postdoctoral component is to educate independent investigators who will contribute to the efforts to identify and characterize the genetic and environmental determinants of SUAD; they are expected to do so at VCU and at other institutions nationwide. To accomplish the overall program goal we will: i) Offer and carefully monitor a multidisciplinary integrated research training program with a range of research opportunities; ii) Meet the needs for training in emerging research areas in SUAD; iii) Provide training to researchers from diverse academic and ethnic backgrounds and intensive mentoring; iv) Provide a specialized curriculum that will merge strengths in SUAD research at our institution; and v) Disseminate course materials, developed software, user guides and example scripts to the wider community by teaching workshops and establishing a website with web casts, pod casts and script libraries. This project seeks support from the NIDA R25 research education program in statistical genetics of substance abuse. Two predoctoral and three postdoctoral participants will pursue individualized courses of training in statistical genetics, and participate in active research projects in the Virginia Institute for Psychiatric and Behavioral Genetics at Virginia Commonwealth University.",Research Education in Statistical Genetics of Substance Abuse,8305142,R25DA026119,"['Addictive Behavior', 'Address', 'Adolescent', 'Alcohol or Other Drugs use', 'Alcohols', 'Animal Model', 'Area', 'Behavioral', 'Behavioral Genetics', 'Biometry', 'Clinical', 'Commit', 'Communities', 'Comorbidity', 'Computer Simulation', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dependence', 'Development', 'Doctor of Philosophy', 'Education', 'Education Projects', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Environment', 'Epidemiologist', 'Exposure to', 'Faculty', 'Fostering', 'Foundations', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Programming', 'Genetic Research', 'Genetics and Medicine', 'Genome Scan', 'Genotype', 'Goals', 'Grant', 'Health', 'Human', 'Human Genetics', 'Individual', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internet', 'Knowledge', 'Libraries', 'Machine Learning', 'Mathematics', 'Measurable', 'Mentors', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Mus', 'National Institute of Drug Abuse', 'O-(glucuronic acid 2-sulfate)-(1--4)-O-(2,5)-anhydromannitol 6-sulfate', 'Participant', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacology and Toxicology', 'Phenotype', 'Positioning Attribute', 'Postdoctoral Fellow', 'Predisposing Factor', 'Prevention', 'Psychiatry', 'Psychology', 'Psychometrics', 'Publishing', 'Queensland', 'Recruitment Activity', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Sampling', 'Scientist', 'Series', 'Statistical Methods', 'Statistical Models', 'Structure', 'Students', 'Substance abuse problem', 'Time', 'Training', 'Training Programs', 'Twin Multiple Birth', 'Universities', 'Virginia', 'Work', 'Writing', 'addiction', 'base', 'career', 'career development', 'cohort', 'computer science', 'design', 'experience', 'field study', 'flexibility', 'genetic epidemiology', 'genome wide association study', 'human subject', 'medical specialties', 'meetings', 'model development', 'multidisciplinary', 'novel', 'population based', 'post-doctoral training', 'pre-doctoral', 'professor', 'programs', 'psychogenetics', 'software development', 'statistics', 'tool', 'web site']",NIDA,VIRGINIA COMMONWEALTH UNIVERSITY,R25,2012,379949,-0.015892657887845628
"Informatic tools for predicting an ordinal response for high-dimensional data    DESCRIPTION (provided by applicant):        Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been fully described nor has software been made generally available. Herein we propose to apply the L1 penalization method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing additional model-based ordinal classification methodologies applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies we have previously described. The specific aims of this application are to: (1) Develop R functions for implementing the stereotype logit model as well as an L1 penalized stereotype logit model for modeling an ordinal response. (2) Empirically examine the performance of the L1 penalized stereotype logit model and competitor ordinal response models by performing a simulation study and applying the models to publicly available microarray datasets. (3) Develop an R package for fitting a random-effects ordinal regression model for clustered ordinal response data. (4) Extend the random-effects ordinal regression model to include an L1 penalty term to accomodate high-dimensional covariate spaces and empirically examine the performance of the L1random-effects ordinal regression model through application to microarray data. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, so that the methodology and software developed in this application will provide unique informatic methods for analyzing such data. Moreover, the ordinal response extensions proposed in this application, though initially conceived of by considering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.               Most histopathological variables are reported on an ordinal scale. Studies involving protocol biopsies where both histopathological assessment and microarray studies are performed at the same time point are increasingly being performed, and the software developed in this application will provide unique informatic tools for analyzing such data. Moreover, the informatic methods proposed in this application, though initially conceived of by con- sidering microarray applications, will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect human preference data and other responses on an ordinal scale.",Informatic tools for predicting an ordinal response for high-dimensional data,8216289,R01LM011169,"['Advocate', 'Behavioral Research', 'Bioconductor', 'Biopsy', 'Biopsy Specimen', 'Cancer Patient', 'Cancer Prognosis', 'Categories', 'Chronic Hepatitis', 'Classification', 'Client satisfaction', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Diagnostic Neoplasm Staging', 'Environment', 'Evaluation', 'Event', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Status', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Logistics', 'Logit Models', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Nodal', 'Outcome', 'Patients', 'Performance', 'Progressive Disease', 'Protocols documentation', 'Quality of life', 'Recurrence', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scoring Method', 'Solid Neoplasm', 'Specimen', 'Stable Disease', 'Staging', 'Stereotyping', 'Techniques', 'Time', 'Trees', 'base', 'forest', 'functional status', 'heuristics', 'indexing', 'liver biopsy', 'malignant breast neoplasm', 'novel', 'partial response', 'preference', 'programs', 'response', 'simulation', 'social', 'software development', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R01,2012,255679,-0.016790531903960848
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,-0.018101162152221417
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8496251,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2012,40000,-0.0023417796892097487
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8330246,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2012,2827142,-0.006661708397445563
"CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING    DESCRIPTION (provided by applicant): Case Studies in Bayesian Statistics and Machine Learning I continues in the tradition of the Case Studies in Bayesian Statistics series. The original series of workshops were held in odd years at Carnegie Mellon University in the early fall. The first edition of the new workshop will be held at Carnegie Mellon University on October 14-15, 2011. The highest level goal of the workshop series is to generate and present successful solutions to difficult substantive problems in a wide variety of areas. The specific objectives of the workshop are to 1. Present and discuss solutions to challenging scientific problems that illustrate the potential for statistical machine learning approaches in substantive research; 2. Present an opportunity for statisticians and computer scientists to present applications-oriented research  that changes the way that data are analyzed in scientific fields; 3. Stimulate discussion of the challenges of the analysis of high-dimensional and complex datasets in a scientifically useful manner; 4. Encourage young researchers, including graduate students, to present their applied work; 5. Provide a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; 6. Expose young researchers to important challenges and opportunities in collaborative research; 7. Include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment; 8. Encourage dissemination of the findings presented at the workshop via well-documented and peer- reviewed journal articles.      PUBLIC HEALTH RELEVANCE: Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.           Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.         ",CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING,8203089,R13CA144626,"['Area', 'Bioinformatics', 'Case Study', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Data Analyses', 'Data Set', 'Disabled Persons', 'Discipline', 'Disease', 'Educational workshop', 'Environment', 'Fostering', 'Goals', 'Hand', 'Health Sciences', 'Image', 'Institutes', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Human Genome Research Institute', 'Participant', 'Peer Review', 'Research', 'Research Personnel', 'Scientist', 'Series', 'Solutions', 'Underrepresented Minority', 'Universities', 'Woman', 'Work', 'anticancer research', 'computer science', 'data modeling', 'falls', 'graduate student', 'interest', 'journal article', 'meetings', 'peer', 'planetary Atmosphere', 'statistics', 'symposium']",NCI,CARNEGIE-MELLON UNIVERSITY,R13,2011,7500,-0.015941778016380866
"MACHINE LEARNING TO FORECAST ZOONOTIC DISEASE EMERGENCE    DESCRIPTION (provided by applicant): As over 70% of emerging infectious diseases are caused by parasites or pathogens transmitted from animals to humans (leading to 'zoonotic' infections), a fundamental issues for public health is identifying the drivers leading to zoonotic diseases in humans. Cross-species transmission of infectious agents depends on numerous traits of hosts, their infectious agents, and environmental factors defining the external context of disease. Previous studies identifying predictors of cross-species transmission have been limited by a focus on single infectious diseases (e.g., rabies, Lyme disease) at restricted spatial scales, in part because large-scale analyses spanning numerous host species and infectious agents are precluded by the many complex interactions, autocorrelations, and sampling biases common in multivariate, high-dimensional data. The proposed research confronts these computational limitations through the innovative application of machine learning algorithms. Specifically, analyses will address three outstanding and interrelated questions in global health: (1) What characteristics signal a predisposition of mammalian host species to be reservoirs of zoonotic disease?; (2) What traits among infectious agents predict their potential to cause zoonotic infection?; (3) What are the most important environmental and anthropogenic predictors of zoonotic outbreaks globally? Analyses will apply a series of supervised, unsupervised and semi-supervised machine learning algorithms to new, global-scale databases containing biological, ecological, environmental, and anthropogenic data for three groups of mammalian hosts (primates, carnivores, and ungulates) and their zoonotic infectious agents. A long-term goal of this research is to empirically develop ""rules of thumb"" about zoonotic diseases by highlighting the key traits of mammalian hosts, infectious agents, and the environmental and human factors describing zoonotic outbreaks in recent history. Ultimately, research proposed herein will provide a basis for predicting the geographic locations, infectious agents, and animal reservoirs from which future zoonoses will emerge.      PUBLIC HEALTH RELEVANCE: This project proposes to investigate the factors driving zoonotic disease outbreaks and cross-species transmission from wild mammals into humans through the innovative application of machine learning algorithms to newly published data describing hundreds of infectious agents, their mammalian host species, human populations, and the global environment. Ultimately, this project aims to predict the locations and species from which future diseases will emerge, and is therefore directly relevant for the improvement of human health.           This project proposes to investigate the factors driving zoonotic disease outbreaks and cross-species transmission from wild mammals into humans through the innovative application of machine learning algorithms to newly published data describing hundreds of infectious agents, their mammalian host species, human populations, and the global environment. Ultimately, this project aims to predict the locations and species from which future diseases will emerge, and is therefore directly relevant for the improvement of human health.         ",MACHINE LEARNING TO FORECAST ZOONOTIC DISEASE EMERGENCE,8061158,F32GM087811,"['Address', 'Algorithms', 'Animals', 'Area', 'Automobile Driving', 'Award', 'Biological', 'Characteristics', 'Climate', 'Communicable Diseases', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Disease', 'Disease Outbreaks', 'Ecology', 'Emerging Communicable Diseases', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Future', 'Geographic Locations', 'Goals', 'Health', 'Human', 'Infection', 'Infectious Agent', 'Location', 'Lyme Disease', 'Machine Learning', 'Mammals', 'Methods', 'Nature', 'Output', 'Parasites', 'Parasitic Diseases', 'Pattern', 'Pattern Recognition', 'Population', 'Precipitation', 'Predisposition', 'Primates', 'Public Health', 'Publications', 'Publishing', 'Rabies', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Sampling Biases', 'Series', 'Signal Transduction', 'Source', 'Training', 'Ungulate', 'Vertebrates', 'West Nile virus', 'Zoonoses', 'Zoonotic Infection', 'anthropogenesis', 'base', 'career development', 'comparative', 'computer science', 'disease transmission', 'global environment', 'global health', 'innovation', 'land use', 'pathogen', 'trait', 'transmission process']",NIGMS,UNIVERSITY OF GEORGIA,F32,2011,51326,0.005285895147766441
"New Machine Learning Tools for Biomedical Data    DESCRIPTION (provided by applicant): With recent biotechnology advances, biomedical investigations have become computationally more complex and more challenging, involving high-dimensional structured data collected at a genomic scale. To respond to the pressing need to analyze such high-dimensional data, the research team proposes to develop powerful statistical and computational tools to model and infer condition-specific gene networks through sparse and structured learning of multiple precision matrices, as for time-varying gene network analyses with microarray data. The approach will be generalized to regression analysis with covariates and to mixture models with phenotype heterogeneity, e.g., unknown disease subtypes.  Statistically, the team will investigate novel penalization or regularization approaches to improve accuracy and efficiency of estimating multiple large precision matrices describing pairwise partial correlations in Gaussian graphical models and Gaussian mixture models. Computationally, innovative strategies will be explored based on the state-of-the art optimization techniques, particularly difference convex programming, augmented Lagrangian method, and the method of coordinate decent. Specific aims include: a) developing computational tools for inferring multiple precision matrices, especially when the size of a matrix greatly exceeds that of samples; b) developing regression approaches for sparse as well as structured learning to associate partial correlations with covariates of interest; c) developing mixture models to infer gene disregulations in the presence of unknown disease subtypes, and to discover novel disease subtypes; d) applying the developed methods to analyze two microarray datasets for i) inference of condition-specific gene networks for E. coli, and ii) new class discovery and prediction for human endothelial cells; e) developing public-domain software.      PUBLIC HEALTH RELEVANCE: This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.           This proposed research is expected not only to contribute valuable analysis tools for the elucidation of condition-specific gene networks, but also to advance statistical methodology and theory in Gaussian graphical models and Gaussian mixture models for high-dimensional data.         ",New Machine Learning Tools for Biomedical Data,8101478,R01GM081535,"['Accounting', 'Address', 'Biological', 'Biomedical Research', 'Biotechnology', 'Blood', 'Blood Cells', 'Cells', 'Communities', 'Complex', 'Computer software', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Detection', 'Disease', 'Endothelial Cells', 'Escherichia coli', 'Floods', 'Gene Expression', 'Gene Expression Regulation', 'Genes', 'Genomics', 'Group Structure', 'Grouping', 'Heterogeneity', 'Human', 'Investigation', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Network-based', 'Phenotype', 'Public Domains', 'Regression Analysis', 'Research', 'Sampling', 'Source', 'Structure', 'Techniques', 'Time', 'Tissue-Specific Gene Expression', 'base', 'cell type', 'computerized tools', 'disorder subtype', 'improved', 'innovation', 'interest', 'novel', 'programs', 'software development', 'theories', 'tool', 'vector']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,290523,-0.01329084816228858
"Optimizing Peripheral Nerve Regeneration using Computational Intelligence based T    DESCRIPTION (provided by applicant):  Peripheral nerve injuries are common diseases that affect a large amount of patients every year.  Tissue engineering has emerged as a powerful approach for developing alternative nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  Some research groups have applied artificial neural networks and decision trees to obtain the best model configuration for the prediction of the tissue engineering strategies.  For the decision trees based methods, it is hard to tell which classification tree is better than the other.  Furthermore, the prediction system using the decision tree algorithm lacks the capability of accumulating the learning experience over time.  On the other hand, Artificial Neural Networks (ANNs) exhibit some remarkable properties, but only the connection weights are trained with fixed topology.  It is hard to find the best fixed topology in advance for each specific tissue engineering strategy.  In this proposal, swarm intelligence (SI) based evolving ANNs technique is proposed to tackle this challenge.  Two swarm intelligence based methods, Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO), will be applied in this project to train the ANN model.  More specifically, ACO will be used to optimize the topology structure of the ANN models, while the PSO is used to adjust the connection weights of the ANN models based on the optimized topology structure.  For this SWarm Intelligence based Reinforcement Learning method for ANNs (SWIRL-ANN) system, both topology and connection weight of artificial neural networks can be evolved automatically and simultaneously so that an optimal classifier for tissue engineering strategies in peripheral nerve regeneration can be achieved.  The research project will include the following phases:  Aim 1:  Predict tissue engineering strategies in peripheral nerve regeneration using SWarm Intelligence based Reinforcement Learning method for ANNs (SWIRL-ANN) analytical and prediction system.  Aim 2:  Validate the efficacy of novel unknown tissue engineered nerve grafts as predicted by using SWIRL-ANN based analytical and prediction system for bridging peripheral nerve gaps in rat sciatic nerve injury model in vivo.      PUBLIC HEALTH RELEVANCE:  Tissue engineering has emerged as a powerful approach for developing nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  In this proposal, swarm intelligence (SI) based evolving artificial neural networks (ANNs) technique is proposed to tackle this challenge.  The proposed research will be helpful to efficiently develop tissue engineered products for tissue and organ replacement.               Tissue engineering has emerged as a powerful approach for developing nerve grafts for peripheral nerve regeneration.  Since tissue engineering strategies in peripheral nerve regeneration involve various possible combinations of variables, it is necessary to develop efficient tools to identify optimal tissue engineering strategies and predict the experimental results based on these tissue engineering strategies for peripheral nerve regeneration.  In this proposal, swarm intelligence (SI) based evolving artificial neural networks (ANNs) technique is proposed to tackle this challenge.  The proposed research will be helpful to efficiently develop tissue engineered products for tissue and organ replacement.            ",Optimizing Peripheral Nerve Regeneration using Computational Intelligence based T,8232817,R15NS074404,"['Advanced Development', 'Affect', 'Algorithms', 'Animal Experiments', 'Animals', 'Ants', 'Area', 'Artificial Intelligence', 'Autologous Transplantation', 'Biocompatible Materials', 'Biological', 'Biological Neural Networks', 'Biomedical Research', 'Breathing', 'Cells', 'Characteristics', 'Classification', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Defect', 'Disease', 'Exhibits', 'Foundations', 'Future', 'Generic Drugs', 'Individual', 'Insecta', 'Intelligence', 'Knowledge', 'Learning', 'Maps', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Nerve', 'Nerve Regeneration', 'Network-based', 'Neural Network Simulation', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Perception', 'Peripheral Nerves', 'Peripheral nerve injury', 'Phase', 'Physicians', 'Process', 'Property', 'Psychological reinforcement', 'Rattus', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Social Behavior', 'Structure', 'Surgical incisions', 'System', 'Techniques', 'Time', 'Tissue Engineering', 'Tissues', 'Training', 'Treatment Protocols', 'Trees', 'Vertebrates', 'Weight', 'Work', 'base', 'experience', 'in vivo', 'in vivo Model', 'insight', 'nerve gap', 'nerve injury', 'novel', 'particle', 'relating to nervous system', 'sciatic nerve', 'social', 'tool', 'vector']",NINDS,STEVENS INSTITUTE OF TECHNOLOGY,R15,2011,423840,-0.015387010474637534
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8107695,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2011,108418,-0.010557416930519476
"HyPhy: Molecular Evolutionary Analyses    DESCRIPTION (provided by applicant): Project Description HyPhy (http://www.hyphy.org) is a scriptable software platform designed to enable flexible and powerful analyses of DNA, RNA, codon, amino acid and other types of sequence data in an evolutionary context. Such analyses have become an indispensable component of most research studies that make use of comparative genomic data. Biologists and bioinformaticians increasingly recognize the benefits of molecular evolutionary analyses. Since its initial release in 2001, HyPhy has become a relatively stable and mature product, and has been downloaded by more than 4,500 unique users, integrated into several popular web-based genomic data analysis servers, cited in over 400 peer-reviewed publications and described in three book chapters, in spite of the fact that the development of the package has never been directly funded. This proposal seeks support to improve the quality, performance, reliability, modularity, documentation and feature sets of the HyPhy system. Specific aims can be divided into four major areas: 1. Software engineering, testing, and documentation of the HyPhy codebase. 2. The development of a high-performance engine for phylogenetic maximum likelihood model fitting and inference. 3. Extension of a newly-initiated toolbox for machine learning applications in molecular evolution. 4. Creation and maintenance of a wiki-themed documentation resource.      PUBLIC HEALTH RELEVANCE: Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.           Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.",HyPhy: Molecular Evolutionary Analyses,8143338,R01GM093939,"['Amino Acids', 'Animal Model', 'Area', 'Biomedical Research', 'Book Chapters', 'Classification', 'Code', 'Codon Nucleotides', 'Collection', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Development', 'Documentation', 'Drug resistance', 'Evolution', 'Funding', 'Gene Mutation', 'Genes', 'Genetic Programming', 'Genome', 'Genomics', 'Goals', 'Human', 'Immune response', 'Internet', 'Language', 'Libraries', 'Likelihood Functions', 'Machine Learning', 'Maintenance', 'Modeling', 'Molecular', 'Molecular Evolution', 'Molecular Sequence Data', 'Online Systems', 'Pathogenicity', 'Pattern', 'Peer Review', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Procedures', 'Publications', 'RNA', 'Research Personnel', 'Resources', 'Software Engineering', 'Study models', 'System', 'Testing', 'Viral', 'Work', 'acronyms', 'base', 'combinatorial', 'comparative genomics', 'design', 'flexibility', 'improved', 'pathogen', 'pressure', 'programs', 'protein structure function', 'public health relevance', 'research study', 'tool', 'user-friendly', 'wiki']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2011,275732,-0.02010502719300724
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8062031,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Health', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2011,359403,-0.004696745350054292
"The Crystallography of Macromolecules    DESCRIPTION (provided by applicant): The proposal ""The Crystallography of Macromolecules"" addresses the limitations of diffraction data analysis methods in the field of X-ray crystallography. The significance of this work is determined by the importance of the technique, which generates uniquely-detailed information about cellular processes at the atomic level. The structural results obtained with crystallography are used to explain and validate results obtain by other biophysical, biochemical and cell biology techniques, to generate hypotheses for detailed studies of cellular process and to guide drug design studies - all of which are highly relevant to NIH mission. The proposal focuses on method development to address a frequent situation, where the crystal size and order is insufficient to obtain a structure from a single crystal. This is particularly frequent in cases of large eukaryotic complexes and membrane proteins, where the structural information is the most valuable to the NIH mission. The diffraction power of a single crystal is directly related to the microscopic order and size of that specimen. It is also one of the main correlates of structure solution success. The method used to solve the problem of data insufficiency in the case of a single crystal is to use multiple crystals and to average data between them, which allows to retrieve even very low signals. However, different crystals of the same protein, even if they are very similar i.e. have the same crystal lattice symmetry and very similar unit cell dimensions, still are characterized by a somewhat different order. This non-isomorphism is often high enough to make their solution with averaged data impossible. Moreover, the use of multiple data sets complicates decision making as each of the datasets contains different information and it is not clear when and how to combine them. The proposed solution relies on hierarchical analysis. First, the shape of the diffraction spot profiles will be modeled using a novel approach (Aim 1). This will form the ground for the next step, in which deconvolution of overlapping Bragg spot profiles from multiple lattices will be achieved (Aim 2). An additional benefit of algorithms developed in Aim 1 is that they will automatically derive the integration parameters and identify artifacts, making the whole process more robust. This is particularly significant for high-throughput and multiple crystal analysis. In Aim 3, comparison of data from multiple crystals will be performed to identify subsets of data that should be merged to produce optimal results. The critical aspect of this analysis will be the identification and assessment of non- isomorphism between datasets. The experimental decision-making strategy is the subject of Aim 4. The Support Vector Machine (SVM) method will be used to evaluate the suitability of available datasets for possible methods of structure solution. In cases of insufficient data it will identify the most significant factor that needs to be improved. Aim 5 is to simplify navigation of data reduction and to integrate the results of previous aims with other improvements in hardware and computing.      PUBLIC HEALTH RELEVANCE: The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.             The goal of the proposal is to develop methods for analysis of X-ray diffraction data with a particular focus on the novel analysis of diffraction spot shape and the streamlining of data analysis in multi-crystal modes. The development of such methods is essential to advance structural studies in thousands of projects, which individually are important for NIH mission.           ",The Crystallography of Macromolecules,8108523,R01GM053163,"['Address', 'Algorithms', 'Anisotropy', 'Biochemical', 'Cell physiology', 'Cells', 'Cellular biology', 'Communities', 'Complex', 'Computer software', 'Computers', 'Crystallography', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Decision Making', 'Dependence', 'Development', 'Dimensions', 'Drug Design', 'Evaluation', 'Funding', 'Goals', 'Ice', 'Image', 'Ligands', 'Machine Learning', 'Maps', 'Membrane Proteins', 'Methods', 'Microscopic', 'Mission', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Noise', 'Output', 'Pattern', 'Phase', 'Problem Solving', 'Procedures', 'Process', 'Proteins', 'Quality Indicator', 'Radiation', 'Relative (related person)', 'Research', 'Resolution', 'Rotation', 'Shapes', 'Signal Transduction', 'Site', 'Solutions', 'Solvents', 'Specimen', 'Spottings', 'Structure', 'System', 'Techniques', 'Technology', 'Twin Multiple Birth', 'United States National Institutes of Health', 'Work', 'X ray diffraction analysis', 'X-Ray Crystallography', 'base', 'beamline', 'cell dimension', 'data reduction', 'detector', 'experience', 'improved', 'independent component analysis', 'indexing', 'macromolecule', 'method development', 'novel', 'novel strategies', 'programs', 'research study', 'statistics', 'success', 'user-friendly']",NIGMS,UT SOUTHWESTERN MEDICAL CENTER,R01,2011,341852,-0.01651083948974723
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,8133946,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computational algorithm', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2011,297497,0.0035038917924628808
"SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID DESCRIPTION (provided by applicant): Advanced instrumentation and cellular imaging techniques using high-throughput 3D electron microscopy are driving a new revolution in the exploration of complex biological systems by providing near seamless views across multiple scales of resolution. These datasets provide the necessary breadth and depth to analyze multicellular, cellular, and subcelluar structure across large swathes of neural tissue. While these new imaging procedures are generating extremely large datasets of enormous value, the quantities are such that no single user or even laboratory team can possibly analyze the full content of their own imaging activities through traditional means. To address this challenge, we propose to further develop and refine a prototype hybrid system for high-throughput segmentation of large neuropil datasets that: 1) advances automatic algorithms for segmentation of cellular and sub-cellular structures using machine learning techniques; 2) couples these techniques to a scalable and flexible process or tool suite allowing multiple users to simultaneously review, edit and curate the results of these automatic approaches; and, 3) builds a knowledge base of training data guiding and improving automated processing. This system will allow project scientists to select areas of interest, execute automatic segmentation algorithms, and distribute workload, curate data, and deposit final results into the Cell Centered Database (Martone et al. 2008) via accessible web-interfaces. Emerging techniques in cellular and subcellular 3D imaging are generating datasets of enormous value to the study of disease processes and to the pursuit of greater insight into the structure and function of the nervous system. To unlock the potential of these data, new solutions are needed to improve the capability to segment and label the individual molecular, subcellular and cellular components within very large volumetric expanses. To address this challenge, we propose a hybrid system for high-throughput segmentation of large neuropil datasets that advances machine learning algorithms for automatic segmentation and couples these techniques to a scalable tool suite for multiple users to simultaneously review, edit and curate results.",SLASH: SCALABLE LARGE ANALYTIC SEGMENTATION HYBRID,8163481,R01NS075314,"['Address', 'Adoption', 'Algorithms', 'Area', 'Automobile Driving', 'Biological', 'Cell membrane', 'Cell physiology', 'Cells', 'Cellular Structures', 'Classification', 'Complex', 'Computer software', 'Computers', 'Computers and Advanced Instrumentation', 'Couples', 'Cytoskeleton', 'Data', 'Data Set', 'Databases', 'Deposition', 'Devices', 'Disease', 'Electron Microscopy', 'Electrons', 'Face', 'Generations', 'Goals', 'Growth', 'Hybrids', 'Image', 'Imaging Techniques', 'Individual', 'Institutes', 'Internet', 'Label', 'Laboratories', 'Machine Learning', 'Manuals', 'Methods', 'Microscopic', 'Mitochondria', 'Molecular', 'Molecular Target', 'Names', 'Nervous System Physiology', 'Nervous system structure', 'Neurofibrillary Tangles', 'Neurons', 'Neuropil', 'Online Systems', 'Organelles', 'Participant', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Scanning Electron Microscopy', 'Scientist', 'Services', 'Solutions', 'Staining method', 'Stains', 'Structure', 'Subcellular structure', 'System', 'Techniques', 'Three-Dimensional Imaging', 'Tissues', 'Training', 'Universities', 'Utah', 'Validation', 'Work', 'Workload', 'biological systems', 'cellular imaging', 'complex biological systems', 'data mining', 'digital imaging', 'electron optics', 'flexibility', 'image processing', 'improved', 'insight', 'interest', 'knowledge base', 'novel', 'prototype', 'relating to nervous system', 'scientific computing', 'tomography', 'tool', 'web interface']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,459174,-0.014425926966765276
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,8112574,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'spatiotemporal', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2011,176524,-0.0028218380975448985
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,8113231,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2011,119416,-0.019097469841772314
"Methods for high-dimensional data in HIV/CVD research    DESCRIPTION (provided by applicant): Recent technological advances have yielded vast quantities of molecular and cellular data, affording unprecedented opportunities to understand complex disease etiologies and to inform clinical management strategies. However, in order to derive information from these rich stores of data we need to develop sound and appropriate analytic techniques. This need is especially relevant in studies at the intersection of human immunodeficiency virus (HIV) and cardiovascular disease (CVD), which are characterized by an elaborate set of interactions among viral and host factors. These factors include viral and host genetic profiles, as well as markers of caloric metabolism, immune activation and inflammation, which work together to determine response to therapy and overall disease progression. A comprehensive assessment of these markers presents several analytical challenges owing to the large number of potentially informative variables and the largely uncharacterized relationship among them. We propose a multi-faceted strategy that focuses on the development and application of integrative statistical approaches. Such approaches will allow us to explore and characterize novel hypotheses relating to the complex relationships among multiple genetic, environmental, demographic, and clinical factors and measures of disease progression. Specifically, this continuation application focuses on advancing and applying statistical methods in two settings: first, we consider population-based genetic association studies of innate-immunity, adipokine, drug metabolism and drug transport genes and markers of immune reconstitution, inflammation and risk of CVD in HIV-infected individuals; and second, we address investigations of metabolic and immunologic profiles that associate with immune recovery, inflammation and risk of CVD. The Specific Aims of the proposed research are to develop and evaluate: (1) Latent class and mixture modeling paradigms for (a) discovering and characterizing multi-locus genotype-trait associations and (b) evaluating unobservable haplotype-trait associations in candidate-gene investigations; and (2) Hierarchical mixture models and machine learning approaches for (a) monitoring quantitative biomarkers in resource-limited settings and (b) characterizing high- dimensional predictors of immune reconstitution and inflammation. IMPACT: This research will lead to the creation of appropriate and carefully evaluated analytic tools to derive information from the rich array of molecular and cellular data now available. Ultimately, this research will advance our ability to translate molecular and cellular level data for clinical decision making, serving at the cornerstone of personalized medicine.      PUBLIC HEALTH RELEVANCE: The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.           The newly available array of data on genetic polymorphisms and cellular level immune factors promises unprecedented opportunities to elucidate complex disease etiology and inform clinical management strategies. Using human immunodeficiency virus (HIV) and cardiovascular disease (CVD) as our model systems, we propose to develop, evaluate and apply new analytic approaches for high-dimensional data. Ultimately, these methods will allow us to derive information from the vast quantities of molecular and cellular data for personalized, clinical decisions and thus serve as a central component of translational medicine.         ",Methods for high-dimensional data in HIV/CVD research,8071406,R01HL107196,"['Address', 'Award', 'Biological Markers', 'Biological Models', 'Biometry', 'Candidate Disease Gene', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Clinical Research', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Collection', 'Development', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug Transport', 'Dyslipidemias', 'Environment', 'Environmental Risk Factor', 'Etiology', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genotype', 'Goals', 'HIV', 'Haplotypes', 'Immune', 'Immunologic Markers', 'Immunologics', 'Immunology', 'Individual', 'Inflammation', 'Integration Host Factors', 'Investigation', 'Laboratories', 'Lead', 'Lipids', 'Machine Learning', 'Measures', 'Medicine', 'Metabolic', 'Metabolic Marker', 'Metabolism', 'Methods', 'Microbiology', 'Modeling', 'Molecular', 'Monitor', 'Natural Immunity', 'Peer Review', 'Pharmacology', 'Publications', 'Recovery', 'Research', 'Research Personnel', 'Resources', 'Software Tools', 'Statistical Computing', 'Statistical Methods', 'Techniques', 'Testing', 'Textbooks', 'Time', 'Translating', 'Translational Research', 'Viral', 'Work', 'adaptive immunity', 'cardiovascular disorder risk', 'clinical decision-making', 'drug metabolism', 'genetic association', 'genetic profiling', 'genome wide association study', 'immune activation', 'insight', 'molecular array', 'novel', 'open source', 'population based', 'professor', 'reconstitution', 'response', 'sound', 'statistics', 'success', 'tool', 'trait', 'translational medicine']",NHLBI,UNIVERSITY OF MASSACHUSETTS AMHERST,R01,2011,438295,-0.05580487269006059
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8044673,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,324859,-0.012799826367921745
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,8120230,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2011,264994,-0.004668668093177952
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8150462,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2011,407746,-0.015356486227960413
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,8077374,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Running', 'Rural', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'high risk', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2011,105194,-0.03583217578330079
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,8139155,R44RR024094,"['AIDS/HIV problem', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cell physiology', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2011,449663,-0.027633074123797865
"ISMB 2011 Conference Support for Students & Young Scientists    DESCRIPTION (provided by applicant): ISMB 2011 Conference Travel Support for Students and Young Scientists.  The Intelligent Systems for Molecular Biology (ISMB) conference in 2011 will be held in Vienna, Austria, as a conference of approximately 1,500-1,700 attendees, including 33-38% students/post doctoral researchers. ISMB brings together graduate students, post doctoral researchers, faculty, research staff and senior scientists of many different nationalities, all of whom are studying or working in computer science, molecular biology, mathematics and/or statistics. The conference brings biologists and computational scientists together to focus on research centered on actual biological problems rather than simply theoretical calculations. The combined focus on ""intelligent systems"" and actual biological data makes ISMB a highly relevant meeting, and many years of producing the event has resulted in a well organized, well attended, and respected annual conference. The ISMB conference presents the latest research methods and results developed through the application of computer programming to the study of biological sciences, including advances in sequencing genomes that may lead to a better understanding of how, for instance, cells interact for the treatment of diseases such as cancer. Additionally, presentations may describe methods and advances associated with the analysis of existing biological literature, including benchmarking experiments, to create a better public understanding of scientific research reports. Overall, ISMB serves to educate attendees on the latest developments that will further drive the research methods and results of the field of computational biology. Students and scientists are able to return to their labs to apply what they have learned as they advance their own research efforts. The scientific program for each ISMB meeting comprises parallel presentation tracks of original research papers, highlights of recently published research, topically focused special sessions on emerging topics, technology demos, tutorial workshops, special interest group meetings and a student symposium organized by and for students. As an example, for ISMB 2010, 234 original research papers were submitted and 48 selected for the Proceedings Track; 126 published papers were submitted and 42 selected for the Highlights Track; nine proposals were submitted and four selected for presentation along with two invited for the Special Sessions Track. In all, well over 200 talks were presented during the course of the 2010 conference, and similar numbers are anticipated for 2011. In all cases, submissions are rigorously reviewed, typically by three members of each track's committee before approval by the track chair, insuring the highest possible quality of work is presented. The specific areas represented in the conference vary each year depending on the areas that researchers find most interesting and innovative, and therefore submit as papers and proposals. This proposal seeks funding to assist students and junior researchers in attending the conference, thus exposing them to the latest research of their own areas as well as areas that may be new to them.      PUBLIC HEALTH RELEVANCE: Relevance Bioinformatics is well established as an essential tool for understanding biological systems. The widespread recognition of bioinformatics has been largely driven by genomic sequence efforts, because laboratory scientists recognize that the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. Biologists are routinely integrating computational tools into their research programs and creating large predictive models based on information found in databases and other electronic resources. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field, as well as exposing what's on the horizon of future discoveries.           Relevance Bioinformatics is well established as an essential tool for understanding biological systems. The widespread recognition of bioinformatics has been largely driven by genomic sequence efforts, because laboratory scientists recognize that the usefulness of genomic data in the quest to develop new and improved treatments for and prevention of disease is highly dependent on one's ability to electronically access and manipulate it. Biologists are routinely integrating computational tools into their research programs and creating large predictive models based on information found in databases and other electronic resources. The Intelligent Systems for Molecular Biology (ISMB) conference series directly addresses these questions by showcasing the latest advances in the field, as well as exposing what's on the horizon of future discoveries.         ",ISMB 2011 Conference Support for Students & Young Scientists,8121309,R13GM097938,"['Address', 'Algorithms', 'Area', 'Austria', 'Benchmarking', 'Binding', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Cells', 'Computational Biology', 'Computational Technique', 'Computer software', 'Computers', 'Computing Methodologies', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Educational workshop', 'Electronics', 'Elements', 'Event', 'Evolution', 'Expert Systems', 'Faculty', 'Feedback', 'Financial Support', 'Funding', 'Future', 'Genomics', 'Graph', 'Group Meetings', 'Human', 'Industry', 'International', 'Knowledge', 'Laboratory Scientists', 'Lead', 'Learning', 'Limited Stage', 'Linguistics', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Metabolic Pathway', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Structure', 'Nationalities', 'Oral', 'Paper', 'Participant', 'Pattern Recognition', 'Peer Review', 'Phylogenetic Analysis', 'Postdoctoral Fellow', 'Published Comment', 'Publishing', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Robotics', 'Role', 'Science', 'Scientist', 'Senior Scientist', 'Sequence Analysis', 'Series', 'Speed', 'Students', 'System', 'Technology', 'Time', 'Training', 'Travel', 'Validation', 'Vendor', 'Work', 'base', 'biological systems', 'career', 'computer program', 'computer science', 'computerized tools', 'cost', 'disorder prevention', 'exhibitions', 'experience', 'genome sequencing', 'graduate student', 'improved', 'information organization', 'innovation', 'interest', 'lectures', 'meetings', 'member', 'multidisciplinary', 'next generation', 'novel', 'parallel computing', 'posters', 'practical application', 'predictive modeling', 'programs', 'research study', 'role model', 'satisfaction', 'skills', 'special interest group', 'statistics', 'symposium', 'tool']",NIGMS,INTERNATIONAL SOCIETY/COMP BIOLOGY,R13,2011,20000,-0.010287248439224508
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,8143297,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'animal tissue', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2011,132786,-0.013053364593811291
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,8079474,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2011,250488,-0.012710273507835443
"Integration and visualization of diverse biological data    DESCRIPTION (provided by applicant): Modern genome-scale experimental techniques enable for the first time in biological research the comprehensive monitoring of the entire molecular regulatory events leading to disease. Their integrative analyses hold the promise of generating specific, experimentally testable hypotheses, paving the way for a systems-level molecular view of complex disease. However, systems-level modeling of metazoan biology must address the challenges of: 1. biological complexity, including individual cell lineages and tissue types, 2. the increasingly large scale of data in higher organisms, and 3. the diversity of biomolecules and interaction mechanisms in the cell. The long-term goal of this research is to address these challenges through the development of bioinformatics frameworks for the study of gene function and regulation in complex biological systems thereby contributing to a greater understanding of human disease. In the initial funding period, we have developed accurate methods for integrating and visualizing diverse functional genomics data in S. cerevisiae and implemented them in interactive web-based systems for the biology community. Our methods have led to experimental discoveries of novel biology, are widely used by the yeast community, and are integrated with the SGD model organism database. We now propose to leverage our previous work to develop novel data integration and analysis methods and implement them in a public system for human data. In the proposed research period, we will create algorithms appropriate for integrating metazoan data in a tissue- and cell-lineage specific manner in health and disease. We will also develop novel hierarchical methods for predicting specific molecular interaction mechanisms and will extend our methods for integrating additional biomolecules. These methods will direct experiments focused on the glomerular kidney filter, a critical and complex component of the human vascular system whose dysfunction directly contributes to microvascular disease. Prediction of these cell-lineage specific functional networks will advance the understanding of the glomerulus function and its role in microvascular disease, leading to better clinical predictors, diagnoses, and treatments. From a technical perspective, application to glomerular biology will enable iterative improvement of the proposed methods based on experimental feedback. The end product of this research will be a general, robust, interactive, and automatically updated system for human data integration and analysis that will be freely available to the biomedical community. We will leverage parallel processing technologies (inspired by Google- type cloud computing solutions) to ensure interactive-analysis speed on the system. This system will allow biomedical researchers to synthesize, analyze, and visualize diverse data in human biology, enabling accurate predictions of biological networks and understanding their cell-lineage specificity and role in disease. Such integrative analyses will provide experimentally testable hypotheses, leading to a deeper understanding of complex disorders and paving the way to molecular-defined tissue targeted therapies and drug development.      PUBLIC HEALTH RELEVANCE: Our general system will enable integrative analysis of human functional genomics data in a cell-lineage and disease-focused manner, allowing biomedical researchers to identify potential clinical biomarkers and to formulate specific hypotheses elucidating the cause and development of a variety of complex disorders. Our application of this system to generate cell-lineage specific functional networks will lead to a better understanding of the glomerulus function and will directly benefit human health through the development of improved predictors, diagnoses, and treatments for microvascular disease.              Our general system will enable integrative analysis of human functional genomics data in a cell-lineage and disease-focused manner, allowing biomedical researchers to identify potential clinical biomarkers and to formulate specific hypotheses elucidating the cause and development of a variety of complex disorders. Our application of this system to generate cell-lineage specific functional networks will lead to a better understanding of the glomerulus function and will directly benefit human health through the development of improved predictors, diagnoses, and treatments for microvascular disease.            ",Integration and visualization of diverse biological data,8041717,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Models', 'Biological Process', 'Biology', 'Cardiovascular Diseases', 'Case Study', 'Cell Lineage', 'Cells', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diabetic Angiopathies', 'Diagnosis', 'Disease', 'Endothelium', 'Ensure', 'Event', 'Feedback', 'Functional disorder', 'Funding', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Grant', 'Health', 'Histocompatibility Testing', 'Human', 'Human Biology', 'Imagery', 'Individual', 'Joints', 'Kidney', 'Kidney Diseases', 'Kidney Glomerulus', 'Lead', 'Machine Learning', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Monitor', 'Mus', 'Online Systems', 'Organism', 'Participant', 'Plasma', 'Progress Reports', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Role', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Sampling', 'Solutions', 'Specificity', 'Speed', 'Structure', 'Structure of glomerular mesangium', 'System', 'Systems Biology', 'Systems Integration', 'Techniques', 'Technology', 'Time', 'Tissues', 'Update', 'Urine', 'Vascular System', 'Work', 'Yeasts', 'base', 'biological research', 'biological systems', 'cell type', 'complex biological systems', 'data integration', 'drug development', 'functional genomics', 'gene function', 'genome database', 'human data', 'human disease', 'human tissue', 'improved', 'model organisms databases', 'novel', 'parallel processing', 'podocyte', 'research study', 'therapy development', 'transcriptomics']",NIGMS,PRINCETON UNIVERSITY,R01,2011,433016,-0.029831473368669162
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8061704,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'information model', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2011,2923298,-0.016486341173536014
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,-0.02001505365637158
"Systems Biology of Plasmodium falciparum: Building and Exploring Network Models    DESCRIPTION (provided by applicant):  There is an urgent need for novel anti-malaria interventions, due to the growing resistance of Plasmodium parasites to available drugs. The long-term objective of our research is to identify potential drug or vaccine targets using an efficient and cost-effective data-mining approach. This proposed study is aimed at the identification of biological networks as the first step toward a systems-level understanding of parasite biology. Compared to functional genomics, which requires a priori information about the identity or function of targets, a systems-level understanding of parasite biology will allow us to identify targets based on their key roles in networks. Specific aim 1 focuses on the harvest of data relevant to three gene families: the proteases, transporters, and transcription factors that are essential components in cellular networks for parasite growth and infection. Our project will encompass genomic data, expression data, and data derived from other high throughput experiments and bioinformatic and statistical analyses. Specific aim 2 focuses on the integration and encapsulation of these data in the form of network models. The models will be inferred using probabilistic graphical methods drawn from the field of engineering. Specific aim 3 focuses on the exploration of these models, particularly in the light of data from wet lab gene mapping and drug resistance experiments. Our ""biological maps"" will enable the visualization of evolutionary forces driving changes in the parasite's phenotype and should allow us to identify network design weaknesses in the parasite--potential vulnerabilities that could result in new malarial control strategies.           n/a",Systems Biology of Plasmodium falciparum: Building and Exploring Network Models,8131721,SC1AI080579,"['Animal Model', 'Antimalarials', 'Area', 'Artificial Intelligence', 'Beryllium', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle Regulation', 'Cell physiology', 'Chromosome Mapping', 'Complex', 'Data', 'Data Set', 'Development', 'Digestion', 'Discipline', 'Drug resistance', 'Electrical Engineering', 'Elements', 'Engineering', 'Family', 'Gene Family', 'Gene Proteins', 'Genes', 'Genomics', 'Growth', 'Harvest', 'Hemoglobin', 'Imagery', 'Immune', 'Infection', 'Intervention', 'Life Cycle Stages', 'Location', 'Malaria', 'Maps', 'Mediating', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Open Reading Frames', 'Parasites', 'Pattern', 'Peptide Hydrolases', 'Pharmaceutical Preparations', 'Phenotype', 'Plant Roots', 'Plasmodium', 'Plasmodium falciparum', 'Prevalence', 'Process', 'Property', 'Proteins', 'Proteomics', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Rupture', 'Signal Transduction', 'Specialist', 'Staging', 'Statistical Models', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Transcription Factor 3', 'Vaccines', 'Work', 'base', 'biological systems', 'comparative genomics', 'computerized data processing', 'cost effective', 'data integration', 'data mining', 'design', 'driving force', 'experience', 'functional genomics', 'insight', 'member', 'mortality', 'network models', 'novel', 'pressure', 'programs', 'prototype', 'quantum', 'research study', 'response', 'stem', 'theories', 'trafficking', 'transcription factor', 'transcriptomics']",NIAID,UNIVERSITY OF TEXAS SAN ANTONIO,SC1,2011,238085,-0.04194508676091908
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8138357,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2011,128304,-0.0023417796892097487
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,8325815,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Systems Development', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2011,100000,-0.0023417796892097487
"COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems.  To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Novel Machine Learning Approaches for Automati,8173654,76201000029C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Health Care Costs', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data mining', 'flexibility', 'innovation', 'novel', 'novel strategies', 'rapid growth']",NLM,NORTHEASTERN UNIVERSITY,N03,2010,377968,-0.004511781114407674
"COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING - Combining multiple types of reasoning to infer plausible,8170612,76201000023C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Diagnosis', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Medical Informatics', 'Patients', 'Population', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'prototype', 'rapid growth']",NLM,"CYCORP, INC.",N03,2010,377967,-0.006312687694283296
"COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat  In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Casual Inference on Narrative and Structured Temporal Dat ,8172635,76201000024C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Disease', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Human', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patients', 'Population', 'Research', 'Research Personnel', 'Research Project Grants', 'Statistical Methods', 'Structure', 'System', 'Text', 'Thinking', 'biomedical scientist', 'clinical care', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,N03,2010,373073,-0.00696709584180517
"COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Automated Reasoning for Application of Clinical,8173644,76201000025C,"['Caring', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Computer Simulation', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Patient Care', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'System', 'Thinking', 'biomedical scientist', 'clinical application', 'evidence based guidelines', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,STANFORD UNIVERSITY,N03,2010,378000,-0.007392780941172887
"COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Developing an Intelligent and Socially Oriented Search Que,8173663,76201000032C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Communities', 'Competence', 'Complement', 'Computers', 'Contracts', 'Data', 'Databases', 'Decision Making', 'Electronic Health Record', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'flexibility', 'innovation', 'novel strategies', 'rapid growth', 'social']",NLM,UNIVERSITY OF MICHIGAN AT ANN ARBOR,N03,2010,301251,-0.005483288072904912
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,8049892,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2010,5742,-0.004186102308437295
"COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Thinking to Support Clinicians and Biomedica,8173959,76201000034C,"['Caring', 'Clinical', 'Clinical Data', 'Clinical Decision Support Systems', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'clinical decision-making', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SAMSUNG SDS AMERICA, INC.",N03,2010,377925,-0.004134055506274633
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173956,76201000033C,"['Biology', 'Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computer software', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Molecular Biology', 'Patients', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'data modeling', 'flexibility', 'innovation', 'novel strategies', 'open source', 'prototype', 'rapid growth']",NLM,UNIVERSITY OF COLORADO DENVER,N03,2010,377982,-0.002563991382656685
"COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a","COMPUTTIONAL THINKING-An Evidence-Based, Open-Database Approach to Diagnostic Dec",8173645,76201000026C,"['Caring', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Face', 'Family', 'Funding', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Patients', 'Population', 'Prevention', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'cost effectiveness', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'novel strategies', 'rapid growth']",NLM,"SIMULCONSULT, INC.",N03,2010,377991,-0.0067646398507370436
"COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Visual Clinical Problem Threading for Case Summarization,8173653,76201000028C,"['Age', 'Caring', 'Chronic Disease', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Complex', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Disease Management', 'Face', 'Family', 'Funding', 'Human', 'Imagery', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Machine Learning', 'Medical', 'Memory', 'Methods', 'Patients', 'Population', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'Visual', 'biomedical scientist', 'design', 'flexibility', 'innovation', 'novel strategies', 'rapid growth']",NLM,"RUTGERS, THE STATE UNIV OF N.J.",N03,2010,300216,-0.005131379526391429
"COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Evidence-Based Expert Systems to Assist in Treatment of De,8173961,76201000035C,"['Caring', 'Childhood', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Competence', 'Complement', 'Computers', 'Contracts', 'Databases', 'Decision Making', 'Development', 'Disease', 'Expert Systems', 'Face', 'Family', 'Funding', 'Goals', 'Human', 'Individual', 'Information Management', 'Intelligence', 'Knowledge', 'Lead', 'Machine Learning', 'Medical', 'Memory', 'Mental Depression', 'Outcome', 'Patients', 'Probability', 'Research', 'Research Personnel', 'Research Project Grants', 'Solutions', 'Speed', 'System', 'Thinking', 'biomedical scientist', 'evidence base', 'flexibility', 'health application', 'innovation', 'novel strategies', 'rapid growth', 'tool', 'treatment planning']",NLM,SRI INTERNATIONAL,N03,2010,377997,-0.006612327122671648
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7913074,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,1248287,-0.010557416930519476
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8121894,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,300000,-0.010557416930519476
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8144973,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,113520,-0.010557416930519476
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8147585,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,151816,-0.010557416930519476
"HyPhy: Molecular Evolutionary Analyses    DESCRIPTION (provided by applicant): Project Description HyPhy (http://www.hyphy.org) is a scriptable software platform designed to enable flexible and powerful analyses of DNA, RNA, codon, amino acid and other types of sequence data in an evolutionary context. Such analyses have become an indispensable component of most research studies that make use of comparative genomic data. Biologists and bioinformaticians increasingly recognize the benefits of molecular evolutionary analyses. Since its initial release in 2001, HyPhy has become a relatively stable and mature product, and has been downloaded by more than 4,500 unique users, integrated into several popular web-based genomic data analysis servers, cited in over 400 peer-reviewed publications and described in three book chapters, in spite of the fact that the development of the package has never been directly funded. This proposal seeks support to improve the quality, performance, reliability, modularity, documentation and feature sets of the HyPhy system. Specific aims can be divided into four major areas: 1. Software engineering, testing, and documentation of the HyPhy codebase. 2. The development of a high-performance engine for phylogenetic maximum likelihood model fitting and inference. 3. Extension of a newly-initiated toolbox for machine learning applications in molecular evolution. 4. Creation and maintenance of a wiki-themed documentation resource.      PUBLIC HEALTH RELEVANCE: Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.           Project Narrative Molecular evolutionary analyses are central to many aspects of basic, translational, and applied biomedical research. Examples include identifying gene mutations that allow pathogens to evade the immune response; prediction of the structure and function of proteins; estimating the evolutionary relatedness of human or other populations; characterizing the magnitude of selective pressure, either natural or artificial, on genes or species.",HyPhy: Molecular Evolutionary Analyses,7937611,R01GM093939,"['Amino Acids', 'Animal Model', 'Area', 'Biomedical Research', 'Book Chapters', 'Code', 'Codon Nucleotides', 'Collection', 'Computer software', 'DNA', 'Data', 'Data Analyses', 'Development', 'Documentation', 'Drug resistance', 'Evolution', 'Funding', 'Gene Mutation', 'Genes', 'Genetic Programming', 'Genome', 'Genomics', 'Goals', 'Human', 'Immune response', 'Internet', 'Language', 'Libraries', 'Machine Learning', 'Maintenance', 'Modeling', 'Molecular', 'Molecular Evolution', 'Molecular Sequence Data', 'Online Systems', 'Pathogenicity', 'Pattern', 'Peer Review', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Population', 'Procedures', 'Publications', 'Research Personnel', 'Resources', 'Software Engineering', 'Study models', 'System', 'Testing', 'Viral', 'Work', 'acronyms', 'base', 'combinatorial', 'comparative genomics', 'design', 'flexibility', 'improved', 'pathogen', 'pressure', 'programs', 'protein structure function', 'public health relevance', 'research study', 'tool', 'user-friendly', 'wiki']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R01,2010,296413,-0.02010502719300724
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,8068069,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,51400,-0.004696745350054292
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7828142,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'innate immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2010,338802,-0.004696745350054292
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,7791405,K25AI064625,"['Address', 'Adverse event', 'Algorithms', 'Behavior', 'Biochemical', 'Biomedical Research', 'Bioterrorism', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Coupled', 'Coupling', 'Cytokine Network Pathway', 'Cytokine Signaling', 'Data', 'Engineering', 'Equation', 'Event', 'Feedback', 'Foundations', 'Future', 'Goals', 'Immune response', 'Immune system', 'Immunization', 'Immunologics', 'Immunology', 'Individual', 'Infectious Disease Immunology', 'Kinetics', 'Knowledge', 'Machine Learning', 'Mentors', 'Methods', 'Military Personnel', 'Modeling', 'Molecular', 'Nature', 'Organism', 'Pathogenesis', 'Patients', 'Pattern', 'Population', 'Proteins', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Risk', 'Role', 'Series', 'Serum', 'Signal Transduction', 'Signaling Protein', 'Simulate', 'Smallpox', 'Smallpox Vaccine', 'Software Tools', 'Systems Biology', 'Testing', 'Time', 'Training Programs', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Vent', 'Work', 'base', 'chemical reaction', 'cytokine', 'design', 'experience', 'interest', 'network models', 'novel', 'programs', 'protein expression', 'research study', 'response', 'simulation', 'software development', 'tool', 'volunteer']",NIAID,UNIVERSITY OF TULSA,K25,2010,113132,-0.005309837165667322
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7912919,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,304151,0.0035038917924628808
"Rational design of cytokine releasing angiogenic constructs    DESCRIPTION (provided by applicant): The development of organized vascular networks necessitates a tightly regulated interplay between variable cells, growth factors and soluble mediators. The applicant's long-term goal is to develop therapeutic angiogenic strategies based on the rational design of cytokine releasing constructs that promote vascular patterning and vessel stability. The objective of this proposal is to i) develop electrospun, three-dimensional constructs with patterned architecture, ii) demonstrate that the spatial and temporal delivery of two model angiogenic growth factors promotes the formation of an organized capillary network and iii) develop a computational model that can predict the biological effect of a growth factor releasing construct as a function of specified fabrication parameters. We hypothesize that guided therapeutic angiogenesis (i.e. patterned vascular networks) can be obtained by controlling the spatial and temporal presentation of soluble mediators at the site of ischemia. In AIM I, we will synthesize bFGF and G-CSF releasing electrospun constructs and determine their programmed delivery as a function of fabrication parameters. In AIM II, we will demonstrate the effect of spatial and temporal control of cytokine delivery in promoting directed angiogenesis in a three-dimensional in vitro angiogenesis model and we will develop a computational model/software that can predict the biological effect of different scaffold configurations. We will validate our model by assessing the angiogenic potential of our growth factor releasing constructs in a murine critical limb ischemic model.      PUBLIC HEALTH RELEVANCE: We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.           Project narrative We are proposing to a) fabricate a growth factor releasing construct that regulates the spatio- temporal delivery of angiogenic cytokines and promotes the formation of organized capillary networks and b) develop a machine learning computational model that can predict the angiogenic potential of our construct as a function of fabrication parameters.",Rational design of cytokine releasing angiogenic constructs,7961101,R21EB012136,"['Animal Model', 'Animals', 'Architecture', 'Biological', 'Blood Vessels', 'Blood capillaries', 'CSF3 gene', 'Cell Proliferation', 'Cell Transplantation', 'Clinical', 'Computer Simulation', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Electrodes', 'Fiber', 'Fibroblast Growth Factor 2', 'Gelatin', 'Goals', 'Growth', 'Growth Factor', 'In Vitro', 'Ischemia', 'Kinetics', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Mediator of activation protein', 'Modeling', 'Mus', 'Natural regeneration', 'Needles', 'Operative Surgical Procedures', 'Output', 'Pattern', 'Pharmacotherapy', 'Polymers', 'Process', 'Research', 'Series', 'Site', 'Specific qualifier value', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Effect', 'Tissue Engineering', 'angiogenesis', 'base', 'capillary', 'cell growth', 'cell motility', 'cytokine', 'design', 'electric field', 'high risk', 'interest', 'mathematical model', 'minimally invasive', 'multitask', 'nanofiber', 'novel', 'pre-clinical', 'programs', 'public health relevance', 'repaired', 'scaffold', 'success', 'therapeutic angiogenesis', 'tissue regeneration']",NIBIB,UNIVERSITY OF MIAMI CORAL GABLES,R21,2010,221192,-0.0028218380975448985
"Prediction of influenza antigenic variants using a novel sparse multitask learnin    DESCRIPTION (provided by applicant): Influenza and influenza related complication lead to more than 200,000 hospitalizations and approximately 36,000 deaths in the United States each year, and vaccination is the primary option for reducing influenza effect. A large amount of global efforts has to be made each year to identify antigenic variants and decide whether new vaccine strains are needed. Current laboratory based antigenic characterization processes are labor intensive and time consuming, and it has been the bottleneck for generating an effective influenza vaccination program. A robust method without such a laboratory characterization is demanding for rapid identification of influenza antigenic variants. This project proposes to develop a novel sparse multitask learning method in predicting influenza antigenic variants solely based on the input of protein sequences, and further to apply this method in mapping antigenic drift pathway of A/H3N2 influenza viruses and studying antigenic drift patterns leading to influenza outbreaks. This method is based on the assumption that influenza antigenicity would be determined by certain features in hemagglutinin (HA) protein sequence and tertiary structure. This assumption was well evidenced that the viruses with conserved HAs generated cross-reactions in serological reactions and also provided cross- protection in both laboratory experiments and field practices. The proposed method is novel since it combines multitask learning and sparse learning. Therefore not only this project will develop significant technology for antigenic variant screen, but also new machine learning methods. This project will facilitate vaccine strain selection since the proposed method can potentially reduce and even eliminate serological assay, one of the most labor intensive procedures, in influenza surveillance. In addition, the antigenicity specific features and the drift patterns causing influenza outbreaks to be identified in this study will enhance our understanding about antigen-antibody interaction thus enhance our knowledge in influenza immunology and serology. Furthermore, the proposed method is potentially applicable in characterizing antigenic properties of other pathogens with significant antigenic variations, for example, rotavirus. The specific aims are the following: (1) Development of a novel sparse multitask learning method in generating antigenic distance matrix using hemagglutinin inhibition (HI) data; (2) Development of a quantitative method for predicting antigenic variants in silicon; (3) Application of this method in studying seasonal influenza antigenic drift pathway and antigenic drift patterns leading to influenza outbreaks. This nature of this study is to address a novel predictive method for measuring antigenic divergence between influenza viruses, which is critical in influenza vaccine strain selection. Thus, we are submitting this project to the broad challenge area (06) Enabling Technologies and fit for the Specific Challenge 06-GM-103: development of predictive method for molecular structure, recognition, and ligand interaction.       PUBLIC HEALTH RELEVANCE: This study is to develop a novel computational method for influenza antigenic variant prediction, which is very useful in influenza vaccine strain selection. This method will also be applied in studying antigenic drift patterns leading to influenza outbreak and epidemics.               Project Narrative This study is to develop a novel computational method for influenza antigenic variant prediction, which is very useful in influenza vaccine strain selection. This method will also be applied in studying antigenic drift patterns leading to influenza outbreak and epidemics.",Prediction of influenza antigenic variants using a novel sparse multitask learnin,7835340,RC1AI086830,"['Address', 'Amino Acid Sequence', 'Antibodies', 'Antigenic Variation', 'Antigens', 'Area', 'Biological Assay', 'Cessation of life', 'Communities', 'Complication', 'Computing Methodologies', 'Cross Reactions', 'Data', 'Development', 'Epidemic', 'Hemagglutinin', 'Hospitalization', 'Immunology', 'Influenza', 'Influenza A Virus, H3N2 Subtype', 'Influenza vaccination', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Ligands', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Membrane Glycoproteins', 'Methods', 'Molecular Structure', 'Mutation', 'Nature', 'Online Systems', 'Pathway interactions', 'Pattern', 'Peptide Sequence Determination', 'Performance', 'Procedures', 'Process', 'Property', 'Reaction', 'Research', 'Rotavirus', 'Seasons', 'Serologic tests', 'Serological', 'Silicon', 'Structure', 'Techniques', 'Technology', 'Testing', 'Time', 'Trees', 'United States', 'Vaccination', 'Vaccines', 'Variant', 'Viral', 'Virus', 'base', 'genetic analysis', 'improved', 'influenza outbreak', 'influenza virus vaccine', 'influenzavirus', 'multitask', 'novel', 'novel vaccines', 'pathogen', 'programs', 'public health relevance', 'research study', 'seasonal influenza', 'tool', 'vector']",NIAID,MISSISSIPPI STATE UNIVERSITY,RC1,2010,412913,-0.024189008522501672
"COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Computational Abduction for Molecular Biology,8173647,76201000027C,[' '],NLM,STANFORD UNIVERSITY,N03,2010,378000,-0.002563991382656685
"COMPUTATIONAL THINKING-Text Mining of clinical narratives: In biomedicine, clinicians and researchers now face formidable challenges in information management, innovation, and decision-making in an era which is seeing extraordinarily rapid growth of knowledge, distributed among a host of databases, and on a scale far larger than can be mastered by an individual. The remarkable speed, memory capacity, and symbol-manipulating power of computers, if properly harnessed, can complement human cognitive strengths so as to enable efficient use of all of the knowledge relevant to solution of clinical and scientific problems. To invigorate research in the arena of computation and cognition, a number of fresh concepts have arisen in recent years: computational intelligence, machine learning, intelligence amplifying systems, flexible competence, human-computer collaboration, and computational thinking. As part of its ¿Medical Advanced Research Projects Initiative,¿ the NLM is funding novel approaches to computational thinking, in order to evaluate the feasibility of using innovative computational approaches to enhance the ability of clinicians and biomedical scientists to solve one or more significant cognitive tasks and bring improvements in medical care to patient, families and the public. n/a",COMPUTATIONAL THINKING-Text Mining of clinical narratives:,8173660,76201000031C,[' '],NLM,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,N03,2010,336943,-0.011049033521226872
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7754089,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2010,2037327,-0.0021871004202032216
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7895558,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2010,116662,-0.019097469841772314
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,8115481,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'operation', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2010,99989,-0.008791494458627463
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7846105,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,326254,-0.012799826367921745
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7935464,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'knowledge base', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2010,267671,-0.004668668093177952
"Functional activity and inter-organismal interactions in the human microbiome    DESCRIPTION (provided by applicant): High-throughput sequencing has provided a tool capable of observing the human microbiome, but characterizing the biological roles and metabolic potential of these microbial communities remains a significant challenge. Increasing evidence points to the functional activity of gene products, rather than community taxonomic composition, as the most robust descriptor of the microflora's relationship with its host and as a potential point of intervention in modulating human health. Existing computational tools for exploring a newly sequenced metagenome rely heavily on sequence homology and do not yet leverage information from the thousands of publicly available functional experimental results. Likewise, no previous methods have provided genome-scale computational tools for biological hypothesis generation regarding specific molecular interactions among the microflora and with a human host. This proposal aims to develop computational methodology to interpret the functional activity of microfloral communities: 1. Integrate functional information from taxonomic, metagenomic, and metatranscriptomic datasets. We will develop methodology to unify these three representations of microbiome composition by incorporating  information from large scale functional genomic data collections. 2. Identify genomic predictors of inter-species functional activity, including host/microflora interactions and points of community-wide regulatory feedback. We will computationally screen microbiome assays for molecular interactions and regulatory motifs spanning multiple organisms in the community. 3. Implement these technologies as publicly available, accessible, and interpretable tools. We will provide freely available, open source, downloadable and web-based implementations of this methodology for use  by the bioinformatic and biological communities. As high-throughput sequencing becomes more widely used to study microbial communities in the human microbiome and in the environment, computational tools will be necessary to summarize their global functional activity and systems-level regulatory interactions. In the long term, by providing methodology to understand the human microbiome at the molecular level, we hope to enable its future use as a diagnostic indicator and as a point of intervention to improve human health.      PUBLIC HEALTH RELEVANCE: DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.              2 Project Narrative DNA sequencing technology has recently allowed us to examine the microorganisms naturally residing in and on the human body, many of which are beneficial and some of which can be harmful. Although we can now gather data on the cellular behavior of these microbes and on their interactions with human beings, computational tools are needed to interpret this information. By developing new software to study these communities of microorganisms, we hope to eventually be able to detect when they may be causing disease and modify their composition to improve human health.",Functional activity and inter-organismal interactions in the human microbiome,8020799,R01HG005969,"['Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cells', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Diagnostic', 'Disease', 'Environment', 'Feedback', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Health', 'Human', 'Human Microbiome', 'Human body', 'Individual', 'Internet', 'Intervention', 'Machine Learning', 'Maps', 'Mentors', 'Metabolic', 'Metagenomics', 'Methodology', 'Methods', 'Microbe', 'Modeling', 'Molecular', 'Online Systems', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Process', 'Proteins', 'Recombinant DNA', 'Research Personnel', 'Resources', 'Role', 'Sequence Homology', 'Signaling Molecule', 'System', 'Systems Biology', 'Taxon', 'Techniques', 'Technology', 'Testing', 'Tissues', 'base', 'computerized tools', 'functional genomics', 'improved', 'member', 'metagenomic sequencing', 'microbial', 'microbial community', 'microbiome', 'microorganism', 'novel', 'open source', 'public health relevance', 'repository', 'tool', 'transcriptomics']",NHGRI,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2010,408559,-0.015356486227960413
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7798186,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'allograft rejection', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2010,289814,-0.01847130105005237
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,7821454,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic Diseases', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Running', 'Rural Community', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'high risk', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2010,100455,-0.03583217578330079
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,8110751,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic Diseases', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Running', 'Rural Community', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'high risk', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2010,50000,-0.03583217578330079
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7941984,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2010,303186,-0.013053364593811291
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7858165,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'allograft rejection', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,253269,-0.012710273507835443
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): Flow cytometry is used to rapidly gather large quantities of data on cell type and function. The manual process of classifying hundreds of thousands of cells forms a bottleneck in diagnostics, high-throughput screening, clinical trials, and large-scale research experiments. The process currently requires a trained technician to identify populations on a digital graph of the data by manually drawing regions. As the complexity of the data increases, this gating task becomes more lengthy and laborious, and it is increasingly clear that minimizing human processing is essential to increasing both throughput and consistency. In clinical tests and diagnostic environments, automated gating would eliminate a complex set of human instructions and decisions in the Standard Operating Procedure (SOP), thereby reducing error and speeding results to the doctor. In many cases, the software will be able to recognize the need for additional tests before the doctor has an opportunity to look at the first report. Currently no software is available to perform complex multi-parameter analyses in an automated and rigorously validated manner. FlowDx will fill an important gap in the evolution of the technology and pave the way for ever larger phenotypic studies and for the translation of this research process to a clinical environment. Specific Aims 1) Fully define the experimental protocol, whereby a researcher can compare two or more classifications of identical data sets to study the differences, biases and effectiveness of human and algorithmic classifiers. 2) Describe and evaluate metrics that compare the performance of classification algorithms. 3) Conduct analytical experiments on our identified use cases, illustrating the potential of this technique to affect clinical analysis. 4) Iteratively implement the tools to automate these experiments, improve the experimental capabilities, and collaborate in new use cases. These aims will be satisfied while maintaining quantitative standards of software quality, establishing measurements in system uptime, throughput and robustness to set the baseline for subsequent iterations.      PUBLIC HEALTH RELEVANCE: FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project 1) Fits the ""translational medicine"" model of the NIH Roadmap 2) Reduces error in the diagnosis of cancer and other diseases 3) Speeds results to physicians. Patients learn the outcome more quickly. Therapeutic intervention is faster. 4) Accommodates large-scale research by allowing greater volumes of complex data to be much more quickly examined, compared, and quantified 5) Reduces the expense of cell analysis by as much as 50% 6) Conforms to 21CFR Part 11 guidance           Narrative FlowDx, a Clinical Cytometry Analysis Software Project is designed to create a new, more efficient, and more effective way of analyzing cells for the presence of cancer, HIV/ AIDS, and other diseases, using a fully automated software system. Using Magnetic Gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural Networks, and Support Vector Machines (SVM), Tree Star software will analyze the cell samples from patients at a much faster rate and with fewer false positives and negatives than the manual method now in use. The FlowDx Project  � Fits the ""translational medicine"" model of the NIH Roadmap  � Reduces error in the diagnosis of cancer and other diseases  � Speeds results to physicians. Patients learn the outcome more quickly.  Therapeutic intervention is faster.  � Accommodates large-scale research by allowing greater volumes of complex data  to be much more quickly examined, compared, and quantified  � Reduces the expense of cell analysis by as much as 50%  � Conforms to 21CFR Part 11 guidance",Clinical Cytometry Analysis Software with Automated Gating,7999420,R44RR024094,"['Acquired Immunodeficiency Syndrome', 'Affect', 'Algorithms', 'Architecture', 'Authorization documentation', 'Automation', 'Biological Assay', 'Biological Neural Networks', 'Biomedical Research', 'Cells', 'Characteristics', 'Classification', 'Client', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Code', 'Complex', 'Computer software', 'Computers', 'Consensus', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Documentation', 'Effectiveness', 'Environment', 'Evolution', 'Flow Cytometry', 'Foundations', 'Graph', 'Grouping', 'HIV', 'Hospitals', 'Human', 'Institution', 'Instruction', 'Label', 'Language', 'Learning', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Medical center', 'Methods', 'Metric', 'Modeling', 'Outcome', 'Patients', 'Performance', 'Physicians', 'Population', 'Probability', 'Procedures', 'Process', 'Protocols documentation', 'Quality Control', 'Records', 'Reporting', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Security', 'Services', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Trees', 'United States National Institutes of Health', 'Universities', 'Work', 'abstracting', 'cancer diagnosis', 'cell type', 'commercial application', 'data integrity', 'design', 'digital', 'encryption', 'high throughput screening', 'improved', 'operation', 'patient privacy', 'public health relevance', 'repository', 'research study', 'response', 'software systems', 'technological innovation', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R44,2010,449663,-0.027633074123797865
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,8112998,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein complex', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,R01,2010,252819,-0.022524662036093726
"Interrogation of systems level mechanisms controlling DNA repair processes    DESCRIPTION (provided by applicant): A comprehensive model of a complete biological system will enable prediction of phenotypic outcomes in face of novel genetic and environmental perturbations. Such predictive power for cellular systems, such as DMA repair, will have tremendous impacts on early detection and diagnosis of genetic disorders and, ultimately in designing preventive and/or curative treatments. However, the full understanding of eukaryotic DMA repair systems presents a daunting challenge due to their enormous complexity, especially given that systems approaches are not yet sufficiently mature to mathematically model such complex processes. At a molecular level, eukaryotic genetic information processing is mirrored in a simplified manner by their evolutionary ancestors, the archaea. In Halobacterium NRC-1 we have a simple yet powerful model system of -2400 genes in which the underlying generalized principles of a systems approach can be delineated. This halophilic archaeon routinely mitigates damage from high salinity, UV radiation and desiccation-rehydration cycles. We will use this model system to address the hypothesis that the decision making process governing cell fate after exposure to DMA damaging events such as UV irradiation is mediated by robust gene regulatory networks that simultaneously process information on spectral characteristics of the impinging radiation, the nature and extent of DMA damage and the potential preparative role of cellular entrainment to diurnal cycles. This hypothesis is motivated by two main observations: a. in systems analysis of UV response, although extraordinarily resistant, not all expected repair genes in this microbe responded to irradiation or damage; b. many of these damage-unresponsive repair genes did, however, respond to day-night entrainment. At a fundamental level this suggested that the resident state of the cell at time of irradiation may influence the type of response elicited. A systems approach is ideally suited to delinate the networks underlying this decision making process. Specifically, we will measure dynamic global changes (degree of damage, mRNA /protein levels, protein-protein and protein-DNA interactions) in wild type and mutant strains (defective in sensors, signal transducers, regulators and repair proteins) subjected to combinatorial changes in incident radiation, type of damage inflicted and resident state of the cell at the time of radiation. Through an integrated analysis of these diverse systems level data we will statistically learn perturbation-induced rewiring of a gene regulatory network that processes environmental perturbations (input) into phenotype (output), i.e. a predictive model for regulatory mechanisms for repair. This basic model will serve as a template for designing systems approaches to model higher complexities of eukaryotic repair processes.           n/a",Interrogation of systems level mechanisms controlling DNA repair processes,7896417,R01GM077398,"['Address', 'Algorithms', 'Animal Model', 'Archaea', 'Architecture', 'Behavior', 'Biological', 'Biological Assay', 'Biological Models', 'Biology', 'Candidate Disease Gene', 'Cell Cycle Progression', 'Cell Size', 'Cell Survival', 'Cell physiology', 'Cells', 'Characteristics', 'Chromosomes, Human, Pair 4', 'Circadian Rhythms', 'Complex', 'Coupled', 'DNA', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'DNA biosynthesis', 'DNA lesion', 'DNA repair protein', 'DNA-Protein Interaction', 'Data', 'Decision Making', 'Desiccation', 'Disease', 'Early Diagnosis', 'Elements', 'Engineering', 'Environment', 'Epitopes', 'Equilibrium', 'Event', 'Exogenous Factors', 'Exposure to', 'Face', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Information Processing Pathway', 'Genetic Transcription', 'Genome', 'Global Change', 'Halobacterium', 'Heart', 'Hour', 'Human', 'Lead', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Messenger RNA', 'Metabolism', 'Microbe', 'Modeling', 'Molecular', 'Mutation', 'Nature', 'Nucleotide Excision Repair', 'Organism', 'Outcome', 'Output', 'Oxidative Stress', 'Pathway interactions', 'Phenotype', 'Physiological Processes', 'Physiology', 'Preventive', 'Process', 'Protein Dynamics', 'Proteins', 'Radiation', 'Readiness', 'Regulation', 'Regulator Genes', 'Regulatory Element', 'Rehydrations', 'Resistance', 'Resources', 'Role', 'Signal Transduction', 'Skin Cancer', 'Source', 'Specific qualifier value', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Tertiary Protein Structure', 'Time', 'Transcript', 'Transducers', 'Translations', 'UV response', 'Ultraviolet Rays', 'biological research', 'biological systems', 'cell type', 'combinatorial', 'coping', 'cryptochrome', 'design', 'gene repair', 'genetic disorder diagnosis', 'halobacteria', 'information processing', 'interest', 'irradiation', 'knockout gene', 'mathematical model', 'mutant', 'novel', 'operation', 'predictive modeling', 'repaired', 'response', 'segregation', 'sensor', 'technology development', 'trait', 'ultraviolet damage', 'ultraviolet irradiation']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,R01,2010,332561,-0.035276588285786344
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7780085,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,3513343,-0.016486341173536014
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8138946,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,745063,-0.016486341173536014
"Enhancing 3dsvm to improve its interoperability and dissemination    DESCRIPTION (provided by applicant): This research plan outlines crucial software enhancements to a program called 3dsvm, which is a command line program and graphical user interface (gui) plugin for AFNI (Cox, 1996). 3dsvm performs support vector machine (SVM) analysis on fMRI data, which constitutes one important approach to performing multivariate supervised learning of neuroimaging data. 3dsvm originally provided the ability to analyze fMRI data as described in (LaConte et al., 2005). Since its first distribution as a part of AFNI, it has been steadily extended to provide new functionality including regression and non-linear kernels, as well as multiclass classification capabilities. In addition to its integration into AFNI, features that make 3dsvm particularly well suited for fMRI analysis are that it is easy to spatially mask voxels (to include/exclude them in the SVM analysis) as well as to flexibly select subsets of a dataset to use as training or testing samples. It has been used to generate results for our own work and for collaborative efforts and has been cited as a resource by others (Mur et al. 2009; Hanke et al. 2009). Despite many positive aspects of 3dsvm, the priorities of PAR-07-417 address a genuine need that this software project has - the ability to focus on improvements that will increase its dissemination and interoperability. A major motivation for PAR-07-417 is to facilitate the improved interface, characterization, and documentation to enhance the extent of sharing and to provide the groundwork for future extensions. Our aims are well aligned with this program announcement. Further, there is a growing need in the neuroimaging community for tools such as 3dsvm. Since 3dsvm is not a new project, is tightly integrated into the software environment of AFNI, and can be further integrated to enable better functionality to support needs as diverse as NIfTI format capabilities to rtFMRI, this proposed project will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.      PUBLIC HEALTH RELEVANCE: This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.           NARRATIVE This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.",Enhancing 3dsvm to improve its interoperability and dissemination,8278135,R03EB012464,[' '],NIBIB,VIRGINIA POLYTECHNIC INST AND ST UNIV,R03,2010,156500,-0.01396040562912997
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7851333,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Belief', 'Bioinformatics', 'Biological Assay', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Phenotype', 'Physiological', 'Protocols documentation', 'Public Health', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly']",NHLBI,WASHINGTON UNIVERSITY,R01,2010,474912,-0.01029240429488398
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8150047,U54NS064808,"['Address', 'Adherence', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Bite', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials Cooperative Group', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Computer software', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Quality', 'Data Reporting', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Disasters', 'Disease', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family history of', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Image', 'Individual', 'Industry', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Manuals', 'Maps', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patients', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Positron-Emission Tomography', 'Principal Investigator', 'Printing', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Reader', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Scanning', 'Scientist', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Support Groups', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'computerized data processing', 'data management', 'data mining', 'electronic data', 'follow-up', 'improved', 'interest', 'meetings', 'patient advocacy group', 'programs', 'quality assurance', 'radiologist', 'sample collection', 'statistics', 'tool', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2010,959195,-0.006661708397445563
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7921043,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'global health', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2010,133651,-0.0023417796892097487
"Systems Biology of Plasmodium falciparum: Building and Exploring Network Models    DESCRIPTION (provided by applicant):  There is an urgent need for novel anti-malaria interventions, due to the growing resistance of Plasmodium parasites to available drugs. The long-term objective of our research is to identify potential drug or vaccine targets using an efficient and cost-effective data-mining approach. This proposed study is aimed at the identification of biological networks as the first step toward a systems-level understanding of parasite biology. Compared to functional genomics, which requires a priori information about the identity or function of targets, a systems-level understanding of parasite biology will allow us to identify targets based on their key roles in networks. Specific aim 1 focuses on the harvest of data relevant to three gene families: the proteases, transporters, and transcription factors that are essential components in cellular networks for parasite growth and infection. Our project will encompass genomic data, expression data, and data derived from other high throughput experiments and bioinformatic and statistical analyses. Specific aim 2 focuses on the integration and encapsulation of these data in the form of network models. The models will be inferred using probabilistic graphical methods drawn from the field of engineering. Specific aim 3 focuses on the exploration of these models, particularly in the light of data from wet lab gene mapping and drug resistance experiments. Our ""biological maps"" will enable the visualization of evolutionary forces driving changes in the parasite's phenotype and should allow us to identify network design weaknesses in the parasite--potential vulnerabilities that could result in new malarial control strategies.           n/a",Systems Biology of Plasmodium falciparum: Building and Exploring Network Models,7910601,SC1AI080579,"['Animal Model', 'Antimalarials', 'Area', 'Artificial Intelligence', 'Be++ element', 'Beryllium', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle Regulation', 'Cell physiology', 'Chromosome Mapping', 'Complex', 'Data', 'Data Set', 'Development', 'Digestion', 'Discipline', 'Drug resistance', 'Electrical Engineering', 'Elements', 'Engineering', 'Family', 'Gene Family', 'Gene Proteins', 'Genes', 'Genomics', 'Growth', 'Harvest', 'Hemoglobin', 'Imagery', 'Immune', 'Infection', 'Intervention', 'Life Cycle Stages', 'Light', 'Location', 'Malaria', 'Maps', 'Mediating', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Open Reading Frames', 'Parasites', 'Pattern', 'Peptide Hydrolases', 'Pharmaceutical Preparations', 'Phenotype', 'Plant Roots', 'Plasmodium', 'Plasmodium falciparum', 'Prevalence', 'Process', 'Property', 'Proteins', 'Proteomics', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Rupture', 'Signal Transduction', 'Specialist', 'Staging', 'Statistical Models', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Transcription Factor 3', 'Vaccines', 'Work', 'base', 'biological systems', 'comparative genomics', 'computerized data processing', 'cost', 'data integration', 'data mining', 'design', 'driving force', 'experience', 'functional genomics', 'insight', 'member', 'mortality', 'network models', 'novel', 'pressure', 'programs', 'prototype', 'quantum', 'research study', 'response', 'stem', 'theories', 'trafficking', 'transcription factor', 'transcriptomics']",NIAID,UNIVERSITY OF TEXAS SAN ANTONIO,SC1,2010,240491,-0.04194508676091908
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,-0.0022804875216768097
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7670456,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Outcome', 'Performance', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,74750,-0.004186102308437295
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant): This proposal is submitted in response to NOT-OD-09-058 NIH Announces the Availability of Recovery Act Funds for Competitive Revision Applications. Health status and outcomes are frequently measured on an ordinal scale. Examples include scoring methods for liver biopsy specimens from patients with chronic hepatitis, including the Knodell hepatic activity index, the Ishak score, and the METAVIR score. In addition, tumor-node-metasis stage for cancer patients is an ordinal scaled measure. Moreover, the more recently advocated method for evaluating response to treatment in target tumor lesions is the Response Evaluation Criteria In Solid Tumors method, with ordinal outcomes defined as complete response, partial response, stable disease, and progressive disease. Traditional ordinal response modeling methods assume independence among the predictor variables and require that the number of samples (n) exceed the number of covariates (p). These are both violated in the context of high-throughput genomic studies. Our currently funded R03 grant, ""Recursive partitioning and ensemble methods for classifying an ordinal response,"" consists of the following three specific aims (SA.1) extend the recursive partitioning and random forest classification methodologies for predicting an ordinal response by developing computational tools for the R programming environment including implementing our ordinal impurity criteria in rpart and implementing the ordinal impurity criteria in randomForest; (SA.2) evaluate the proposed ordinal classification methods in comparison to existing nominal and continuous response methods using simulated, benchmark, and gene expression datasets; and (SA.3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Recently, penalized models have been successfully applied to high-throughput genomic datasets in fitting linear, logistic, and Cox proportional hazards models with excellent performance. However, extension of penalized models to the ordinal response setting has not been described. Herein we propose to extend the L1 penalized method to ordinal response models to enable modeling of common ordinal response data when a high-dimensional genomic data comprise the predictor space. This study will expand the scope of our current research by providing a model-based ordinal classification methodology applicable for high-dimensional datasets to accompany the heuristic based classification tree and random forest ordinal methodologies considered in the parent grant. The specific aims of this competitive revision application are to: Aim 1) Extend the L1 penalized methodology to enable predicting an ordinal response by developing computational tools for the R programming environment; Aim 2) Using simulated, benchmark, and gene expression datasets, evaluate L1 penalized ordinal response models by comparing error rates from our L1 fitting algorithm to those obtained when using a forward variable selection modeling strategy and our ordinal random forest approach; and Aim 3) Evaluate methods for assessing important covariates from L1 penalized ordinal response models.           This project will develop L1 penalized ordinal response models and implement them in the R programming environment. By conducting extensive comparisons of various ordinal response modeling methods using simulated, benchmark, and gene expression datasets, we will be able to make a recommendation regarding ordinal response modeling to the scientific community. This research is significant since the ordinal response modeling methods developed during the project period will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.",Recursive partitioning and ensemble methods for classifying an ordinal response,7805045,R03LM009347,"['Advocate', 'Algorithms', 'Applications Grants', 'Area', 'Behavioral Research', 'Benchmarking', 'Bioconductor', 'Biopsy Specimen', 'Cancer Patient', 'Chronic Hepatitis', 'Classification', 'Clinical', 'Communities', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Drug toxicity', 'Economics', 'Education', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Gene Expression', 'Genomics', 'Grant', 'Health', 'Health Status', 'Health Surveys', 'Hepatic', 'Human', 'In complete remission', 'Informatics', 'Lesion', 'Literature', 'Location', 'Logistics', 'Machine Learning', 'Mathematics', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Neoplasm Metastasis', 'Occupations', 'Outcome', 'Patients', 'Performance', 'Positioning Attribute', 'Progressive Disease', 'Recommendation', 'Recovery', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Sample Size', 'Sampling', 'Science', 'Scoring Method', 'Simulate', 'Solid Neoplasm', 'Stable Disease', 'Staging', 'Technology', 'Translational Research', 'Travel', 'Trees', 'United States National Institutes of Health', 'base', 'computerized tools', 'cost', 'forest', 'heuristics', 'improved', 'indexing', 'interest', 'liver biopsy', 'meetings', 'neglect', 'novel', 'parent grant', 'partial response', 'preference', 'programs', 'research study', 'response', 'simulation', 'social', 'software development', 'symposium', 'tool', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2009,75000,-0.023254648657721352
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7622614,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2009,1224323,-0.010557416930519476
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7685518,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Computer software', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Series', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'new therapeutic target', 'novel', 'pathway tools', 'programs', 'reconstruction']",NLM,SRI INTERNATIONAL,R01,2009,175647,-0.0168652105606876
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7577491,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Patients', 'Play', 'Population', 'Process', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'longitudinal analysis', 'particle', 'public health relevance', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2009,342223,-0.004696745350054292
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,8004341,K25AI064625,"['Address', 'Adverse event', 'Algorithms', 'Behavior', 'Biochemical', 'Biomedical Research', 'Bioterrorism', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Coupled', 'Coupling', 'Cytokine Network Pathway', 'Cytokine Signaling', 'Data', 'Engineering', 'Equation', 'Event', 'Feedback', 'Foundations', 'Future', 'Goals', 'Immune response', 'Immune system', 'Immunization', 'Immunologics', 'Immunology', 'Individual', 'Infectious Disease Immunology', 'Kinetics', 'Knowledge', 'Machine Learning', 'Mentors', 'Methods', 'Military Personnel', 'Modeling', 'Molecular', 'Nature', 'Organism', 'Pathogenesis', 'Patients', 'Pattern', 'Population', 'Proteins', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Risk', 'Role', 'Series', 'Serum', 'Signal Transduction', 'Signaling Protein', 'Simulate', 'Smallpox', 'Smallpox Vaccine', 'Software Tools', 'Systems Biology', 'Testing', 'Time', 'Training Programs', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Vent', 'Work', 'base', 'chemical reaction', 'cytokine', 'design', 'experience', 'interest', 'network models', 'novel', 'programs', 'protein expression', 'research study', 'response', 'simulation', 'software development', 'tool', 'volunteer']",NIAID,UNIVERSITY OF TULSA,K25,2009,39676,-0.005309837165667322
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,7612652,K25AI064625,"['Address', 'Adverse event', 'Algorithms', 'Behavior', 'Biochemical', 'Biomedical Research', 'Bioterrorism', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Coupled', 'Coupling', 'Cytokine Network Pathway', 'Cytokine Signaling', 'Data', 'Engineering', 'Equation', 'Event', 'Feedback', 'Foundations', 'Future', 'Goals', 'Immune response', 'Immune system', 'Immunization', 'Immunologics', 'Immunology', 'Individual', 'Infectious Disease Immunology', 'Kinetics', 'Knowledge', 'Machine Learning', 'Mentors', 'Methods', 'Military Personnel', 'Modeling', 'Molecular', 'Nature', 'Organism', 'Pathogenesis', 'Patients', 'Pattern', 'Population', 'Proteins', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Risk', 'Role', 'Series', 'Serum', 'Signal Transduction', 'Signaling Protein', 'Simulate', 'Smallpox', 'Smallpox Vaccine', 'Software Tools', 'Systems Biology', 'Testing', 'Time', 'Training Programs', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Vent', 'Work', 'base', 'chemical reaction', 'cytokine', 'design', 'experience', 'interest', 'network models', 'novel', 'programs', 'protein expression', 'research study', 'response', 'simulation', 'software development', 'tool', 'volunteer']",NIAID,UNIVERSITY OF ALABAMA AT BIRMINGHAM,K25,2009,70245,-0.005309837165667322
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7670408,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'biological systems', 'complex biological systems', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,311541,0.0035038917924628808
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7582301,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2009,2057843,-0.0021871004202032216
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7652367,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2009,124786,-0.019097469841772314
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7913595,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2009,108000,-0.019097469841772314
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7633119,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2009,18437,-0.010386559178528968
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7652508,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'intelligence genetics', 'novel', 'programs', 'resistant strain', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2009,363929,-0.008791494458627463
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,-0.0022804875216768097
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,-0.0022804875216768097
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7582189,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Body of uterus', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,312453,-0.012799826367921745
"Ontology-based Information Network to Support Vaccine Research    DESCRIPTION (provided by applicant): Since the introduction of Edward Jenner's smallpox vaccine in 1796, vaccines have proven invaluable for their ability to stimulate the immune system and to confer protection against pathogenic organisms. Progress in modern vaccine research has been accompanied by a dramatic increase in the number of vaccine-related papers in the published literature. It has become increasingly challenging to identify and annotate vaccine data from this large and diverse literature which no one scientist or team can fully master. Although vaccine databases exist that emphasize commercialized vaccines, no public central repository is available to store research data concerning commercial vaccines, vaccines in clinical trials, or vaccine candidates in early stages of development, in a fashion that render such data available for advanced analyses. To fill this need, we have developed VIOLIN (http://www.violinet.org), a web-based database system for annotation, storage, and analysis of published vaccine data. An ontology represents consensus-based controlled vocabularies of terms and relations, with associated definitions which are logically formulated in such a way as to promote automated reasoning. A bottleneck of vaccine research and further VIOLIN development is the lack of a vaccine ontology, which in turn makes a significant obstacle for vaccine data standardization, retrieval, integration, and advanced analysis and prediction. Our goal is to develop the community-based Vaccine Ontology (VO) and apply it to efficient vaccine literature mining and analysis of protective immune mechanisms. We will focus on two model pathogens: Escherichia coli and Brucella species. This project contains three specific aims: (1) develop a community-based Vaccine Ontology (VO), and apply it to establish a vaccine knowledgebase and to promote vaccine data integration and query through Semantic Web. The VO development will be achieved through collaboration with vaccine researchers, the Infectious Disease Ontology (IDO) Initiative, and the National Center for Biomedical Ontology (NCBO); (2) develop a VO-based natural language processing (NLP) system and apply it for more efficient retrieval of Brucella and E. coli vaccine information, automated annotation of journal articles with VO terms, and VO improvement. This task will be achieved by collaboration with the National Center for Integrative Biomedical Informatics (NCIBI). (3) analyze and predict vaccine targets and protective immune networks attributable to the interactions between host and vaccine. This will be achieved mainly by VO-based literature mining and a novel genome- and literature-based statistical methodology. This project will be implemented by a strong collaborative team and supported from a large user community. The Vaccine Ontology and its applications to literature mining and for studying protective immunity against Brucella spp. and E. coli will lay a strong foundation for further advanced informatics research on vaccines against infectious diseases in the post-genomics and information era.            Narrative: Vaccines stimulate the immune system and confer protection against pathogenic microorganisms. A bottleneck of vaccine research is the lack of an ontology (consensus- based controlled vocabularies of terms and relations) to ensure consistency of literature curation and support automated reasoning. The goal of this project is to develop a community-based Vaccine Ontology and apply it to vaccine literature mining and analysis of vaccine-induced immune mechanisms.",Ontology-based Information Network to Support Vaccine Research,7735790,R01AI081062,"['Algorithms', 'Attenuated Live Virus Vaccine', 'Automated Annotation', 'Bacterial Genes', 'Brucella', 'Brucella Vaccine', 'Clinical Trials', 'Collaborations', 'Communicable Diseases', 'Communities', 'Consensus', 'Controlled Vocabulary', 'Data', 'Databases', 'Development', 'Dictionary', 'Ensure', 'Escherichia coli', 'Escherichia coli Vaccines', 'Foundations', 'Genome', 'Genomics', 'Goals', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Informatics', 'Information Networks', 'Information Retrieval', 'Journals', 'Laboratories', 'Literature', 'MeSH Thesaurus', 'Methodology', 'Methods', 'Modeling', 'National Center for Integrative Biomedical Informatics', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Preparation', 'Process', 'Proteins', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Smallpox Vaccine', 'Staging', 'Standardization', 'Structure', 'Subunit Vaccines', 'System', 'Testing', 'Training', 'Vaccine Research', 'Vaccines', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'data integration', 'editorial', 'gene function', 'genome-wide', 'interest', 'journal article', 'microorganism', 'novel', 'novel vaccines', 'pathogen', 'programs', 'repository', 'research study', 'statistics', 'text searching', 'user-friendly', 'vaccine candidate', 'vaccine development', 'vaccine evaluation', 'web interface']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2009,270375,-0.004668668093177952
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7663288,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Computer Systems Development', 'Computer software', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Source', 'Structure', 'System', 'Systems Integration', 'Technology', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'meetings', 'models and simulation', 'open source', 'outreach', 'protein complex', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2009,437938,-0.007109608055983488
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7599555,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2009,290671,-0.01847130105005237
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7790821,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Support', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'flexibility', 'innovation', 'interest', 'meetings', 'prevent', 'product development', 'public health relevance', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2009,4047,-0.003769909722569461
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,7656889,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic Diseases', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Running', 'Rural Community', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'high risk', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2009,96901,-0.03583217578330079
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,7918523,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic Diseases', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Running', 'Rural Community', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'high risk', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2009,49991,-0.03583217578330079
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7788875,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Ontology', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'RNA', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'empowered', 'genome wide association study', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'histone modification', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'public health relevance', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2009,142123,-0.018507595625984974
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7595813,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'biological systems', 'comparative', 'computer based statistical methods', 'data integration', 'design', 'flexibility', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface', 'web site']",NIGMS,PRINCETON UNIVERSITY,R01,2009,243004,-0.013921647802622495
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7666186,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'biobank', 'cancer microarray', 'cancer type', 'design', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'transplant database', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,256073,-0.012710273507835443
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7804332,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2009,363247,-0.013053364593811291
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7570061,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein complex', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,R01,2009,280910,-0.022524662036093726
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7625039,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Base Management', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data exchange', 'data format', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2009,536571,-0.0025633403762972385
"Interrogation of systems level mechanisms controlling DNA repair processes    DESCRIPTION (provided by applicant): A comprehensive model of a complete biological system will enable prediction of phenotypic outcomes in face of novel genetic and environmental perturbations. Such predictive power for cellular systems, such as DMA repair, will have tremendous impacts on early detection and diagnosis of genetic disorders and, ultimately in designing preventive and/or curative treatments. However, the full understanding of eukaryotic DMA repair systems presents a daunting challenge due to their enormous complexity, especially given that systems approaches are not yet sufficiently mature to mathematically model such complex processes. At a molecular level, eukaryotic genetic information processing is mirrored in a simplified manner by their evolutionary ancestors, the archaea. In Halobacterium NRC-1 we have a simple yet powerful model system of -2400 genes in which the underlying generalized principles of a systems approach can be delineated. This halophilic archaeon routinely mitigates damage from high salinity, UV radiation and desiccation-rehydration cycles. We will use this model system to address the hypothesis that the decision making process governing cell fate after exposure to DMA damaging events such as UV irradiation is mediated by robust gene regulatory networks that simultaneously process information on spectral characteristics of the impinging radiation, the nature and extent of DMA damage and the potential preparative role of cellular entrainment to diurnal cycles. This hypothesis is motivated by two main observations: a. in systems analysis of UV response, although extraordinarily resistant, not all expected repair genes in this microbe responded to irradiation or damage; b. many of these damage-unresponsive repair genes did, however, respond to day-night entrainment. At a fundamental level this suggested that the resident state of the cell at time of irradiation may influence the type of response elicited. A systems approach is ideally suited to delinate the networks underlying this decision making process. Specifically, we will measure dynamic global changes (degree of damage, mRNA /protein levels, protein-protein and protein-DNA interactions) in wild type and mutant strains (defective in sensors, signal transducers, regulators and repair proteins) subjected to combinatorial changes in incident radiation, type of damage inflicted and resident state of the cell at the time of radiation. Through an integrated analysis of these diverse systems level data we will statistically learn perturbation-induced rewiring of a gene regulatory network that processes environmental perturbations (input) into phenotype (output), i.e. a predictive model for regulatory mechanisms for repair. This basic model will serve as a template for designing systems approaches to model higher complexities of eukaryotic repair processes.           n/a",Interrogation of systems level mechanisms controlling DNA repair processes,7646234,R01GM077398,"['Address', 'Algorithms', 'Animal Model', 'Archaea', 'Architecture', 'Behavior', 'Biological', 'Biological Assay', 'Biological Models', 'Biology', 'Candidate Disease Gene', 'Cell Cycle Progression', 'Cell Size', 'Cell Survival', 'Cell physiology', 'Cells', 'Characteristics', 'Chromosomes, Human, Pair 4', 'Circadian Rhythms', 'Complex', 'Coupled', 'DNA', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'DNA biosynthesis', 'DNA lesion', 'DNA repair protein', 'DNA-Protein Interaction', 'Data', 'Decision Making', 'Desiccation', 'Disease', 'Early Diagnosis', 'Elements', 'Engineering', 'Environment', 'Epitopes', 'Equilibrium', 'Event', 'Exogenous Factors', 'Exposure to', 'Face', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Information Processing Pathway', 'Genetic Transcription', 'Genome', 'Global Change', 'Halobacterium', 'Heart', 'Hour', 'Human', 'Lead', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Messenger RNA', 'Metabolism', 'Microbe', 'Modeling', 'Molecular', 'Mutation', 'Nature', 'Nucleotide Excision Repair', 'Operative Surgical Procedures', 'Organism', 'Outcome', 'Output', 'Oxidative Stress', 'Pathway interactions', 'Phenotype', 'Physiological Processes', 'Physiology', 'Preventive', 'Process', 'Protein Dynamics', 'Proteins', 'Radiation', 'Readiness', 'Regulation', 'Regulator Genes', 'Regulatory Element', 'Rehydrations', 'Resistance', 'Resources', 'Role', 'Signal Transduction', 'Skin Cancer', 'Source', 'Specific qualifier value', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Tertiary Protein Structure', 'Time', 'Transcript', 'Transducers', 'Translations', 'UV response', 'Ultraviolet Rays', 'biological research', 'biological systems', 'cell type', 'combinatorial', 'coping', 'cryptochrome', 'design', 'gene repair', 'genetic disorder diagnosis', 'halobacteria', 'information processing', 'interest', 'irradiation', 'knockout gene', 'mathematical model', 'mutant', 'novel', 'predictive modeling', 'repaired', 'response', 'segregation', 'sensor', 'technology development', 'trait', 'ultraviolet damage', 'ultraviolet irradiation']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,R01,2009,335920,-0.035276588285786344
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7941562,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,1038804,-0.016486341173536014
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7581087,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,3437506,-0.016486341173536014
"Systems Biology of Plasmodium falciparum: Building and Exploring Network Models    DESCRIPTION (provided by applicant):  There is an urgent need for novel anti-malaria interventions, due to the growing resistance of Plasmodium parasites to available drugs. The long-term objective of our research is to identify potential drug or vaccine targets using an efficient and cost-effective data-mining approach. This proposed study is aimed at the identification of biological networks as the first step toward a systems-level understanding of parasite biology. Compared to functional genomics, which requires a priori information about the identity or function of targets, a systems-level understanding of parasite biology will allow us to identify targets based on their key roles in networks. Specific aim 1 focuses on the harvest of data relevant to three gene families: the proteases, transporters, and transcription factors that are essential components in cellular networks for parasite growth and infection. Our project will encompass genomic data, expression data, and data derived from other high throughput experiments and bioinformatic and statistical analyses. Specific aim 2 focuses on the integration and encapsulation of these data in the form of network models. The models will be inferred using probabilistic graphical methods drawn from the field of engineering. Specific aim 3 focuses on the exploration of these models, particularly in the light of data from wet lab gene mapping and drug resistance experiments. Our ""biological maps"" will enable the visualization of evolutionary forces driving changes in the parasite's phenotype and should allow us to identify network design weaknesses in the parasite--potential vulnerabilities that could result in new malarial control strategies.           n/a",Systems Biology of Plasmodium falciparum: Building and Exploring Network Models,7669377,SC1AI080579,"['Animal Model', 'Antimalarials', 'Area', 'Artificial Intelligence', 'Be++ element', 'Beryllium', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle Regulation', 'Cell physiology', 'Chromosome Mapping', 'Complex', 'Data', 'Data Set', 'Development', 'Digestion', 'Discipline', 'Drug resistance', 'Electrical Engineering', 'Elements', 'Engineering', 'Family', 'Gene Family', 'Gene Proteins', 'Genes', 'Genomics', 'Growth', 'Harvest', 'Hemoglobin', 'Imagery', 'Immune', 'Infection', 'Intervention', 'Life Cycle Stages', 'Light', 'Location', 'Malaria', 'Maps', 'Mediating', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Open Reading Frames', 'Parasites', 'Pattern', 'Peptide Hydrolases', 'Pharmaceutical Preparations', 'Phenotype', 'Plant Roots', 'Plasmodium', 'Plasmodium falciparum', 'Prevalence', 'Process', 'Property', 'Proteins', 'Proteomics', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Rupture', 'Signal Transduction', 'Specialist', 'Staging', 'Statistical Models', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Transcription Factor 3', 'Vaccines', 'Work', 'base', 'biological systems', 'comparative', 'computerized data processing', 'cost', 'data integration', 'data mining', 'design', 'driving force', 'experience', 'functional genomics', 'insight', 'member', 'mortality', 'network models', 'novel', 'pressure', 'programs', 'prototype', 'quantum', 'research study', 'response', 'stem', 'theories', 'trafficking', 'transcription factor', 'transcriptomics']",NIAID,UNIVERSITY OF TEXAS SAN ANTONIO,SC1,2009,240426,-0.04194508676091908
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7692401,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2009,135000,-0.0023417796892097487
"HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence    DESCRIPTION (provided by applicant):      Even as many developed countries strengthen their traditional clinically-based surveillance capacity, a basic level of health information infrastructure is still lacking in many parts of the developing world, including areas that are most vulnerable to emerging health threats. To help fill this gap, electronic news outlets, disease reporting networks, discussion sites, and blogs are proving to be invaluable data sources for a new generation of public health surveillance systems that operate across international borders- almost all major outbreaks investigated by WHO, including SARS, are first identified by these informal online sources. These data sources provide local and timely information about disease outbreaks and related events around the world, including areas relatively invisible to day-to-day global public health efforts due to lack of infrastructure or political suppression of information. However, since this information is dispersed and largely unstructured, there is lack of organization and integration between sources, precluding a global view of all ongoing disease threats. In order to construct such an integrated view of current emerging infections, we deployed an early prototype called Health Map, a freely available multi-stream real-time knowledge management system that aggregates and maps health alerts across numerous key data sources, including WHO alerts, newswires, mailing lists. Leveraging our previous National Library of Medicine funded work in real-time health monitoring; we propose extensive development of this new digital resource to address technological and methodological barriers to achieving a synthesized, comprehensive, and customized view of global health. The principal objective of Health Map development is to provide access to the greatest amount of possibly useful information across the widest variety of geographical areas and disease agents, without overwhelming the user with an excess of information, or obscuring important and urgent elements. We will specifically address the challenge of providing structure to unstructured data while still enabling the astute user to notice the subtle pattern that could be an early indication of a significant outbreak. Development of Health Map will proceed along the four key components of surveillance: (1) data acquisition (evaluation and integration of multiple electronic sources) (2) information characterization (disease and location categorization and severity rating of unstructured data), (3) signal interpretation (multi-stream signal detection, spatiotemporal modeling and risk assessment) and (4) knowledge dissemination (tiered dissemination of global infectious disease alerts and flexible data visualization tools). In each area, we will make critical enhancements to the existing prototype. We plan rigorous evaluation to ensure that Health Map successfully leverages electronic sources for surveillance, communication, and intervention. We will also conduct comprehensive user testing, usability studies, user behavior analysis as part of our effort. Our goal is to make Health Map a vital knowledge resource tailored for both the general public and public health decision-makers.        Although an enormous amount of information about current infectious disease outbreaks is found in Web- accessible information sources, this information is dispersed and not well-organized, making it almost impossible for public health officials and concerned citizens to know about all ongoing emerging disease threats. Through rigorous development of our online HealthMap system, we plan to provide a synthesized, timely and comprehensive view of current global infectious disease outbreaks by acquiring, integrating and disseminating a wide variety of Internet-based information sources. Our goal is to make HealthMap a vital knowledge resource tailored for both the general public and public health decision-makers.",HealthMap: Knowledge Management for Emerging Infectious Disease Intelligence,7928674,G08LM009776,"['Accounting', 'Address', 'Area', 'Behavior', 'Biological Neural Networks', 'Characteristics', 'Code', 'Communicable Diseases', 'Communication', 'Complement', 'Computer Systems Development', 'Data', 'Data Quality', 'Data Sources', 'Decision Making', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Dictionary', 'Disease', 'Disease Outbreaks', 'Electronics', 'Elements', 'Emerging Communicable Diseases', 'Ensure', 'Evaluation', 'Event', 'Funding', 'General Population', 'Generations', 'Geographic Locations', 'Goals', 'Health', 'Health Professional', 'Health Status', 'Imagery', 'Infection', 'Information Resources', 'Information Resources Management', 'Information Retrieval', 'Intelligence', 'International', 'Internet', 'Intervention', 'Knowledge', 'Language', 'Location', 'Mails', 'Maps', 'Medical', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Natural Language Processing', 'Network-based', 'Operating System', 'Pattern', 'Population', 'Population Surveillance', 'Populations at Risk', 'Process', 'Public Health', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Risk Assessment', 'Risk Estimate', 'Sampling', 'Severe Acute Respiratory Syndrome', 'Severities', 'Signal Transduction', 'Site', 'Source', 'Statistical Models', 'Stream', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trees', 'United States National Library of Medicine', 'Update', 'Work', 'base', 'data acquisition', 'demographics', 'digital', 'experience', 'flexibility', 'improved', 'interest', 'mortality', 'news', 'prototype', 'spatiotemporal', 'syndromic surveillance', 'tool', 'usability', 'web-accessible']",NLM,BOSTON CHILDREN'S HOSPITAL,G08,2009,55179,-0.0023417796892097487
"DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE Cardiovascular disease (CVD) and its associated risk factors such as hypertension and dyslipidemia constitute a major public-health burden due to increased mortality and morbidity and rising health care costs. Massive epidemiological data are needed to detect the small effects of many individual genes and the environment on these traits. However, sample sizes needed to make powerful inferences may only be reached by integrating multiple epidemiological studies. Meaningful integration of information from multiple studies requires the development of data ontologies which make it possible to integrate information across studies in an optimum manner so as to maximize the information content and hence the statistical power for detecting small effect sizes. A second compounding problem of data integration is that software applications that manage such study data are typically non-interoperable, i.e. “silos” of data, and are incapable of being shared in a syntactically and semantically meaningful manner. Consequently, an infrastructure that integrates across studies in an interoperable manner is needed to ensure that epidemiological cardiovascular research remains a viable and major player in the biomedical informatics revolution which is currently underway. The cancer Biomedical Informatics Grid (caBIGTM) is addressing these problems in the cancer domain by developing software systems that are able to exchange information or that are syntactically interoperable by accessing metadata that is semantically annotated using controlled vocabularies. Our overarching goal is to develop ontologies for integrating cardiovascular epidemiological data from multiple studies. Specifically, we propose three Aims: First, develop cardiovascular data ontologies and vocabularies for each of three disparate multi-center epidemiological studies that facilitate data integration across the studies and data mining for various phenotypes. Second, adopt a technology infrastructure that leverages the cardiovascular data ontologies and vocabularies using Model Driven Architecture (MDA) and caBIGTM tools to facilitate the integration and widespread sharing of cardiovascular data sets. Third, facilitate seamless data sharing and promote widespread data dissemination among research communities cutting across clinical, translational and epidemiological domains, primarily through collaboration with the established CardioVascular Research Grid (CVRG). Cardiovascular disease (CVD) is a leading cause of mortality and morbidity which contributes substantially to rising health care costs and consequently constitutes a major public health burden. Therefore, understanding the genetic and environmental effects on these CVD traits is important. Massive epidemiological study data are needed to detect the small individual effects of genes and their interactions, and integration of multiple epidemiological studies are necessary for generating large sample sizes. Unfortunately, integrating information from multiple studies in a meaningful manner requires the development of data ontologies (language and grammar). Our proposal addresses this need, and does this in a way that is informative and user-friendly from the End User’s point of view.",DEVELOPMENT OF DATA ONTOLOGIES FOR INTEGRATING MULTI-CENTER CARDIOVASCULAR STUDIE,7558424,R01HL094286,"['Address', 'Adopted', 'Architecture', 'Area', 'Belief', 'Bioinformatics', 'Biological Assay', 'Budgets', 'Cardiovascular Diseases', 'Cardiovascular system', 'Clinical', 'Clinical Research', 'Collaborations', 'Common Data Element', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Controlled Vocabulary', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Dimensions', 'Disease', 'Dyslipidemias', 'Electrocardiogram', 'Elements', 'Ensure', 'Environment', 'Epidemiologic Studies', 'Epidemiology', 'Equipment', 'Failure', 'Family Study', 'Ferrets', 'Genes', 'Genetic', 'Genotype', 'Goals', 'Grant', 'Health Care Costs', 'Human', 'Hypertension', 'Individual', 'Language', 'Length', 'Literature', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morbidity - disease rate', 'National Cancer Institute', 'Natural Language Processing', 'Nature', 'Ontology', 'Peer Review', 'Phenotype', 'Physiological', 'Preparation', 'Protocols documentation', 'Public Health', 'Published Comment', 'Publishing', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Factors', 'Sample Size', 'Scientist', 'Solutions', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Time', 'Time Study', 'Vocabulary', 'Work', 'anticancer research', 'base', 'bench to bedside', 'biomedical informatics', 'cancer Biomedical Informatics Grid', 'cardiovascular disorder risk', 'data integration', 'data mining', 'data sharing', 'design', 'experience', 'graphical user interface', 'interest', 'meetings', 'mortality', 'software development', 'software systems', 'tool', 'trait', 'user-friendly', 'working group']",NHLBI,WASHINGTON UNIVERSITY,R01,2009,488000,-0.01029240429488398
"Novel TIRF microscopy analyzing trafficking & signaling at the cell cortex In 3-month paid research experiences, over two summers, 4  talented undergraduate engineering and computer science students will be recruited to contribute in a substantial way to the progress of sub-aims of my parent New Innovator (DP2) Award;  the DP2's main specific aims are to develop novel microscopes (Aim 1) and analytical tools (Aim 2) to study membrane trafficking at the cell cortex (Aim 3).  Specifically, students will work on one of two projects: i)  Design and construct and improve electronic circuits to control the galvometric XY mirror on our multi-angle FRAP/TIRFM (Aim 1B) and/or new auto-sensing calibration devices (Aim 2) or ii) Develop models or software to visualize and track vesicles in 3D by multi-angle TIRFM (Aim 2 ). Their advances will directly benefits goals of the DP2 and the new electronics and software's performance will be benchmarked and iteratively improved. Training and oversight will come from the PI and the DP2-supported senior scientist, Dr. Polejaev.  Students will benefit from the infrastructure of the Yale 'CINEMA' lab imaging center, which is under the PI's directorship.         Two outstanding US engineering students have already been identified, Noah Pestana and Isaac Anderson, both of whom have expressed a high interest in participating this summer on projects (i) and (ii), respectively. In the second summer, a particular emphasis will be made to recruit minority undergraduate students through the Yale 'STARS' program for minorities.   All proposed activity are within the scope of the parent DP2 and capitalizes on early successes and will accelerate the tempo of two of the approved specific aims. Consistent with the recovery act's goal, this funding will provide full-time summer employment for 4 undergraduate students and accelerate scientific achievement of the parent DP2 award.  n/a",Novel TIRF microscopy analyzing trafficking & signaling at the cell cortex,7892704,DP2OD002980,"['1-Phosphatidylinositol 3-Kinase', 'Abbreviations', 'Accounting', 'Acoustics', 'Address', 'Adipocytes', 'Affect', 'Algorithms', 'Area', 'Arts', 'Attenuated', 'Automobile Driving', 'Award', 'Back', 'Binding', 'Biochemical', 'Biochemistry', 'Biological', 'Biology', 'Boxing', 'Buffers', 'Caliber', 'Calibration', 'Cell Line', 'Cell membrane', 'Cell surface', 'Cells', 'Cellular biology', 'Clathrin', 'Cluster Analysis', 'Collaborations', 'Collection', 'Collimator', 'Color', 'Coma', 'Communities', 'Complex', 'Computational Biology', 'Computer Vision Systems', 'Computers', 'Conflict (Psychology)', 'Confocal Microscopy', 'Coupled', 'Coupling', 'Cues', 'Cytoskeleton', 'Data', 'Data Set', 'Defect', 'Development', 'Diabetes Mellitus', 'Diffusion', 'Dimensions', 'Dimerization', 'Disadvantaged', 'Discipline', 'Docking', 'Down-Regulation', 'Drops', 'Dyes', 'Employee Strikes', 'Endocytosis', 'Engineering', 'Ensure', 'Environment', 'Event', 'Exocytosis', 'Eye', 'Face', 'Feedback', 'Fiber', 'Figs - dietary', 'Flare', 'Fluorescein-5-isothiocyanate', 'Fluorescence', 'Fluorescence Microscopy', 'Fluorescence Recovery After Photobleaching', 'Fluorescent Dyes', 'Functional disorder', 'Funding', 'Genetic Screening', 'Germany', 'Glass', 'Glucose Transporter', 'Glycerol', 'Goals', 'Grant', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Insulin', 'Interdisciplinary Study', 'Investments', 'Joints', 'Kinetics', 'Knowledge', 'Label', 'Laboratories', 'Lasers', 'Learning', 'Legal patent', 'Length', 'Life', 'Light', 'Lighting', 'Link', 'Lipids', 'Location', 'Macromolecular Complexes', 'Malignant Neoplasms', 'Maps', 'Masks', 'Measures', 'Mediating', 'Membrane', 'Membrane Microdomains', 'Membrane Protein Traffic', 'Methodology', 'Methods', 'Microscope', 'Microscopy', 'Microtubules', 'Modeling', 'Molecular', 'Monitor', 'Morphologic artifacts', 'Motivation', 'Motor', 'Nature', 'Non-Insulin-Dependent Diabetes Mellitus', 'Optics', 'Organelles', 'PTEN gene', 'Paper', 'Parasites', 'Pathway interactions', 'Penetration', 'Performance', 'Phosphatidylinositols', 'Phosphotransferases', 'Photobleaching', 'Physiologic pulse', 'Planet Mars', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Private Sector', 'Probability', 'Process', 'Proteins', 'Publications', 'Pupil', 'Quantum Dots', 'RNA Interference', 'Radial', 'Randomized', 'Reagent', 'Recruitment Activity', 'Refractive Indices', 'Regulation', 'Relative (related person)', 'Reporter', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Support', 'Resolution', 'Risk', 'Role', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Seminal', 'Series', 'Side', 'Signal Pathway', 'Signal Transduction', 'Silicon Dioxide', 'Simulate', 'Site', 'Small Interfering RNA', 'Solid', 'Solutions', 'Sorting - Cell Movement', 'Source', 'Spain', 'Spatial Distribution', 'Specific qualifier value', 'Specimen', 'Speed', 'Spottings', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Thick', 'Time', 'Total Internal Reflection Fluorescent', 'Touch sensation', 'Training', 'Transfection', 'Tubulin', 'Vesicle', 'Visual', 'Wolves', 'Work', 'analytical method', 'basal insulin', 'base', 'blood glucose regulation', 'cell cortex', 'cell motility', 'cell type', 'cellular imaging', 'density', 'design', 'extracellular', 'flexibility', 'flotillin', 'fluorescence imaging', 'fluorescence microscope', 'fluorophore', 'handbook', 'holistic approach', 'image processing', 'improved', 'innovation', 'insight', 'instrument', 'instrumentation', 'insulin signaling', 'interest', 'lens', 'medical schools', 'meetings', 'micromanipulator', 'migration', 'millisecond', 'nanometer', 'novel', 'object shape', 'photoactivation', 'prototype', 'receptor', 'research study', 'response', 'scaffold', 'simulation', 'single molecule', 'success', 'tool', 'trafficking', 'trans-Golgi Network', 'trend', 'user-friendly', 'virtual']",OD,YALE UNIVERSITY,DP2,2009,50463,-0.012259861210246342
"Recursive partitioning and ensemble methods for classifying an ordinal response    DESCRIPTION (provided by applicant):       Classification methods applied to microarray data have largely been those developed by the machine learning community, since the large p (number of covariates) problem is inherent in high-throughput genomic experiments. The random forest (RF) methodology has been demonstrated to be competitive with other machine learning approaches (e.g., neural networks and support vector machines). Apart from improved accuracy, a clear advantage of the RF method in comparison to most machine learning approaches is that variable importance measures are provided by the algorithm. Therefore, one can assess the relative importance each gene has on the predictive model. In a large number of applications, the class to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I,II,III, IV); drug toxicity (none, mild, moderate, severe); or response to treatment classified as complete response, partial response, stable disease, and progressive disease. These responses are ordinal; while there is an inherent ordering among the responses, there is no known underlying numerical relationship between them. While one can apply standard nominal response methods to ordinal response data, in so doing one loses the ordered information inherent in the data. Since ordinal classification methods have been largely neglected in the machine learning literature, the specific aims of this proposal are to (1) extend the recursive partitioning and RF methodologies for predicting an ordinal response by developing computational tools for the R programming environment; (2) evaluate the proposed ordinal classification methods against alternative methods using simulated, benchmark, and gene expression datasets; (3) develop and evaluate methods for assessing variable importance when interest is in predicting an ordinal response. Novel splitting criteria for classification tree growing and methods for estimating variable importance are proposed, which appropriately take the nature of the ordinal response into consideration. In addition, the Generalized Gini index and ordered twoing methods will be studied under the ensemble learning framework, which has not been previously conducted. This project is significant to the scientific community since the ordinal classification methods to be made available from this project will be broadly applicable to a variety of health, social, and behavioral research fields, which commonly collect responses on an ordinal scale.           n/a",Recursive partitioning and ensemble methods for classifying an ordinal response,7470967,R03LM009347,"['Algorithms', 'Behavioral Research', 'Benchmarking', 'Biological Neural Networks', 'Class', 'Classification', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Discriminant Analysis', 'Drug toxicity', 'Environment', 'Gene Expression', 'Genes', 'Genomics', 'Goals', 'Health', 'Health Surveys', 'Image Analysis', 'In complete remission', 'Individual', 'Learning', 'Literature', 'Machine Learning', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Neoplasm Metastasis', 'Northern Blotting', 'Numbers', 'Outcome', 'Performance', 'Polymerase Chain Reaction', 'Process', 'Progressive Disease', 'Relative (related person)', 'Simulate', 'Stable Disease', 'Staging', 'Standards of Weights and Measures', 'Structure', 'Technology', 'Time', 'Trees', 'computerized tools', 'forest', 'improved', 'indexing', 'interest', 'neglect', 'novel', 'partial response', 'predictive modeling', 'programs', 'research study', 'response', 'social', 'tumor']",NLM,VIRGINIA COMMONWEALTH UNIVERSITY,R03,2008,74521,-0.004186102308437295
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7363692,K22LM008389,"['Adverse event', 'Area', 'Arts', 'Attention', 'Biological Neural Networks', 'Caregivers', 'Chicago', 'Childhood', 'Classification', 'Clinical', 'Computer software', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Element', 'Data Set', 'Databases', 'Decision Trees', 'Detection', 'Disease', 'Early Intervention', 'Ensure', 'Etiology', 'Event', 'Excision', 'Foundations', 'Genomics', 'Heart Arrest', 'Hour', 'Laboratories', 'Length', 'Life', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Noise', 'Numbers', 'Patients', 'Pediatric Intensive Care Units', 'Performance', 'Physiologic Monitoring', 'Physiological', 'Population', 'Predictive Value', 'Procedures', 'Process', 'Purpose', 'Range', 'Receiver Operating Characteristics', 'Regression Analysis', 'Research Personnel', 'Sample Size', 'Sensitivity and Specificity', 'Series', 'Severity of illness', 'Social Sciences', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Validation', 'Work', 'abstracting', 'base', 'computer based statistical methods', 'data mining', 'data modeling', 'inclusion criteria', 'mortality', 'predictive modeling', 'programs', 'prospective', 'size', 'tool', 'trend', 'vector']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2008,135000,-0.022263823408531076
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7499147,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Depth', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Numbers', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'foot', 'insight', 'member', 'novel', 'quality assurance', 'scale up', 'size', 'symposium', 'theories', 'tool']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2008,1200000,-0.010557416930519476
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7504002,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2008,176002,-0.0168652105606876
"The Statistical and Computational Analysis of Flow Cytometry Data    DESCRIPTION (provided by applicant):  Flow cytometry is a data-rich technology that plays a critical role in basic research and clinical therapy for a variety of human diseases. Recent technological developments have greatly increased the areas of application and data throughput, and corresponding innovative analysis methods are needed. In order to be able to take advantage of these new capabilities researchers need access to high quality analysis tools that will help to identify subpopulations of cells with particular characteristics. The methods we are proposing include advanced methods for machine learning and visualization. We will apply our methods to a number of different scenarios such as the analysis of longitudinal data, and the analysis of data arising from clinical studies. PUBLIC HEALTH RELEVANCE: The aims of this project are to provide statistical and computational methods for the analysis of flow cytometry data. The impact of these tools will be to provide better, more reliable, tools for the analysis of flow cytometry data. The domain of application spans all diseases, but current applications are focused on HIV disease and cancer.          n/a",The Statistical and Computational Analysis of Flow Cytometry Data,7431959,R01EB008400,"['AIDS/HIV problem', 'Address', 'Antibodies', 'Antigens', 'Area', 'Basic Science', 'Biological', 'Cancer Vaccines', 'Cations', 'Cells', 'Characteristics', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Computer Analysis', 'Computer software', 'Computing Methodologies', 'Condition', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Disease', 'Ensure', 'Event', 'Flow Cytometry', 'Future', 'Genomics', 'HIV', 'Hypersensitivity', 'Imagery', 'Immune response', 'Immunity', 'Intervention', 'Lasers', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Medical', 'Methods', 'Names', 'Noise', 'Numbers', 'Patients', 'Play', 'Population', 'Process', 'Public Health', 'Rate', 'Reagent', 'Research Infrastructure', 'Research Personnel', 'Role', 'Rosa', 'Sampling', 'Shapes', 'Software Tools', 'Staining method', 'Stains', 'Statistical Methods', 'Surface', 'Technology', 'Transplantation', 'Vaccine Research', 'Variant', 'Work', 'graft vs host disease', 'human disease', 'immune function', 'innovation', 'instrument', 'instrumentation', 'leukemia/lymphoma', 'particle', 'size', 'sound', 'tool']",NIBIB,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2008,376423,-0.004696745350054292
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,7491749,K25AI064625,"['Address', 'Adverse event', 'Algorithms', 'Behavior', 'Biochemical', 'Biomedical Research', 'Bioterrorism', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Coupled', 'Coupling', 'Cytokine Network Pathway', 'Cytokine Signaling', 'Data', 'Engineering', 'Equation', 'Event', 'Feedback', 'Foundations', 'Future', 'Goals', 'Immune response', 'Immune system', 'Immunization', 'Immunologics', 'Immunology', 'Individual', 'Infectious Disease Immunology', 'Kinetics', 'Knowledge', 'Machine Learning', 'Mentors', 'Methods', 'Military Personnel', 'Modeling', 'Molecular', 'Nature', 'Organism', 'Pathogenesis', 'Patients', 'Pattern', 'Population', 'Proteins', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Risk', 'Role', 'Series', 'Serum', 'Signal Transduction', 'Signaling Protein', 'Simulate', 'Smallpox', 'Smallpox Vaccine', 'Software Tools', 'Systems Biology', 'Testing', 'Time', 'Today', 'Training Programs', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Vent', 'Work', 'base', 'chemical reaction', 'cytokine', 'design', 'experience', 'interest', 'network models', 'novel', 'programs', 'protein expression', 'research study', 'response', 'simulation', 'software development', 'tool', 'volunteer']",NIAID,UNIVERSITY OF ALABAMA AT BIRMINGHAM,K25,2008,106794,-0.005309837165667322
"A Simulation Tool to Enable Identification of Critical Network Interactions Using    DESCRIPTION (provided by applicant): One of the main challenges in the discovery of intracellular biomarkers and identification of therapeutic targets is the lack of a mechanistic understanding of the complex underlying pathways. The tremendous increase in both the quantity and diversity of cellular data represents a significant challenge to researchers seeking to construct biologically relevant interaction maps, and objectively extract specific actionable information. Machine learning based clustering algorithms serve as a preliminary statistical data analysis metric, but they fail to capture the data in the proper biological context. While chemical kinetics based models have proved to be effective in elucidating the pathway mechanisms, accurate estimates for the model parameters are severely lacking and are often impossible to obtain owing to the inherent difficulties involved in making dynamic measurements of specific intracellular phenomena. Additionally, methods for rational prioritization and selection of critical intracellular interactions (in the absence of kinetic information) are sorely lacking. Therefore, there is a clear need for innovative software tools that enable quantitative analysis of available microarray data in a biological pathway context, ultimately leading to the objective identification of critical biological interactions, providing a direction for more focused future efforts. We propose to address this challenge by developing an automated software platform that utilizes microarray data to select and merge relevant canonical biological pathway models thereby placing significantly expressed genes in their biological context. The analysis software will utilize a microarray expression-weighted metric to objectively rank the most critical interactions within the network model using a novel chemical kinetics-free Boolean dynamics algorithm. In the Phase I effort, we will develop a software tool composed of an R library that enables the automated generation of a pathway model from a given microarray dataset. Additionally, a methodology, and associated R library will be developed to objectively rank critical interactions in the pathway model, using a microarray data expression-weighted metric. Demonstration and validation of proposed algorithm will be carried out using a well characterized lipopolysaccharide (LPS) stimulated RAW 264.7 macrophage system. In Phase II, we will extend the scope of the algorithmic framework to include proteomic and metabolomic weighting in the objective ranking of critical interactions, and add workflow improvements through the addition of a graphical user interface (GUI). Experimental verification and validation of critical interactions identified in Phase I will be carried out using gene-silencing techniques. We also intend to establish collaborative partnerships with commercial entities. The proposing team has extensive experience in the areas of systems biology and bioinformatics (CFDRC) and microarray data analysis (Shawn Levy, University of Vanderbilt). CFDRC has a strong track record in the commercialization of software and hardware. PUBLIC HEALTH RELEVANCE:  Recently, there has been a tremendous increase in both the amount and diversity of cellular data available to researchers, representing a clear need for the development of advanced computational analysis software to enable the discovery of biomarkers of disease states, and identification of new therapeutic targets. However, currently available analysis tools do not consider the data in a proper biological context. This research proposes to develop an automated software platform that utilizes available data to develop and analyze mathematical models of complex processes in an automated fashion, resulting in the identification of critical intracellular processes.             n/a",A Simulation Tool to Enable Identification of Critical Network Interactions Using,7482734,R43GM084890,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Bioinformatics', 'Biological', 'Biological Markers', 'Complex', 'Computer Analysis', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Future', 'Gene Silencing', 'Generations', 'Genes', 'Genomics', 'Kinetics', 'Lead', 'Libraries', 'Lipopolysaccharides', 'Machine Learning', 'Maps', 'Measurement', 'Methodology', 'Methods', 'Metric', 'Microarray Analysis', 'Modeling', 'Nature', 'Pathway Analysis', 'Pathway interactions', 'Phase', 'Process', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Software Tools', 'Statistical Data Interpretation', 'System', 'Systems Biology', 'Techniques', 'Title', 'Universities', 'Urination', 'Validation', 'Weight', 'base', 'chemical kinetics', 'commercialization', 'editorial', 'experience', 'graphical user interface', 'innovation', 'macrophage', 'mathematical model', 'metabolomics', 'network models', 'novel', 'novel therapeutics', 'simulation', 'therapeutic target', 'tool']",NIGMS,CFD RESEARCH CORPORATION,R43,2008,99571,-0.0027192079902638886
"Stochastic dynamics for multiscale biology    DESCRIPTION (provided by applicant):  Complex biological systems are increasingly subject to investigation by mathematical modeling in general and stochastic simulation in particular. Advanced mathematical methods will be used to generate next-generation computational methods and algorithms for (1) formulating these models, (2) simulating or sampling their stochastic dynamics, (3) reducing them to simpler approximating models for use in multiscale simulation, and (4) optimizing their unknown or partly known parameters to fit observed behaviors and/or measurements. The proposed methods are based on advances in applied statistical and stochastic mathematics, including advances arising from operator algebra, quantum field theory, stochastic processes, statistical physics, machine learning, and related mathematically grounded fields. A central technique in this work will be the use of the operator algebra formulation of the chemical master equation.       The biological systems to be studied include and are representative of high-value biomedical target systems whose complexity and spatiotemporal scale requires improved mathematical and computational methods, to obtain the scientific understanding underlying future medical intervention. Cancer research is broadly engaged in signal transduction systems and complexes with feedback, for which the yeast Ste5 MARK pathway is a model system. DNA damage sensing (through ATM) and repair control (though p53 and Mdm2) are at least equally important to cancer research owing to the central role that failure of these systems play in many cancers. The dendritic spine synapse system is central to neuroplasticity and therefore human learning and memory. It is critical to understand this neurobiological system well enough to protect it against neurodegenerative diseases and environmental insults. The project seeks fundamental mathematical breakthroughs in stochastic and multiscale modeling that will enable the scientific understanding of these complex systems necessary to create effective medical interventions of the future.           n/a",Stochastic dynamics for multiscale biology,7596501,R01GM086883,"['Affinity', 'Algorithms', 'Behavior', 'Binding', 'Binding Sites', 'Biological', 'Biological Models', 'Biology', 'Chemicals', 'Chromosome Pairing', 'Complex', 'Computing Methodologies', 'DNA Damage', 'Dendritic Spines', 'Diffusion', 'Drug Formulations', 'Equation', 'Equilibrium', 'Evolution', 'Failure', 'Feedback', 'Free Energy', 'Future', 'Graph', 'Human', 'Intervention', 'Investigation', 'Laws', 'Learning', 'M cell', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Medical', 'Memory', 'Methods', 'Modeling', 'Molecular', 'Neurobiology', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Numbers', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Production', 'Purpose', 'Rate', 'Reaction', 'Role', 'Sampling', 'Scheme', 'Semantics', 'Signal Transduction', 'Simulate', 'Site', 'Speed', 'Stochastic Processes', 'Surface', 'Synapses', 'System', 'TP53 gene', 'Techniques', 'Testing', 'Time', 'Transcriptional Regulation', 'Validation', 'Vertebral column', 'Work', 'Yeasts', 'anticancer research', 'base', 'concept', 'improved', 'indexing', 'interest', 'mathematical model', 'models and simulation', 'multi-scale modeling', 'next generation', 'novel', 'quantum', 'reaction rate', 'repaired', 'simulation', 'spatiotemporal', 'syntax', 'theories']",NIGMS,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,319129,0.0035038917924628808
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7367958,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2008,2037396,-0.0021871004202032216
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7473948,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Condition', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Range', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2008,122190,-0.019097469841772314
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7489320,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2008,52898,-0.010386559178528968
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7495734,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2008,353327,-0.008791494458627463
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7407451,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2008,291451,-0.01847130105005237
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7457647,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2008,437938,-0.007109608055983488
"Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid    DESCRIPTION (provided by applicant): The objective of this project is the development of an innovative technique to avoid disclosure of confidential data in public use tabular data. Our proposed technique, called Optimal Data Switching (OS), overcomes the limitations and disadvantages found in currently deployed disclosure limitation methods. Statistical databases for public use pose a critical problem of identifying how to make the data available for analysis without disclosing information that would infringe on privacy, violate confidentiality, or endanger national security. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. Yet, the possibility of extracting certain sensitive elements of information from the data can jeopardize the welfare of these organizations and potentially, in some instances, the welfare of the society in which they operate. The challenge is, therefore, to represent the data in a form that permits accurate analysis for supporting research, decision-making and policy initiatives, while preventing an unscrupulous or ill-intentioned party from exploiting the data for harmful consequences. Our goal is to build on the latest advances in optimization, to which the OptTek Systems, Inc. (OptTek) research team has made pioneering contributions, to provide a framework based on optimal data switching, enabling the Centers for Disease Control and Prevention (CDC) and other organizations to effectively meet the challenge of confidentiality protection. The framework we propose is structured to be easy to use in a wide array of application settings and diverse user environments, from client-server to web-based, regardless of whether the micro-data is continuous, ordinal, binary, or any combination of these types. The successful development of such a framework, and the computer-based method for implementing it, is badly needed and will be of value to many types of organizations, not only in the public sector but also in the private sector, for whom the incentive to publish data is both economic as well as scientific. Examples in the public sector are evident, where organizations like CDC and the U.S. Census Bureau exist for the purpose of collecting, analyzing and publishing data for analysis by other parties. Numerous examples are also encountered in the private sector, notably in banking and financial services, healthcare (including drug companies and medical research institutions), market research, oil exploration, computational biology, renewable and sustainable energy, retail sales, product development, and a wide variety of other areas. PUBLIC HEALTH RELEVANCE: In the process of accumulating and disseminating public health data for reporting purposes, various uses, and statistical analysis, we must guarantee that individual records describing each person or establishment are protected. Organizations in both the public and private sectors have a major stake in this confidentiality protection problem, given the fact that access to data is essential for advancing research and formulating policy. This project proposes the development of a robust methodology and practical framework to deliver an efficient and effective tool to protect the confidentiality in published tabular data.                      n/a",Optimal Micro-Data Switching: An Enhanced Framework and Decision Tool for Confid,7535414,R43MH086138,"['Accounting', 'American', 'Area', 'Cells', 'Censuses', 'Centers for Disease Control and Prevention (U.S.)', 'Client', 'Computational Biology', 'Confidentiality', 'Data', 'Data Analyses', 'Data Reporting', 'Data Set', 'Databases', 'Decision Analysis', 'Decision Making', 'Development', 'Disadvantaged', 'Disclosure', 'Economics', 'Elements', 'Ensure', 'Environment', 'Goals', 'Health Personnel', 'Healthcare', 'Incentives', 'Individual', 'Inferior', 'Institution', 'Machine Learning', 'Market Research', 'Medical Research', 'Methodology', 'Methods', 'National Security', 'Oils', 'Online Systems', 'Persons', 'Pharmaceutical Preparations', 'Policies', 'Policy Making', 'Privacy', 'Private Sector', 'Problem Solving', 'Process', 'Property', 'Provider', 'Public Health', 'Public Sector', 'Publishing', 'Purpose', 'Records', 'Research', 'Research Methodology', 'Respondent', 'Sales', 'Services', 'Social Welfare', 'Societies', 'Solutions', 'Structure', 'Support of Research', 'System', 'Techniques', 'Time', 'United States National Institutes of Health', 'base', 'computer framework', 'data mining', 'desire', 'innovation', 'interest', 'prevent', 'tool']",NIMH,"OPTTEK SYSTEMS, INC.",R43,2008,99843,-0.003769909722569461
"Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission    DESCRIPTION (provided by applicant): Chagas disease, caused by the unicellular parasite Trypanosoma cruzi, is a preventable disease. Nevertheless more people in the Americas die from Chagas disease than any other parasitic infection. This application will provide tutorial-based training to the applicant in mathematical modeling and modern computer science at the University of Pennsylvania and the Bloomberg School of Public Health of Johns Hopkins University. Training will be geared towards developing the skills necessary to address complex problems hindering the control of Chagas disease. The long term goal of the research is to integrate diagnostic testing of children at high risk of T. cruzi infection into control programs focused mainly on vector elimination, and to better guide vector elimination campaigns. In addition to tutorial-based training in the United States, entomologic and clinical research will be conducted in Peru in coordination with a research grant awarded by   the Fogarty International Center to Dr. Cesar Naquira and collaborators. In Arequipa, a city of nearly one million inhabitants in southern Peru, T. cruzi and Chagas disease have become urban problems. A quiet epidemic of Chagas disease infection is progressing across the city. The specific aims of this application are: 1) To develop techniques to understand and control T. cruzi transmission in epidemic situations, and, 2. To optimize the spatio-temporal ordering of the Chagas disease vector control   campaign in Arequipa, and to elucidate best-practice strategies for vector control elsewhere. In developing these mathematical techniques the candidate will receive quantitative training that will complement his previous work on infectious disease and allow him to develop into a well-rounded independent investigator. RELEVANCE: Integrating diagnosis of children for T cruzi infection into Chagas control campaigns has the potential to greatly reduce morbidity and mortality due to Chagas disease in Peru and elsewhere. Optimizing the spatio-temporal order of vector control campaigns can increase the probability that these eliminate future transmission of T cruzi.          n/a",Mathematical Techniques for Control of Epidemic Trypanosoma cruzi Transmission,7510774,K01AI079162,"['Address', 'Algorithms', 'Americas', 'Area', 'Award', 'Brazil', 'Capital', 'Chagas Disease', 'Child', 'Chile', 'Cities', 'Clinical Research', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Vectors', 'Ecology', 'Empirical Research', 'Epidemic', 'Future', 'Goals', 'Grant', 'Health', 'Housing', 'Infection', 'Infection Control', 'Insect Vectors', 'Insecticides', 'Knowledge', 'Machine Learning', 'Mentors', 'Modeling', 'Morbidity - disease rate', 'Parasites', 'Parasitic Diseases', 'Parasitic infection', 'Patients', 'Pattern', 'Pennsylvania', 'Peru', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Protocols documentation', 'Public Health', 'Public Health Schools', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Residual state', 'Retinal Cone', 'Risk', 'Running', 'Rural Community', 'Solutions', 'South America', 'Techniques', 'Training', 'Triatoma', 'Tropical Medicine', 'Trypanosoma cruzi', 'Tuberculosis', 'United States', 'Universities', 'Work', 'base', 'chemotherapy', 'computer science', 'disease transmission', 'disorder control', 'international center', 'killings', 'mathematical model', 'mortality', 'novel', 'programs', 'pyrethroid', 'rural area', 'skills', 'success', 'theories', 'transmission process', 'urban area', 'vector', 'vector control']",NIAID,UNIVERSITY OF PENNSYLVANIA,K01,2008,94531,-0.03583217578330079
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7404447,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2008,243004,-0.013921647802622495
"Integrated Analysis of Genome-Wide Array Data    DESCRIPTION (provided by applicant): This project will develop an integrated desktop application to combine data from expression array, RNA transcript array, proteomics, SNP array (for polymorphism an analysis, as well as LOH and copy number determination), methylation array, histone modification array, promoter array, and microRNA array and metabolomics technologies. Current approaches to analysis of individual `omic' technologies suffer from problems of fragmentation, that present an incomplete view of the workings of the cell. However, effective integration into a single analytic platform is non-trivial. There is a need for a consistent approach, infrastructure, and interface between array types, to maximize ease of use, while recognizing and accommodating the specific computational and statistical requirements, and biological context, of each array. A central challenge is the need to create and work with lists of genomic regions of interest (GROIs) for each sample: we propose three novel approaches to aid in identification of GROIs. These lists must then be integrated with rectangular (sample by feature) data arrays to facilitate statistical analysis. Integration between array types occurs at the computational level, through a unified software package, statistically, through tools that seek statistical relationships between features from different arrays, biologically, through use of annotations (particularly gene ontology, protein- protein and protein-DNA interactions, and pathway membership) that document functional relationships between features, and through genomic interactions that suggest relationships between features that map to the same regions of the genome. The end product will support analysis of each platform separately, with a comprehensive suite of data management, statistical and heuristic analytic tools and the means to place findings of interest into a meaningful biological context through cross-reference to extensive biobases. Beyond that, a range of methods - statistical, biological and genomic - will be available to explore interactions and associations between platforms. PDF created with PDF Factory trial version www.pdffactory.com. PUBLIC HEALTH RELEVANCE: While the large-scale array technologies have provided an unprecedented capability to model cellular processes, both in normal functioning and disease states, this capability is utterly dependent on the availability of complex data management, computational, statistical and informatic software tools.  The utility of the next generation of arrays - which focus on critical regulation and control functions of the cell - will be stymied by an initial lack of suitable bioinformatic tools.  This proposal initiates an accelerated development of an integrated software package intended to empower biologists in the application and analysis of these powerful new technologies, with broadly reaching impact at all levels of biological and clinical research, and across every discipline.          n/a",Integrated Analysis of Genome-Wide Array Data,7538527,R43HG004677,"['Algorithms', 'Alternative Splicing', 'Binding', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Bite', 'Cell physiology', 'Cells', 'Classification', 'Clinical Data', 'Clinical Research', 'Complex', 'Computer software', 'DNA copy number', 'DNA-Protein Interaction', 'Data', 'Data Linkages', 'Development', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'GDF15 gene', 'Gene Expression', 'Genes', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Heating', 'Histones', 'Imagery', 'Individual', 'Informatics', 'Internet', 'Joints', 'Link', 'Loss of Heterozygosity', 'Machine Learning', 'Maps', 'Methylation', 'MicroRNAs', 'Modeling', 'Modification', 'Numbers', 'Ontology', 'PLAB Protein', 'Pathway interactions', 'Phase', 'Polymorphism Analysis', 'Process', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'RNA', 'Range', 'Regulation', 'Research Infrastructure', 'Resources', 'Sampling', 'Software Tools', 'Sorting - Cell Movement', 'Statistical Methods', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Text', 'Transcript', 'Work', 'base', 'data management', 'genome-wide analysis', 'heuristics', 'high throughput technology', 'interest', 'metabolomics', 'new technology', 'next generation', 'novel', 'novel strategies', 'prognostic', 'promoter', 'tool', 'tool development']",NHGRI,EPICENTER SOFTWARE,R43,2008,157474,-0.018507595625984974
"Statistical Model Building for High Dimensional Biomedical Data    DESCRIPTION (provided by applicant):  Typical of current large-scale biomedical data is the feature of small number of observed samples and the widely observed sample heterogeneity. Identifying differentially expressed genes related to the sample phenotye (e.g., cancer disease development) and predicting sample phenotype based on the gene expressions are some central research questions in the microarray data analysis. Most existing statistical methods have ignored sample heterogeneity and thus loss power.       This project proposes to develop novel statistical methods that explicitly address the small sample size and sampe heterogeneity issues, and can be applied very generally. The usefulness of these methods will be shown with the large-scale biomedical data originating from the lung and kidney transplant research projects. The transplant projects aimed to improve the molecular diagnosis and therapy of lung/kidney allograft rejection by identifying molecular biomarkers to predict the allograft rejection for critical early treatment and rapid, noninvasive, and economical testing.       The specific aims are 1) Develop novel statistical methods for differential gene expression detection that explicitly model sample heterogeneity. 2) Develop novel statistical methods for classifying high-dimensional biomedical data and incorporating sample heterogeneity. 3) Develop novel statistical methods for jointly analyzing a set of genes (e.g., genes in a pathway). 4) Use the developed models and methods to answer research questions relevant to public health in the lung and kidney transplant projects; and implement and validate the proposed methods in user-friendly and well-documented software, and distribute them to the scientific community at no charge.       It is very important to identify new biomarkers of allograft rejection in lung and kidney transplant recipients. The rapid and reliable detection and prediction of rejection in easily obtainable body fluids may allow the rapid advancement of clinical interventional trials. We propose to study novel methods for analyzing the large-scale biomedical data to realize their full potential of molecular diagnosis and prognosis of transplant rejection prediction for critical early treatment.          n/a",Statistical Model Building for High Dimensional Biomedical Data,7386333,R01GM083345,"['Address', 'Adopted', 'Algorithms', 'Allografting', 'Biological Markers', 'Body Fluids', 'Cations', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Collection', 'Communities', 'Computer software', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Disease', 'Early treatment', 'Effectiveness', 'Experimental Designs', 'Gene Expression', 'Genes', 'Genomics', 'Graft Rejection', 'Heterogeneity', 'Individual', 'Internet', 'Joints', 'Kidney Transplantation', 'Least-Squares Analysis', 'Literature', 'Lung', 'Lung diseases', 'Machine Learning', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Methods', 'Minnesota', 'Modeling', 'Molecular', 'Molecular Diagnosis', 'None or Not Applicable', 'Numbers', 'Oncogene Activation', 'Outcome', 'Outcome Measure', 'Pathway interactions', 'Patients', 'Personal Satisfaction', 'Phenotype', 'Principal Component Analysis', 'Probability', 'Procedures', 'Public Health', 'Purpose', 'Relative (related person)', 'Research', 'Research Project Grants', 'Research Proposals', 'Resources', 'Sample Size', 'Sampling', 'Silicon Dioxide', 'Statistical Methods', 'Statistical Models', 'Technology', 'Testing', 'Tissue-Specific Gene Expression', 'Transplant Recipients', 'Transplantation', 'Universities', 'Ursidae Family', 'Work', 'base', 'cancer microarray', 'cancer type', 'design', 'desire', 'improved', 'interest', 'kidney allograft', 'method development', 'novel', 'outcome forecast', 'predictive modeling', 'simulation', 'software development', 'sound', 'theories', 'user friendly software', 'user-friendly']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,255036,-0.012710273507835443
"Clinical Cytometry Analysis Software with Automated Gating    DESCRIPTION (provided by applicant): The proposed Clinical Cytometry Analysis Software Project described in this grant application is designed to create a new, more efficient and effective way of analyzing cells for the presence of cancer, HIV/AIDS and other disease, using a fully automated software system. Using modern data mining techniques (pattern recognition, feature recognition, image analysis) we will design software which will analyze data (the cell samples from patients) at a much faster rate and with fewer false positives and negatives than the manual method now in use. Objectives: Assemble and validate algorithms in software that can automatically classify regions of interest in flow cytometry data. We will demonstrate that the particular populations required by our use cases can be validly, rigorously and repeatably identified automatically. Develop and validate graphical and statistical results that satisfy FDA requirements for medical device software, simplify regulatory compliance by the clinical user, and automatically deliver analysis results to diagnostic expert systems and/or LIMS systems. Satisfy the  translational medicine  goals outlined in the NIH Roadmap. This software will bring the clinician streamlined testing currently only available in research labs. Methods: Four use cases have been selected, one employing synthetic data and three clinical data; Leukemia/Lymphoma test, Analysis of longitudinal Graft vs. Host Disease (GvHD) in bone marrow transplant specimens for predictive markers and HIV/AIDS - Gag-specific T cell cytokine response profile assay. For each we have access to a substantial body of existing data, analyzed by experts. Beginning with the autogating routines in our own FlowJo software, we will test and expand the application of Magnetic gating, Probability Clustering, Subtractive Cluster Analysis, Artificial Neural and Support Vector Machines (SVM). Using a sampling of human operators to establish a control range, we will test each of these five techniques against the four use cases in cooperation with our collaborators. Events in the manually classified samples are given a weighted score based on the frequency with which they are included by all the operators. A single operator's score or the gating algorithm's score is compared with the cumulative score of the expert group and a match rating is computed. Additional validation techniques include combinatory validation on internal measures with respect to Pareto optimality, and Predictive Power/Stability self consistency checks using resampled or perturbed data measured with external indices such as the adjusted Rand index and the Variation of Information index.  PUBLIC HEALTH RELEVANCE: By eliminating the operator's time, we estimate that the cost of clinical flow cytometry analysis can be reduced to half the current figure while delivering the results much faster. By eliminating the subjectivity and human error of manually created regions and reducing the range of variability of the so created, there would result fewer false positives and false negatives, improving the clinical outcome for those patients needing therapy but undetected by current methods. An order of magnitude increase in speed means faster therapeutic intervention.  A less expensive test improves outcome by making the test accessible to more patients.          n/a",Clinical Cytometry Analysis Software with Automated Gating,7482923,R43RR024094,"['AIDS/HIV problem', 'Algorithms', 'Applications Grants', 'B-Lymphocytes', 'Biological Assay', 'Biological Neural Networks', 'Bone Marrow Transplantation', 'Cells', 'Characteristics', 'Class', 'Classification', 'Clinical', 'Clinical Data', 'Cluster Analysis', 'Complex', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Cytometry', 'Data', 'Data Analyses', 'Data Files', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Educational process of instructing', 'Environment', 'Evaluation', 'Event', 'Expert Systems', 'Facility Construction Funding Category', 'Flow Cytometry', 'Frequencies', 'Funding', 'Future', 'Gagging', 'Generations', 'Goals', 'Grant', 'Graph', 'Human', 'Image Analysis', 'Individual', 'Instruction', 'Knowledge', 'Legal patent', 'Life Cycle Stages', 'Machine Learning', 'Magnetism', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Medical', 'Medical Device', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Noise', 'Numbers', 'Outcome', 'Output', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Phase', 'Population', 'Probability', 'Process', 'Public Health', 'Publishing', 'Range', 'Rate', 'Regulation', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Scientist', 'Score', 'Software Design', 'Software Engineering', 'Software Tools', 'Solutions', 'Specific qualifier value', 'Specimen', 'Speed', 'Standards of Weights and Measures', 'Structure', 'System', 'T-Lymphocyte', 'Target Populations', 'Techniques', 'Testing', 'Therapeutic Intervention', 'Time', 'Training', 'Tube', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Voting', 'Weight', 'base', 'clinical Diagnosis', 'commercialization', 'cost', 'cytokine', 'data mining', 'design', 'improved', 'indexing', 'innovation', 'interest', 'leukemia/lymphoma', 'novel', 'predictive modeling', 'relating to nervous system', 'research study', 'response', 'software systems', 'statistics', 'tool', 'translational medicine']",NCRR,"TREE STAR, INC.",R43,2008,100854,-0.023622903021202308
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7433931,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2008,322087,-0.007598835962337546
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7346975,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Endopeptidases', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Numbers', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein structure', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,R01,2008,280910,-0.022524662036093726
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.       n/a",Bioconductor: an open computing resource for genomics,7495201,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2008,805222,-0.004093148095818884
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7440169,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Class', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Numbers', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'Spectrum Analysis', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2008,535031,-0.0025633403762972385
"Interrogation of systems level mechanisms controlling DNA repair processes    DESCRIPTION (provided by applicant): A comprehensive model of a complete biological system will enable prediction of phenotypic outcomes in face of novel genetic and environmental perturbations. Such predictive power for cellular systems, such as DMA repair, will have tremendous impacts on early detection and diagnosis of genetic disorders and, ultimately in designing preventive and/or curative treatments. However, the full understanding of eukaryotic DMA repair systems presents a daunting challenge due to their enormous complexity, especially given that systems approaches are not yet sufficiently mature to mathematically model such complex processes. At a molecular level, eukaryotic genetic information processing is mirrored in a simplified manner by their evolutionary ancestors, the archaea. In Halobacterium NRC-1 we have a simple yet powerful model system of -2400 genes in which the underlying generalized principles of a systems approach can be delineated. This halophilic archaeon routinely mitigates damage from high salinity, UV radiation and desiccation-rehydration cycles. We will use this model system to address the hypothesis that the decision making process governing cell fate after exposure to DMA damaging events such as UV irradiation is mediated by robust gene regulatory networks that simultaneously process information on spectral characteristics of the impinging radiation, the nature and extent of DMA damage and the potential preparative role of cellular entrainment to diurnal cycles. This hypothesis is motivated by two main observations: a. in systems analysis of UV response, although extraordinarily resistant, not all expected repair genes in this microbe responded to irradiation or damage; b. many of these damage-unresponsive repair genes did, however, respond to day-night entrainment. At a fundamental level this suggested that the resident state of the cell at time of irradiation may influence the type of response elicited. A systems approach is ideally suited to delinate the networks underlying this decision making process. Specifically, we will measure dynamic global changes (degree of damage, mRNA /protein levels, protein-protein and protein-DNA interactions) in wild type and mutant strains (defective in sensors, signal transducers, regulators and repair proteins) subjected to combinatorial changes in incident radiation, type of damage inflicted and resident state of the cell at the time of radiation. Through an integrated analysis of these diverse systems level data we will statistically learn perturbation-induced rewiring of a gene regulatory network that processes environmental perturbations (input) into phenotype (output), i.e. a predictive model for regulatory mechanisms for repair. This basic model will serve as a template for designing systems approaches to model higher complexities of eukaryotic repair processes.           n/a",Interrogation of systems level mechanisms controlling DNA repair processes,7473118,R01GM077398,"['Address', 'Algorithms', 'Animal Model', 'Archaea', 'Architecture', 'Behavior', 'Biological', 'Biological Assay', 'Biological Models', 'Biology', 'Candidate Disease Gene', 'Cell Cycle Progression', 'Cell Size', 'Cell Survival', 'Cell physiology', 'Cells', 'Characteristics', 'Chromosomes, Human, Pair 4', 'Circadian Rhythms', 'Complex', 'Computer information processing', 'Condition', 'Coupled', 'DNA', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'DNA biosynthesis', 'DNA lesion', 'DNA repair protein', 'DNA-Protein Interaction', 'Data', 'Decision Making', 'Desiccation', 'Disease', 'Early Diagnosis', 'Elements', 'Engineering', 'Environment', 'Epitopes', 'Equilibrium', 'Event', 'Exogenous Factors', 'Exposure to', 'Face', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Information Processing Pathway', 'Genetic Transcription', 'Genome', 'Global Change', 'Halobacterium', 'Heart', 'Hour', 'Human', 'Lead', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Messenger RNA', 'Metabolism', 'Microbe', 'Modeling', 'Molecular', 'Mutation', 'Nature', 'Nucleotide Excision Repair', 'Numbers', 'Operative Surgical Procedures', 'Organism', 'Outcome', 'Output', 'Oxidative Stress', 'Pathway interactions', 'Phenotype', 'Physiological Processes', 'Physiology', 'Preventive', 'Process', 'Protein Dynamics', 'Proteins', 'Radiation', 'Readiness', 'Regulation', 'Regulator Genes', 'Regulatory Element', 'Rehydrations', 'Resistance', 'Resources', 'Role', 'Signal Transduction', 'Skin Cancer', 'Source', 'Specific qualifier value', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Tertiary Protein Structure', 'Time', 'Transcript', 'Transducers', 'Translations', 'UV response', 'Ultraviolet Rays', 'biological research', 'cell type', 'combinatorial', 'coping', 'cryptochrome', 'day', 'design', 'gene repair', 'genetic disorder diagnosis', 'halobacteria', 'interest', 'irradiation', 'knockout gene', 'mathematical model', 'mutant', 'novel', 'predictive modeling', 'repaired', 'response', 'segregation', 'sensor', 'technology development', 'trait', 'ultraviolet damage', 'ultraviolet irradiation']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,R01,2008,335920,-0.035276588285786344
"Causal Discovery Algorithms for Translational Research with High-Throughput Data Project Summary Causal Discovery Algorithms for Translational Research with High-Throughput Data The long-term goal of this project is to provide to the biomedical community next-generation causal algorithms to facilitate discovery of disease molecular pathways and causative as well as predictive biomarkers and molecular signatures from high-throughput data. Such knowledge and methods are necessary toward earlier and more accurate diagnosis and prognosis, personalized medicine, and rational drug design. If successful, the proposed research will have significant and wide methodological and practical implications spanning several areas of biomedicine with a primary focus and immediate benefits in high-throughput diagnostics and personalized medicine. It will provide significantly improved computational methods and deeper theoretical understanding related to producing molecular signatures and understanding mechanisms of disease and concomitant leads for new drugs. It will provide evidence about applicability of novel causal methods in other types of data. It will generate insights in specific pathways of lung cancer in humans. It will deepen our understanding and solutions to the Rashomon effect in ¿omics¿ data. The proposed research will also shed light on the operational value of the stability heuristic. Finally the research will engage the international research community to address open computational causal discovery problems relevant to high-throughput and other biomedical data. ¿ Aim 1. Evaluate and characterize several novel causal algorithms for biomarker selection, molecular signature creation and reverse network engineering using real, simulated, resimulated, and experimental datasets. Study generality of the methods by means of applicability to non-¿omics¿ datasets. ¿ Aim 2. Evaluate and characterize, novel and state of the art causal algorithms against state-of-the-art non-causal and quasi-causal algorithms. ¿ Aim 3. Systematically investigate the Rashomon effect as it applies to biomarker and signature multiplicity. ¿ Aim 4. Systematically investigate the utility of applying the stability heuristic for causal discovery. ¿ Aim 5. Derive novel biomarkers, pathways and hypotheses for lung cancer. ¿ Aim 6. Induce novel solutions through an international causal discovery competition. ¿ Aim 7. Disseminate findings. n/a",Causal Discovery Algorithms for Translational Research with High-Throughput Data,7643514,R56LM007948,"['AKT1 gene', 'AKT2 gene', 'AKT3 gene', 'Address', 'Affect', 'Algorithms', 'Area', 'Arts', 'Benchmarking', 'Bioinformatics', 'Biologic Characteristic', 'Biological Markers', 'Biology', 'Biometry', 'Book Chapters', 'Books', 'Cancer cell line', 'Causations', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Communities', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Set', 'Depth', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Discipline', 'Disease', 'Drug Design', 'Educational process of instructing', 'Educational workshop', 'Engineering', 'Ensure', 'Epidermal Growth Factor Receptor', 'European', 'Evaluation', 'Event', 'Excision', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Hereditary Disease', 'Home environment', 'Human', 'Human Cell Line', 'Inferior', 'Information Retrieval', 'Institution', 'International', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Malignant neoplasm of lung', 'Marker Discovery', 'Medicine', 'Methods', 'Modality', 'Molecular', 'Molecular Profiling', 'Neighborhoods', 'Noise', 'Numbers', 'Online Systems', 'Outcome', 'Output', 'Paper', 'Pathway interactions', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Process', 'Proteomics', 'Protocols documentation', 'Public Domains', 'Publishing', 'Quality Control', 'Random Allocation', 'Randomized', 'Rate', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Sample Size', 'Sampling', 'Schedule', 'Score', 'Services', 'Simulate', 'Solutions', 'Standards of Weights and Measures', 'Structure', 'Testing', 'Text', 'Thinking', 'Tissues', 'Translational Research', 'Variant', 'Work', 'base', 'c-erbB-1 Proto-Oncogenes', 'clinically relevant', 'computer based statistical methods', 'computer science', 'contextual factors', 'coping', 'data mining', 'design', 'drug development', 'heuristics', 'human data', 'human tissue', 'improved', 'innovation', 'insight', 'journal article', 'member', 'new technology', 'next generation', 'novel', 'novel diagnostics', 'outcome forecast', 'reconstruction', 'research study', 'software systems', 'symposium', 'theories', 'tool']",NLM,VANDERBILT UNIVERSITY,R56,2008,4434,-0.022225994119487148
"System-wide Study of Transcriptional Control of Metabolism    DESCRIPTION (provided by applicant): This proposal is in response to the NIH call for Exploratory Collaborations with National Centers for Biomedical Computing, PAR-06-223, and it will involve a collaboration between Columbia University's MAGNet NCBC and a team at Los Alamos National Laboratory. The aim of the proposal is a system-wide tudy of integrated transcriptional and metabolic networks in Eschericia coli K-12 strain, aiming at a similar analysis of a pathogen, Bacillus anthracis, at a later date. LANL hosts an experimental research program on bacterial metabolomics. Metabolites serve several functions. The most common one is being the precursors to various cellular components. They are also regulators of cellular functions by means of modulating metabolic reactions or binding to transcription factors and subsequently regulating gene expression. Conversely, the genes regulated by a transcription factor often encode enzymes, modulating the speed of metabolic reactions. Thus, to understand and ultimately predict the cellular response to an environmental change of interest (e.g., pathogen entry into its host environment), we must integrate the analysis of the transcriptome and metabolome. To address this need, we will work with the laboratories of Pat Unkefer and John Dunbar, which will produce data sets of about 300 joint metabolic/transcriptional profiles of E.coli under different steady-state growth conditions. The resources of MAGnet NCBC, specifically the algorithms within the geWorkbench bioinformatics platform produced by the center, will be leveraged to reconstruct cellular networks. Specifically, we expect that ARACNE, an algorithm originally developed by MAGNet for high-fidelity analysis of transcriptional networks in mammalian cells, is well positioned for reconstruction of metabolic networks from high throughput system-wide metabolic activity data, provide that appropriate modifications to deal with the specifics of the metabolic data are made. We will also adapt the algorithm to discover modulated interactions, that is, metabolic interactions that are conditional on the activity of a modulator gene (enzyme), or transcriptional interactions that require the presence of a metabolite to proceed. Such integrated genome/metabolome analysis has not been attempted yet. It will be a giant leap towards a complete understanding of cellular processes in an important organism. Because of the comparatively small size of bacterial genomes and metabolomes, it will be possible to perform system-wide analyses of interactions for the entire integrated genome and metabolome. While important in its own right, especially in view of the pathogenic nature of B. anthracis, this research would also represent an important test bed for a subsequent study of metabolic diseases in higher animals, including humans.           n/a",System-wide Study of Transcriptional Control of Metabolism,7387471,R21GM080216,"['Accounting', 'Address', 'Affect', 'Affinity', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Architecture', 'Autistic Disorder', 'Automobile Driving', 'B-Cell Lymphomas', 'B-Lymphocytes', 'Bacillus anthracis', 'Bacterial Genome', 'Beds', 'Benchmarking', 'Binding', 'Biochemical', 'Biochemical Pathway', 'Biochemical Reaction', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Cadherins', 'Cell Adhesion Molecules', 'Cell Separation', 'Cell physiology', 'Cell-Cell Adhesion', 'Cells', 'Chemicals', 'Code', 'Collaborations', 'Commit', 'Communities', 'Complex', 'Computational Biology', 'Computational Technique', 'Computational algorithm', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Computers', 'Concentration measurement', 'Condition', 'Crystallography', 'DNA Binding', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Discipline', 'Disease', 'Dissection', 'Documentation', 'Drug Formulations', 'Drug Interactions', 'Educational workshop', 'Electrical Engineering', 'Engineering', 'Ensure', 'Environment', 'Enzyme Gene', 'Enzymes', 'Escherichia coli', 'Family', 'Gene Expression', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Health Sciences', 'Human', 'Imagery', 'Immune system', 'In Vitro', 'Informatics', 'Institutes', 'Institution', 'Internet', 'Java', 'Joints', 'Knowledge', 'Laboratories', 'Language', 'Life', 'Literature', 'Machine Learning', 'Mails', 'Malignant Neoplasms', 'Mammalian Cell', 'Manuals', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Mediating', 'Metabolic', 'Metabolic Control', 'Metabolic Diseases', 'Metabolism', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Multimedia', 'Nature', 'Noise', 'Online Systems', 'Ontology', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Personal Satisfaction', 'Phenotype', 'Phosphotransferases', 'Physics', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Prokaryotic Cells', 'Property', 'Protein Analysis', 'Protein Family', 'Proteins', 'Proteome', 'Publishing', 'RNA', 'Range', 'Rate', 'Reaction', 'Research', 'Research Activity', 'Research Personnel', 'Research Proposals', 'Resolution', 'Resources', 'Sampling', 'Semantics', 'Sequence Analysis', 'Services', 'Signal Transduction', 'Site', 'Software Engineering', 'Solutions', 'Source Code', 'Specific qualifier value', 'Specificity', 'Speed', 'Structural Protein', 'Structure', 'Structure of germinal center of lymph node', 'Students', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Transcriptional Regulation', 'United States National Institutes of Health', 'Universities', 'Validation', 'Virulence', 'Visual', 'Work', 'base', 'biocomputing', 'biomedical informatics', 'computer framework', 'computer studies', 'computerized tools', 'concept', 'data acquisition', 'data mining', 'data modeling', 'design', 'environmental change', 'experience', 'forging', 'hazard', 'improved', 'innovation', 'interest', 'interoperability', 'knowledge base', 'member', 'metabolomics', 'microbial', 'multidisciplinary', 'nervous system disorder', 'novel', 'pathogen', 'pathogenic bacteria', 'professor', 'programs', 'protein protein interaction', 'reconstruction', 'research study', 'response', 'simulation', 'size', 'software development', 'text searching', 'tool', 'transcription factor']",NIGMS,LOS ALAMOS NAT SECTY-LOS ALAMOS NAT LAB,R21,2008,220227,-0.026570692119768816
"Systems Biology of Plasmodium falciparum: Building and Exploring Network Models    DESCRIPTION (provided by applicant):  There is an urgent need for novel anti-malaria interventions, due to the growing resistance of Plasmodium parasites to available drugs. The long-term objective of our research is to identify potential drug or vaccine targets using an efficient and cost-effective data-mining approach. This proposed study is aimed at the identification of biological networks as the first step toward a systems-level understanding of parasite biology. Compared to functional genomics, which requires a priori information about the identity or function of targets, a systems-level understanding of parasite biology will allow us to identify targets based on their key roles in networks. Specific aim 1 focuses on the harvest of data relevant to three gene families: the proteases, transporters, and transcription factors that are essential components in cellular networks for parasite growth and infection. Our project will encompass genomic data, expression data, and data derived from other high throughput experiments and bioinformatic and statistical analyses. Specific aim 2 focuses on the integration and encapsulation of these data in the form of network models. The models will be inferred using probabilistic graphical methods drawn from the field of engineering. Specific aim 3 focuses on the exploration of these models, particularly in the light of data from wet lab gene mapping and drug resistance experiments. Our ""biological maps"" will enable the visualization of evolutionary forces driving changes in the parasite's phenotype and should allow us to identify network design weaknesses in the parasite--potential vulnerabilities that could result in new malarial control strategies.           n/a",Systems Biology of Plasmodium falciparum: Building and Exploring Network Models,7483692,SC1AI080579,"['Animal Model', 'Antimalarials', 'Area', 'Artificial Intelligence', 'Be++ element', 'Beryllium', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle Regulation', 'Cell physiology', 'Chromosome Mapping', 'Complex', 'Data', 'Data Set', 'Development', 'Digestion', 'Discipline', 'Drug resistance', 'Electrical Engineering', 'Elements', 'Endopeptidases', 'Engineering', 'Family', 'Gene Family', 'Gene Proteins', 'Genes', 'Genomics', 'Growth', 'Harvest', 'Hemoglobin', 'Imagery', 'Immune', 'Infection', 'Intervention', 'Life Cycle Stages', 'Light', 'Location', 'Malaria', 'Maps', 'Mediating', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Open Reading Frames', 'Parasites', 'Pattern', 'Peptide Hydrolases', 'Pharmaceutical Preparations', 'Phenotype', 'Plant Roots', 'Plasmodium', 'Plasmodium falciparum', 'Prevalence', 'Process', 'Property', 'Proteins', 'Proteomics', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Rupture', 'Signal Transduction', 'Specialist', 'Staging', 'Statistical Models', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Transcription Factor 3', 'Vaccines', 'Work', 'base', 'comparative', 'computerized data processing', 'cost', 'data integration', 'data mining', 'design', 'driving force', 'experience', 'functional genomics', 'insight', 'member', 'mortality', 'network models', 'novel', 'pressure', 'programs', 'prototype', 'quantum', 'research study', 'response', 'stem', 'theories', 'trafficking', 'transcription factor', 'transcriptomics']",NIAID,UNIVERSITY OF TEXAS SAN ANTONIO,SC1,2008,242920,-0.04194508676091908
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7458835,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,503603,0.016653967186893125
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7688793,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2008,58299,0.016653967186893125
"MBRS IMSD Program at the University of Kansas    DESCRIPTION (provided by applicant):  Within the three short years since the University of Kansas (KU) IMSD program began, twelve minority students, including six American Indians, will have applied to graduate school. This is a continuation proposal to allow us to build on these successes. KU IMSD works in concert with other NIH funded programs (Bridge, RISE, IRACDA) and takes full advantage of the juxtaposition of a Research I Institution (KU) and one of the largest tribal colleges (Haskell Indian Nations University). KU IMSD consists of four components: 1) providing research experiences to American Indian and other minority students, with a particular a focus on recruiting Haskell students from the Bridge or RISE program; 2) enhancing and modifying the curriculum; 3) offering an interdisciplinary seminar series and 4) providing financial aid and mentoring. The undergraduate research experience takes a broad, interdisciplinary approach to placing students into one of 77 possible KU labs and includes opportunities for students to share their research results at local and national meetings. The program will also support several American Indian graduate students who have obtained BA's from Haskell. IMSD supported curricular enhancements that have shown remarkable results over the past three years will be continued for gatekeeper courses in biology, chemistry and math. In biology alone, the average grade point average of American Indian students who completed the introductory biology course has increased from 0.86 to 2.94 over the past six years. An integrative seminar series will bring together students from the IMSD, Bridge, and RISE programs to foster community and learning. Support from KU IMSD for undergraduate researchers will be a cornerstone in providing financial support along with KU scholarships targeted for American Indian students. Mentoring will be provided from faculty (research advisors), the IMSD Program Coordinator (individual and group meetings) and peers (other IMSD students). Evaluation and tracking procedures will allow for regular adjustment of activities during the course of the program and assessment of whether goals have been met. If funded for another four years, the KU/Haskell collaboration provides the opportunity to significantly impact the number of American Indian scientists in this country.         n/a",MBRS IMSD Program at the University of Kansas,7389728,R25GM062232,"['Address', 'Administrative Personnel', 'Administrator', 'Adopted', 'Advertising', 'Alaska', 'Algorithms', 'American Indians', 'Appendix', 'Area', 'Arrhythmia', 'Artificial Intelligence', 'Arts', 'Attention', 'Authorization documentation', 'Behavior', 'Behavioral Sciences', 'Biodiversity', 'Biogenesis', 'Bioinformatics', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Books', 'Bovine Spongiform Encephalopathy', 'Budgets', 'Calculi', 'Cells', 'Chaos Theory', 'Characteristics', 'Charge', 'Chemistry', 'Cities', 'Class', 'Collaborations', 'Communities', 'Computer Assisted', 'Computer software', 'Computers', 'Conservatism', 'Country', 'Coupled', 'DNA', 'DNA Structure', 'Daily', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Depth', 'Development', 'Discipline', 'Disease', 'Distant', 'Doctor of Philosophy', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Elements', 'Engineering', 'Enrollment', 'Environment', 'Equilibrium', 'Ethics', 'Evaluation', 'Event', 'Exercise', 'Exposure to', 'Faculty', 'Family', 'Feedback', 'Feeling', 'Fees', 'Film', 'Financial Support', 'Fostering', 'Foundations', 'Friends', 'Funding', 'Future', 'GYPA gene', 'Gatekeeping', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Glycophorin A', 'Goals', 'Grant', 'Graph', 'Group Meetings', 'Growth', 'Habits', 'Hand', 'Health', 'Health Sciences', 'Heart', 'Helix (Snails)', 'Home environment', 'Hour', 'Housing', 'Human', 'Human Resources', 'Humanities', 'Indigenous', 'Individual', 'Informatics', 'Information Systems', 'Institutes', 'Institution', 'Internet', 'Internships', 'Invasive', 'Journals', 'Judgment', 'Kansas', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Light', 'Link', 'Literature', 'Malignant Neoplasms', 'Mathematics', 'Medical', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Minority', 'Minority Groups', 'Modeling', 'Modems', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Monitor', 'Motivation', 'Museums', 'Music', 'Mutation', 'National Institute of General Medical Sciences', 'Natural History', 'Newsletter', 'Numbers', 'Oklahoma', 'Oral', 'Oral cavity', 'Organic Chemistry', 'Other Minority', 'Outcome', 'Pamphlets', 'Paper', 'Parents', 'Participant', 'Performance', 'Personality', 'Pharmaceutical Chemistry', 'Placement', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Poverty', 'Preparation', 'Principal Investigator', 'Problem Solving', 'Procedures', 'Process', 'Productivity', 'Program Development', 'Progress Reports', 'Proteins', 'Published Comment', 'Purpose', 'Qualifying', 'Quantum Mechanics', 'RNA', 'Race', 'Radio', 'Range', 'Reading', 'Recommendation', 'Records', 'Recruitment Activity', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Review Committee', 'Rewards', 'Schedule', 'Scholarship', 'Schools', 'Science', 'Scientist', 'Selection Criteria', 'Series', 'Services', 'Shock', 'Site', 'Slide', 'Snow', 'Social Functioning', 'Source', 'Sports', 'Standards of Weights and Measures', 'Students', 'Supervision', 'Support Groups', 'Surveys', 'Techniques', 'Technology', 'Text', 'Textbooks', 'Thinking', 'Time', 'Time Management', 'Today', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Update', 'Ursidae Family', 'Visit', 'Visualization software', 'Wages', 'Week', 'Work', 'Writing', 'abstracting', 'aging nutrition', 'analytical method', 'base', 'biomedical scientist', 'career', 'cohort', 'college', 'computer generated', 'computer science', 'concept', 'cost', 'court', 'data acquisition', 'day', 'digital', 'ear helix', 'expectation', 'experience', 'falls', 'follow-up', 'gene therapy', 'high school', 'i(19)', 'image visualization', 'improved', 'instructor', 'interdisciplinary approach', 'interest', 'lectures', 'member', 'novel', 'peer', 'posters', 'preference', 'professor', 'programs', 'protein structure', 'role model', 'satisfaction', 'size', 'skills', 'success', 'symposium', 'theories', 'tool', 'tribal college']",NIGMS,UNIVERSITY OF KANSAS LAWRENCE,R25,2008,373102,-0.008720535482448276
"Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi    DESCRIPTION (provided by applicant): The study of complex biological systems increasingly depends on vast amounts of dynamic information from diverse sources. The scientific analysis of the parasite Trypanosoma cruzi (T.cruzi), the principal causative agent of human Chagas disease, is the driving biological application of this proposal. Approximately 18 million people, predominantly in Latin America, are infected with the T.cruzi parasite. As many as 40 percent of these are predicted eventually to suffer from Chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. Research on T. cruzi is therefore an important human disease related effort. It has reached a critical juncture with the quantities of experimental data being generated by labs around the world, due in large part to the publication of the T.cruzi genome in 2005. Although this research has the potential to improve human health significantly, the data being generated exist in independent heterogeneous databases with poor integration and accessibility. The scientific objectives of this research proposal are to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for T.cruzi. This is in collaboration with the National Center for Biomedical Ontologies (NCBO) and will leverage its resources to achieve the objectives of this proposal as well as effectively to disseminate results to the broader life science community, including researchers in human pathogens. The PSE allows the dynamic integration of local and public data to answer biological questions at multiple levels of granularity. The PSE will utilize state-of- the-art semantic technologies for effective querying of multiple databases and, just as important, feature an intuitive and comprehensive set of interfaces for usability and easy adoption by biologists. Included in the multimodal datasets will be the genomic data and the associated bioinformatics predictions, functional information from metabolic pathways, experimental data from mass spectrometry and microarray experiments, and textual information from Pubmed. Researchers will be able to use and contribute to a rigorously curated T.cruzi knowledge base that will make it reusable and extensible. The resources developed as part of this proposal will be also useful to researchers in T.cruzi related kinetoplastids, Trypanosoma brucei and Leishmania major (among other pathogenic organisms), which use similar research protocols and face similar informatics challenges. PUBLIC HEALTH RELEVANCE: The scientific objective of this proposal is to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for Trypanosoma cruzi, a parasite that infects approximately 18 million people, predominantly in Latin America. As many as 40 percent of those infected are predicted to eventually suffer from Chagas disease, the leading cause of heart disease and sudden death in middle-aged adults in the region. Facilitating T.cruzi research through the PSE, with the aim of identifying vaccine, diagnostic, and therapeutic targets, is an important human disease related endeavor.          n/a",Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi,7428761,R01HL087795,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Animal Model', 'Anti-Retroviral Agents', 'Architecture', 'Archives', 'Area', 'Arts', 'Automobile Driving', 'Beds', 'Behavior', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Body of uterus', 'Buffaloes', 'California', 'Caring', 'Chagas Disease', 'Childhood', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Computers', 'Controlled Vocabulary', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Doctor of Public Health', 'Drops', 'Drosophila genus', 'Educational Activities', 'Educational workshop', 'Electronics', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evolution', 'Face', 'Feedback', 'Foundations', 'Future', 'Gene Mutation', 'Generations', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Geographic Locations', 'Goals', 'HIV', 'HIV Infections', 'Health', 'Heart Diseases', 'Homologous Gene', 'Human', 'Human Resources', 'Imagery', 'Immunologic Deficiency Syndromes', 'Immunology', 'Individual', 'Infection', 'Informatics', 'Information Management', 'Information Resources', 'Information Services', 'Information Technology', 'International', 'Internet', 'Interruption', 'Knowledge', 'Laboratories', 'Laboratory Organism', 'Language', 'Latin America', 'Lead', 'Learning', 'Leishmania major', 'Libraries', 'Link', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Medical Informatics', 'Medicine', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Mind', 'Mining', 'Modeling', 'Mutation', 'Natural Language Processing', 'Nature', 'Online Mendelian Inheritance In Man', 'Online Systems', 'Ontology', 'Operative Surgical Procedures', 'Oregon', 'Organism', 'Orthologous Gene', 'Outcome', 'Parasites', 'Pathogenesis', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Philosophy', 'Physiology', 'Prevention strategy', 'Principal Investigator', 'Problem Solving', 'Process', 'Proteomics', 'Protocols documentation', 'PubMed', 'Public Health', 'Publications', 'Publishing', 'Purpose', 'Randomized Clinical Trials', 'Range', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Resources', 'San Francisco', 'Science', 'Scientist', 'Semantics', 'Services', 'Site', 'Software Tools', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Structure', 'Study models', 'Sudden Death', 'Sum', 'System', 'TAF8 gene', 'Talents', 'Techniques', 'Technology', 'Terminology', 'Testing', 'Thinking', 'Training', 'Treatment Protocols', 'Trypanosoma brucei brucei', 'Trypanosoma cruzi', 'USA Georgia', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Vaccines', 'Vertical Disease Transmission', 'Victoria Austrailia', 'Virtual Library', 'Virus', 'Western Asia Georgia', 'Work', 'Zebrafish', 'abstracting', 'base', 'biocomputing', 'biomedical scientist', 'college', 'computer based Semantic Analysis', 'computer science', 'concept', 'data integration', 'design', 'desire', 'fundamental research', 'human disease', 'improved', 'indexing', 'innovative technologies', 'knowledge base', 'member', 'metabolomics', 'middle age', 'novel', 'novel strategies', 'open source', 'outreach', 'pandemic disease', 'pathogen', 'prevent', 'programs', 'protein protein interaction', 'repository', 'research and development', 'research study', 'syntax', 'theories', 'therapeutic target', 'tool', 'usability']",NHLBI,WRIGHT STATE UNIVERSITY,R01,2008,393930,-0.014800000627844152
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7222736,K22LM008389,"['Adverse event', 'Area', 'Arts', 'Attention', 'Biological Neural Networks', 'Caregivers', 'Chicago', 'Childhood', 'Classification', 'Clinical', 'Computer software', 'Critical Care', 'Critical Illness', 'Data', 'Data Analyses', 'Data Element', 'Data Set', 'Databases', 'Decision Trees', 'Detection', 'Disease', 'Early Intervention', 'Ensure', 'Etiology', 'Event', 'Excision', 'Foundations', 'Genomics', 'Heart Arrest', 'Hour', 'Laboratories', 'Length', 'Life', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Noise', 'Numbers', 'Patients', 'Pediatric Intensive Care Units', 'Performance', 'Physiologic Monitoring', 'Physiological', 'Population', 'Predictive Value', 'Procedures', 'Process', 'Purpose', 'Range', 'Receiver Operating Characteristics', 'Regression Analysis', 'Research Personnel', 'Sample Size', 'Sensitivity and Specificity', 'Series', 'Severity of illness', 'Social Sciences', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Validation', 'Work', 'abstracting', 'base', 'computer based statistical methods', 'data mining', 'data modeling', 'inclusion criteria', 'mortality', 'predictive modeling', 'programs', 'prospective', 'size', 'tool', 'trend', 'vector']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2007,135000,-0.022263823408531076
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7226297,R01HL065462,"['Accounting', 'Algorithms', 'Arts', 'Biological', 'Class', 'Classification', 'Communities', 'Computer software', 'Condition', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease regression', 'Documentation', 'Effectiveness', 'Employee Strikes', 'Environment', 'Etiology', 'Gene Expression', 'Gene Expression Profiling', 'Genes', 'Genomics', 'Goals', 'Heart failure', 'Knowledge', 'Least-Squares Analysis', 'Machine Learning', 'Mass Spectrum Analysis', 'Mechanics', 'Medical', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Profiling', 'Motivation', 'Pan Genus', 'Patients', 'Performance', 'Principal Investigator', 'Procedures', 'Property', 'Proteomics', 'Public Domains', 'Research Personnel', 'Sample Size', 'Sampling', 'Scheme', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Tissue-Specific Gene Expression', 'Weight', 'Work', 'base', 'forest', 'gene function', 'improved', 'novel', 'protein protein interaction', 'response', 'statistics', 'tool']",NHLBI,UNIVERSITY OF MINNESOTA,R01,2007,141753,-0.029992649983428057
"A RuleFit Product for Classification and Regression Prediction and data exploration are important aspects of modern commercial and scientific life. Regression methods predict dependent variables (e.g., tumor growth, severity of disease), while classification methods predict class membership (e.g., tumor or disease type). Both use a vector of independent variables to make the predictions. Because they are often superior predictors, can handle large numbers observations and large numbers of variables, can often yield insight into the data not provided by other methods, and because they can adapt to arbitrarily complex relationships, modern machine learning methods based on tree ensembles such as RANDOM FORESTS and MART have become leading modern analytical methods. Here we propose to commercially implement RULEFIT, a recent innovative method extending the RANDOM FORESTS and MART approaches, that shows strong evidence of being consistently more accurate than either ensemble. RULEFIT also includes groundbreaking new methods for variable selection in the face of huge numbers of predictors, and for identifying interactions, and ranking their importance. Optionally, RULEFIT extracts ""rules"" of special interest: succinct statements of conditions under which an outcome is especially likely or unlikely, or especially large or small. The primary output of RULEFIT is a numeric value reecting a prediction of the value of the dependent variable or the probability of a class membership. RULEFIT is likely to become a leading technique in the machine learning and statistics. It builds on RANDOM FORESTS and MART and includes all their useful benefits such as variable selection, data exploration, data reduction, outlier detection, and missing value imputation, while enhancing and extending these benefits.  COMMERCIAL POTENTIAL The market for advanced analytical tools has been growing strongly over the last decade and the growth shows no signs of diminishing. Modelers and data analysts in both university- based and commercial settings are increasingly aware of the power and value of new analytical tools derived from modern statistics and machine learning research. The increased accuracy of the new methods and the acceleration they provide to the analysis of complex data are fueling demand for this new technology. The advances embedded in the proposed product represent substantial improvements to existing technology and include methods to solve vexing problems in contemporary data analysis, and thus should find a welcoming market.  There are further reasons to forecast robust commercial potential for this product. The applicant organization has a strong track record in the industry and is widely recognized as a developer of high quality software. We have been working with consultant Friedman since 1990 and have gained exclusive rights to the proprietary sourcecode for a number of his innovations. These include CART, MARS, MART and PRIM. With the addition of RULEFIT and its associated sub-components, these products represent a unique collection of pedigreed tools. We have also forged a similar relationship with the (late) Leo Breiman and have the exclusive rights to commercialization of Breiman's Random Forests sourcecode. Our proposed package thus occupies a distinctive position in machine learning software which cannot be replicated by other vendors. Keywords: machine learning; classi?cation; prediction; supervised learning; variable importance; inter- action detection; Justi?cation Dr. Steinberg has extensive experience in software development for advanced statistical and machine learning methods, particularly in the area of classi?cation and regression trees, sur- vival analysis, adaptive modeling, RANDOM FORESTS and MART. He will oversee all aspects of the project. He will will work with Dr. Cardell, Professor Friedman, Mr. Colla, and with the Salford Systems software development engineer in creating and studying the software and methods used in this proposal. He will also be responsible for the architecture of the Phase I software. Professor Friedman and Dr. Cardell will provide technical support as follows: Dr. Fried- man is an expert on machine learning methods and is one of the developers of the RULEFIT technique. Regular consultation with him will be in this area. Dr. Cardell is an expert in asymptotic theory, and in the design of Monte Carlo and other tests for the evaluation of ma- chine learning algorithms. He also has extensive experience in machine learning, including adaptive modeling, neural networks, logistic regression, and classi?cation methods. He will review core algorithms of RULEFIT for possible improvement and extension and design the Monte Carlo tests. Mr. Colla has extensive experience in software development and with machine learning methods, including work on the commercial implementations of CART, MARS, RANDOM FORESTS, and MART. Working with Dr. Cardell, he will be responsible for much of the new software coding. 5 Project Description Page 7 Principal Investigator/Program Director (Last, first, middle): Steinberg, Dan Prediction models based upon classification and regression tree ensembles have become important in medical and other research. There are currently no commercial products available that implement the proposed RuleFit methodology. These methods have significant advantages over existing techniques, and will aid researchers in obtaining the best possible predictions.   n/a",A RuleFit Product for Classification and Regression,7268612,R43CA124294,"['Acceleration', 'Agreement', 'Algorithms', 'Architecture', 'Area', 'Beds', 'Build-it', 'Cations', 'Class', 'Classification', 'Code', 'Collection', 'Comparative Study', 'Complex', 'Computer software', 'Condition', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Detection', 'Disease', 'Disease regression', 'Engineering', 'Evaluation', 'Face', 'Generations', 'Growth', 'Industry', 'Information Systems', 'Investigation', 'Learning', 'Left', 'Life', 'Linear Models', 'Literature', 'Logistic Regressions', 'Machine Learning', 'Marketing', 'Measures', 'Medical', 'Medical Research', 'Methodology', 'Methods', 'Modeling', 'Neural Network Simulation', 'Numbers', 'Outcome', 'Output', 'Painless', 'Pattern', 'Performance', 'Phase', 'Plant Leaves', 'Play', 'Positioning Attribute', 'Principal Investigator', 'Probability', 'Rate', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Rights', 'Role', 'Sampling', 'Severity of illness', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Trees', 'Universities', 'Variant', 'Vendor', 'Work', 'analytical method', 'analytical tool', 'base', 'commercialization', 'data mining', 'data structure', 'design', 'evaluation/testing', 'experience', 'forest', 'forging', 'graphical user interface', 'innovation', 'insight', 'interest', 'loss of function', 'man', 'new technology', 'novel', 'professor', 'programs', 'prototype', 'relating to nervous system', 'research study', 'software development', 'statistics', 'theories', 'tool', 'tumor', 'tumor growth', 'vector']",NCI,SALFORD SYSTEMS,R43,2007,91700,-0.0006020178630525415
"Pathway Prediction and Assessment Integrating Multiple Evidence Types    DESCRIPTION (provided by applicant):    Metabolic pathway databases provide a biological framework in which relationships among an organism's genes may be revealed. This context can be exploited to boost the accuracy of genome annotation, to discover new targets for therapeutics, or to engineer metabolic pathways in bacteria to produce a historically expensive drug cheaply and quickly. But, knowledge of metabolism in ill-characterized species is limited and dependent on computational predictions of pathways. Our ultimate target is to develop methods for the prediction of novel metabolic pathways in any organism, coupled with robust assessment of the validity of any predicted pathway. We hypothesize that integrating evidence from multiple levels of an organism's metabolic network - from the fit of a pathway within the network to evolutionary relationships between pathways - will allow us to assess pathway validity and to predict novel metabolic pathways. We have successfully applied machine learning methods to the problem of identifying missing enzymes in metabolic pathways and believe similar methods will prove fruitful in this application. Our preliminary studies have identified several properties of predicted metabolic pathways that differ between sets of true positive pathway predictions (i.e., pathways known to occur in an organism) and sets of false positive pathway predictions. We will expand on these features and develop methods to address the following specific aims:      1) Identify features that are informative in distinguishing between correct and incorrect pathway predictions in computationally-generated pathway/genome databases based on predictions for highly-curated organisms (e.g., Escherichia coli and Arabidopsis thaliana).   2) Develop methods for computing the probability that a pathway is correctly predicted. Informative features identified in Specific Aim #1 will be integrated into a classifier that will compute the probability that a predicted pathway is correct given the associated evidence.   3) Extend the Pathologic program (the Pathway Tools algorithm used to infer the metabolic network of an organism) to predict alternate, previously unknown pathways in an organism. We will search the MetaCyc reaction space (comprising almost 6000 reactions) for novel subpathways, explicitly constraining our search using organism-specific evidence (i.e., homology, experimental evidence, etc.) at each step.          n/a",Pathway Prediction and Assessment Integrating Multiple Evidence Types,7301424,R01LM009651,"['Address', 'Algorithms', 'Antimalarials', 'Bacteria', 'Biochemical Pathway', 'Biological', 'Coupled', 'Data', 'Data Set', 'Databases', 'Development', 'Engineering', 'Enzymes', 'Escherichia', 'Escherichia coli', 'Future', 'Gene Expression', 'Genes', 'Genome', 'Gold', 'Government', 'Knowledge', 'Laboratory Research', 'Left', 'Machine Learning', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Molecular Profiling', 'Mouse-ear Cress', 'Organism', 'Pathologic', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Probability', 'Prodrugs', 'Property', 'Proteins', 'Reaction', 'Relative (related person)', 'Research', 'Research Institute', 'Research Personnel', 'Scientist', 'Score', 'Series', 'Software Tools', 'Standards of Weights and Measures', 'Techniques', 'Training', 'Validation', 'Yeasts', 'base', 'design', 'genome database', 'metabolomics', 'novel', 'programs', 'reconstruction', 'therapeutic target', 'tool']",NLM,SRI INTERNATIONAL,R01,2007,173307,-0.0168652105606876
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,7208003,K25AI064625,"['Address', 'Adverse event', 'Algorithms', 'Behavior', 'Biochemical', 'Biomedical Research', 'Bioterrorism', 'Cells', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Coupled', 'Coupling', 'Cytokine Network Pathway', 'Cytokine Signaling', 'Data', 'Engineering', 'Equation', 'Event', 'Feedback', 'Foundations', 'Future', 'Goals', 'Immune response', 'Immune system', 'Immunization', 'Immunologics', 'Immunology', 'Individual', 'Infectious Disease Immunology', 'Kinetics', 'Knowledge', 'Machine Learning', 'Mentors', 'Methods', 'Military Personnel', 'Modeling', 'Molecular', 'Nature', 'Organism', 'Pathogenesis', 'Patients', 'Pattern', 'Population', 'Proteins', 'Proteomics', 'Public Health', 'Research', 'Research Personnel', 'Risk', 'Role', 'Series', 'Serum', 'Signal Transduction', 'Signaling Protein', 'Simulate', 'Smallpox', 'Smallpox Vaccine', 'Software Tools', 'Systems Biology', 'Testing', 'Time', 'Today', 'Training Programs', 'Vaccination', 'Vaccine Research', 'Vaccines', 'Vent', 'Work', 'base', 'chemical reaction', 'cytokine', 'design', 'experience', 'interest', 'network models', 'novel', 'programs', 'protein expression', 'research study', 'response', 'simulation', 'software development', 'tool', 'volunteer']",NIAID,UNIVERSITY OF ALABAMA AT BIRMINGHAM,K25,2007,103762,-0.005309837165667322
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7246847,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2007,2183988,-0.0021871004202032216
"Computational Biology of Transcriptional Networks in Aging    DESCRIPTION (provided by applicant):   Funding is sought for a five year mentored training period at Brown University for Dr. Nicola Neretti to transition from physics to independent investigator in computational biology. The candidate has a Physics PhD from Brown University, and has been working in the field of signal processing and machine learning. During the past year he has been part of a collaborative project with Dr. John Sedivy (Department of Molecular and Cell Biology and Biochemistry) which resulted in a published paper about the analysis of gene expression array data to target c-Myc-activated genes with a correlation-based method. He has established other productive collaborations with members of the Center for Computational Molecular Biology (CCMB) at Brown University. The principal mentor will be Dr. Marc Tatar (Department of Ecology and Evolutionary Biology). The secondary mentors will be Dr. Charles Lawrence (Dep. Applied Mathematics) and Dr. John  Sedivy. The work plan for the five years is to split the training/research effort evenly between computation and biology. For the training requirements the candidate plans to attend courses and workshops in genetics, biochemistry, molecular biology, bioinformatics, and related fields. Dr. Neretti also plans to complete lab rotations in the laboratory of Dr. Marc Tatar and Dr. John Sedivy, to acquire first hand experience in generating the biological data he will later analyze. The main focus of the research effort will be to use microarray data in time course experiments with a high temporal resolution to elucidate the complex interactions among genes and develop novel analytic techniques in functional genomic. In particular, the candidate proposes to integrate the results of gene clustering/graph analysis (e.g. correlation and tagged correlation based clustering) obtained from the time course data with the information available in genetic/pathway databases relevant to the process of senescence. This will allow the evaluation of given hypotheses about functional relationships among genes and the identification of novel dependencies, which can then be directly tested via experiments in model systems of aging. By using this approach the candidate proposes to address key questions in aging research such as what transcriptional changes are under the control of a nutrient sensing system, the temporal and hierarchical relationship of these changes, the magnitude of change that is biologically relevant, and whether genes within a functional metabolic network are co-regulated at the transcriptional level.                   n/a",Computational Biology of Transcriptional Networks in Aging,7259194,K25AG028753,"['Address', 'Affect', 'Age', 'Aging', 'Algorithms', 'Animal Model', 'Biochemical Pathway', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biology', 'Caenorhabditis elegans', 'Cells', 'Cellular biology', 'Collaborations', 'Complex', 'Computational Biology', 'Computational Molecular Biology', 'Condition', 'Data', 'Databases', 'Dependency', 'Disease', 'Doctor of Philosophy', 'Drosophila genus', 'Ecology', 'Educational workshop', 'Evaluation', 'Five-Year Plans', 'Funding Applicant', 'Gene Cluster', 'Gene Expression', 'Gene Mutation', 'Genes', 'Genetic', 'Genetic Transcription', 'Genotype', 'Goals', 'Graph', 'Growth', 'Hand', 'Homologous Gene', 'Insulin', 'Insulin Receptor', 'Laboratories', 'Longevity', 'Machine Learning', 'Mammals', 'Mathematics', 'Mentors', 'Metabolic', 'Metabolism', 'Methods', 'Molecular', 'Molecular Analysis', 'Molecular Biology', 'Mutation', 'Nematoda', 'Nutrient', 'Organism', 'Paper', 'Pathway interactions', 'Physics', 'Play', 'Process', 'Publishing', 'Range', 'Regulatory Element', 'Reproduction', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Response Elements', 'Role', 'Rotation', 'Sampling', 'Series', 'Signal Transduction', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'base', 'c-myc Genes', 'computerized data processing', 'design', 'detection of nutrient', 'dietary restriction', 'experience', 'fly', 'functional genomics', 'improved', 'insulin signaling', 'member', 'novel', 'nutrition', 'receptor', 'research study', 'response', 'senescence']",NIA,BROWN UNIVERSITY,K25,2007,102389,-0.019097469841772314
"Experimental and Computational Studies of Concept Learning    DESCRIPTION (provided by applicant): This research is aimed at developing better understanding of how people bring their prior knowledge to the table when learning about new concepts. Both experimental studies and computational models of these processes will be used to further understanding of this fundamental aspect of human cognition. The proposal focuses on effects and interactions that show that memorized exemplars of a problem are involved with concept learning, on processes involved in unsupervised sorting without feedback, and on how these two processes interact with pre-existing concepts and relational knowledge. New computational models will incorporate exemplars and unsupervised learning into an existing model of knowledge and supervised learning, accounting for a variety of previously observed and newly predicted effects. Experiments involving human participants will investigate interactions of prior knowledge with frequency, exposure, and concept structure. Experiments are paired with the modeling so that new empirical discoveries will go hand-in-hand with theoretical development. If successful, this model will be the only one in the field that accounts for this range of phenomena, encompassing both statistical learning and use of prior knowledge in concept acquisition. Relevance to Public Health: Categorization and category learning are fundamental aspects of cognition, allowing people to intelligently respond to the world. As categorization can be impaired by neurological disorders such as Parkinson's disease, dementia, and amnesia, a rigorous understanding of the processes involved in normal populations aides the research and treatment of disorders in patients. This project will provide a detailed computational model of concept learning, which can then serve as a model to investigate what has gone wrong when the process is disrupted in clinical populations.           n/a",Experimental and Computational Studies of Concept Learning,7275769,F32MH076452,"['Accounting', 'Amnesia', 'Categories', 'Clinical', 'Cognition', 'Computer Simulation', 'Development', 'Disease', 'Feedback', 'Frequencies', 'Goals', 'Hand', 'Human', 'Individual', 'Intelligence', 'Intuition', 'Knowledge', 'Learning', 'Machine Learning', 'Modeling', 'Parkinson&apos', 's Dementia', 'Participant', 'Patients', 'Population', 'Process', 'Public Health', 'Range', 'Research', 'Role', 'Sorting - Cell Movement', 'Structure', 'Testing', 'Thinking', 'base', 'computer studies', 'concept', 'experience', 'insight', 'nervous system disorder', 'research study', 'satisfaction', 'theories']",NIMH,NEW YORK UNIVERSITY,F32,2007,51278,-0.010386559178528968
"Efficient software and algorithms for analyzing markers data on general pedigree    DESCRIPTION (provided by applicant): Our long-term objective is to develop an efficient, extensible, modular, and accessible software toolbox that facilitates statistical methods for analyzing complex pedigrees. The toolbox will consist of novel algorithms that extend state of the art algorithms from graph theory, statistics, artificial intelligence, and genetics. This tool will enhance capabilities to analyze genetic components of inherited diseases. The specific aim of this project is to develop an extensible software system for efficiently computing pedigree likelihood for complex diseases in the presence of multiple polymorphic markers, and SNP markers, in fully general pedigrees taking into account qualitative (discrete) and quantitative traits and a variety of disease models. Our experience shows that by building on top of the insight gained within the last decade from the study of computational probability, in particular, from the theory of probabilistic networks, we can construct a software system whose functionality, speed, and extensibility is unmatched by current linkage software. We plan to integrate these new methods into an existing linkage analysis software, called superlink, which is already gaining momentum for analyzing large pedigrees. We will also continue to work with several participating genetic units in research hospitals and improve the software quality and reliability as we proceed with algorithmic improvements. In this project we will develop novel algorithms for more efficient likelihood calculations and more efficient maximization algorithms for the most general pedigrees. These algorithms will remove redundancy due to determinism, use cashing of partial results effectively, and determine close-to-optimal order of operations taking into account these enhancements. Time-space trade-offs will be computed that allow to use memory space in the most effective way, and to automatically determine on which portions of a complex pedigree exact computations are infeasible. In such cases, a combination of exact computations with intelligent use of approximation techniques, such as variational methods and sampling, will be employed. In particular we will focus on advancing sampling schemes such as MCMC used in the Morgan program and integrating it with exact computation. A serious effort will be devoted for quality control, interface design, and integration with complementing available software with the active help of current users of Superlink and Morgan. PUBLIC SUMMARY: The availability of extensive DMA measurements and new computational techniques provides the opportunity to decipher genetic components of inherited diseases. The main aim of this project is to deliver a fully tested and extremely strong software package to deliver the best computational techniques to genetics researchers.          n/a",Efficient software and algorithms for analyzing markers data on general pedigree,7318595,R01HG004175,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Artificial Intelligence', 'Arts', 'Breeding', 'Complement', 'Complex', 'Computational Technique', 'Computer software', 'Data', 'Disease', 'Disease Resistance', 'Disease model', 'Genes', 'Genetic', 'Genetic Counseling', 'Graph', 'Hospitals', 'Human', 'Inherited', 'Measurement', 'Memory', 'Methods', 'Numbers', 'Operative Surgical Procedures', 'Polymorphic Microsatellite Marker', 'Probability', 'Quality Control', 'Range', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scheme', 'Single Nucleotide Polymorphism', 'Speed', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Work', 'base', 'computer studies', 'design', 'experience', 'genetic analysis', 'genetic linkage analysis', 'genetic pedigree', 'improved', 'insight', 'novel', 'programs', 'size', 'software systems', 'statistics', 'theories', 'tool', 'trait']",NHGRI,UNIVERSITY OF CALIFORNIA IRVINE,R01,2007,372000,-0.008791494458627463
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,-0.0022804875216768097
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7269383,R01RR014477,[' '],NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2007,307022,-0.02724331198114096
"Comparative Visualization and Analysis for GCxGC    DESCRIPTION (provided by applicant): Project Summary. This project will investigate and develop effective information technologies for comparative analysis and visualization of complex data generated by comprehensive two-dimensional gas chromatography (GCxGC). GCxGC is an emerging technology that provides an order-of-magnitude greater separation capacity, significantly better signal-to-noise ratio, and higher dimensional retention-structure relations than traditional GC. The principal challenge for utilization of GCxGC, in a wide range of public-health and other applications, is the difficulty of analyzing and interpreting the large, complex data it generates. The quantity and complexity of GCxGC data necessitates the investigation and development of new information technologies. This project will develop and demonstrate innovative methods and tools for comparative analysis of GCxGC datasets. The expected results of this research and development include a PCA-based method for chemical fingerprinting, decision trees with chemical constraints for sample classification, genetic programming for template and constraint-based matching and classification, and visualization methods for comparative GCxGC analyses. These methods will be implemented in commercial software that will support researchers and laboratory analysts in a wide range of commercial applications, including health care, environmental monitoring, and chemical processing. The power of GCxGC, supported by effective information technologies, will enable better understanding of chemical compositions and processes, a foundation for future scientific advances and discoveries. Relevance to Public Health. Today, a few advanced laboratories are pioneering GCxGC for a variety of applications such as environmental monitoring of exposure profiles in air, soil, food, and water; identification and quantification of toxic products in blood, urine, milk, and breath samples; and qualitative and quantitative metabolomics to provide a holistic view of the biochemical status or biochemical phenotype of an organism. Many analyses in these applications require detailed chemical comparisons of samples, e.g..monitoring changes, comparison to reference standards, chemical matching or ""fingerprinting"", and classification. GCxGC is a powerful new technology for such comparative analyses. This proposal will provide innovative information technologies to support users in these applications.           n/a",Comparative Visualization and Analysis for GCxGC,7270029,R44RR020256,"['Air', 'Archives', 'Biochemical', 'Blood', 'Chemicals', 'Classification', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Trees', 'Development', 'Emerging Technologies', 'Environmental Monitoring', 'Fingerprint', 'Food', 'Foundations', 'Future', 'Gas Chromatography', 'Genetic Programming', 'Goals', 'Healthcare', 'Image', 'Imagery', 'Information Technology', 'Investigation', 'Laboratories', 'Language', 'Machine Learning', 'Marketing', 'Methods', 'Milk', 'Monitor', 'Noise', 'Organism', 'Pattern', 'Phase', 'Phenotype', 'Principal Component Analysis', 'Process', 'Public Health', 'Range', 'Reference Standards', 'Reporting', 'Research Personnel', 'Sales', 'Sampling', 'Schedule', 'Scientific Advances and Accomplishments', 'Signal Transduction', 'Software Tools', 'Soil', 'Statistical Methods', 'Structure', 'System', 'Techniques', 'Technology', 'Today', 'Trademark', 'Urine', 'Water', 'base', 'chemical fingerprinting', 'commercial application', 'comparative', 'innovation', 'innovative technologies', 'instrument', 'metabolomics', 'new technology', 'research and development', 'tool', 'two-dimensional']",NCRR,"GC IMAGE, LLC",R44,2007,239373,-0.013865318673767155
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7247404,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2007,292160,-0.01847130105005237
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7287965,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2007,446875,-0.007109608055983488
The RPI Exploratory Center for Cheminformatics (RMI) No abstract available n/a,The RPI Exploratory Center for Cheminformatics (RMI),7472067,P20HG003899,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2007,364010,-0.000630248710519874
Carolina Exploratory Center for Cheminformatics Research No abstract available n/a,Carolina Exploratory Center for Cheminformatics Research,7472715,P20HG003898,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,P20,2007,373960,-0.000630248710519874
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7214148,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2007,240927,-0.013921647802622495
"Generation and Description of Dendritic Morphology    DESCRIPTION (provided by applicant): This continuing project is directed at describing dendrite structure in a compact yet sufficiently complete and detailed fashion to allow the computer generation of morphologically accurate neuronal models. Dendrite morphology plays a fundamental role in physiological and pathological brain function by subserving and shaping network connectivity and by integrating the complex pattern of synaptic inputs received by the neuron. A parsimonious and algorithmic description of dendritic shape is a crucial step towards the quantitative characterization of the structure-activity relationship in the nervous system and it constitutes an effective way to represent, compress, store, exchange, and amplify extremely complex neuroanatomical data. Neuroanatomical algorithms and models have been developed to simulate and quantitatively analyze the three-dimensional structure of dendritic trees in the same format used to represent experimentally reconstructed neurons.      The specific aims of this project are: (1) to expand and improve neuroanatomically plausible algorithms of dendritic structure and development by including determinants of three-dimensional branch orientation and dependence of growth upon local and global influences (e.g. diameter and neuronal size, respectively); (2) to enhance and distribute the analysis, modeling, and data basing software in order to provide experimental and computational neuroscientists with web-based tools to query, retrieve, measure, classify, and synthesize dendritic morphology data; (3) to continue the experimental reconstruction and analysis of hippocampal pyramidal cells and spinal motoneurons with different experimental protocols and in early postnatal periods; and to integrate these data with detailed biophysical models of neuronal electrophysiology. The informatics and neuroscience components of this research are deeply intertwined and span a variety of scientific approaches, including ""wet"" experiments, computational simulations, statistical analysis and data mining. This will require the design and implementation of novel neuroinformatics tools for data handling and integration, and their distribution to the wider neuroscience community.         n/a",Generation and Description of Dendritic Morphology,7233290,R01NS039600,"['Algorithms', 'Archives', 'Atlases', 'Brain', 'Caliber', 'Cells', 'Class', 'Classification', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Computers', 'Data', 'Data Analyses', 'Databases', 'Dendrites', 'Dependence', 'Development', 'Developmental Process', 'Electrophysiology (science)', 'Environment', 'Generations', 'Goals', 'Growth', 'Hippocampus (Brain)', 'Image', 'Imagery', 'Informatics', 'Internet', 'Java', 'Lead', 'Length', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Microscopic', 'Mining', 'Modeling', 'Morphology', 'Motor Neurons', 'Neonatal', 'Nervous system structure', 'Neurons', 'Neurosciences', 'One-Step dentin bonding system', 'Online Systems', 'Pattern', 'Physiological', 'Play', 'Protocols documentation', 'Pyramidal Cells', 'Research', 'Research Personnel', 'Research Proposals', 'Role', 'Series', 'Shapes', 'Simulate', 'Software Tools', 'Spinal', 'Structure', 'Structure-Activity Relationship', 'Surface', 'Synapses', 'System', 'Techniques', 'Trees', 'data mining', 'data modeling', 'density', 'design', 'digital', 'improved', 'insight', 'models and simulation', 'neuroinformatics', 'novel', 'postnatal', 'programs', 'reconstruction', 'research study', 'simulation', 'size', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'user-friendly', 'virtual']",NINDS,GEORGE MASON UNIVERSITY,R01,2007,61912,-0.02283936508325793
"Systems analysis of oxygen regulation in Halobacterium    DESCRIPTION (provided by applicant): To withstand environmental onslaught, biological systems mount global programs to coordinate the induction of protection and repair mechanisms. This proposal poses the hypothesis that the transcriptional networks underlying such responses to diverse stressors are interrelated. Halobacterium, a halophilic archaeon, has been chosen as a model for this study because it routinely negotiates an array of adverse conditions in its extreme environment, including anoxia, metal stress, and radiation damage. This proposal will investigate the inter-relationship of these responses using global approaches. Given that basal genetic information processing pathways in Halobacterium are mediated by eukaryotic-like proteins, findings from this study will have a direct impact on understanding how complex eukaryotic organisms elicit orthogonal responses in disease-perturbed or infection states. Specifically, I will (1) Characterize key transcriptional regulators responsible for mediating responses to fluctuating oxygen concentrations and identify regulons under their direct and indirect control; (2) Through statistical analysis of integrated datasets, evaluate the extent of cross-regulation of the anoxic response with other environmental perturbations; (3) Experimentally test new hypotheses generated by statistical analysis. These proposed experiments are expected to result in a transcriptional network model that addresses how organisms maintain homeostasis despite stress.           n/a",Systems analysis of oxygen regulation in Halobacterium,7261251,F32GM078980,"['Address', 'Aerobic', 'Algorithms', 'Anoxia', 'Archaea', 'Behavioral', 'Binding Sites', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Phenomena', 'Cells', 'Collection', 'Complex', 'Computer software', 'Condition', 'Couples', 'Data', 'Data Set', 'Defect', 'Disease', 'Electrophoretic Mobility Shift Assay', 'Environment', 'Equilibrium', 'Experimental Designs', 'Face', 'Facility Construction Funding Category', 'Fellowship', 'Gene Targeting', 'Genes', 'Genetic Information Processing Pathway', 'Genome', 'Goals', 'Growth', 'Halobacterium', 'Homeostasis', 'Hydrogen Peroxide', 'Individual', 'Infection', 'Information Systems', 'Knock-out', 'Laboratories', 'Learning', 'Light', 'Localized', 'Machine Learning', 'Manuscripts', 'Maps', 'Mediating', 'Mediation', 'Metals', 'Modeling', 'Molecular Biology', 'Mutate', 'Names', 'Organism', 'Oxidation-Reduction', 'Oxidative Stress', 'Oxygen', 'Oxygen measurement, partial pressure, arterial', 'Play', 'Preparation', 'Property', 'Proteins', 'Proteomics', 'Protocols documentation', 'Radiation', 'Regulation', 'Regulator Genes', 'Regulon', 'Relative (related person)', 'Role', 'Stress', 'Study models', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Testing', 'Time', 'TimeLine', 'Training', 'Transcription Initiation Site', 'Transcriptional Regulation', 'Work', 'biological adaptation to stress', 'cell injury', 'chromatin immunoprecipitation', 'halobacteria', 'high throughput screening', 'in vivo', 'insight', 'metal poisoning', 'mutant', 'network models', 'novel', 'programs', 'repaired', 'research study', 'response', 'stressor', 'transcription factor']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,F32,2007,48796,-0.010666554641226645
"Robust computational framework for predictive ADME-Tox modeling    DESCRIPTION (provided by applicant):    This proposal seeks to establish a universally applicable and robust predictive ADME-Tox modeling framework based on rigorous Quantitative Structure Activity/Property Relationships (QSAR/QSPR) modeling. The framework has been refined in the course of many years of our research in the areas of QSPR methodology development and application to experimental datasets that led to novel analytical approaches, descriptors, model validation schemes, overall QSPR workflow design, and multiple end-point studies. This proposal focuses on the design of optimized QSPR protocols for the development of reliable predictors of critically important ADME-Tox properties. The ADME properties will include, but not limited to, water solubility, membrane permeability, P450 metabolism inhibition and induction, metabolic stability, human intestinal absorption, bioavailability, transporters and PK data; a variety of toxicological end-points vital to human health will be explored; they are available from recent initiatives on development and standardization of toxicity data, such as the US FDA, NIEHS, and EPA DSS-Tox and other database projects. The ultimate goal of this project is sharing both modeling software and specialized predictors with the research community via a web-based Predictive ADME-Tox Portal. The project objectives will be achieved via concurrent development of QSPR methodology (Specific Aim 1), building highly predictive, robust QSPR models of known ADME-Tox properties (Specific Aim 2), and the deployment of both modeling software and individual predictors via a specialized web-portal (Specific Aim 3). To achieve the goals of this project focusing on the development and delivery of specialized tools and rigorous predictors, we have assembled a research team of mostly senior investigators with complimentary skills and track records of accomplishment in the areas of computational drug discovery, experimental toxicology, statistical modeling, and software development and integration; two of the team members have had recent industrial experience before transitioning to academia. To the best of our knowledge, the results of this proposal will lead to the first publicly available in silico ADME-Tox modeling framework and predictors that can be used by the research community to analyze any set of chemicals (i.e., virtual and real compound sets). The framework will have a significant impact on compound prioritization, chemical library design, and candidate selection for preclinical and clinical development.            n/a",Robust computational framework for predictive ADME-Tox modeling,7244058,R21GM076059,"['Academia', 'Acute', 'Address', 'Area', 'Biological Availability', 'Cardiotoxicity', 'Cell Membrane Permeability', 'Chemicals', 'Chronic', 'Clinical', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer software', 'Computers', 'Consensus', 'Cytochrome P450', 'Data', 'Data Collection', 'Data Set', 'Databases', 'Descriptor', 'Development', 'Drug Kinetics', 'End Point', 'Ensure', 'Environment', 'Goals', 'Health', 'Hepatotoxicity', 'Human', 'Individual', 'Internet', 'Intestinal Absorption', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Letters', 'Lung', 'Machine Learning', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Organ', 'Pharmacologic Substance', 'Postdoctoral Fellow', 'Property', 'Protocols documentation', 'Quantitative Structure-Activity Relationship', 'Records', 'Recruitment Activity', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Scheme', 'Scientist', 'Screening procedure', 'Secure', 'Source', 'Specialist', 'Standardization', 'Standards of Weights and Measures', 'Statistical Models', 'Statistically Significant', 'Structure', 'Students', 'Techniques', 'Technology', 'Testing', 'Toxic effect', 'Toxicology', 'Training', 'United States Environmental Protection Agency', 'United States Food and Drug Administration', 'United States National Institutes of Health', 'Validation', 'base', 'carcinogenicity', 'career', 'cluster computing', 'combinatorial', 'computer framework', 'data mining', 'design', 'drug discovery', 'experience', 'genotoxicity', 'innovation', 'knowledge of results', 'member', 'method development', 'neurotoxicity', 'novel', 'open source', 'pre-clinical', 'programs', 'protocol development', 'reproductive', 'skills', 'small molecule libraries', 'software development', 'tool', 'virtual', 'water solubility']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R21,2007,328325,-0.007598835962337546
"Bayesian Methods and Experimental Design for Molecular Biology Experiments    DESCRIPTION (provided by applicant): The goal of this proposal is to provide a suite of software tools for bioinformatics and systems biology researchers who are using molecular biology (Omics) data to identify the best experimental design and to analyze the resulting experimental data using Bayesian tools. A common problem for most bioinformatics experiments is low power due to low replication. This problem can be alleviated economically when an increase in adoption and use of a specific platform leads to a decrease in associated costs, thereby enabling an increase in samples allocated per treatment. Yet, many bioinformatics experiments remain underpowered as researchers use the offsets of decreased costs to explore more complex questions. When designing an experiment, the allocation of samples to treatment regimens, and the choice of treatments to test, are traditionally the only variables to manipulate. Bayesian experimental design provides a framework to find the optimal design out of n possible designs subject to a utility function that can include such items as time and material costs.      Bayesian statistical methods have been gaining substantial favor in bioinformatics and systems biology as they provide a highly flexible framework for fitting and exploring complex models. Bayesian models also provides to domain experts such as biologists and physicians easily interpretable models through posterior probabilities which are more naturally understood than the traditional p-value. While a number of open source tools based on Bayesian models are available, most are applied best in the context of a specific research data analysis problem or model and are not integrated into a single, complete system for data analysis.      We propose to research and develop a statistical analysis software package S+OBAYES (for S-PLUS and R) with generalized tools for Bayesian design of experiments, empirical and fully Bayesian analysis, and modeling and simulation using modern commercial software development practices. These tools will provide functionality for finding the optimal choice and layout of experimental treatments for molecular biology experiments and for fitting Bayesian linear and non-linear models to a variety of data types including time series. We propose to validate the software in molecular biology research problems such as the detection of differential gene, protein, and metabolite abundance. The benefits of this work will be a commercial-quality software package with validated statistical methodology and interactive visualization tools that will appeal to molecular biologists and systems biology investigators. The results of the proposed work will expedite discoveries in basic science, early disease detection, and drug discovery and development.          n/a",Bayesian Methods and Experimental Design for Molecular Biology Experiments,7325828,R43GM083023,"['Address', 'Adoption', 'Algorithms', 'Animal Genetics', 'Arizona', 'Basic Science', 'Bayesian Analysis', 'Bayesian Method', 'Bioconductor', 'Bioinformatics', 'Biological Markers', 'Biological Sciences', 'Biometry', 'Biotechnology', 'Cations', 'Chromosome Mapping', 'Code', 'Communities', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Department of Defense', 'Depth', 'Detection', 'Development', 'Disease', 'Educational process of instructing', 'Educational workshop', 'Employment', 'Ensure', 'Experimental Designs', 'Exposure to', 'Factor Analysis', 'Foundations', 'Funding', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Government', 'Government Agencies', 'Health', 'Imagery', 'Industry', 'Information Systems', 'Institution', 'Iowa', 'Libraries', 'Linear Models', 'Machine Learning', 'Manuals', 'Manuscripts', 'Maps', 'Marketing', 'Mass Spectrum Analysis', 'Measures', 'Medical Informatics', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular', 'Molecular Biology', 'Non-linear Models', 'Numbers', 'Pathway interactions', 'Phase', 'Physicians', 'Population Study', 'Principal Investigator', 'Probability', 'Property', 'Proteome', 'Proteomics', 'Proxy', 'Quantitative Trait Loci', 'Research', 'Research Personnel', 'Rice', 'Risk Factors', 'SNP genotyping', 'Sampling', 'Science', 'Scientist', 'Series', 'Services', 'Simulate', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Software Tools', 'Software Validation', 'Solutions', 'Speed', 'Standards of Weights and Measures', 'Statistical Methods', 'Statistical Models', 'Systems Biology', 'Techniques', 'Telecommunications', 'Testing', 'Time', 'Time Series Analysis', 'Training', 'Treatment Protocols', 'Universities', 'Validation', 'Washington', 'Wisconsin', 'Work', 'animal breeding', 'base', 'cost', 'design', 'drug discovery', 'experience', 'human subject', 'improved', 'interest', 'lecturer', 'models and simulation', 'open source', 'professor', 'programs', 'protein metabolite', 'research and development', 'research study', 'skills', 'software development', 'statistics', 'success', 'theories', 'tool', 'treatment effect']",NIGMS,INSIGHTFUL CORPORATION,R43,2007,103995,-0.0004396002011679114
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7176188,R01AI064913,"['Accounting', 'Affinity', 'Algorithms', 'Alleles', 'Allergens', 'Amino Acid Sequence', 'Amino Acids', 'Antibodies', 'Antigen-Presenting Cells', 'Archives', 'Area', 'Asthma', 'B-Lymphocyte Epitopes', 'B-Lymphocytes', 'Binding', 'Binding Sites', 'Biological Neural Networks', 'Biomedical Research', 'Child', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computing Methodologies', 'Databases', 'Dengue Hemorrhagic Fever', 'Dengue Virus', 'Descriptor', 'Discriminant Analysis', 'Doctor of Philosophy', 'Drug Design', 'Endopeptidases', 'Epitopes', 'Escape Mutant', 'Flavivirus', 'Food', 'Goals', 'Histamine Release', 'Homology Modeling', 'Hypersensitivity', 'IgE', 'Immunotherapy', 'Internet', 'Lead', 'Least-Squares Analysis', 'Length', 'Machine Learning', 'Major Histocompatibility Complex', 'Mediating', 'Methods', 'Modeling', 'Numbers', 'Online Systems', 'Outcome', 'Peptide Hydrolases', 'Peptide Mapping', 'Peptides', 'Pharmaceutical Preparations', 'Procedures', 'Proteins', 'Quantitative Structure-Activity Relationship', 'Research', 'Side', 'Software Tools', 'Structure', 'Surface', 'T-Cell Receptor', 'T-Lymphocyte', 'T-Lymphocyte Epitopes', 'Test Result', 'Testing', 'Vaccine Design', 'Validation', 'Variant', 'Viral Proteins', 'Work', 'base', 'chemical property', 'computerized tools', 'env Gene Products', 'immunogenic', 'improved', 'mathematical model', 'novel', 'novel strategies', 'pathogen', 'practical application', 'protein structure', 'response', 'software development', 'three dimensional structure', 'three-dimensional modeling', 'tool', 'vaccine development']",NIAID,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,2007,286350,-0.022524662036093726
"Metabolomic Assessment of Estrogenic Endocrine Disruptor    DESCRIPTION (provided by applicant)     Estrogenic endocrine disruptors (EEDs) are a group of structurally diverse compounds that include pharmaceuticals, dietary supplements, industrial chemicals and environmental contaminants.  They can elicit a number of adverse health effects such as hormone dependent cancers, reproductive tract abnormalities, compromised reproductive fitness, and impaired cognitive abilities.  In order to fully assess the potential adverse effects of synthetic and natural EEDs, a more comprehensive understanding of their molecular, metabolic, and tissue level effects is required within the context of a whole organism.  This collaborative proposal will elucidate the pathways, networks and signaling cascades perturbed by EEDs using the complementary multidisciplinary expertise of its team members in the areas of toxicology, molecular biology, endocrinology, multinuclear NMR spectroscopy, data management and advanced data analysis.  The comparative effects of ethynyl estradiol (EE), genistein (GEN), and o, p'-dichlorodiphenyltrichloroethane (DDT) on metabolite levels will be assessed in urine, serum and liver extracts by multinuclear (i. e., 1H, 13C, 31P) NMR spectroscopy, and complemented with histopathology examination and gene expression data from ongoing microarray studies in both mouse and rat models.  All data will be stored and archived in dbZach, a MIAME-compliant toxicogenomic supportive database that facilitates data analysis, the integration of disparate data sets, the exchange of data between investigators, and the deposition of data into public repositories.  Advanced statistical approaches, modeling and data integration tools such as neural networks, data fusion, and Baysean inference will be used to fuse these disparate data sets in order to elucidate the conserved biological networks that are of importance in response to endogenous estrogens.  Moreover, EED perturbed pathways associated with elicited effects will be further defined.  Results from these studies will not only further define the physiologic and toxic mechanisms of action of estrogenic compounds but will also demonstrate the synergy of fusing complementary microarray, metabolomic and histopathology data into a comprehensive integrative computational model.  This approach will also demonstrate the ability to maximize knowledge extraction from all disparate data available within the proposed innovative data management system when used with the advanced information tools that will be developed.            n/a",Metabolomic Assessment of Estrogenic Endocrine Disruptor,7240459,R01ES013927,"['Adverse effects', 'Affect', 'Apical', 'Archives', 'Area', 'Biochemical Pathway', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Cell Proliferation', 'Chemicals', 'Class', 'Classification', 'Clinical Chemistry', 'Cognitive', 'Complement', 'Computer Simulation', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Deposition', 'Development', 'Disease Progression', 'Dose', 'Endocrine Disruptors', 'Endocrinology', 'Engineering', 'Environmental Pollution', 'Estradiol', 'Estrogens', 'Funding', 'Future', 'Gene Expression', 'Genistein', 'Health', 'Hepatic', 'Histopathology', 'Hormones', 'Knowledge Extraction', 'Lead', 'Link', 'Lipids', 'Liver Extract', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Metabolic', 'Metabolism', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Profiling', 'Monitor', 'Multinuclear NMR', 'Mus', 'NMR Spectroscopy', 'Numbers', 'Organ Weight', 'Outcome', 'Pathway interactions', 'Pattern Recognition', 'Pharmacologic Substance', 'Physiological', 'Principal Investigator', 'Process', 'Rattus', 'Reporting', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk Assessment', 'Rodent', 'Sampling', 'Screening procedure', 'Serum', 'Signal Transduction', 'Spectrum Analysis', 'System', 'Techniques', 'Time', 'Tissues', 'Toxic effect', 'Toxicogenomics', 'Toxicology', 'Urine', 'Whole Organism', 'aqueous', 'comparative', 'data integration', 'data management', 'dichlorodiphenyltrichloroethane', 'dietary supplements', 'estrogenic endocrine disruptor', 'experience', 'fitness', 'innovation', 'member', 'metabolic abnormality assessment', 'metabolomics', 'multidisciplinary', 'programs', 'repository', 'reproductive', 'research study', 'response', 'tool']",NIEHS,MICHIGAN STATE UNIVERSITY,R01,2007,543226,-0.0025633403762972385
"Interrogation of systems level mechanisms controlling DNA repair processes    DESCRIPTION (provided by applicant): A comprehensive model of a complete biological system will enable prediction of phenotypic outcomes in face of novel genetic and environmental perturbations. Such predictive power for cellular systems, such as DMA repair, will have tremendous impacts on early detection and diagnosis of genetic disorders and, ultimately in designing preventive and/or curative treatments. However, the full understanding of eukaryotic DMA repair systems presents a daunting challenge due to their enormous complexity, especially given that systems approaches are not yet sufficiently mature to mathematically model such complex processes. At a molecular level, eukaryotic genetic information processing is mirrored in a simplified manner by their evolutionary ancestors, the archaea. In Halobacterium NRC-1 we have a simple yet powerful model system of -2400 genes in which the underlying generalized principles of a systems approach can be delineated. This halophilic archaeon routinely mitigates damage from high salinity, UV radiation and desiccation-rehydration cycles. We will use this model system to address the hypothesis that the decision making process governing cell fate after exposure to DMA damaging events such as UV irradiation is mediated by robust gene regulatory networks that simultaneously process information on spectral characteristics of the impinging radiation, the nature and extent of DMA damage and the potential preparative role of cellular entrainment to diurnal cycles. This hypothesis is motivated by two main observations: a. in systems analysis of UV response, although extraordinarily resistant, not all expected repair genes in this microbe responded to irradiation or damage; b. many of these damage-unresponsive repair genes did, however, respond to day-night entrainment. At a fundamental level this suggested that the resident state of the cell at time of irradiation may influence the type of response elicited. A systems approach is ideally suited to delinate the networks underlying this decision making process. Specifically, we will measure dynamic global changes (degree of damage, mRNA /protein levels, protein-protein and protein-DNA interactions) in wild type and mutant strains (defective in sensors, signal transducers, regulators and repair proteins) subjected to combinatorial changes in incident radiation, type of damage inflicted and resident state of the cell at the time of radiation. Through an integrated analysis of these diverse systems level data we will statistically learn perturbation-induced rewiring of a gene regulatory network that processes environmental perturbations (input) into phenotype (output), i.e. a predictive model for regulatory mechanisms for repair. This basic model will serve as a template for designing systems approaches to model higher complexities of eukaryotic repair processes.           n/a",Interrogation of systems level mechanisms controlling DNA repair processes,7317856,R01GM077398,"['Address', 'Algorithms', 'Animal Model', 'Archaea', 'Architecture', 'Behavior', 'Biological', 'Biological Assay', 'Biological Models', 'Biology', 'Candidate Disease Gene', 'Cell Cycle Progression', 'Cell Size', 'Cell Survival', 'Cell physiology', 'Cells', 'Characteristics', 'Chromosomes, Human, Pair 4', 'Circadian Rhythms', 'Complex', 'Computer information processing', 'Condition', 'Coupled', 'DNA', 'DNA Damage', 'DNA Repair', 'DNA Repair Gene', 'DNA Repair Pathway', 'DNA biosynthesis', 'DNA lesion', 'DNA repair protein', 'DNA-Protein Interaction', 'Data', 'Decision Making', 'Desiccation', 'Disease', 'Early Diagnosis', 'Elements', 'Engineering', 'Environment', 'Epitopes', 'Equilibrium', 'Event', 'Exogenous Factors', 'Exposure to', 'Face', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Information Processing Pathway', 'Genetic Transcription', 'Genome', 'Global Change', 'Halobacterium', 'Heart', 'Hour', 'Human', 'Lead', 'Learning', 'Life', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Messenger RNA', 'Metabolism', 'Microbe', 'Modeling', 'Molecular', 'Mutation', 'Nature', 'Nucleotide Excision Repair', 'Numbers', 'Operative Surgical Procedures', 'Organism', 'Outcome', 'Output', 'Oxidative Stress', 'Pathway interactions', 'Phenotype', 'Physiological Processes', 'Physiology', 'Preventive', 'Process', 'Protein Dynamics', 'Proteins', 'Radiation', 'Readiness', 'Regulation', 'Regulator Genes', 'Regulatory Element', 'Rehydrations', 'Resistance', 'Resources', 'Role', 'Signal Transduction', 'Skin Cancer', 'Source', 'Specific qualifier value', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Tertiary Protein Structure', 'Time', 'Transcript', 'Transducers', 'Translations', 'UV response', 'Ultraviolet Rays', 'biological research', 'cell type', 'combinatorial', 'coping', 'cryptochrome', 'day', 'design', 'gene repair', 'genetic disorder diagnosis', 'halobacteria', 'interest', 'irradiation', 'knockout gene', 'mathematical model', 'mutant', 'novel', 'predictive modeling', 'repaired', 'response', 'segregation', 'sensor', 'technology development', 'trait', 'ultraviolet damage', 'ultraviolet irradiation']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,R01,2007,335920,-0.035276588285786344
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methodswithin the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data,  earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user  nterface to support access to information resources for enhanced decision-making by practitioners. The ong-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3  Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technologywork; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C). n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7284424,P01HK000027,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2007,1889501,-0.017573013494115167
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7185305,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Biology, Other', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Computer information processing', 'Data', 'Databases', 'Depth', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'functional genomics', 'genetic element', 'genome database', 'human disease', 'interest', 'model organisms databases', 'repository', 'size', 'tool']",NHGRI,JACKSON LABORATORY,P41,2007,3146180,-0.016486341173536014
"System-wide Study of Transcriptional Control of Metabolism    DESCRIPTION (provided by applicant): This proposal is in response to the NIH call for Exploratory Collaborations with National Centers for Biomedical Computing, PAR-06-223, and it will involve a collaboration between Columbia University's MAGNet NCBC and a team at Los Alamos National Laboratory. The aim of the proposal is a system-wide tudy of integrated transcriptional and metabolic networks in Eschericia coli K-12 strain, aiming at a similar analysis of a pathogen, Bacillus anthracis, at a later date. LANL hosts an experimental research program on bacterial metabolomics. Metabolites serve several functions. The most common one is being the precursors to various cellular components. They are also regulators of cellular functions by means of modulating metabolic reactions or binding to transcription factors and subsequently regulating gene expression. Conversely, the genes regulated by a transcription factor often encode enzymes, modulating the speed of metabolic reactions. Thus, to understand and ultimately predict the cellular response to an environmental change of interest (e.g., pathogen entry into its host environment), we must integrate the analysis of the transcriptome and metabolome. To address this need, we will work with the laboratories of Pat Unkefer and John Dunbar, which will produce data sets of about 300 joint metabolic/transcriptional profiles of E.coli under different steady-state growth conditions. The resources of MAGnet NCBC, specifically the algorithms within the geWorkbench bioinformatics platform produced by the center, will be leveraged to reconstruct cellular networks. Specifically, we expect that ARACNE, an algorithm originally developed by MAGNet for high-fidelity analysis of transcriptional networks in mammalian cells, is well positioned for reconstruction of metabolic networks from high throughput system-wide metabolic activity data, provide that appropriate modifications to deal with the specifics of the metabolic data are made. We will also adapt the algorithm to discover modulated interactions, that is, metabolic interactions that are conditional on the activity of a modulator gene (enzyme), or transcriptional interactions that require the presence of a metabolite to proceed. Such integrated genome/metabolome analysis has not been attempted yet. It will be a giant leap towards a complete understanding of cellular processes in an important organism. Because of the comparatively small size of bacterial genomes and metabolomes, it will be possible to perform system-wide analyses of interactions for the entire integrated genome and metabolome. While important in its own right, especially in view of the pathogenic nature of B. anthracis, this research would also represent an important test bed for a subsequent study of metabolic diseases in higher animals, including humans.           n/a",System-wide Study of Transcriptional Control of Metabolism,7234993,R21GM080216,"['Accounting', 'Address', 'Affect', 'Affinity', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Animals', 'Architecture', 'Autistic Disorder', 'Automobile Driving', 'B-Cell Lymphomas', 'B-Lymphocytes', 'Bacillus anthracis', 'Bacterial Genome', 'Beds', 'Benchmarking', 'Binding', 'Biochemical', 'Biochemical Pathway', 'Biochemical Reaction', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Cadherins', 'Cell Adhesion Molecules', 'Cell Separation', 'Cell physiology', 'Cell-Cell Adhesion', 'Cells', 'Chemicals', 'Code', 'Collaborations', 'Commit', 'Communities', 'Complex', 'Computational Biology', 'Computational Technique', 'Computational algorithm', 'Computer Analysis', 'Computer Simulation', 'Computer software', 'Computers', 'Concentration measurement', 'Condition', 'Crystallography', 'DNA Binding', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Discipline', 'Disease', 'Dissection', 'Documentation', 'Drug Formulations', 'Drug Interactions', 'Educational workshop', 'Electrical Engineering', 'Engineering', 'Ensure', 'Environment', 'Enzyme Gene', 'Enzymes', 'Escherichia coli', 'Family', 'Gene Expression', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genetic', 'Genetic Determinism', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Growth', 'Head', 'Health Sciences', 'Human', 'Imagery', 'Immune system', 'In Vitro', 'Informatics', 'Institutes', 'Institution', 'Internet', 'Java', 'Joints', 'Knowledge', 'Laboratories', 'Language', 'Life', 'Literature', 'Machine Learning', 'Mails', 'Malignant Neoplasms', 'Mammalian Cell', 'Manuals', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Mediating', 'Metabolic', 'Metabolic Control', 'Metabolic Diseases', 'Metabolism', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Modification', 'Molecular', 'Molecular Profiling', 'Molecular Structure', 'Multimedia', 'Nature', 'Noise', 'Online Systems', 'Ontology', 'Organism', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Personal Satisfaction', 'Phenotype', 'Phosphotransferases', 'Physics', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Prokaryotic Cells', 'Property', 'Protein Analysis', 'Protein Family', 'Proteins', 'Proteome', 'Publishing', 'RNA', 'Range', 'Rate', 'Reaction', 'Research', 'Research Activity', 'Research Personnel', 'Research Proposals', 'Resolution', 'Resources', 'Sampling', 'Semantics', 'Sequence Analysis', 'Services', 'Signal Transduction', 'Site', 'Software Engineering', 'Solutions', 'Source Code', 'Specific qualifier value', 'Specificity', 'Speed', 'Structural Protein', 'Structure', 'Structure of germinal center of lymph node', 'Students', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissues', 'Training', 'Transcriptional Regulation', 'United States National Institutes of Health', 'Universities', 'Validation', 'Virulence', 'Visual', 'Work', 'base', 'biocomputing', 'biomedical informatics', 'computer framework', 'computer studies', 'computerized tools', 'concept', 'data acquisition', 'data mining', 'data modeling', 'design', 'environmental change', 'experience', 'forging', 'hazard', 'improved', 'innovation', 'interest', 'interoperability', 'knowledge base', 'member', 'metabolomics', 'microbial', 'multidisciplinary', 'nervous system disorder', 'novel', 'pathogen', 'pathogenic bacteria', 'professor', 'programs', 'protein protein interaction', 'reconstruction', 'research study', 'response', 'simulation', 'size', 'software development', 'text searching', 'tool', 'transcription factor']",NIGMS,LOS ALAMOS NAT SECTY-LOS ALAMOS NAT LAB,R21,2007,257655,-0.026570692119768816
"Development of rapid detection tests for Brucella species    DESCRIPTION (provided by applicant): Brucellosis is a re-emerging zoonotic disease that affects humans and a variety of farm animals. As well as a threat to public health it has considerable economic importance. The etiologic agent responsible for human infection, Brucella melitensis, is classified by both NIAID and CDC as a Category B biothreat pathogen. Antibiotics are only partially effective at controlling the disease and there is no vaccine approved for use in humans. Thus, prompt and accurate diagnosis is the key to containing infection. Currently, the most common diagnostic tests for Brucella are based on indirect serology which are lacking in specificity and sensitivity as well as speed of execution. Thus, there is a pressing need to develop improved diagnostics, especially technologies that may be used at field level. This investigation proposes to address this issue and develop a superior and robust technique for diagnosing Brucella infection with highly specific, non-cross reactive antibody reactions. We will achieve this goal by screening the Brucella genome using novel proteomic chip technology to identify unique antigens that will yield highly specific and accurate diagnoses. This work will be undertaken in Phase 1 of the application. In Phase 2, when antigens have been selected, we anticipate that we will proceed to develop ELISA and immunoblot assays. In parallel, both ELISA and lateral flow ""dipstick"" tests for Brucella antigen will be constructed. We envisage that the antigen assays will have particular utility in detecting Brucella disseminated in the context of the bioterrorism event. The diagnostic tools developed within this application will be rigorously evaluated for sensitivity and specificity using large and diverse panels of human and animal Brucella-positive sera available through our collaborators. The project proposes to develop a highly sensitive and robust diagnostic test for brucellosis, also known as ""undulant fever"", an infectious disease that causes serious illness in farm animals and humans. The current techniques for diagnosing infection are somewhat unreliable and based on outmoded methods. This investigation will use state- of-the-art technology to identify new structures or antigens on Brucella bacteria that may be used to develop a test with superior diagnostic performance.          n/a",Development of rapid detection tests for Brucella species,7278655,R43AI068166,"['Acute', 'Address', 'Affect', 'Algorithms', 'Animals', 'Antibiotics', 'Antibodies', 'Antigen Targeting', 'Antigens', 'Area', 'Artificial Intelligence', 'Arts', 'Bacteria', 'Bioinformatics', 'Biological Assay', 'Bioterrorism', 'Brucella', 'Brucella abortus', 'Brucella melitensis', 'Brucellosis', 'Categories', 'Centers for Disease Control and Prevention (U.S.)', 'Chronic', 'Clinical', 'Communicable Diseases', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Economics', 'Enzyme-Linked Immunosorbent Assay', 'Event', 'Fluorescence', 'Genes', 'Genome', 'Goals', 'Human', 'Immune Sera', 'Immunoassay', 'Immunoblotting', 'Immunodominant Antigens', 'Infection', 'Investigation', 'Lateral', 'Livestock', 'Methods', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Peptides', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Phase II Clinical Trials', 'Printing', 'Proteomics', 'Public Health', 'Reaction', 'Sampling', 'Screening procedure', 'Sensitivity and Specificity', 'Serologic tests', 'Serum', 'Speed', 'Structure', 'Study of serum', 'System', 'Techniques', 'Technology', 'Testing', 'Vaccines', 'Work', 'Yersinia enterocolitica', 'base', 'biothreat', 'disorder control', 'improved', 'novel', 'pathogen', 'prototype', 'rapid detection', 'response', 'tool']",NIAID,"INBIOS INTERNATIONAL, INC.",R43,2007,278818,-0.008944485456935394
"Accelerating metabolic discovery using characterization data    DESCRIPTION (provided by applicant): The long term goal of this project is to develop methods that will allow researchers to gain insight into the metabolic networks of organisms for which we have little or no high-throughput data. Such metabolic networks can reveal aspects of the organism's metabolism that might make it vulnerable to new or existing therapies. A core data set using genomic and other omic data from data-rich bacteria that are related to the organisms of interest will be assembled. The statistical tools needed to integrate these data and to infer metabolic networks using these core data plus characterization (phenotypic) data will then be built. Using the statistical inference algorithms, the characterization data can be leveraged to reveal the metabolic networks of data-poor bacteria for which we have only characterization data. This approach can eliminate the need for genome sequencing, gene expression experiments and the like for thousands of Gram-negative facultative rod bacteria (GNF). There are five tasks in the project: (1) assemble the data sets from data-rich organisms that will be used to inform the inference algorithm. These data include (a) the genomic sequences and annotation information, (b) extant pathway data and (c) gene expression data. All these data contain some level of information about the connectivity within the metabolic network; (2) process the genomic data to enhance its predictive value; (3) develop a data integration algorithm; (4) investigate modeling frameworks to be used for Bayesian data fusion and network inference; (5) validate the metabolic networks. Deliverables from this project should include: (1) a set of pathway genome databases for 35 GNF, This group includes 20 strains classified as category A or B biothreat agents, (2) a core dataset that integrates all the information we have relevant to the metabolic pathways in the 35 sequenced GNF, (3) a probabilistic graphical modeling framework capable of integrating disparate types of data and inferring networks from the integrated data, (4) a method for using characterization data, along with deliverables 2 and 3, to infer metabolic networks for bacterial strains for which we have only characterization data. The ability to rapidly construct models of metabolic networks means researchers will be able to respond to emerging infectious agents or biothreats more quickly. Relevance The methods developed as part of this proposal will allow us to quickly make metabolic maps for thousands of bacteria. Such maps can guide researchers to promising new targets for therapeutic or preventative measures against pathogenic bacteria. The fight against well-known pathogens and biothreat agents, as well as against new, emerging pathogens will be greatly aided by these tools.              n/a",Accelerating metabolic discovery using characterization data,7267998,R21AI067543,"['Adopted', 'Algorithms', 'American Type Culture Collection', 'Artificial Intelligence', 'Bacteria', 'Bacteriology', 'Biochemical', 'Biochemical Pathway', 'Biological Models', 'Biology', 'Bypass', 'Categories', 'Cholera', 'Code', 'Data', 'Data Collection', 'Data Set', 'Depth', 'Disease', 'Electronics', 'Facility Construction Funding Category', 'Gammaproteobacteria', 'Gene Expression', 'Genomics', 'Goals', 'Gram&apos', 's stain', 'Infectious Agent', 'Information Networks', 'Manuals', 'Maps', 'Measures', 'Meta-Analysis', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Methods', 'Modeling', 'Nosocomial Infections', 'Numbers', 'Nutritional', 'Organism', 'Outcome', 'Oxygen', 'Pathway interactions', 'Plague', 'Predictive Value', 'Process', 'Prophylactic treatment', 'Proteomics', 'Research Personnel', 'Salmonella typhi', 'Shapes', 'Shigella', 'Shigella Infections', 'Signal Transduction', 'Source', 'Statistical Methods', 'Techniques', 'Testing', 'Time', 'Typhoid Fever', 'Variant', 'Vibrio cholerae', 'Work', 'Writing', 'Yersinia pestis', 'biothreat', 'computerized data processing', 'data integration', 'design', 'falls', 'fight against', 'genome database', 'genome sequencing', 'gram negative facultative rods', 'innovation', 'insight', 'interest', 'network models', 'novel', 'pathogen', 'pathogenic bacteria', 'programs', 'research study', 'retinal rods', 'routine Bacterial stain', 'sound', 'success', 'therapeutic target', 'tool', 'transcriptomics']",NIAID,AMERICAN TYPE CULTURE COLLECTION,R21,2007,185677,-0.012116348760989059
"Systems Biology of Plasmodium falciparum: Building and Exploring Network Models    DESCRIPTION (provided by applicant):  There is an urgent need for novel anti-malaria interventions, due to the growing resistance of Plasmodium parasites to available drugs. The long-term objective of our research is to identify potential drug or vaccine targets using an efficient and cost-effective data-mining approach. This proposed study is aimed at the identification of biological networks as the first step toward a systems-level understanding of parasite biology. Compared to functional genomics, which requires a priori information about the identity or function of targets, a systems-level understanding of parasite biology will allow us to identify targets based on their key roles in networks. Specific aim 1 focuses on the harvest of data relevant to three gene families: the proteases, transporters, and transcription factors that are essential components in cellular networks for parasite growth and infection. Our project will encompass genomic data, expression data, and data derived from other high throughput experiments and bioinformatic and statistical analyses. Specific aim 2 focuses on the integration and encapsulation of these data in the form of network models. The models will be inferred using probabilistic graphical methods drawn from the field of engineering. Specific aim 3 focuses on the exploration of these models, particularly in the light of data from wet lab gene mapping and drug resistance experiments. Our ""biological maps"" will enable the visualization of evolutionary forces driving changes in the parasite's phenotype and should allow us to identify network design weaknesses in the parasite--potential vulnerabilities that could result in new malarial control strategies.           n/a",Systems Biology of Plasmodium falciparum: Building and Exploring Network Models,7287978,SC1GM081068,"['Animal Model', 'Antimalarials', 'Area', 'Artificial Intelligence', 'Be++ element', 'Beryllium', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle Regulation', 'Cell physiology', 'Chromosome Mapping', 'Complex', 'Data', 'Data Set', 'Development', 'Digestion', 'Discipline', 'Drug resistance', 'Electrical Engineering', 'Elements', 'Endopeptidases', 'Engineering', 'Family', 'Gene Family', 'Gene Proteins', 'Genes', 'Genomics', 'Growth', 'Harvest', 'Hemoglobin', 'Imagery', 'Immune', 'Infection', 'Intervention', 'Life Cycle Stages', 'Light', 'Location', 'Malaria', 'Maps', 'Mediating', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Open Reading Frames', 'Parasites', 'Pattern', 'Peptide Hydrolases', 'Pharmaceutical Preparations', 'Phenotype', 'Plant Roots', 'Plasmodium', 'Plasmodium falciparum', 'Prevalence', 'Process', 'Property', 'Proteins', 'Proteomics', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Rupture', 'Signal Transduction', 'Specialist', 'Staging', 'Statistical Models', 'System', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Transcription Factor 3', 'Vaccines', 'Work', 'base', 'comparative', 'computerized data processing', 'cost', 'data integration', 'data mining', 'design', 'driving force', 'experience', 'functional genomics', 'insight', 'member', 'mortality', 'network models', 'novel', 'pressure', 'programs', 'prototype', 'quantum', 'research study', 'response', 'stem', 'theories', 'trafficking', 'transcription factor', 'transcriptomics']",NIGMS,UNIVERSITY OF TEXAS SAN ANTONIO,SC1,2007,247625,-0.04194508676091908
"Computational Models of Infectious Disease Threats DESCRIPTION (provided by applicant):  Microbial threats, including bioterrorism and naturally emerging infectious diseases, pose a serious challenge to national security in the United States and to health worldwide.  This proposal describes the creation of a center for computational modeling of infectious diseases at the Johns Hopkins Bloomberg School of Public Health, with the collaboration of key experts at the Brookings Institution, the National Aeronautic and Space Administration, the University of Maryland, and Imperial College (London).  The overarching aim of this project is to integrate the most advanced and powerful techniques of epidemiological data analysis with those of computer simulation (agent-based modeling) to produce a unified computational epidemiology that is scientifically sound, highly visual and user-friendly, and responsive to biosecurity and public health policy requirements.  Data analysis will be guided by the insight that epidemic patterns over space and time can be approached as nearly decomposable systems, in which frequency components of the incidence signal can be isolated and studied.  Wavelet transforms, and empiric mode decomposition using Hilbert-Huang Transforms, will be used to sift nonlinear, nonstationary epidemiological data, allowing frequency band patterns to be defined.  Isolated frequency modes will then be associated with external forcing (weather, social contact patterns) and internal dynamics (Kermack-McKendrick predator-prey models).  Results of the epidemiological data decomposition analysis, along with the knowledge of infectious disease experts, will instruct the creation and development of agent-based models.  Such models feature populations of mobile individuals in artificial societies that interact locally with other individuals.  Features of the basic model include variable social network structures, individual susceptibility and immunity, incubation periods, transmission rates, contact rates, and other selectable parameters.  After the agent-based model is calibrated to generate epidemic patterns consistent with real world epidemiology, preventive strategies including vaccination, contact tracing, isolation, quarantine, and other public health measures will be systematically introduced and their impact evaluated.  Methods will be developed for assessing the utility of individual models, and for making decisions based on combined results from more than one model.  Infectious diseases to be studied initially include smallpox, SARS, dengue, West Nile, and unknown but hypothetically plausible agents.  As part of a Cooperative Agreement, the Center will work with other research groups, a bioinformatics core group, and the NIGMS to develop data sets, software and methods, agent-based models, and visualization tools.  In an infectious disease epidemic emergency the Center will redirect its activities to serve the nation's security, as guided by the NIGMS. n/a",Computational Models of Infectious Disease Threats,7284239,U01GM070708,"['AIDS therapy', 'AIDS/HIV problem', 'Academy', 'Acquired Immunodeficiency Syndrome', 'Affect', 'Airborne Particulate Matter', 'Algorithms', 'American', 'Americas', 'Animal Experimentation', 'Appendix', 'Archives', 'Area', 'Arthropod Vectors', 'Award', 'Bacteria', 'Beds', 'Bioinformatics', 'Biological', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Books', 'Borrelia', 'Climate', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Condition', 'Contact Tracing', 'Data', 'Data Analyses', 'Data Set', 'Decision Making', 'Decision Theory', 'Demography', 'Dengue', 'Dengue Hemorrhagic Fever', 'Detection', 'Development', 'Dialysis procedure', 'Disease', 'Docking', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Ecology', 'Economics', 'Educational process of instructing', 'Ehrlichia', 'Emergency Situation', 'Emerging Communicable Diseases', 'Encephalitis', 'Engineering', 'Environmental Engineering technology', 'Environmental Health', 'Epidemic', 'Epidemiologic Methods', 'Epidemiologic Studies', 'Epidemiology', 'Event', 'Evolution', 'Facility Construction Funding Category', 'Faculty', 'Foot-and-Mouth Disease', 'Frequencies', 'Game Theory', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Geography', 'Glass', 'Goals', 'HIV', 'Hantavirus', 'Head', 'Health', 'Health Policy', 'Healthcare', 'Hepatitis E', 'Human', 'Human Resources', 'Hygiene', 'Imagery', 'Immunity', 'Immunology', 'Incidence', 'Individual', 'Infectious Agent', 'Infectious Disease Epidemiology', 'Influenza', 'Informatics', 'Information Services', 'Institute of Medicine (U.S.)', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internal Medicine', 'International', 'Internet', 'Intervention', 'Joints', 'Journals', 'Knowledge', 'Laboratories', 'Laboratory Research', 'Laboratory Study', 'Lead', 'Legal patent', 'Leptospira', 'Libraries', 'Location', 'London', 'Lung', 'Machine Learning', 'Maintenance', 'Malaria', 'Maryland', 'Master&apos', 's Degree', 'Mathematical Biology', 'Mathematics', 'Measles', 'Measures', 'Mechanics', 'Methods', 'Microbiology', 'Military Personnel', 'Modeling', 'Modified Smallpox', 'Molecular', 'National Institute of General Medical Sciences', 'National Security', 'New York', 'Nonlinear Dynamics', 'Nonparametric Statistics', 'Observational Study', 'Oceanography', 'Outcome', 'Paper', 'Pattern', 'Physical Dialysis', 'Play', 'Policies', 'Policy Maker', 'Population', 'Positioning Attribute', 'Predisposition', 'Pregnancy Outcome', 'Prevention strategy', 'Preventive', 'Principal Investigator', 'Prion Diseases', 'Procedures', 'Process', 'Provider', 'Proxy', 'Public Health', 'Public Health Schools', 'Public Policy', 'Publications', 'Publishing', 'Purpose', 'Quarantine', 'Rate', 'Recording of previous events', 'Reference Standards', 'Relative (related person)', 'Research', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Rickettsia', 'Risk Assessment', 'Rodent', 'Role', 'Route', 'Schedule', 'Schools', 'Science', 'Scientist', 'Screening procedure', 'Security', 'Series', 'Severe Acute Respiratory Syndrome', 'Signal Transduction', 'Simulate', 'Smallpox', 'Social Network', 'Social Sciences', 'Societies', 'Software Tools', 'Space Flight', 'Statistical Computing', 'Statistical Models', 'Structure', 'Students', 'System', 'Systems Analysis', 'Testing', 'Theoretical model', 'Time', 'Time Series Analysis', 'Training', 'Tropical Medicine', 'U-Series Cooperative Agreements', 'Uncertainty', 'United States', 'United States National Academy of Sciences', 'United States National Aeronautics and Space Administration', 'Universities', 'Vaccination', 'Variant', 'Vector-transmitted infectious disease', 'Violence', 'Viral', 'Viral Hemorrhagic Fevers', 'Virus', 'Virus Diseases', 'Visual', 'Weather', 'West Nile virus', 'Work', 'base', 'biosecurity', 'c new', 'college', 'computer science', 'concept', 'design', 'disease natural history', 'disease transmission', 'disorder prevention', 'disorder risk', 'editorial', 'experience', 'improved', 'indexing', 'infectious disease model', 'insight', 'interest', 'mathematical model', 'member', 'microbial', 'models and simulation', 'network models', 'pathogen', 'peer', 'predictive modeling', 'prevent', 'professor', 'programs', 'remote sensing', 'respiratory', 'simulation', 'skills', 'social', 'social organization', 'sound', 'theories', 'tool', 'transmission process', 'user-friendly', 'vaccination strategy']",NIGMS,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,U01,2007,588968,0.016653967186893125
"MBRS IMSD Program at the University of Kansas    DESCRIPTION (provided by applicant):  Within the three short years since the University of Kansas (KU) IMSD program began, twelve minority students, including six American Indians, will have applied to graduate school. This is a continuation proposal to allow us to build on these successes. KU IMSD works in concert with other NIH funded programs (Bridge, RISE, IRACDA) and takes full advantage of the juxtaposition of a Research I Institution (KU) and one of the largest tribal colleges (Haskell Indian Nations University). KU IMSD consists of four components: 1) providing research experiences to American Indian and other minority students, with a particular a focus on recruiting Haskell students from the Bridge or RISE program; 2) enhancing and modifying the curriculum; 3) offering an interdisciplinary seminar series and 4) providing financial aid and mentoring. The undergraduate research experience takes a broad, interdisciplinary approach to placing students into one of 77 possible KU labs and includes opportunities for students to share their research results at local and national meetings. The program will also support several American Indian graduate students who have obtained BA's from Haskell. IMSD supported curricular enhancements that have shown remarkable results over the past three years will be continued for gatekeeper courses in biology, chemistry and math. In biology alone, the average grade point average of American Indian students who completed the introductory biology course has increased from 0.86 to 2.94 over the past six years. An integrative seminar series will bring together students from the IMSD, Bridge, and RISE programs to foster community and learning. Support from KU IMSD for undergraduate researchers will be a cornerstone in providing financial support along with KU scholarships targeted for American Indian students. Mentoring will be provided from faculty (research advisors), the IMSD Program Coordinator (individual and group meetings) and peers (other IMSD students). Evaluation and tracking procedures will allow for regular adjustment of activities during the course of the program and assessment of whether goals have been met. If funded for another four years, the KU/Haskell collaboration provides the opportunity to significantly impact the number of American Indian scientists in this country.         n/a",MBRS IMSD Program at the University of Kansas,7209749,R25GM062232,"['Address', 'Administrative Personnel', 'Administrator', 'Adopted', 'Advertising', 'Alaska', 'Algorithms', 'American Indians', 'Appendix', 'Area', 'Arrhythmia', 'Artificial Intelligence', 'Arts', 'Attention', 'Authorization documentation', 'Behavior', 'Behavioral Sciences', 'Biodiversity', 'Biogenesis', 'Bioinformatics', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Books', 'Bovine Spongiform Encephalopathy', 'Budgets', 'Calculi', 'Cells', 'Chaos Theory', 'Characteristics', 'Charge', 'Chemistry', 'Cities', 'Class', 'Collaborations', 'Communities', 'Computer Assisted', 'Computer software', 'Computers', 'Conservatism', 'Country', 'Coupled', 'DNA', 'DNA Structure', 'Daily', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Depth', 'Development', 'Discipline', 'Disease', 'Distant', 'Doctor of Philosophy', 'Education', 'Educational Curriculum', 'Educational process of instructing', 'Educational workshop', 'Elements', 'Engineering', 'Enrollment', 'Environment', 'Equilibrium', 'Ethics', 'Evaluation', 'Event', 'Exercise', 'Exposure to', 'Faculty', 'Family', 'Feedback', 'Feeling', 'Fees', 'Film', 'Financial Support', 'Fostering', 'Foundations', 'Friends', 'Funding', 'Future', 'GYPA gene', 'Gatekeeping', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Programming', 'Geographic Information Systems', 'Glycophorin A', 'Goals', 'Grant', 'Graph', 'Group Meetings', 'Growth', 'Habits', 'Hand', 'Health', 'Health Sciences', 'Heart', 'Helix (Snails)', 'Home environment', 'Hour', 'Housing', 'Human', 'Human Resources', 'Humanities', 'Indigenous', 'Individual', 'Informatics', 'Information Systems', 'Institutes', 'Institution', 'Internet', 'Internships', 'Invasive', 'Journals', 'Judgment', 'Kansas', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Light', 'Link', 'Literature', 'Malignant Neoplasms', 'Mathematics', 'Medical', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Minority', 'Minority Groups', 'Modeling', 'Modems', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Monitor', 'Motivation', 'Museums', 'Music', 'Mutation', 'National Institute of General Medical Sciences', 'Natural History', 'Newsletter', 'Numbers', 'Oklahoma', 'Oral', 'Oral cavity', 'Organic Chemistry', 'Other Minority', 'Outcome', 'Pamphlets', 'Paper', 'Parents', 'Participant', 'Performance', 'Personality', 'Pharmaceutical Chemistry', 'Placement', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Poverty', 'Preparation', 'Principal Investigator', 'Problem Solving', 'Procedures', 'Process', 'Productivity', 'Program Development', 'Progress Reports', 'Proteins', 'Published Comment', 'Purpose', 'Qualifying', 'Quantum Mechanics', 'RNA', 'Race', 'Radio', 'Range', 'Reading', 'Recommendation', 'Records', 'Recruitment Activity', 'Regulation', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Review Committee', 'Rewards', 'Schedule', 'Scholarship', 'Schools', 'Science', 'Scientist', 'Selection Criteria', 'Series', 'Services', 'Shock', 'Site', 'Slide', 'Snow', 'Social Functioning', 'Source', 'Sports', 'Standards of Weights and Measures', 'Students', 'Supervision', 'Support Groups', 'Surveys', 'Techniques', 'Technology', 'Text', 'Textbooks', 'Thinking', 'Time', 'Time Management', 'Today', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Update', 'Ursidae Family', 'Visit', 'Visualization software', 'Wages', 'Week', 'Work', 'Writing', 'abstracting', 'aging nutrition', 'analytical method', 'base', 'biomedical scientist', 'career', 'cohort', 'college', 'computer generated', 'computer science', 'concept', 'cost', 'court', 'data acquisition', 'day', 'digital', 'ear helix', 'expectation', 'experience', 'falls', 'follow-up', 'gene therapy', 'high school', 'i(19)', 'image visualization', 'improved', 'instructor', 'interdisciplinary approach', 'interest', 'lectures', 'member', 'novel', 'peer', 'posters', 'preference', 'professor', 'programs', 'protein structure', 'role model', 'satisfaction', 'size', 'skills', 'success', 'symposium', 'theories', 'tool', 'tribal college']",NIGMS,UNIVERSITY OF KANSAS LAWRENCE,R25,2007,430160,-0.008720535482448276
"Systems Biology of Cell Decision Processes    DESCRIPTION (provided by applicant):  The remarkable complexity of biological systems demands a systematic approach to their analysis.  The goal of this proposal is to establish an MIT Center of Excellence in Cell Decision Processes (CDP) around an interdisciplinary team of cell biologists, computer scientists and microsystems engineers tackling the systems biology of protein networks and signal transduction in mammalian cells. The basic hypothesis of the CDP Center is that understanding cell decision processes requires the development of network models that combine quantitative rigor with molecular detail. These models will be hybrids containing highly specific representations of critical reactions and abstract representations of the system as a whole. Effective models will endure and capture the accumulation of knowledge over time in a rigorous and portable format.  The CDP Center will follow a research paradigm that links systematic experiments and numerical modeling in a four-step feedback loop of manipulation-measurement-mining and modeling. The biological focus of the Center will be the signaling events that control apoptosis. The measurement of protein concentrations, modification state and activity will be undertaken for signaling molecules at many points in the apoptotic network. The measurements will then be analyzed using several modeling approaches ranging from highly specified to highly abstract.  To collect sufficient data for systematic modeling, the CDP Center will automate and standardize biochemical methods, develop compact array-based assays and design novel microfabricated devices with integrated microfluidics and label-free sensors. To organize and systematize the data, informatic methods will be developed that support rigorous approaches to inference. Finally, physico-chemical, structure-systems and Bayesian models will be used to generate biological hypotheses for experimental verification.  The research activities of the CDP Center will be complemented by graduate and undergraduate education and by an outreach program targeted at the scientific community in general and minority-serving institutions in particular.         n/a",Systems Biology of Cell Decision Processes,7285298,P50GM068762,"['Accountability', 'Accounting', 'Address', 'Adopted', 'Algorithms', 'Amino Acid Sequence', 'Animals', 'Annual Reports', 'Apoptosis', 'Apoptotic', 'Appendix', 'Area', 'Artificial Intelligence', 'Attention', 'Authorship', 'Automation', 'Award', 'BCL2L11 gene', 'Belief', 'Binding', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Phenomena', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Engineering', 'Boxing', 'Budgets', 'Cancer Center', 'Cell Extracts', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Chemical Engineering', 'Chemical Structure', 'Chemicals', 'Child', 'Classification', 'Code', 'Collaborations', 'Color', 'Commit', 'Communities', 'Complement', 'Complex', 'Computational Biology', 'Computer software', 'Computers', 'Computing Methodologies', 'Core Facility', 'Data', 'Data Analyses', 'Databases', 'Detection', 'Development', 'Devices', 'Discipline', 'Disputes', 'Doctor of Philosophy', 'Drug Formulations', 'Drug Industry', 'Education', 'Education and Outreach', 'Educational Curriculum', 'Educational process of instructing', 'Electrical Engineering', 'Electronics', 'Engineering', 'Ensure', 'Equilibrium', 'Equipment', 'Event', 'Evolution', 'Experimental Models', 'Facility Construction Funding Category', 'Faculty', 'Feedback', 'Figs - dietary', 'Foundations', 'Funding', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome Components', 'Goals', 'Grant', 'Gray unit of radiation dose', 'Head', 'High Performance Computing', 'High Pressure Liquid Chromatography', 'Historically Black Colleges and Universities', 'Home environment', 'Human', 'Human Resources', 'Hybrids', 'Image', 'Individual', 'Informatics', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internet', 'Interruption', 'Investments', 'Joints', 'Knowledge', 'Label', 'Lead', 'Leadership', 'Learning', 'Letters', 'Life', 'Link', 'Mammalian Cell', 'Manuals', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Metabolic', 'Methods', 'Microfluidics', 'Microscopy', 'Mining', 'Minority-Serving Institution', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Profiling', 'Molecular and Cellular Biology', 'Monitor', 'Mus', 'NCI Center for Cancer Research', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paper', 'Pathway Analysis', 'Pathway interactions', 'Peptide Sequence Determination', 'Performance', 'Phase', 'Phosphorylation', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Productivity', 'Property', 'Protein Analysis', 'Proteins', 'Proteomics', 'Protons', 'Publications', 'Purpose', 'Range', 'Reaction', 'Reliance', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resource Allocation', 'Resources', 'Risk', 'Role', 'Route', 'Running', 'Schools', 'Science', 'Scientist', 'Seeds', 'Semiconductors', 'Series', 'Services', 'Signal Transduction', 'Signaling Molecule', 'Soaps', 'Source', 'Specific qualifier value', 'Students', 'Supervision', 'Support of Research', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'To specify', 'Training', 'Training Programs', 'Transgenic Organisms', 'Underrepresented Minority', 'United States National Institutes of Health', 'Visit', 'Work', 'abstracting', 'anticancer research', 'base', 'chemical reaction', 'college', 'computer based statistical methods', 'computer science', 'computerized data processing', 'concept', 'data mining', 'data modeling', 'design', 'desire', 'drug discovery', 'experience', 'fluid flow', 'forging', 'fundamental research', 'instrument', 'instrumentation', 'interest', 'intracellular protein transport', 'member', 'microsystems', 'molecular modeling', 'network models', 'novel', 'novel strategies', 'open source', 'outreach', 'outreach program', 'programs', 'protein transport', 'research facility', 'research study', 'response', 'sensor', 'size', 'success', 'systems research', 'technology development', 'tool', 'virtual']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,P50,2007,2952859,-0.02474154492195754
"Systems Biology of Cell Decision Processes The remarkable complexity of biological systems demands a systematic approach to their analysis. The goal of this proposal is to establish an MIT Center of Excellence in Cell Decision Processes (CDP) around an interdisciplinary team of cell biologists, computer scientists and microsystems engineers tackling the systems biology of protein networks and signal transduction in mammalian cells. The basic hypothesis of the CDP Center is that understanding cell decision processes requires the development of network models that combine quantitative rigor with molecular detail. These models will be hybrids containing highly specific representations of critical reactions and abstract representations of the system as a whole. Effective models will endure and capture the accumulation of knowledge over time in a rigorous and portable format.  The CDP Center will follow a research paradigm that links systematic experiments and numerical modeling in a four-step feedback loop of manipulation-measurement-mining and modeling. The biological focus of the Center will be the signaling events that control apoptosis. The measurement of protein concentrations, modification state and activity will be undertaken for signaling molecules at many points in the apoptotic network. The measurements will then be analyzed using several modeling approaches ranging from highly specified to highly abstract.  To collect sufficient data for systematic modeling, the CDP Center will automate and standardize biochemical methods, develop compact array-based assays and design novel microfabricated devices with integrated microfluidics and label-free sensors. To organize and systematize the data, inforrnatic methods will be developed that support rigorous approaches to inference. Finally, physico-chemical, structure-systems and Bayesian models will be used to generate biological hypothesis for experimental verification.  The research activities of the CDP Center will be complemented by graduate and undergraduate education and by an outreach program targeted at the scientific community in general and minority-serving institutions in particular n/a",Systems Biology of Cell Decision Processes,7479966,P50GM068762,"['Accountability', 'Accounting', 'Address', 'Adopted', 'Algorithms', 'Amino Acid Sequence', 'Animals', 'Annual Reports', 'Apoptosis', 'Apoptotic', 'Appendix', 'Area', 'Artificial Intelligence', 'Attention', 'Authorship', 'Automation', 'Award', 'BCL2L11 gene', 'Belief', 'Binding', 'Biochemical', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Phenomena', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Engineering', 'Boxing', 'Budgets', 'Cancer Center', 'Cell Extracts', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Cellular biology', 'Chemical Engineering', 'Chemical Structure', 'Chemicals', 'Child', 'Classification', 'Code', 'Collaborations', 'Color', 'Commit', 'Communities', 'Complement', 'Complex', 'Computational Biology', 'Computer software', 'Computers', 'Computing Methodologies', 'Core Facility', 'Data', 'Data Analyses', 'Databases', 'Detection', 'Development', 'Devices', 'Discipline', 'Disputes', 'Doctor of Philosophy', 'Drug Formulations', 'Drug Industry', 'Education', 'Education and Outreach', 'Educational Curriculum', 'Educational process of instructing', 'Electrical Engineering', 'Electronics', 'Engineering', 'Ensure', 'Equilibrium', 'Equipment', 'Event', 'Evolution', 'Experimental Models', 'Facility Construction Funding Category', 'Faculty', 'Feedback', 'Figs - dietary', 'Foundations', 'Funding', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome Components', 'Goals', 'Grant', 'Gray unit of radiation dose', 'Head', 'High Performance Computing', 'High Pressure Liquid Chromatography', 'Historically Black Colleges and Universities', 'Home environment', 'Human', 'Human Resources', 'Hybrids', 'Image', 'Individual', 'Informatics', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'Internet', 'Interruption', 'Investments', 'Joints', 'Knowledge', 'Label', 'Lead', 'Leadership', 'Learning', 'Letters', 'Life', 'Link', 'Mammalian Cell', 'Manuals', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Metabolic', 'Methods', 'Microfluidics', 'Microscopy', 'Mining', 'Minority-Serving Institution', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Molecular Profiling', 'Molecular and Cellular Biology', 'Monitor', 'Mus', 'NCI Center for Cancer Research', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Outcome', 'Outcomes Research', 'Paper', 'Pathway Analysis', 'Pathway interactions', 'Peptide Sequence Determination', 'Performance', 'Phase', 'Phosphorylation', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Productivity', 'Property', 'Protein Analysis', 'Proteins', 'Proteomics', 'Protons', 'Publications', 'Purpose', 'Range', 'Reaction', 'Reliance', 'Research', 'Research Activity', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resource Allocation', 'Resources', 'Risk', 'Role', 'Route', 'Running', 'Schools', 'Science', 'Scientist', 'Seeds', 'Semiconductors', 'Series', 'Services', 'Signal Transduction', 'Signaling Molecule', 'Soaps', 'Source', 'Specific qualifier value', 'Students', 'Supervision', 'Support of Research', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Testing', 'Time', 'TimeLine', 'To specify', 'Training', 'Training Programs', 'Transgenic Organisms', 'Underrepresented Minority', 'United States National Institutes of Health', 'Visit', 'Work', 'abstracting', 'anticancer research', 'base', 'chemical reaction', 'college', 'computer based statistical methods', 'computer science', 'computerized data processing', 'concept', 'data mining', 'data modeling', 'design', 'desire', 'drug discovery', 'experience', 'fluid flow', 'forging', 'fundamental research', 'instrument', 'instrumentation', 'interest', 'intracellular protein transport', 'member', 'microsystems', 'molecular modeling', 'network models', 'novel', 'novel strategies', 'open source', 'outreach', 'outreach program', 'programs', 'protein transport', 'research facility', 'research study', 'response', 'sensor', 'size', 'success', 'systems research', 'technology development', 'tool', 'virtual']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,P50,2007,48090,-0.02540577357963121
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,7111722,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2006,314029,-0.02724331198114096
"Predicting Cardiac Arrest in Pediatric Critical Illness    DESCRIPTION (provided by applicant):  The broad purpose of this proposal is to create a framework for bedside decision support to predict life threatening events before they happen. The specific hypothesis is that models predicting cardiac arrest can be generated from physiologic and laboratory data obtained in the 12 hours preceding the event using logistic regression analysis (LR) and data mining techniques such as support vector machines (SVM), neural networks (NN), Bayesian networks (BN) and decision tree classification (DTC). We further hypothesize that a support vector machine technique will yield the model with the best performance. Specific Aim 1 is to acquire and prepare data for eligible patients by merging information from physiologic, laboratory, and clinical databases and selecting data from twelve hours prior to either a cardiac arrest or the maximum severity of illness. Noise will be removed with automated methods that can be used in real time. Missing data elements will be imputed by statistical methods that are regarded as state of the art. Since the optimum time window to investigate before an arrest has not been established, and since there is no standard process of abstracting trend information, we will generate multiple candidate data sets in an effort to determine the optimum combination of parameters. Data dimensionality will be reduced by three separate feature selection methods, each of which will be used in subsequent modeling procedures. Specific Aim 2 is to create cardiac arrest prediction models from the candidate data sets using LR, SVM, NN, BN and DTC. We will assess model performance with sensitivity, specificity, positive predictive value, negative predictive value, and area under the Receiver Operating Characteristics curve (AUROC) using 10- fold cross validation. We will then assess the ability to generalize by testing the model on unseen data. We will determine the impact of training sample size on model performance by varying the percentage of data used during the 10-fold cross validation for each modeling technique's best performing model. We will then perform a false prediction analysis to determine the etiology of the false prediction. Specific Aim 3 is to determine which modeling process and configuration parameters performs the best, and to determine optimum timing windows for: time to analyze pre-arrest and size of feature window. The significance of this proposal is that successful prediction and early intervention could save thousands of lives annually.          n/a",Predicting Cardiac Arrest in Pediatric Critical Illness,7106109,K22LM008389,"['clinical research', 'heart arrest', 'model']",NLM,BAYLOR COLLEGE OF MEDICINE,K22,2006,135000,-0.022263823408531076
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,7015648,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2006,500182,-0.0018362094507481824
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7007706,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2006,153843,-0.038083631571043094
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,7012338,R01AI064913,"['B lymphocyte', 'T lymphocyte', 'allergens', 'aminoacid', 'antigens', 'artificial intelligence', 'bioinformatics', 'chemical property', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'dengue virus', 'immunoglobulin E', 'major histocompatibility complex', 'mathematical model', 'molecular biology information system', 'peptides', 'physical property', 'protein binding', 'stereochemistry', 'structural biology']",NIAID,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,2006,258041,-0.022524662036093726
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,7068069,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2006,19532,-0.00798455134765196
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,7243612,R33RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R33,2006,350636,-0.003841455981341637
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,7056185,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2006,145987,-0.029992649983428057
"The RPI Exploratory Center for Cheminformatics (RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics (RMI),7125575,P20HG003899,"['Internet', 'NIH Roadmap Initiative tag', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2006,377226,-0.006284365554920959
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,7284550,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2006,498959,-0.03166647403784591
"24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS    DESCRIPTION (provided by applicant):    This conference grant (R13) application requests funds to partially cover the cost of planning, organizing, publicizing and hosting the 24th Annual Symposium on Nonhuman Primate Models for AIDS. The symposium will be held October 4-7, 2006, at the Omni Hotel at CNN Center in downtown Atlanta, Georgia, and will be hosted by the Yerkes National Primate Research Center, Emory University. This meeting is the premier forum for the presentation and exchange of the most recent scientific advances in AIDS research utilizing the nonhuman primate model. The latest findings in primate pathogenesis, immunology, genomics, virology, vaccines and therapeutics will be presented. It is anticipated more than 300 scientists from the United States and other countries will attend. The symposium will encompass five half-day scientific sessions and an evening poster session. The scientific sessions will be: Virology, Pathogenesis, Immunology, Vaccines and Therapeutics/Genomics. Each session will have an invited Chair, a scientific leader in the field, who will give a 30-minute state-of-the-field presentation to open the session, and a Co-Chair from the Scientific Committee, who will moderate the session and entertain questions. In addition, there will be an invited keynote speaker and a banquet speaker, who will address scientific approaches and concerns regarding the global AIDS crisis and related issues of public health. A Scientific Program Committee consisting of eight-ten members drawn from the Yerkes/Emory community and other institutions will review abstracts and assign oral or poster presentations for each of the scientific sessions. Committee members will include leaders in the field from a variety of scientific disciplines. Criteria for selection of oral presentations will include relevance of the topic as well as originality and quality of the information contained in the abstract. Those giving talks will be invited to submit their presentations in manuscript form for publication in the Journal of Medical Primatology. A poster session will include meritorious presentations that cannot be accommodated in one of the platform sessions. A local Organizing Committee will handle arrangements and logistics for the symposium. Feedback from the participants will be obtained through written questionnaires or oral comments to members of the organizing committee. This format has been successfully followed using NCRR support for the previous Annual symposium.           n/a",24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS,7114527,R13RR022961,"['AIDS', 'Primates', 'disease /disorder model', 'meeting /conference /symposium', 'travel']",NCRR,EMORY UNIVERSITY,R13,2006,63089,-0.005397591241465466
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,7099789,K25AI064625,"['Poxviridae disease', 'cytokine', 'model']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,K25,2006,27000,-0.005309837165667322
"Cytokine Signaling Network Response to Smallpox Vaccine    DESCRIPTION (provided by applicant):  The identification of the molecular and cellular basis for adverse events observed in patients immunized against smallpox is of great public health interest. This is especially true today given efforts to defend the U.S. population and military against potential bioterrorism agents. We have shown recently that adverse vents following smallpox vaccination correlate with systemic cytokine patterns, suggesting a role for cytokines in the pathogenesis of adverse events. A challenge to further delineating the immunological mechanisms of adverse events is that cytokines rarely act in isolation to induce an immune response, but rather they work in a complex network to which immune system cells respond. Cytokines are small signaling proteins that integrate the activities of immune system cells. While it is often well understood how they act individually, the behavior of cytokines as part of a signaling network is less well known and likely depends on the nature of the infecting organism. We propose to develop and evaluate a comprehensive strategy to identify detailed kinetic cytokine network models associated with adverse events following smallpox vaccination. This strategy will be developed and evaluated using proteomic time-series data available from 103 volunteers that are part of an ongoing NIAID/NIH-sponsored trial to evaluate the Aventis Pasteur Smallpox Vaccine (APSV). We will develop software tools using machine learning algorithms to automatically discover cytokine signaling network models from observed time-series cytokine expression levels. Once the underlying cytokine network model has been estimated, our goal is to use the dynamic model as a simulation tool to suggest ways to create vaccines that minimize the risk of adverse events associated with vaccination. The software tools developed in this proposal will be generally applicable for biomedical research to understand the biochemical interactions in time-series data, and, thus, a useful and novel software package will be made available to the vaccine research community. The experience and knowledge gained during the collaboration on this important research problem in immunology coupled with the didactic and mentoring portions of the training program will create a firm foundation upon which I can develop and test significant hypotheses for future studies on the immunology of infectious diseases.             n/a",Cytokine Signaling Network Response to Smallpox Vaccine,7389130,K25AI064625,[' '],NIAID,UNIVERSITY OF ALABAMA AT BIRMINGHAM,K25,2006,73818,-0.005309837165667322
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6916483,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,321788,-0.02724331198114096
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6946761,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2005,100000,0.0023709473712852838
"Simulation Algorithms for Spatial Pattern Recognition    DESCRIPTION (provided by applicant):    This SBIR project is developing methods and software for the specification, construction and simulation of neutral spatial models, and for applying these neutral models within the framework of probabilistic pattern recognition. Results will allow epidemiologists, environmental scientists and image analysts across a broad range of commercial disciplines to more accurately identify patterns in spatial data by removing the bias towards false positives that is caused by unrealistic null hypotheses such as ""complete spatial randomness"" (CSR). This project will accomplish 5 aims:      1. Conduct a requirements analysis to specify the neutral models and functionality to incorporate in the software.   2. Develop and test a software prototype to evaluate feasibility of the proposed models.   3. Propose a topology of neutral models and develop strategies to generate them and to conduct sensitivity analysis for investigating the impact of implicit assumptions (i.e. spatial autocorrelation or non-uniform risk) and number of realizations on test results.   4. Incorporate the neutral models in the first commercially established software package that allows for user-specified alternate hypothesis in spatial statistical tests.   5. Apply the software and methods to demonstrate the approach and its unique benefits for exposure and health risk assessment.      Feasibility of this project was demonstrated in the Phase I. This Phase II project will accomplish aims three through five. These technologic, scientific and commercial innovations will revolutionize our ability to identify, document and assess the probability of spatial patterns relative to neutral models that incorporate realistic local, spatial and multivariate dependencies. The neutral models and methods in this proposal make possible, for the first time ever, evaluation of the sensitivity of the results of cluster or boundary analyses to specification of the null hypothesis.         n/a",Simulation Algorithms for Spatial Pattern Recognition,6863029,R44CA092807,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'human data', 'image processing', 'imaging /visualization /scanning', 'statistics /biometry', 'visual cortex']",NCI,BIOMEDWARE,R44,2005,498368,-0.0018362094507481824
"Shifting Conceptions of Human Identity DESCRIPTION (provided by applicant):  . One of the most important questions raised by the ongoing achievements of the Human Genome Project is how this new biological knowledge - and the powers it confers - will affect our identity and self-understanding as human beings. This book project focuses on one key aspect of this complex issue: exploring the extent to which human identity can be reconciled with deliberate design or partial redesign. The author proposes to shed new light on this question by comparing the debates surrounding two areas of scientific innovation that are not normally associated with each other, but that are in fact deeply related: the enterprise of human genetic intervention and the enterprise of building intelligent machines. Both these enterprises entail ""pushing the limits"" of traditional concepts of what it means to be human; and both ultimately confront their makers with the same core ""family"" of questions: What are the defining features of human personhood? To what extent can those features be modified or extended, before human personhood begins to break down? Can some (or all) of those features find embodiment in an entity other than a human being? These kinds of questions are no longer the sole province of science fiction writers, but have been taken up with increasing seriousness by mainstream scientists and technologists, as well as by a wide array of ""science watchers"" in academia, legislative circles, and the news media.   . Through documentary research and interviews, this project aims to deepen our understanding of the history and sociology of the debates surrounding these powerful new technologies, electro-mechanical and biological, that are perceived as destabilizing human identity. The intended audience for the book is a broad one: scientists and technological practitioners interested in the social and cultural reception of their research; legislators and other policymakers with a stake in the governance of science; general educated readers who are concerned about the role of science and technology in shaping our collective future. n/a",Shifting Conceptions of Human Identity,6915830,R03HG003298,"['adult human (21+)', 'artificial intelligence', 'behavioral /social science research tag', 'biotechnology', 'books', 'clinical research', 'ethics', 'genetic manipulation', 'history of life science', 'human subject', 'identity', 'interview', 'robotics', 'self concept', 'sociology /anthropology']",NHGRI,VANDERBILT UNIVERSITY,R03,2005,75833,-0.02667018196340612
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,-0.0013779146393269933
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6908174,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2005,375000,-0.013149391556868626
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6850134,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2005,297104,-0.010418463813694432
"CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE   The applicant is an Instructor in Pediatrics at Harvard Medical School and an associate in bioinformatics and pediatric endocrinology at Children's Hospital, Boston. The applicant completed an NLM-funded fellowship in informatics and received a Masters Degree in Medical Informatics from MIT. Since completing his fellowship less than two years ago, he has first-authored six publications, co-authored eight publications, senior authored two publications, and co-authored a book on microarray analysis. The applicant plans to pursue a career in basic research in diabetes genomics and bioinformatics, with a joint appointment in both an academic pediatric endocrinology department and a medical informatics program. The mentor is Dr. Isaac Kohane, director of the Children's Hospital Informatics Program with a staff of 20 including 10 faculty and extensive computational resources, funded through several NIH grants.       The past 10 years have led to a variety of measurements tools in molecular biology that are near comprehensive in nature. For example, RNA expression detection microarrays can provide systematic quantitative information on the expression of over 40,000 unique RNAs within cells. Yet microarrays are just one of at least 30 large-scale measurement or experimental modalities available to investigators in molecular biology. We see scientific value in being able to integrate multiple large-scale data sets from all biological modalities to address biomedical questions that could otherwise not be answered. We recognize that the full agenda of working out the details for all possible inferential processes between all near-comprehensive modalities is too large. The goal of this project is to serve as a model automated system for gathering data related to particular experimental characteristic and perform inferential operators on these data. For this application, we are focusing on a pragmatic subset. Specifically, we propose intersecting near comprehensive data sets by phenotype, and intersecting lists of significant and related genes within these data sets in an automated manner.      The central hypothesis for this application is that integrating large-scale data sets across measurement  modalities is a synergistic process to create new knowledge and testable hypothesis in the area of diabetes, and inferential processes involving intersection across genes can be automated. n/a",CREATION AND APPLICATION OF A DIABETES KNOWLEDGE BASE,7125331,K22LM008261,"['RNA interference', 'adipocytes', 'artificial intelligence', 'automated data processing', 'cell differentiation', 'clinical research', 'computer system design /evaluation', 'diabetes mellitus genetics', 'human data', 'information systems', 'insulin sensitivity /resistance', 'noninsulin dependent diabetes mellitus', 'obesity', 'phenotype', 'quantitative trait loci', 'vocabulary', 'weight gain']",NLM,STANFORD UNIVERSITY,K22,2005,152083,-0.038083631571043094
"Computational tools for T- and B-cell epitope prediction DESCRIPTION (provided by applicant): In the proposed work, we will develop software tools to predict T- and B-cell epitopes of allergenic and viral proteins. The approach is based on novel quantitative descriptors of the physical-chemical properties of amino acids developed recently by our group. The primary goal of the new approach is to use a minimal number of variables to establish the classification procedures and QSAR models. The novel descriptors of physical-chemical properties of amino acids will be used in combination with a partial least squares approach to reduce the number of variables in the discriminant analysis and in artificial neural networks. Algorithms based on multivariate classification, K-nearest-neighbor methods, support vector machines and neural networks will be developed and assessed by cross-validation for their ability to predict T- and B-cell epitopes in proteins. The resulting QSAR models/database approach can then be used to identify immunogenic epitopes in the proteins of pathogens for vaccine development and drug design. IgE epitopes, archived in our web-based, relational Structural Database of Allergenic Proteins (SDAP), will be used to develop the Bcell epitope prediction methods. Stereochemical variability plots will also be used to predict functional and immunological determinants on proteins from Dengue virus (DV). This information can aid in the design of vaccines that better stimulate neutralizing T- and B-cell responses to diverse variants of DV. The validated suite of software tools to identify and classify immunogenic peptides will be made available to the scientific community as a Web server, similar to SDAP. Collaborations with experimental groups will enable the practical applications of the tools, which include predicting the allergenicity of novel foods and drugs, improving specific immunotherapies for allergy and asthma, and vaccine design. n/a",Computational tools for T- and B-cell epitope prediction,6916795,R01AI064913,"['B lymphocyte', 'T lymphocyte', 'allergens', 'aminoacid', 'antigens', 'artificial intelligence', 'bioinformatics', 'chemical property', 'chemical structure', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'dengue virus', 'immunoglobulin E', 'major histocompatibility complex', 'mathematical model', 'molecular biology information system', 'peptides', 'physical property', 'protein binding', 'stereochemistry', 'structural biology']",NIAID,UNIVERSITY OF TEXAS MEDICAL BR GALVESTON,R01,2005,264250,-0.022524662036093726
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6937143,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2005,20000,-0.00798455134765196
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6914863,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2005,167918,-0.003841455981341637
"Statistical Methods for Genomic and Proteomic Data    DESCRIPTION (provided by applicant): We propose developing, evaluating and comparing statistical methods in analyzing and interpreting microarray data, including a heart failure dataset collected in the co-Principal Investigator's lab. Some of the proposed methods will incorporate or be applied to other types of genomic or proteomic data. In Aim A.1, we consider detecting differential gene expression. A weighted permutation scheme is proposed to improve permutation-based inference procedures, and these methods will be compared with several recently proposed parametric and semi-parametric methods. We also propose incorporating existing biological data in the statistical methods. In Aim A.2, we study a clustering-based classification (CBC) method for gene function prediction using microarray data. CBC will be compared with other state-of-the-art supervised machine learning algorithms, such as support vector machines and random forests. Other sources of biological data, such as protein-protein interaction data, will be incorporated in the proposed method. In Aim A.3, we consider sample classification and prediction based on gene expression profiles in a general framework called penalized partial least squares (PPLS). PPLS will be compared with other supervised machine learning algorithms. We will extend PPLS to combine microarray data from multiple studies. We plan to implement the proposed statistical methods in R and make the software publicly and freely available.         n/a",Statistical Methods for Genomic and Proteomic Data,6922406,R01HL065462,"['clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'functional /structural genomics', 'human data', 'mathematical model', 'microarray technology', 'model design /development', 'proteomics', 'statistics /biometry']",NHLBI,UNIVERSITY OF MINNESOTA TWIN CITIES,R01,2005,174500,-0.029992649983428057
"The RPI Exploratory Center for Cheminformatics(RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics(RMI),7032113,P20HG003899,"['Internet', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2005,375639,-0.006284365554920959
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS    DESCRIPTION (provided by applicant): The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methods within the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data, earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user interface to support access to information resources for enhanced decision-making by practitioners. The long-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3 Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technology work; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C).             n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7084856,P01CD000261,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2005,1270432,-0.017710684672158795
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6848697,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2005,1446246,-0.03166647403784591
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6943136,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2005,467823,-0.030854685001902013
"Computer cluster for computational biology DESCRIPTION (provided by applicant):    The present application aims to establish a computer Cluster for Computational Biology and Bioinformatic (CCBB). The cluster will consists of 256 dual nodes connected with Giganet switches to enable rapid communication between the processors. The cluster will enable the integration of the two approaches and make it possible to effectively address the highly demanding computational tasks of the field. It will serve a small group of investigators, supported by the NIH, and their close collaborators. The hardware needs of computational biology and bioinformatic applications, and of the team of investigators listed in this application can be summarized as follows:   1. Significant computer power for complex and expensive simulations.   2. Large storage capacity for the whole cluster (shared) and (separately) for the individual nodes.   3. Large and rapidly accessible memory for effective statistical analysis, application of machine learning techniques, and biological discovery.   4. Fast network for information updates across the network.   In addition CCBB will have high level of databases and software integration including   1. Updates of important ""mirrors"" of shared databases (such as NR, swissprot, human EST, human genome, protein databank, etc.)   2. Local installation and frequent upgrade of widely used software packages (e.g. BLAST, Pfam, CHARMm etc.)   3. Help in porting novel software for optimal use on the CCBB hardware platform.   The combined unification of optimal hardware and software for computational biology and bioinformatic will make the new cluster; an outstanding resource for NIH related research n/a",Computer cluster for computational biology,6877645,S10RR020889,"['bioinformatics', 'biomedical equipment purchase', 'computational biology', 'computer network', 'computer program /software', 'computer system hardware', 'computers']",NCRR,CORNELL UNIVERSITY ITHACA,S10,2005,500000,-0.003709925226590543
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6799187,R01RR014477,"['X ray crystallography', 'artificial intelligence', 'automated data processing', 'chemical structure', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'crystallization', 'data collection methodology /evaluation', 'image processing', 'mathematics', 'method development', 'protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,321983,-0.02724331198114096
"LiFESim: Software for health science education (NCRR)    DESCRIPTION (provided by applicant): Stottler Henke Associates in collaboration with Teachers College, Columbia University, proposes to build a software system for teaching scientific inquiry in the context of nutrition science, The goal of the proposed research is to develop a computer based instructional system - called LiFESim - that teaches accurate and detailed information about food and the food system -- from production of food on the farm through food processing and transportation, to impacts of food on personal health and on the natural environment in terms of waste and pollution. The software will complement an existing health science curriculum, developed at Teachers College for 4th-6th graders, called ""Linking Food and the Environment"" or LIFE, developed from an NIH Science Education Partnership Award (SEPA) RR 12374 (1997-2004). Our system will be based on the paradigm of role-playing simulation used in such popular computer games as SimCity and The Sims: students using the software assume roles in a simulated environment and learn from the consequences of the decisions that they make in those roles. Using the simulation paradigm students will be able to explore the dynamics of large-scale systems, such as those the food transportation system in ways that are not possible with the existing curriculum. For example, the simulation would allow students to explore the impact of changes in transportation patterns on food delivery. Our system will provide explicit coaching in applying scientific methods for investigation. We will also explore learning strategies that will encourage students to critically examine - and hopefully improve - their dietary choices. We will complement simulation-based learning with two other artificial intelligence based methodologies - the use of lifelike pedagogical agents, and the use of case-based reasoning. During Phase I, we will develop a set of detailed instructional goals, use these to develop an initial system design, develop a limited prototype of the system, and then develop and perform an informal pilot study to evaluate the viability of our design. The pilot study will be conducted over a one-month period at schools in Hayward, California and New York City. Our Phase II effort will focus on developing an extensive design and performing detailed use testing of the system developed during Phase I.         n/a",LiFESim: Software for health science education (NCRR),6790401,R43RR019780,"['artificial intelligence', 'bioengineering /biomedical engineering', 'clinical research', 'computer assisted instruction', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'education evaluation /planning', 'educational resource design /development', 'environmental contamination', 'food', 'food processing /preparation', 'health education', 'human subject', 'interactive multimedia', 'nutrition', 'nutrition related tag', 'science education']",NCRR,"STOTTLER HENKE ASSOCIATES, INC.",R43,2004,100000,-0.007669043339568485
"BioMediator: Biologic Data Integration& Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration& Analysis System,6805962,R01HG002288,"['artificial intelligence', 'bioengineering /biomedical engineering', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2004,100000,0.0023709473712852838
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6703756,R44CA093112,"['artificial intelligence', 'clinical research', 'computer data analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'mathematics', 'statistics /biometry']",NCI,"CYTEL, INC",R44,2004,411387,-0.00023776650457032605
"AI Software for Science Education Related to Drug Abuse DESCRIPTION (provided by applicant): This Phase I SBIR proposal is aimed at advancing the state of the art in chemistry education software in a critically important respect demanded by students, teachers, administrators and Quantum Simulations, Inc. customers. The focus of this innovation is the development of meaningful interactive tutoring and assessment capabilities for chemistry problem solving. Empowerment of students to make the proper decisions about drugs through experiencing the scientific process requires a solid education in basic chemistry. Chemical formulas comprise much of the fundamental ""language"" of chemistry in which students must be fluent in order to succeed. The topic of writing and understanding chemical formulas, a cornerstone of all general chemistry classes, is a reasonable starting point for the development of an AI assessment system for student learning in chemistry. A solid understanding of chemical formulas is a prerequisite to success in chemistry required not only for literacy to make informed decisions about drugs from a scientific standpoint, but also to enable and prepare students to pursue careers in research related to drug abuse. Quantum has already successfully developed and commercialized an ITS for writing chemical formulas which will be used as the starting point for the present work. The proposed technology will benefit all students; however, it is specifically targeted to help those who have the greatest need, such as students of average or marginal performance and students from historically underserved groups, by lowering barriers to accessing high-quality science instructional software.  Quantum customers include textbook publishers, software providers, hardware vendors and distance learning companies. A prominent textbook publisher, Holt, Rinehart and Winston, has entered into two long-term contracts with Quantum, resulting in rapid dissemination to an established end user base. Quantum intends to employ an identical business model to commercialize the results of this project. n/a",AI Software for Science Education Related to Drug Abuse,6831228,R43DA018455,"['adolescence (12-20)', 'artificial intelligence', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'drug abuse education', 'educational resource design /development', 'human subject', 'science education']",NIDA,"QUANTUM SIMULATIONS, INC.",R43,2004,61750,-0.01873589337383663
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,-0.0013779146393269933
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,-0.0013779146393269933
"Development of Bioinformatic Tools for Virtual Cloning  DESCRIPTION (provided by applicant): The elaboration of the sequences of the human genome and those of many cellular and viral parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease. It has also resulted in the formation of an entirely new field, bioinformatics, which promises to manage and analyze the vast amount of data being generated. Bioinformatics needs to supply tools for data analysis and tools for experimental design. Most of the scientific and corporate resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology. Essentially every experiment in biology now begins with cloning one or more pieces of DNA. Commercial software that facilitates virtual DNA cloning does exist, but it lacks any automation features and depends on primitive and/or fragmentary gene and vector databases. It is inadequate in planning the hundreds or thousands of clones necessary to address questions posed by the proteomics initiatives, because the lack of knowledge integration. In Phase I of this SBIR grant, we have built and tested a virtual cloning expert system, along with a very useful gene database and a uniquely annotated vector database that serve as a knowledge base for automated DNA manipulations. A collection of automated cloning modules and databases is now functional. In Phase II we will complete the virtual cloning expert system and develop a flexible platform for automated experimental design, data management and analysis. We will also construct a user database, improve the user interface and establish security protocols. The results will be a complete program suite as a stable and marketable product. n/a",Development of Bioinformatic Tools for Virtual Cloning,6788945,R44HG003506,"['artificial intelligence', 'bioinformatics', 'biomedical automation', 'computer assisted sequence analysis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'data management', 'experimental designs', 'expression cloning', 'genetic library', 'high throughput technology', 'molecular biology information system', 'molecular cloning']",NHGRI,"VIRMATICS, LLC",R44,2004,375000,-0.013149391556868626
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6701378,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2004,297104,-0.010418463813694432
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6849505,R01AR042739,"['artificial intelligence', 'bone density', 'bone fracture', 'clinical research', 'computer assisted diagnosis', 'densitometry', 'diagnosis design /evaluation', 'disease /disorder proneness /risk', 'hip', 'human subject', 'information systems', 'limbs', 'mathematical model', 'noninvasive diagnosis', 'osteoporosis', 'photon absorptiometry', 'radiography', 'spine', 'women&apos', 's health']",NIAMS,UNIVERSITY OF CHICAGO,R01,2004,105415,-0.010418463813694432
"Neuroinformatic Analysis of Olfactory Coding DESCRIPTION (provided by applicant): Our goal is to use neuroinformatics to help resolve the conflicting findings from which two models of olfactory coding have emerged. One model proposes that very many low-specificity neural responses represent each odorant and the other model suggests that fewer, more specific olfactory receptors bind to particular molecular features and that the combination of these specific responses characterizes each odorant. Since much of the data supporting the low-specificity model has been collected without regard for the exquisite spatial heterogeneity of the olfactory system, it is possible that the differences in conclusions could be resolved if the distinct types of data that are collected by various laboratories were placed into spatial register with one another. To that end, we have been building an archive of the spatial patterns of glomerular responses evoked by a wide range of odorants, and we have been able to test hypotheses regarding strategies of olfactory coding by calculating homologies across glomerular-layer response patterns. To facilitate our analytical task, and to make it feasible for others to place their data in register with this odorant response archive, we propose to continue to develop analytical and visualization software for olfactory bulb data. We also propose to extend this approach to both the olfactory epithelium and olfactory cortex to be able to understand both the initial coding and synthetic levels of the olfactory system.  These efforts will be freely available via the web site on which our olfactory activity archive is posted. We propose to improve the site by incorporating meta-data, as well as data from labs using other species and other types of data, such as lesions and neurophysiological data that can be located in space. Finally, the wide range of odorants that we must test to capture a sense of the system also will necessitate the use of an informatics approach to allow us to test hypotheses regarding the complex means by which chemical structure is represented in the system. The combination of these approaches should help resolve the differences between the conflicting models of olfactory coding. n/a",Neuroinformatic Analysis of Olfactory Coding,6803809,R01DC006516,"['artificial intelligence', 'automated data processing', 'bioinformatics', 'chemoreceptors', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'image processing', 'information retrieval', 'laboratory mouse', 'laboratory rat', 'mathematical model', 'meta analysis', 'neural information processing', 'nucleic acid sequence', 'olfactions', 'olfactory lobe', 'olfactory stimulus', 'respiratory epithelium', 'sensory mechanism', 'stimulus /response']",NIDCD,UNIVERSITY OF CALIFORNIA IRVINE,R01,2004,20000,-0.00798455134765196
"Vortex Tubed Thermocycler with Intelligent Software    DESCRIPTION (provided by applicant): A novel system is proposed for the rapid identification of DNA. The system comprises of a unique thermocycler platform built around the extraordinary vortex tube, detection optics, intelligent software to provide users with information on most ideal operating conditions, and virtual insight into the PCR process as it progresses. An intelligent user interface will allow DNA amplification/detection on an unprecedented timescale (less than 10 minutes). A multi-disciplinary team that consists of a chemical engineer, a mechanical engineer, and two biochemists has been assembled for this project. The efficiency of the vortex tube will be optimized by the use of computational fluid dynamics. Heat transfer between the gas phase and the cuvets will be improved through computational fluid dynamic calculations. The intelligent software consists of a detailed mathematical model that uses similar starting conditions as the initial cuvet composition to model the amplification progress and it performs a virtual PCR in parallel with the actual process. The virtual PCR will become a quantitative tool point-of-care diagnosis of a wide variety of heritable and infectious diseases. The virtues of the intelligent vortex tube PCRJet are: speed, versatility, reliability, portability and low cost.         n/a",Vortex Tubed Thermocycler with Intelligent Software,6810083,R21RR020219,"['DNA', 'artificial intelligence', 'bacterial DNA', 'bioengineering /biomedical engineering', 'bioinformatics', 'biomedical equipment development', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'diagnostic tests', 'mathematical model', 'neoplasm /cancer genetics', 'nucleic acid amplification techniques', 'nucleic acid purification', 'nucleic acid quantitation /detection', 'optics', 'polymerase chain reaction', 'portable biomedical equipment', 'virus DNA']",NCRR,UNIVERSITY OF NEBRASKA LINCOLN,R21,2004,178840,-0.003841455981341637
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6733529,R01LM007273,"['Internet', 'behavioral /social science research tag', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'confidentiality', 'data management', 'decision making', 'health care facility information system', 'health care policy', 'human data', 'human rights', 'information dissemination', 'information retrieval', 'mathematical model', 'medical records', 'model design /development', 'patient oriented research', 'statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2004,406979,-0.02437540144633732
"Attractors of Complex Signal Transduction Systems   DESCRIPTION (provided by the applicant): Biochemical signal transduction is a        critical basic cellular process by which information travels from the receptors      on the surface of a cell to the nucleus where responses to the signals are           generated. Originally thought to be a relatively simple system that merely           passed along signals, signal transduction has proven to be a highly complicated      system of molecules whose interactions have yet to be fully understood.              Furthermore, a number of disease states such as cancer, diabetes, and                neurological disorders have their origins in malfunctioning signal transduction      components. Until signal transduction is understood in its totality, rational        intervention of these diseases will be significantly hampered. While                 conventional laboratory methods have been used to study individual components        or pathways involved in signal transduction, they have less utility in studying      how signal transduction systems work as a whole and therefore do not detect          higher levels or organization and regulation that are known to exist in other        complex systems. In this application, it is proposed that signal transduction        systems be mathematically modeled and analyzed in order to determine the role        of complexity in the system. It is hypothesized that signal transduction             involves both the movement and processing of information in such a way as to         provide a cell the capacity for decision making, something every cell must           possess at some level. The most advanced techniques in the field of chaos            theory and neural networks (the basis of artificial intelligence) will be            applied to determine whether biochemical signal transduction systems have the        fundamental features of a complex system, how those features might be used for       cell level decision making, and how mutations in the system might affect the         higher level functions of the system. The elucidation of such higher levels of       function in signal transduction systems would greatly enhance our understanding      of this basic cellular function and make the rational design of therapy more         effective.                                                                                                                                                                n/a",Attractors of Complex Signal Transduction Systems,6745076,R01GM067272,"['biological information processing', 'biological signal transduction', 'computational neuroscience', 'computer simulation', 'mathematical model', 'mathematics', 'model design /development']",NIGMS,UNIVERSITY OF NEBRASKA OMAHA,R01,2004,314225,-0.026933599140012674
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6795323,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2004,754129,-0.030854685001902013
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6699330,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2004,1405759,-0.03166647403784591
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6777517,R43EB000608,"['archives', 'clinical research', 'computer data analysis', 'computer system design /evaluation', 'confidentiality', 'data management', 'health care policy', 'health related legal', 'human data', 'imaging /visualization /scanning', 'information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2004,149200,-0.02240941625949693
"Tree Ensemble Regression and Classification Methods    DESCRIPTION (provided by applicant):    This SBIR aims to produce next generation classification and regression software based upon ensembles of decision trees: bagging, random forests, and boosting. The prediction accuracy of these methods has caused much excitement in the machine learning community, and both challenges and complements the data modeling culture prevalent among biostatisticians. Recent research extends the methodology to likelihood based methods used in biostatistics, leading to models for survival data and generalized forest models. Generalized forest models extend regression forests in the same way that generalized linear models extend linear models.      This software would apply broadly, including to medical diagnosis, prognostic modeling, and detecting cancer; and for modeling patient characteristics like blood pressure, discrete responses in clinical trials, and count data.      Phase I work will prototype software for survival data, and investigate the performance of ensemble methods on simulated and real data. For survival applications, we will assess out-of-bag estimates of performance, and investigate measures of variable importance and graphics that help clinicians understand the results. Experience writing prototypes and using them on data will lead to a preliminary software design that serves as the foundation of Phase II work.      Phase II will expand upon this work to create commercial software. We will research and implement algorithms for a wider range of applications including generalized forest models, classification, and least squares regression. We will also implement robust loss criteria that enable good performance on noisy data, and make adaptations to handle large data sets.      This proposed software will enable medical researchers to obtain high prediction accuracy, and complement traditional tools like discriminant analysis, linear and logistic regression models, and the Cox model.         n/a",Tree Ensemble Regression and Classification Methods,6832086,R43CA105724,"['clinical research', 'computer assisted medical decision making', 'computer graphics /printing', 'computer human interaction', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'human data', 'mathematical model', 'method development', 'model design /development', 'neoplasm /cancer classification /staging', 'neoplasm /cancer diagnosis', 'neoplasm /cancer remission /regression', 'prognosis', 'statistics /biometry']",NCI,INSIGHTFUL CORPORATION,R43,2004,99937,-0.003148954715079108
"Artificial Intelligence Methods for Crystallization DESCRIPTION (provided by applicant): It is widely believed that crystallization is the rate-limiting step in most X-ray structure determinations. We have therefore been developing computational tools to facilitate this process, including the XtalGrow suite of programs. Here we propose to improve the power and scope of these tools along two fronts: 1. Initial screening, the (iterative) set of experiments that hopefully, yields one or more preliminary ""hits"" (crystalline material that is demonstrably protein); and 2. Optimization experiments that begin with an initial hit and end with diffraction-quality crystals. A central concept of this proposal is that this tool building requires a knowledge-based foundation. Therefore, one of the broad goals of the proposal is to develop a framework for the acquisition and encoding of knowledge in computationally tractable forms; specifically, forms that will yield more effective crystallization procedures. We are interested in how the data interact and how that can be used to improve the crystallization process. While available data, both in the literature and from other projects in the laboratory will continue to be used wherever possible, our analysis has also demonstrated the need to be pro-active i.e. to gather selected data required to complete the knowledge base. We propose to do this by: I. Deepening the data representations is several areas including additional protein characteristics, incorporating a hierarchy of chemical additives and acquiring detailed response data. 11. Improving the efficiency of crystallization screens: Initial crystallization screens would be improved by applying inductive reasoning to the refinement of Bayesian belief nets; procedures would also be developed for dealing with the absence of promising results by identifying unexplored regions of the parameter space and using additional measurements, such as dynamic light scattering and cloud point determinations to further refine the Bayesian belief nets and steer experimentation in more promising directions. Optimization screens would be improved by applying Case-Based and Bayesian methods here as well as by further developments of automated image analysis. III. Improving the ""user friendliness,"" integration and automation of the entire system. n/a",Artificial Intelligence Methods for Crystallization,6682996,R01RR014477,"['X ray crystallography', ' artificial intelligence', ' automated data processing', ' chemical structure', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' crystallization', ' data collection methodology /evaluation', ' image processing', ' mathematics', ' method development', ' protein structure function']",NCRR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,298672,-0.02724331198114096
"BioMediator: Biologic Data Integration & Analysis System DESCRIPTION (provided by applicant):    The broad long-term objectives of this proposal are to collaborate with a group of biology researchers with real world needs to develop and distribute a general-purpose system (BioMediator) to permit integration and analysis of diverse types of biologic data. BioMediator will combine information from a variety of different public and private sources (e.g. experimental data) to help answer biologic questions. BioMediator builds on the foundations laid by the currently funded GeneSeek data integration system. The GeneSeek system was originally developed to query only public domain data sources (both structured and semi-structured) to assist in the curation of the GeneClinics genetic testing knowledge base. The specific aims leading to the development of the BioMediator system are: 1) Interface to additional public domain biological data sources (e.g. pathway databases, function databases). 2) Incorporate access to private databases of experimental results (e.g. proteomics and expression array data). 3) Extend model to include analytic tools operating across distributed biological data sources (e.g. across a set of both proteomic and expression array data). 4) Evolve centralized BioMediator system into a model peer to peer data sharing and analysis system. 5) Distribute and maintain BioMediator production software as a resource for the biological community. The health relatedness of the project is that biologists seeking to understand the molecular basis of human health and disease are struggling with large and increasing volumes of diverse data (mutation, expression array, proteomic) that need to be brought together (integrated) and analyzed in order to develop and test hypotheses about disease mechanisms and normal physiology. The research design is to develop BioMediator by combining and leverage recent developments in a) the domain of open source analytic tools for biologic data and b) ongoing theoretical and applied research by members of the current GeneSeek research team on both general purpose and biologic data integration systems. The methods are:  a) to use an iterative rapid prototyping software development model evaluated in a real-world test bed and b) to expand the existing GeneSeek research team (with expertise in informatics, computer science, and software development) to include biological expertise (four biologists forming a biology working group) and biostatistics expertise. The goal is to ensure the BioMediator system 1) meets the needs of a group of end users acquiring, integrating and analyzing diverse biologic data sets, 2) does so in a scaleable and expandable manner drawing on the latest theoretical developments in data analysis and integration. n/a",BioMediator: Biologic Data Integration & Analysis System,6681249,R01HG002288,"['artificial intelligence', ' bioengineering /biomedical engineering', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' information retrieval', ' molecular biology information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2003,100000,0.0023709473712852838
"Markov Chain Monte Carlo and Exact Logistic Regression    DESCRIPTION (provided by applicant): Today, software for fitting logistic regression models to binary data belongs in the toolkit of every professional biostatistician, epidemiologist, and social scientist. A natural follow-up to this development is the adoption of exact logistic regression by mainstream biostatisticians and data analysts for any setting in which the accuracy of a statistical analysis based on large-sample maximum likelihood theory is in doubt. Cutting-edge researchers in biometry and numerous other fields have already recognized that it is necessary to supplement inference based on large-sample methods with exact inference for small, sparse and unbalanced data. The LogXact software package developed by Cytel Software Corporation fills this need. It has been used since its inception in 1993 to produce exact inferences for data generated from a wide range fields including clinical trials, epidemiology, disease surveillance, insurance, criminology, finance, accounting, sociology and ecology. In all these applications exact logistic regression was adopted because the limitations of the corresponding asymptotic procedures were clearly recognized in advance by the investigators and the exact inference was computationally feasible. But most of the time it will not be obvious whether asymptotic or exact methods are applicable. Ideally one would prefer to run both types of analyses if there is any doubt about the appropriateness of the asymptotic inference. However, because of the computational limits of the exact algorithms, investigators are currently inhibited from attempting the exact analysis. There is uncertainty about the how long the computations will take and even whether they will produce any results at all before the computer runs out of memory. The current project eliminates this uncertainty by introducing a new generation of numerical algorithms that utilize network based Monte Carlo rejection sampling. The Phase 1 progress report has demonstrated that these new algorithms can speed up the computations by factors of 50 to 1000 relative to what is currently available in LogXact. More importantly they can predict how long a job will take so that the user may decide whether to proceed at once or at a better time. The Phase 2 effort aims to incorporate this new generation of computing algorithms into future versions of LogXact.         n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6587476,R44CA093112,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' human data', ' mathematical model', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R44,2003,400084,-0.00023776650457032605
"Software to Handle Missing Values in Large Data DESCRIPTION (provided by applicant):    This SBIR aims to produce commercial software for handling missing data in large data sets, where the goal is data mining and knowledge discovery. There may be a large number of subjects, variables, or both. Examples include microarray data, surveys, genomic data, and high throughput screening data.      Handling missing data is one important step of careful data preparation, which is key to the success of an entire project. Missing values often arise in medical data. This is an obstacle because many data mining tools either require complete data or are not robust to missing data.      Principled methods of handling missing data are computationally intensive. Therefore computational feasibility is a challenge to handling missing values in large data sets.      Phase I work will explore strategies such as sampling, constraining parameters, and monotone data algorithms for model based techniques. Factor analysis and multivariate linear mixed effects models will be used to reduce the number of parameters. A variable-by-variable approach using a popular data mining technique, recursive partitioning, will also be used to impute missing values.      For each of the methods, we will write prototype software and test performance on missing data patterns simulated on real data. Several ad hoc techniques will serve as a baseline for comparison.   Experience writing prototypes and using them in simulations will lead to preliminary software design that will serve as the foundation of Phase II work.       This proposed software will enable medical researchers to gain more from their data mining efforts: maximally extracting information and achieving unbiased predictions, despite missing data. n/a",Software to Handle Missing Values in Large Data,6690119,R43RR017862,"['artificial intelligence', ' clinical research', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' human data', ' mathematical model', ' statistics /biometry']",NCRR,INSIGHTFUL CORPORATION,R43,2003,99847,-0.0038588931114742933
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,-0.0013779146393269933
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6626641,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2003,482862,-0.03465166514223818
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6702676,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2003,205127,-0.010418463813694432
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6628097,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2003,297104,-0.010418463813694432
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6646557,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2003,237000,-0.0214552568284855
"Attractors of Complex Signal Transduction Systems   DESCRIPTION (provided by the applicant): Biochemical signal transduction is a        critical basic cellular process by which information travels from the receptors      on the surface of a cell to the nucleus where responses to the signals are           generated. Originally thought to be a relatively simple system that merely           passed along signals, signal transduction has proven to be a highly complicated      system of molecules whose interactions have yet to be fully understood.              Furthermore, a number of disease states such as cancer, diabetes, and                neurological disorders have their origins in malfunctioning signal transduction      components. Until signal transduction is understood in its totality, rational        intervention of these diseases will be significantly hampered. While                 conventional laboratory methods have been used to study individual components        or pathways involved in signal transduction, they have less utility in studying      how signal transduction systems work as a whole and therefore do not detect          higher levels or organization and regulation that are known to exist in other        complex systems. In this application, it is proposed that signal transduction        systems be mathematically modeled and analyzed in order to determine the role        of complexity in the system. It is hypothesized that signal transduction             involves both the movement and processing of information in such a way as to         provide a cell the capacity for decision making, something every cell must           possess at some level. The most advanced techniques in the field of chaos            theory and neural networks (the basis of artificial intelligence) will be            applied to determine whether biochemical signal transduction systems have the        fundamental features of a complex system, how those features might be used for       cell level decision making, and how mutations in the system might affect the         higher level functions of the system. The elucidation of such higher levels of       function in signal transduction systems would greatly enhance our understanding      of this basic cellular function and make the rational design of therapy more         effective.                                                                                                                                                                n/a",Attractors of Complex Signal Transduction Systems,6774537,R01GM067272,"['biological information processing', ' biological signal transduction', ' computational neuroscience', ' computer simulation', ' mathematical model', ' mathematics', ' model design /development']",NIGMS,UNIVERSITY OF NEBRASKA OMAHA,R01,2003,89424,-0.026933599140012674
"Attractors of Complex Signal Transduction Systems   DESCRIPTION (provided by the applicant): Biochemical signal transduction is a        critical basic cellular process by which information travels from the receptors      on the surface of a cell to the nucleus where responses to the signals are           generated. Originally thought to be a relatively simple system that merely           passed along signals, signal transduction has proven to be a highly complicated      system of molecules whose interactions have yet to be fully understood.              Furthermore, a number of disease states such as cancer, diabetes, and                neurological disorders have their origins in malfunctioning signal transduction      components. Until signal transduction is understood in its totality, rational        intervention of these diseases will be significantly hampered. While                 conventional laboratory methods have been used to study individual components        or pathways involved in signal transduction, they have less utility in studying      how signal transduction systems work as a whole and therefore do not detect          higher levels or organization and regulation that are known to exist in other        complex systems. In this application, it is proposed that signal transduction        systems be mathematically modeled and analyzed in order to determine the role        of complexity in the system. It is hypothesized that signal transduction             involves both the movement and processing of information in such a way as to         provide a cell the capacity for decision making, something every cell must           possess at some level. The most advanced techniques in the field of chaos            theory and neural networks (the basis of artificial intelligence) will be            applied to determine whether biochemical signal transduction systems have the        fundamental features of a complex system, how those features might be used for       cell level decision making, and how mutations in the system might affect the         higher level functions of the system. The elucidation of such higher levels of       function in signal transduction systems would greatly enhance our understanding      of this basic cellular function and make the rational design of therapy more         effective.                                                                                                                                                                n/a",Attractors of Complex Signal Transduction Systems,6640799,R01GM067272,"['biological information processing', ' biological signal transduction', ' computational neuroscience', ' computer simulation', ' mathematical model', ' mathematics', ' model design /development']",NIGMS,UNIVERSITY OF NEBRASKA OMAHA,R01,2003,208750,-0.026933599140012674
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6620783,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2003,380761,-0.02437540144633732
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6626535,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2003,1357166,-0.03166647403784591
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6647589,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2003,729100,-0.030854685001902013
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6694270,R43EB000608,"['archives', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data management', ' health care policy', ' health related legal', ' human data', ' imaging /visualization /scanning', ' information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2003,250800,-0.02240941625949693
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6636516,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2003,374063,-0.010663398064252148
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6526728,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2002,372289,-0.01793223409194912
"Improving Quantum Chemistry Calculations DESCRIPTION (provided by applicant): We propose to extend the functionality of our commercial quantum chemistry program, Q-Chem, to effectively treat molecules containing transition metals. This enhanced capability will provide Q-Chem's end-users with the ability to accurately model complex molecules such as proteins, enzymes, and catalysts of industrial importance. While remarkable progress has been made over the last several years in the accurate modeling of systems containing transition metals, current numerical methods for achieving SCF convergence in these systems are problematic at best, resulting in long execution times or, in some cases, complete failure to find a solution. However, a novel computational technique developed at Q-Chem has been shown to dramatically improve convergence for organic molecules with known SCF convergence problems. We propose to adapt this method for use with transition metals. Our goal is to achieve the same robust SCF convergence that is realized for most organic molecules, thereby greatly increasing productivity and extending the capability of scientists to study molecules such as enzymes and industrial catalysts. During Phase (I, our efforts will be to further extend Q-Chem's capability in the molecular biology arena. This proposal seeks to improve the quantum chemical treatment of molecular systems containing transition metals. Transition metal elements are essential to natural biological processes. The technology developed in this research will enable the computer modeling of those systems that are difficult to handle with the current methodologies and therefore increase of the applications of computational modeling. PROPOSED COMMERCIAL APPLICATION: Transition-metal elements play a vital role in biological systems.  The success of this project will improve the performance of modeling of transition-metal complexes and making the modelings possible for the systems that current algorithms fail.  The resulting work will be made available to researchers in health industry and universities through our commercial software Q-Chem. n/a",Improving Quantum Chemistry Calculations,6484828,R43GM065617,"['artificial intelligence', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' heavy metals', ' mathematical model', ' mathematics', ' model design /development', ' quantum chemistry']",NIGMS,"Q-CHEM, INC.",R43,2002,109642,-0.008144942400048372
"Cluster Comparison Methods & the NCI Expression Dataset There is a significant commercial and academic need for new tools that provide quantitative cluster comparison metrics. It is important for pharmaceutical and biotechnology companies to be able to critically evaluate the utility of using different clustering techniques on large high dimensional datasets, in order to make the most informed decisions based upon the clustering results. We propose to evaluate and build bluster comparison metrics, integrating them with high dimensional visualization techniques, so that not only an overall scope, but the cluster distributions can be compared in an intuitive visual fashion. In carrying out our analysis, we will focus on the NCI (approximately 1,400) compound, subset, 118 known mechanism of action compound gene expression dataset analyzed by Scherf, et.al (2000). IN A FOLLOW ON Phase II SBIR Proposal, we will create a robust software package for commercial release where cluster comparison metrics are integrated with the most valuable visualization tools we identify in the Phase I research. PROPOSED COMMERCIAL APPLICATIONS: The Specific Aims of this Phase I proposal will allow us to create new tools where cluster comparison metrics are integrated with high dimensional visualization techniques, so that not only an overall score, but the cluster distributions can be compared in an intuitive visual fashion. We will use the publicly available NCI DIS compound subset, gene expression dataset of Scherf, e.g. al. (2000) to carry out these aims, as ell as data mine this dataset for new discoveries. n/a",Cluster Comparison Methods & the NCI Expression Dataset,6484325,R43CA096179,"['artificial intelligence', ' cancer information system', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' informatics', ' information retrieval', ' mathematics']",NCI,"ANVIL INFORMATICS, INC.",R43,2002,98438,-0.021033147421023822
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6538226,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2002,66954,-0.01644162857208845
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6490198,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2002,331492,-0.01385723712548444
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6489213,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2002,474210,-0.03465166514223818
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6497411,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2002,297104,-0.010418463813694432
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6558149,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2002,10000,-0.010418463813694432
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6526274,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2002,237000,-0.0214552568284855
"Human Subject Research Enhancements Program We propose to enhance the data consistency and integrity of oversight and tracking systems for human subjects research at Mayo Foundation. Our specific aims include: 1) a comprehensive information modeling exercise to understand the interrelationships and dependencies of administrative and clinical data elements related to human subjects research oversight; 2) building common application components that will simplify the creation of research protocols, IRB application, research subject enrollment and consent, and administrative tracking; 3) providing full text and natural language processing based indices to project abstracts, applications, minutes, and administrative notes, to facilitate the authorized searching and retrieval of materials human subject related to human subject review; and 4) coordinating the information model, modular software tools, and textual indexing, as preliminary work for a competitive informatics proposal for adverse event recognition, pattern detection, and the consistent recording of drugs, devices and outcomes measures. n/a",Human Subject Research Enhancements Program,6591449,S07RR018225,"['abstracting', ' behavioral /social science research tag', ' clinical research', ' computer system design /evaluation', ' data collection methodology /evaluation', ' data management', ' health science research support', ' human rights', ' information systems']",NCRR,"MAYO CLINIC COLL OF MEDICINE, ROCHESTER",S07,2002,1,-0.027684208371734593
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6520234,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2002,163400,0.0024168720874926217
"Preserving Privacy in Medical Data Sets Privacy is a fundamental right and needs to be protected.  For health care related d information, there are regulations for disclosure.  These regulations were motivated by the public's concern of breaches of confidentiality that might result in discrimination.  The recent progress in electronic medical record technology, the Internet, and the genetic revolution, together with media reports on violations of privacy have generated increasing interest in this topic.  A common belief is that sensitive information is more easily available with the use of networked computers. Since total lack of disclosure is not realistic, current regulations require that the ""minimal amount"" of information be given to a certain party.  A thorough study on what constitutes ""minimal"" for particular types of applications and a ""usefulness index"" is lacking.  An exact quantification of the potential for privacy breach in de-identified or anonymized databases is also lacking.  Definition and quantification of these indices is important for decision-making.  As we demonstrate, de-identified data sets can still be used for inference and therefore may disclose sensitive information.  The use of machine learning methods to verify the remaining functional dependencies in a de- identified data set leads to better understanding of the possible inferences.  Anonymization techniques based on logic, statistics, database theory, and machine learning methods can help in the protection of privacy. We will formally define and study anonymity in databases, from a theoretical and a practical standpoint.  We will develop and implement algorithms to anonymize data sets that will be in accordance with the balance of anonymity and ""usefulness"" of the disclosed data sets.  We will also develop and implement algorithms to verify the anonymity of a given data set and indicate the type of records that are at highest risk for a privacy attack.  We will make our methods and documented tools freely available to researchers via the WWW. n/a",Preserving Privacy in Medical Data Sets,6421732,R01LM007273,"['Internet', ' behavioral /social science research tag', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' decision making', ' health care facility information system', ' health care policy', ' human data', ' human rights', ' information dissemination', ' information retrieval', ' mathematical model', ' medical records', ' model design /development', ' patient oriented research', ' statistics /biometry']",NLM,BRIGHAM AND WOMEN'S HOSPITAL,R01,2002,384388,-0.02437540144633732
"Attractors of Complex Signal Transduction Systems   DESCRIPTION (provided by the applicant): Biochemical signal transduction is a        critical basic cellular process by which information travels from the receptors      on the surface of a cell to the nucleus where responses to the signals are           generated. Originally thought to be a relatively simple system that merely           passed along signals, signal transduction has proven to be a highly complicated      system of molecules whose interactions have yet to be fully understood.              Furthermore, a number of disease states such as cancer, diabetes, and                neurological disorders have their origins in malfunctioning signal transduction      components. Until signal transduction is understood in its totality, rational        intervention of these diseases will be significantly hampered. While                 conventional laboratory methods have been used to study individual components        or pathways involved in signal transduction, they have less utility in studying      how signal transduction systems work as a whole and therefore do not detect          higher levels or organization and regulation that are known to exist in other        complex systems. In this application, it is proposed that signal transduction        systems be mathematically modeled and analyzed in order to determine the role        of complexity in the system. It is hypothesized that signal transduction             involves both the movement and processing of information in such a way as to         provide a cell the capacity for decision making, something every cell must           possess at some level. The most advanced techniques in the field of chaos            theory and neural networks (the basis of artificial intelligence) will be            applied to determine whether biochemical signal transduction systems have the        fundamental features of a complex system, how those features might be used for       cell level decision making, and how mutations in the system might affect the         higher level functions of the system. The elucidation of such higher levels of       function in signal transduction systems would greatly enhance our understanding      of this basic cellular function and make the rational design of therapy more         effective.                                                                                                                                                                n/a",Attractors of Complex Signal Transduction Systems,6583980,R01GM067272,"['biological information processing', ' biological signal transduction', ' computational neuroscience', ' computer simulation', ' mathematical model', ' mathematics', ' model design /development']",NIGMS,UNIVERSITY OF NEBRASKA OMAHA,R01,2002,209000,-0.026933599140012674
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6533705,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2002,688297,-0.030854685001902013
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6489015,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2002,1329187,-0.03166647403784591
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6520333,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2002,363261,-0.010663398064252148
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6525584,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2002,128860,-0.007935997624638487
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6388359,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2001,362594,-0.01793223409194912
"Markov Chain Monte Carlo and Exact Logistic Regression   DESCRIPTION (provided by applicant): Logistic regression is a very popular           model for the analysis of binary data with widespread applicability in the           physical, behavioral and biomedical sciences. Parameter inference for this           model is usually based on maximizing the unconditional likelihood function.          However unconditional maximum likelihood inference can produce inconsistent          point estimates, inaccurate p-values and inaccurate confidence intervals for         small or unbalanced data sets and for data sets with a large number of               parameters relative to the number of observations. Sometimes the method fails        entirely as no estimates can be found that maximize the unconditional                likelihood function. A methodologically sound alternative approach that has          none of the aforementioned drawbacks is the exact conditional approach in which      one generates the permutation distributions of the sufficient statistics for         the parameters of interest conditional on fixing the sufficient statistics of        the remaining nuisance parameters at their observed values. The major stumbling      block to this approach is the heavy computational burden it imposes. Monte           Carlo methods attempt to overcome this problem by sampling from the reference        set of possible permutations instead of enumerating them all. Two competing          Monte Carlo methods are network based sampling and Markov Chain Monte Carlo          (MCMC) sampling. Network sampling suffers from memory limitations while MCMC         sampling can produce incorrect results if the Markov chain is not ergodic or if      the process is not in the steady state. We propose a novel approach which            combines the network and MCMC sampling, draws upon the strengths of each of          them and overcomes their individual limitations. We propose to implement this        hybrid network-MCMC method in our LogXact software and as an external procedure      in the SAS system.                                                                   PROPOSED COMMERCIAL APPLICATION:  There is great demand for logistic regression software that can handle small, sparse or  unbalanced data sets by exact methods.  Our LogXact package is the only software that  can provide exact inference for data sets which are not ""toy problems"".  Yet even  LogXact quickly breaks down on moderate sized problems.  The new generation of hybrid  network-MCMC algorithms will handle substantially larger problems that nevertheless need  exact inference.  The commercial potential is considerable since such data sets are common  in scientific studies.                                                                                      n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6404971,R43CA093112,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R43,2001,113111,-0.0014816842360050632
"Simulation Algorithms for Spatial Pattern Recognition   DESCRIPTION (provided by applicant): A new generation of satellites is imaging       the earth's surface with unprecedented spatial and spectral resolution. With         the ability to identify local features related to environmental exposures, this      high-resolution imagery is gong to revolutionize health risk assessment. The         realization of this potential depends critically on our ability to recognize         spatial patterns on these large images. This project will develop fast spatial       null models for use in statistical pattern recognition, and will accomplish 4        aims.                                                                                                                                                                     (1) Implement fast simulation algorithms conditioned on properties of the data,      and on spatial functions;                                                            (2) Assess project feasibility by evaluating the performance of these                algorithms on existing high-resolution, hyperspectral imagery;                       (3) Implement the simulation algorithms in 2 commercial spatial analysis             software packages;                                                                   (4) Apply the software and methods to demonstrate the approach and unique            benefits for risk assessment.                                                                                                                                             The phase 1 research will address the first two aims; aims three and four will       be accomplished in phase 2 once feasibility is demonstrated. The technologic         and scientific innovations from this project are expected to greatly enhance         our ability to extract knowledge from high resolution imagery.                       PROPOSED COMMERCIAL APPLICATION:  The imminent launch of over a dozen satellites capable of high-resolution imagery is giving  health researchers powerful new data for relating environmental features to health   outcomes, but existing software packages cannot undertake spatial analysis of these  extraordinarly large data sets.   The fast simulation algorithms from this research will  be incorporated into 2 commercial software packages, providing advanced spatial  analysis for large imagery.                                                                                     n/a",Simulation Algorithms for Spatial Pattern Recognition,6401389,R43CA092807,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' data management', ' image processing', ' imaging /visualization /scanning', ' statistics /biometry']",NCI,BIOMEDWARE,R43,2001,170490,-0.01281131084849832
"Functional Genomics Software   DESCRIPTION (Applicant's abstract): A substantial commercial potential exists        for software tools that allow a biomedical research scientist to use genomic         data to form experimentally testable hypotheses. These will be used to exploit       genomic sequence data to understand the aetiology of disease, to improve             diagnostic tools, and to develop more effective therapies. The Master Catalog,       a commercial product developed jointly by EraGen Biosciences and the Benner          laboratory at the University of Florida, provides a convenient framework for         implementing heuristics that do this. The Master Catalog is a naturally              organized database that contains evolutionary trees, multiple sequence               alignments, and reconstructed evolutionary intermediates for all of the              proteins in the GenBank database. The Benner laboratory has developed and            anecdotally tested heuristics that date events in the molecular history,             provide evidence for and against functional recruitment within a protein             family, detect distant homologs, associate individual residues important for         functional changes with a crystal structure, find metabolic and regulatory           pathways, and correlate events in the molecular record with the history of life      on Earth. This Phase I proposal seeks to validate a set of these heuristics          more broadly to determine their suitability for database-wide application. In        Phase II, we will implement these within the Master Catalog, and launch a            commercial bioinformatics product to support functional analysis of genomic          databases.                                                                           PROPOSED COMMERCIAL APPLICATION:  In its present version, the Master Catalog is a successful commercial product within a  niche: ""best in class"" of bioinformatics databases.  Adding a validated set of heuristics  for extracting functional information from genome databases will make it the software  of choice for most functional genomics work, and be a central tool in the pharmaceutical/  biotechnology industries.  Academic versions and student versions will find markets in most  universities.                                                                                      n/a",Functional Genomics Software,6337786,R41HG002331,"['artificial intelligence', ' biochemical evolution', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' functional /structural genomics', ' informatics', ' molecular biology information system', ' nucleic acid sequence']",NHGRI,"ERAGEN BIOSCIENCES, INC.",R41,2001,96855,-0.027623651109408794
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6343026,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2001,376147,-0.01385723712548444
"Applying Usability to A Knowledge Based System A cancer genetics-tracking database will be redesigned using usability engineering techniques to improve the functionality and usability of the current system. This is important because it will lead to a system that is easier to use and learn, will decrease the chance of errors, and will increase productivity, and user satisfaction. The current state of informatics offers the potential for the creation of tools to assist in the reduction of medical errors. The redesign of this tracking database will be completed through a three-phase process. The first phase will use the results of a usability analysis to redesign and prototype the cancer genetics-tracking database. In the second phase, usability studies will then be conducted to ensure that the system is functional, easy to use, easy to learn, and meets the goals of the users. The usability studies will include heuristic evaluations, keystroke level models, talk-aloud methods, and cognitive walkthrough techniques. The system will be modified based upon the results of these studies. Research will be compiled on the advantages and disadvantages of ICD coding Vs. SNOMED followed by the selection of the most useful system for coding medical information. In the third phase the final redesign will be compared to the old system using a within-subject design to determine if the redesign decreases the error rate, increases productivity, and user satisfaction. This will be followed-up with a survey to determine the perceived usability of the redesigned application. Throughout the redesign process, specific usability guidelines will be developed for designing healthcare software that is computational and knowledge- based in nature.  n/a",Applying Usability to A Knowledge Based System,6340157,F38LM007188,"['artificial intelligence', ' cancer information system', ' cancer registry /resource', ' computer assisted medical decision making', ' computer human interaction', ' computer system design /evaluation', ' family genetics', ' human data', ' neoplasm /cancer genetics']",NLM,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,F38,2001,68753,-0.01644162857208845
"KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY  The proposed work extends previous research performed by this                                                                     investigator in the field of knowledge-based systems for the analysis of             histopathologic material, mainly neoplastic tissue; and mostly focusing on           adenocarcinoma of the prostate. The present application concerns the creation        of methods for the definition and analysis of novel histopathologic features         predominantly derived from nuclear image data, with the objective of defining        ""prototype identities."" According to the investigator, these are fundamental         complexes of histologic features unique to individual tissue samples, or small       groups of such samples, that should convey useful predictive value for               individual patients. Many of these features are not normally discernable to the      eye, even to the experienced observer. They encompass a large number of primary      and derived nuclear morphometric measures, including what are referred to as         ""weak features,"" that is those that have not in the past shown strong                correlative utility by standard statistical measures. Derived measures include       feature and heterogeneity profiles. These data will form the inputs to a logic       network (""inference network"") designed to perform an identification process and      ultimately to generate the prototype identities.                                                                                                                          n/a",KNOWLEDGED BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY,6286183,R01CA053877,"['artificial intelligence', ' breast neoplasms', ' computer assisted diagnosis', ' computer system design /evaluation', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' digital imaging', ' histopathology', ' human tissue', ' hyperplasia', ' image processing', ' information system analysis', ' neoplasm /cancer diagnosis', ' prognosis', ' prostate neoplasms']",NCI,UNIVERSITY OF ARIZONA,R01,2001,465813,-0.03465166514223818
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6333620,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2001,297104,-0.010418463813694432
Computerized Radiographic Analysis of Bone Structure No abstract available n/a,Computerized Radiographic Analysis of Bone Structure,6487190,R01AR042739,"['artificial intelligence', ' bone density', ' bone fracture', ' clinical research', ' computer assisted diagnosis', ' densitometry', ' diagnosis design /evaluation', ' disease /disorder proneness /risk', ' hip', ' human subject', ' information systems', ' limbs', ' mathematical model', ' noninvasive diagnosis', ' osteoporosis', ' photon absorptiometry', ' radiography', ' spine', "" women's health""]",NIAMS,UNIVERSITY OF CHICAGO,R01,2001,10000,-0.010418463813694432
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6351629,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2001,106893,-0.0032796205290322594
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6392266,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2001,232139,0.00016964752238535807
"Intelligent Information Systems for Systems Biology DESCRIPTION (Provided by Applicant): Our Center will attack the challenges created by the large quantity of data generated from new high throughput technologies. We have teamed biologists, computer scientists and computational scientists from several Universities to build an experienced and distinguished team. Our first major tool building project will be an Object Oriented Framework for the integration of data and tools for genomics, proteomics, DNA arrays and protein-protein interactions. This tool will follow the data from the source through model building. It will build on existing open source tools such as a data acquisition package from particle physics (ROOT), a public database system (MYSQL or PostgreSQL), statistics tools (""R""), graphics libraries, a variety of software tools that have been developed at ISB and new tools needed for the new technologies. We stress the use of an open source system as a means to build the community, creating a functioning system that can be tailored for research and education. We then propose to augment this system with tools for analysis, visualization and model building. We will use yeast as a model system owing to the wide range of data that it available for it. Finally, we propose some novel educational programs designed to put graduate students together into interdisciplinary teams for problem solving. n/a",Intelligent Information Systems for Systems Biology,6401728,P20GM064361,"['analytical method', ' artificial intelligence', ' biotechnology', ' computer program /software', ' data management', ' educational resource design /development', ' functional /structural genomics', ' high throughput technology', ' mathematical model', ' method development', ' microarray technology', ' model design /development', ' molecular biology', ' molecular biology information system', ' protein protein interaction', ' proteomics', ' technology /technique development', ' yeasts']",NIGMS,INSTITUTE FOR SYSTEMS BIOLOGY,P20,2001,237000,-0.0214552568284855
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6387141,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2001,163400,0.0024168720874926217
"Educational Tools for Neuroscience   DESCRIPTION (provided by applicant): SHAI proposes to bring two instructional        technologies together to compliment neuroscience lectures and distance               learning. Specifically we want to embed Computer Simulations of experiments and      the chemical, genetic, and physiological systems that underlie them within an        Intelligent Tutoring System. Simulations are excellent tools for revealing the       structure and dynamics of systems to students. They can also serve as a basis        of interactive experiments where students can ""discover"" the answers to              questions. Intelligent Tutoring Systems (ITS) are an emerging educational            technology based on artificial intelligence research. They play the role of          tutor, in that they guide students with appropriate information or                   demonstrations when they are having difficulty with a lesson. They also              adaptively plan the presentation of new lessons based on evaluations of a            student's past performance and knowledge level. The objective of this phase I        proposal is to develop a prototype of NeuroTutor, a simulation-based ITS to          provide students with individualized instruction in a simulation centered            environment. Steps to reaching this objective include designing a curriculum,        developing instructional, presentations and support, developing appropriate          methods for Student Modeling and Diagnosis, and implementing a limited               prototype.                                                                           PROPOSED COMMERCIAL APPLICATION:  This project has a sizeable commercialization potential.  Medical schools and university  neuroscience courses from a significant market.  Moreover the technologies to be   developed are transferable to other domains in the natural and social sciences, business  and medicine.  The technologies used are appropriate for use in distance learning programs,  and can be used by individuals to educate themselves.                                                                                     n/a",Educational Tools for Neuroscience,6403961,R43MH065842,"['computer assisted instruction', ' computer simulation', ' educational resource design /development', ' interactive multimedia', ' neurobiology', ' science education']",NIMH,"STOTTLER HENKE ASSOCIATES, INC.",R43,2001,100000,-0.002575631459955266
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6449653,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2001,658874,-0.030854685001902013
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6230917,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2001,1291932,-0.03166647403784591
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6484619,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,SCRIPPS RESEARCH INSTITUTE,R01,2001,337739,-0.010663398064252148
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6385653,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2001,127154,-0.007935997624638487
"GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES The broad long-term objectives of this proposal are to create and evaluate an infrastructure (GeneSeek) to permit searching across heterogeneous source databases (genomic and citation databases) for relevant information needed for curation of an existing database of clinical knowledge (GeneClinics).  The Specific Aims are: 1) to use a novel, general purpose knowledge representation language to capture the schema of an existing database of clinical knowledge (GeneClinics genetic testing database), 2) to build a shared schema for mediating cross database queries, by extending the schema of GeneClinics and incorporating pertinent schema elements from other structured and semi-structured information sources, 3) to create and test interfaces to the targeted genetic information sources (databases and other structured information) from the shared query mediation schema, 4) to adapt the existing Tukwila data integration system to implement cross database query planning, query execution, and query result aggregation in the context of our shared query mediation schema and the multiple structured (genetic) information sources to create the GeneSeek data integration system, 5) to evaluate the performance of the Tukwila based GeneSeek data integration system and the shared data schema for precision and recall in finding relevant information for curation of a clinical database (GeneClinics genetic testing database). The broad health relatedness of the project is that data integration tools are needed to help clinicians apply the ever- growing body of medical information to patient care.  The tools are needed by curators of databases of medical knowledge as well as by the care providers themselves.  Nowhere is the growth in information more apparent than in the Human Genome project thus the choice of genetics as a domain to test this data integration system.  The specific genetics database whose curation the GeneSeek system will be evaluated against the GeneClinics database.  If successful these data integration systems could be more broadly applied to other domains in biomedicine.  The research design is to apply recent developments in data integration from the artificial intelligence and database areas of computer science to a real world clinical genetics data integration problem to evaluate the applicability of this system to biomedical information retrieval tasks.  The methods are to expand an existing collaboration between the current GeneClinics content and informatics teams and investigators in the Department of Computer science to: 1) enhance the Tukwila data integration architecture and its related CARIN knowledge representation language, and 2) to use these tools and the existing GeneClinics data model to implement and evaluate this data integration system in the specific domain of medical genetics.  n/a",GENESEEK: DATA INTEGRATION SYSTEM FOR GENETIC DATABASES,6031661,R01HG002288,"['artificial intelligence', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' experimental designs', ' information retrieval', ' molecular biology information system', ' vocabulary development for information system']",NHGRI,UNIVERSITY OF WASHINGTON,R01,2000,354198,-0.01793223409194912
ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS No abstract available n/a,ENHANCED ANATOMICAL KNOWLEDGE SOURCES FOR UMLS,6361843,01LM003528,"['anatomy', ' artificial intelligence', ' vocabulary development for information system']",NLM,UNIVERSITY OF WASHINGTON,N01,2000,228331,-0.009372139930767779
"SELECTING AMONG MATHEMATICAL MODELS OF COGNITION DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling       of cognition, it is important to have well-justified criteria for choosing       among differing explanations (i.e., models) of observed data.  This project      investigates those criteria as well as their instantiation in five model         selection methods.                                                                                                                                                Two lines of research will be undertaken.  In the first, a thorough              investigation of model complexity will be conducted.  Comprehensive              simulations re intended to determine complexity's contribution to model fit      and to model selection.  An analytical solution will also be sought with the     hope of quantifying model complexity.                                                                                                                             The second line of work examines the utility of each of the five selection       methods in choosing among models in three topic areas in cognitive               psychology (information integration, categorization, connectionist               modeling), the end goal being to identify their merits and shortcomings.                                                                                          Findings should provide a better understanding of model selection than           currently available and serve as a useful guide for researchers comparing        the suitability of quantitative models of cognition.                                                                                                               n/a",SELECTING AMONG MATHEMATICAL MODELS OF COGNITION,6185788,R01MH057472,"['artificial intelligence', ' choice', ' cognition', ' computer simulation', ' information dissemination', ' mathematical model', ' psychometrics']",NIMH,OHIO STATE UNIVERSITY,R01,2000,77332,0.0031401022715554106
"NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating             Characteristic (ROC) analysis is recognized widely as the best way of            measuring and specifying the accuracies of diagnostic procedures, because it     is able to distinguish between actual differences in discrimination              capacity, on one hand, and apparent differences that are due only to             decision-threshold effects, on the other.  Key methodological needs remain       to be satisfied before ROC analysis can address all of the practically           important situations that arise in diagnostic applications, however.  This       project employs signal detection theory and computer simulation to address       several of those needs, by:  (1) refining and continuing distribution of         software developed previously by the applicants for fitting ROC curves and       for testing the statistical significance of differences between ROC curve        estimates; (2) developing and evaluating new algorithms for ROC                  curve-Fitting and statistical testing, based on their recently-developed         ""proper"" binormal model, that should provide more meaningful results in          experimental situations that involve small samples of cases; (3)                 investigating the usefulness of a form of ROC methodology that is based on       mixture distributions in order to rduce the need for diagnostic truth in ROC     experiments; (4) investigating the effect of case-saple difficulty on the        statistical power tests for differences between ROC curves, in order to          determine the optimal difficulty of cases that shouldbe studied on rank          diagnostic systems; and (5) developing methods for training artificial           neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC           analysis and signal detection theory.                                             n/a",NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY,6181168,R01GM057622,"['artificial intelligence', ' computer assisted diagnosis', ' computer system design /evaluation', ' method development', ' statistics /biometry']",NIGMS,UNIVERSITY OF CHICAGO,R01,2000,218176,-0.018886064853082613
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,-0.016810761585801266
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,-0.016810761585801266
"KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA Abstractions of time-stamped clinical data are useful for planning               therapy, for monitoring therapy, and for creating high-level summaries of        time-oriented clinical databases.  Temporal abstractions also support            explanations by an intelligent patient-record system and can be used for         representation of the goals and intentions of clinical guidelines and            protocols.                                                                                                                                                        We propose to reengineer and expand the scope of the RESUME system, a            prototype computer program that implements the knowledge-based temporal-         abstraction method, a conceptual and computational framework that we have        developed for abstraction of time-stamped clinical data into clinically          meaningful interval-based concepts. RESUME has been evaluated with highly        encouraging results in several clinical areas. We will address the               practical and theoretical issues of representation, acquisition,                 maintenance, and reuse of temporal-abstraction knowledge. Our specific           aims are defined by a four-step research plan:                                                                                                                    1. We will define formally the knowledge requirements for five                   computational modules (mechanisms) we employ, thus facilitating the              acquisition, maintenance, reuse, and sharing of the required knowledge.                                                                                           2. We will enhance, expand, and redesign five computational temporal-            abstraction mechanisms:                                                          (a) Automatic formation of meaningful contexts for interpretation of             clinical data.                                                                   (b) Classification of clinical data that have equivalent time stamps into        higher-level concepts.                                                           (c) Temporal inference (e.g., the join of certain interval-based clinical        abstractions into longer ones).                                                  (d) Interpolation between temporally disjoint clinical abstractions,             including a development of a probabilistic representation and semantics.         (e) Matching of predefined and runtime temporal patterns, given time-            stamped data and conclusions.                                                                                                                                     3. We will develop a tool for automated acquisition, from expert                 physicians, of temporal-abstraction knowledge, using techniques from the         PROTEGE-II project for designing knowledge-based systems.                                                                                                         4. We will validate and evaluate our methodology and its implementation.         (a) We will assess the value of the knowledge-acquisition tool in several        experiments.                                                                     (b) We will validate the performance of the computational mechanisms in          the domain of therapy of patients who have insulin-dependent diabetes by         collaboration with expert endocrinologists.                                      (c) We will evaluate the overall framework within EON, a project in which        researchers are implementing an integrated architecture for protocol-based       care.                                                                             n/a",KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA,6185217,R29LM006245,"['abstracting', ' artificial intelligence', ' computer assisted medical decision making', ' computer program /software', ' data collection methodology /evaluation', ' health care facility information system', ' human data', ' time resolved data']",NLM,STANFORD UNIVERSITY,R29,2000,121082,-0.01273925349407013
"VIRUS STRUCTURE DETERMINATION SOFTWARE Virus structure determination using electron microscopy has become a useful research tool aimed at understanding viral assembly and infectivity to facilitate the design of anti-viral drugs and virus-based gene delivery systems. Our long term goal is to broaden the group of people able to determine virus structures by providing an integrated software suite for three-dimensional virus structure determination using electron microscopy. The software suite, called Tumbleweed, will allow easy, efficient, and routine determination of icosahedral virus structures from electron micrographs. Novel aspects of Tumbleweed will include a comprehensive suite of tools for icosahedral structure determination, incorporation of an expert system to guide users through the reconstruction procedure, and data analysis tools to ensure that structures are determined accurately. Tumbleweed will also provide a consistent easy to use graphical user interface to all reconstruction tools including data analysis, data management, and data logging. In addition, Tumbleweed will provide tools for image selection, quality assessment, and structure visualization that can be used with any electron microscopy structure determination method. Thus, in addition to virologists, target users include electron microscopists and structural biologists. The result of this Phase Il SBIR will be a completely integrated and tested software package allowing easy, efficient, and routine virus structure determination. PROPOSED COMMERCIAL APPLICATIONS: Tumbleweed is targeted at a large group of users with varied knowledge, experience, and research goals. Virologists will be attracted to the extensive user guidance and intuitive design. Structural biologists and electron microscopists performing virus structure determination will be attracted to the concept of a complete integrated software package. Last, all electron microscopists and structural biologists will be attracted to the integration of all general data processing into a single extendible package. Combined this group should provide a large user-base allowing QED Labs to commercialize Tumbleweed successfully.  n/a",VIRUS STRUCTURE DETERMINATION SOFTWARE,6071498,R44GM058327,"['artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' electron microscopy', ' molecular dynamics', ' structural biology', ' virus assembly', ' virus morphology']",NIGMS,QED LABS,R44,2000,364001,-0.01385723712548444
"TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES DESCRIPTION (Adapted from the applicant's abstract):                                                                                                              The proposed research will build, refine, and test in operational use, a set     of software tools designed to help support, maintain, and iteratively            revalidate computer-based clinical guidelines as they evolve over time.  The     project will focus on the domain of childhood immunization, and will build       upon IMM/Serve, a childhood immunization forecasting program that takes as       input a child's immunization history, and produces recommendations as to         which vaccinations are due and which vaccinations should be scheduled next.      The effort required to modify and validate such a program as the clinical        field evolves over time is a challenging task.  It will be extremely             important to have a robust set of tools to assist in this process.  Partial      prototype versions of certain of these tools already exist.                                                                                                       1.  The project will refine and extend computer-based tools for immunization     knowledge maintenance.  These tools will include:  a) IMM/Def, a program         which automatically generates the rule-based logic for the most complex          portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program          which automatically generates a set of test cases to help test the kernel        logic.  The project will also develop an organized set of strategies for         immunization test case generation, and implement those strategies in the         refined version of IMM/Test.                                                                                                                                      2.  The project will build a Web site to support immunization knowledge          maintenance.                                                                                                                                                      3.  The project will keep a detailed record of all modifications and             customization of the knowledge, and will represent all the variations of the     knowledge using a standardized format such as GLIF, the Guideline                Interchange Format being developed as a standard for exchanging guidelines       between sites.                                                                                                                                                    4.  The project will link IMM/Serve to a database designed to hold               IMM/Serve's analysis of a set of cases, so that the resulting package can be     used as a tool to perform compliance assessment.                                                                                                                  5.  A set of evaluation studies will be carried out to help assess the           efficacy of the tools and to help improve their functionality.                    n/a",TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES,6185229,R01LM006682,"['Internet', ' artificial intelligence', ' computer assisted medical decision making', ' computer assisted patient care', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' human data', ' immunization', ' information systems', ' medical records', ' pediatrics']",NLM,YALE UNIVERSITY,R01,2000,317318,-0.015159517377729826
BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN DESCRIPTION (Taken from application abstract):  Reminder systems are expert       n/a,BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN,6151393,R29LM006233,"['artificial intelligence', ' automated medical record system', ' behavioral /social science research tag', ' belief', ' computer assisted medical decision making', ' computer assisted patient care', ' computer system design /evaluation', ' health care facility information system', ' health services research tag', ' human data', ' patient care management']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R29,2000,103781,-0.004888040067814894
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6186179,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2000,234591,0.00016964752238535807
"THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES DESCRIPTION (Taken from application abstract):  Recent advances in               understanding neuronal functions have led to an expanded scientific interest     in defining the organization of the nervous system so the spatial correlates     of these functions can be identified.  This interest has been formalized as      the Human Brain Project, an ambitious multi disciplinary effort to map the       nervous system from the organismal to the macromolecular levels.  One of the     greatest challenges of this effort is to preserve the complex                    three-dimensional relationships that occur between neuronal structures.          This problem will require new methods for data acquisition as well as data       visualization.                                                                                                                                                    The project described here is an interdisciplinary effort to derive              three-dimensional reconstructions of synaptic architecture from stereo           electron micrographs acquired from multiple viewpoints.  The collaboration       combines advanced ultrastructural visualization techniques with massively        parallel computational methods and an innovative set of pattern recognition,     stereo correspondence and depth mapping algorithms.  Our goal is to              integrate structural information from numerous images into a single,             high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.     The immediate result of this collaboration will be an improved understanding     of the spatial relationships between synaptic macromolecules.  More              importantly, the project will produce a set of computational tools that can      be applied to stereo image data sets of various areas of the nervous system,     from the macroscopic to the molecular level.  Finally, our studies will          advance the state-of-the-art of parallel computation and interactive             reconstruction methods that can provide novel solutions to difficult             problems of neuroscience visualization.                                           n/a",THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES,6185220,R01LM006326,"['artificial intelligence', ' bioimaging /biomedical imaging', ' brain mapping', ' cell cell interaction', ' computational neuroscience', ' computer program /software', ' computer system design /evaluation', ' cytoskeleton', ' data collection', ' electron microscopy', ' image processing', ' imaging /visualization /scanning', ' macromolecule', ' method development', ' nerve endings', ' neurons', ' parallel processing', ' stereophotography', ' structural biology', ' synapses']",NLM,UNIVERSITY OF MARYLAND BALTIMORE,R01,2000,117821,-0.01143690032023878
"CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be       taken to advance our understanding of the genetic and associated co-             variates affecting the inheritance of complex diseases. To that end, this        proposal will develop improved quantitative methods to detect genetic            factors contributing to increased susceptibility to complex disorders and        implement these methods in software for distribution to the research             community.                                                                                                                                                        The methods will concentrate on the use of classification techniques             applied to allele sharing data and other risk factors which affect the           trait. Allele sharing methods for mapping genes will be extended to              include the classification methods known as latent class models, cluster         analysis, and artificial neural networks, as well as a novel use of              logistic regression Co-variates such as gender, parental diagnosis, or           other concomitant factors will be systematically studied through                 applications to both stimulated and existing data sets. An additional goal       is to determine the optimal distribution of relative pairs (e.g. siblings,       first cousins) for these methods. Of great importance to this proposal is        the development of well-documented, user-friendly software and                   documentation which will be distributed to the scientific community via          the Internet. Existing software developed by the PI will be extensively          expanded for latent class models. Existing cluster analysis software will        be modified and combined for ease of use.                                                                                                                         This proposal consists of theoretical exploration, computer simulation,          data analysis, and software development. First, solutions of theoretical         questions relating to classification techniques will be pursued; second,         adaptation of computer programs to implement the analytic methods, and           investigation into alternative research strategies will be accomplished.         The new strategies will be applied to stimulated data, and finally, to           existing data sets of pedigrees in which a complex trait has been                diagnosed. Findings from this research may contribute to the ability to          locate susceptibility loci in complex traits and to the clarification of         those etiological mechanisms responsible for susceptibility.                      n/a",CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI,6168495,R01AA012239,"['alleles', ' analytical method', ' artificial intelligence', ' biomedical resource', ' computer program /software', ' computer simulation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' family genetics', ' gene environment interaction', ' gene expression', ' genetic disorder', ' genetic disorder diagnosis', ' genetic mapping', ' genetic markers', ' genetic susceptibility', ' human data', ' mathematical model', ' model design /development', ' quantitative trait loci', ' statistics /biometry']",NIAAA,WASHINGTON UNIVERSITY,R01,2000,180260,-0.019481211237562992
"STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES   DESCRIPTION (Adapted from the Applicant's Abstract): This proposed project has       three primary objectives. Objective 1 is to develop improved strategies for          fitting more accurate classification and regression tree (i.e., CART) models.        Objective 2 is to develop a formal framework to allow statistical inference on       tree models. Objective 3 is to develop and distribute public-domain software         that will allow applied data analysts to implement the methods we develop in         the first two objectives. To meet these objectives we will integrate                 statistical and computational machine learning approaches. We believe our work       can have a significant impact in biomedical data analysis by combining the           strengths of statistics for developing objective criteria for model selection        and for providing a framework for assessing and quantifying uncertainty              associated with a model, with the strengths of machine learning for fitting          models to large and complex datasets.                                                                                                                                     n/a",STATISTICAL METHODS FOR RECURSIVELY PARTITIONED TREES,6090912,R01GM061218,"['classification', ' computer assisted medical decision making', ' computer program /software', ' computer simulation', ' experimental designs', ' human data', ' information system analysis', ' mathematical model', ' model design /development', ' statistics /biometry']",NIGMS,BARNES-JEWISH HOSPITAL,R01,2000,214602,0.0024168720874926217
"AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY Molecular microscopy has become an increasingly important tool for structural biology but the methodology is very labor intensive and very slow.  It is generally recognized that the development of improved capabilities for three-dimensional electron microscopy are critical for progress in emerging integrative research in molecular cell biology.  We aim to develop a system for rapid routine structure determination of macromolecular assemblies.  Our ultimate goal is to develop an integrated system that can produce a three-dimensional electron density map of a structure within a few hours of inserting a specimen in the electron microscope.  The motivation for this work is to provide answers to interesting biological questions. We will initially use our work on motor-microtubule complexes and actomyosin as the driver for the development of the integrated system.  By tightly coupling the development of the new system with its implementation in a laboratory whose primary goal is answering fundamental questions in cell biology, we will obtain immediate and invaluable feedback as to how the system is used in practice. Developing this system will involve devising new approaches and integrating the results of several ongoing research projects. The primary specific aims are:  (1) To remove the requirement for using film to acquire the high magnification electron micrographs.  This will require the development of feature recognition algorithms and new imaging strategies that take into account the characteristics of currently available digital cameras.  (2) Improve and automate our existing software for helical image analysis.  We will incorporate new methods for determining the helical parameters of an unknown specimen, methods for improving the resolution, and methods for analyzing non-helical specimens.  (3) Integration of the acquisition and the analysis steps.  This will require incorporation of machine learning techniques to produce a system that is highly efficient in terms of throughput and data quality. The general framework for integrated acquisition and analysis to be developed will be readily extendible to other specimens (helical tubes, single particles, two-dimensional crystals). Thus, once the system has been successfully implemented it will be made generally available to the scientific community.  The system we plan to develop has the potential to revolutionize the field of three-dimensional electron microscopy and make this approach accessible to a wide community.  n/a",AN INTEGRATED SYSTEM FOR MOLECULAR MICROSCOPY,6193019,R01GM061939,"['actins', ' bioimaging /biomedical imaging', ' biomedical equipment development', ' digital imaging', ' electron density', ' electron microscopy', ' image processing', ' microtubules', ' myosins', ' structural biology']",NIGMS,UNIVERSITY OF ILLINOIS URBANA-CHAMPAIGN,R01,2000,478050,-0.010663398064252148
"LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE   DESCRIPTION (Adapted from Applicant's Abstract): Morphometric tools developed        under this grant combine techniques from geometry, computer vision, statistics,      and biomathematics in powerful new strategies for analysis of data about size        and shape. This fourth funding period is directed to three extensions of the         established core methodology, along with continued dissemination. Aim 1.             Thin-plate spline interpolant aids the scientist's eye in detecting                  localization of interesting shape differences. Over the present funding period       the applicants reported having developed an algebraic/statistical formalization      of this tactic, the method of creases. Aim 1 of the renewal is to standardize        the parameterization of this feature, to provide protocols for significance          tests, and to produce ""a grammar of grids"" for uniting multiple creases into         coherent summaries of empirical deformations. Aim 2. The standard Procrustes         methods for discrete point landmarks have been extended for data sets of             outlines. Aim 2 of the renewal is to further extend these tools for realistic        data sets that combine discrete point landmarks and curves or surfaces               arbitrarily. The applicants proposed to formalize statistical spaces for such        structures and extend them to anticipate the emerging resource of neural tract       directional data (directions without curves). Aim 3. The best current                strategies for formal statistical inferences about shape exploit permutation         tests of Procrustes distance or its modifications. Under new Aim 3, the              applicants proposed to combine this approach with spline-based high-pass or          low-pass filters and extend it further to support studies of correlations of         shape with other measurement sets, including other aspects of shape. Finally,        as it has been for the past twelve years, Aim 4 is to continue bringing all          these methodological developments to the attention of many different biomedical      communities, by primary scientific papers, essays on methodology per se,             videotapes, and software and documentation free over the Internet. The work          proposed is expected to extend to the medical imaging community's most               sophisticated data resources, carefully labeled images and volumes, a                state-of-the-art biometric toolkit for analysis and visualization carefully          tuned to the special needs of such data.                                                                                                                                  n/a",LANDMARK BASED METHODS FOR BIOMETRIC ANALYSIS OF SHAPE,6180399,R01GM037251,"['bioimaging /biomedical imaging', ' cardiovascular system', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' craniofacial', ' human data', ' image processing', ' mathematical model', ' morphology', ' neuroanatomy', ' statistics /biometry']",NIGMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2000,150497,-0.007935997624638487
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10224492,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,233900,-0.012220076971830595
"Integrative Predictors of Temporomandibular Osteoarthritis ABSTRACT This application proposes the development of efficient web-based data management, mining, and analytics, to integrate and analyze clinical, biological, and high dimensional imaging data from TMJ OA patients. Based on our published results, we hypothesize that patterns of condylar bone structure, clinical symptoms, and biological mediators are unrecognized indicators of the severity of progression of TMJ OA. Efficiently capturing, curating, managing, integrating and analyzing this data in a manner that maximizes its value and accessibility is critical for the scientific advances and benefits that such comprehensive TMJ OA patient information may enable. High dimensional databases are increasingly difficult to process using on-hand database management tools or traditional processing applications, creating a continuing demand for innovative approaches. Toward this end, the DCBIA at the Univ. of Michigan has partnered with the University of North Carolina, the University of Texas MD Anderson Cancer Center and Kitware Inc. Through high-dimensional quantitative characterization of individuals with TMJ OA, at molecular, clinical and imaging levels, we will identify phenotypes at risk for more severe prognosis, as well as targets for future therapies. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA. Due to its ubiquitous design in the web, DSCI software installation will no longer be required. Our long-term goal is to create software and data repository for Osteoarthritis of the TMJ. Such repository requires maintaining the data in a distributed computational environment to allow contributions to the database from multi-clinical centers and to share trained models for TMJ classification. In years 4 and 5 of the proposed work, the dissemination and training of clinicians at the Schools of Dentistry at the University of North Carol, Univ. of Minnesota and Oregon Health Sciences will allow expansion of the proposed studies. In Aim 1, we will test state-of-the-art neural network structures to develop a combined software module that will include the most efficient and accurate neural network architecture and advanced statistics to mine imaging, clinical and biological TMJ OA markers identified at baseline. In Aim 2, we propose to develop novel data analytics tools, evaluating the performance of various machine learning and statistical predictive models, including customized- Gaussian Process Regression, extreme boosted trees, Multivariate Varying Coefficient Model, Lasso, Ridge and Elastic net, Random Forest, pdfCluster, decision tree, and support vector machine. Such automated solutions will leverage emerging computing technologies to determine risk indicators for OA progression in longitudinal cohorts of TMJ health and disease. PROJECT NARRATIVE This application proposes the development of efficient web-based data management, mining, and analytics of clinical, biological, and high dimensional imaging data from TMJ OA patients. The proposed web-based system, the Data Storage for Computation and Integration (DSCI), will remotely compute machine learning, image analysis, and advanced statistics from prospectively collected longitudinal data on patients with TMJ OA.",Integrative Predictors of Temporomandibular Osteoarthritis,10017950,R01DE024450,"['3-Dimensional', 'Age', 'Architecture', 'Arthritis', 'Benchmarking', 'Biological', 'Biological Markers', 'Blood', 'Bone remodeling', 'Bone structure', 'Cancer Center', 'Chronic', 'Classification', 'Clinical', 'Clinical Markers', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Diagnosis', 'Country', 'Custom', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Data Storage and Retrieval', 'Database Management Systems', 'Databases', 'Decision Trees', 'Degenerative polyarthritis', 'Dental', 'Development', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Environment', 'Fibrocartilages', 'Future', 'Gaussian model', 'Goals', 'Hand', 'Health', 'Health Sciences', 'Image', 'Image Analysis', 'Individual', 'Inflammation Mediators', 'Inflammatory', 'Internet', 'Joints', 'Lasso', 'Longitudinal cohort', 'Machine Learning', 'Mandibular Condyle', 'Mediator of activation protein', 'Medicine', 'Methods', 'Michigan', 'Mining', 'Minnesota', 'Modeling', 'Molecular', 'Morphology', 'North Carolina', 'Online Systems', 'Oregon', 'Outcome', 'Pain', 'Paper', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Phenotype', 'Process', 'Property', 'Proteins', 'Publishing', 'Replacement Arthroplasty', 'Resolution', 'Risk', 'Saliva', 'School Dentistry', 'Scientific Advances and Accomplishments', 'Severities', 'Slice', 'Structure', 'Study models', 'Symptoms', 'System', 'Technology', 'Temporomandibular Joint', 'Temporomandibular joint osteoarthritis', 'Testing', 'Texas', 'Three-Dimensional Imaging', 'Training', 'Trees', 'Universities', 'University of Texas M D Anderson Cancer Center', 'Work', 'X-Ray Computed Tomography', 'analytical tool', 'base', 'bone', 'cadherin 5', 'cartilage degradation', 'clinical center', 'clinical diagnostics', 'cone-beam computed tomography', 'craniofacial', 'craniomaxillofacial', 'data warehouse', 'deep learning', 'deep neural network', 'design', 'high dimensionality', 'imaging biomarker', 'improved', 'innovation', 'joint destruction', 'machine learning algorithm', 'neural network', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'outcome forecast', 'predictive modeling', 'prospective', 'quantitative imaging', 'random forest', 'repository', 'scale up', 'screening', 'serial imaging', 'software repository', 'statistical and machine learning', 'statistics', 'subchondral bone', 'support vector machine', 'tool']",NIDCR,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,513041,-0.012220076971830595
"Integrating Ethics into Machine Learning for Precision Medicine The application of new computerized methods of data analysis to vast collections of medical, biological, and other data is emerging as a central feature of a broad vision of precision medicine (PM) in which systems based on artificial intelligence (AI) assist clinicians in treatment, diagnosis, or prognosis. The use of AI to analyze big data for clinical decision-making opens up a new domain for ELSI inquiry to address a possible future when the implications of genetics and genomics become embedded into algorithms, pervasive yet implicit and difficult to identify. Thus, an important target of inquiry is the development and developers of these algorithms. There are three distinctive features of the application of AI, and in particular machine learning (ML), to the domain of PM that create the need for ELSI inquiry. First, the process of developing ML-based systems for PM goals is technically and organizationally complex. Thus, members of development teams will likely have different expertise and assumptions about norms, responsibilities, and regulation. Second, machine learning does not solely operate through predetermined rules, and is thus difficult to hold accountable for its conclusions or reasoning. Third, designers of ML systems for PM may be subject to diverse and divergent interests and needs of multiple stakeholders, yet unaware of the associated ethical and values implications for design. These distinctive features of ML in PM could lead to difficulties in detecting misalignment of design with values, and to breakdown in responsibility for realignment. Because machine learning in the context of precision medicine is such a new phenomenon, we have very little understanding of actual practices, work processes and the specific contexts in which design decisions are made. Importantly, we have little knowledge about the influences and constraints on these decisions, and how they intersect with values and ethical principles. Although the field of machine learning for precision medicine is still in its formative stage, there is growing recognition that designers of AI systems have responsibilities to ask such questions about values and ethics. In order to ask these questions, designers must first be aware that there are values expressed by design. Yet, there are few practical options for designers to learn how to increase awareness. Our specific aims are: Aim 1 To map the current state of ML in PM by identifying and cataloging existing US-based ML in PM  projects and by exploring a range of values expressed by stakeholders about the use of ML in PM through  a combination of multi-method review, and interviews of key informants and stakeholders. Aim 2 To characterize decisions and rationales that shape ML in PM and explore whether and how  developers perceive values as part of these rationales through interviews of ML developers and site visits. Aim 3 To explore the feasibility of using design rationale as a framework for increasing awareness of the  existence of values, and multiple sources of values, in decisions about ML in PM through group-based  exercises with ML developers from academic and commercial settings. The overall goal of this project is to understand how to encourage and enable people who are developing artificial intelligence for personalized health care to be aware of values in their daily practice. We will examine actual practices and contexts in which design decisions are made for precision medicine applications, and use this information to design group-based workshop exercises to increase awareness of values.",Integrating Ethics into Machine Learning for Precision Medicine,9941090,R01HG010476,"['Address', 'Algorithms', 'Artificial Intelligence', 'Awareness', 'Big Data', 'Biological', 'Cataloging', 'Catalogs', 'Clinical', 'Collection', 'Complex', 'Computers', 'Data', 'Data Analyses', 'Development', 'Diagnosis', 'Educational workshop', 'Electronic Health Record', 'Engineering', 'Ethics', 'Evolution', 'Exercise', 'Expert Systems', 'Foundations', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Interview', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Maps', 'Medical', 'Methods', 'Outcome', 'Process', 'Regulation', 'Research', 'Resources', 'Sampling', 'Scholarship', 'Scientist', 'Shapes', 'Site Visit', 'Source', 'System', 'Time', 'Vision', 'Work', 'base', 'biobank', 'clinical decision-making', 'computerized', 'design', 'ethical legal social implication', 'genomic data', 'informant', 'innovation', 'interest', 'member', 'new technology', 'outcome forecast', 'personalized health care', 'precision medicine']",NHGRI,STANFORD UNIVERSITY,R01,2020,605875,-0.007862874086829579
"Machine learning approaches for improved accuracy and speed in sequence annotation Summary/Abstract Alignment of biological sequences is a key step in understanding their evolution, function, and patterns of activity. Here, we describe Machine Learning approaches to improve both accuracy and speed of highly- sensitive sequence alignment. To improve accuracy, we develop methods to reduce erroneous annotation caused by (1) the existence of low complexity and repetitive sequence and (2) the overextension of alignments of true homologs into unrelated sequence. We describe approaches based on both hidden Markov models and Artificial Neural Networks to dramatically reduce these sorts of sequence annotation error. We also address the issue of annotation speed, with development of a custom Deep Learning architecture designed to very quickly filter away large portions of candidate sequence comparisons prior to the relatively-slow sequence-alignment step. The results of these efforts will be incorporated into forks of the open source sequence alignment tools HMMER, MMSeqs, and (where appropriate) BLAST; we will also work with community developers of annotation pipelines, such as RepeatMasker and IMG/M, to incorporate these approaches. The development and incorporation into these widely used bioinformatics tools will lead to widespread impact on sequence annotation efforts. Narrative Modern molecular biology depends on effective methods for creating sequence alignments quickly and accurately. This proposal describes a plan to develop novel Machine Learning approaches that will dramatically increase the speed of highly-sensitive sequence alignment, and will also address two significant sources of erroneous sequence annotation, (i) the presence of repetitive sequence in biological sequences, and (ii) the tendency for sequence alignment algorithms to extend alignments beyond the boundaries of true homology. The proposed methods represent a mix of applications of hidden Markov models and Artificial Neural Networks, and build on prior success in applying such methods to the problem of sensitive sequence annotation.",Machine learning approaches for improved accuracy and speed in sequence annotation,10020995,R01GM132600,"['Address', 'Algorithms', 'Architecture', 'Bioinformatics', 'Biological', 'Classification', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Consumption', 'Custom', 'DNA Transposable Elements', 'Data Set', 'Deletion Mutation', 'Descriptor', 'Development', 'Error Sources', 'Evolution', 'Foundations', 'Genome', 'Genomics', 'Hour', 'Human', 'Human Genome', 'Industry Standard', 'Insertion Mutation', 'Institutes', 'Intervention', 'Joints', 'Label', 'Letters', 'Licensing', 'Machine Learning', 'Manuals', 'Masks', 'Methods', 'Modeling', 'Modernization', 'Molecular Biology', 'Network-based', 'Nucleotides', 'Pattern', 'Pilot Projects', 'Proteins', 'Repetitive Sequence', 'Sequence Alignment', 'Sequence Analysis', 'Source', 'Speed', 'Statistical Models', 'Takifugu', 'Work', 'annotation  system', 'artificial neural network', 'base', 'bioinformatics tool', 'computing resources', 'convolutional neural network', 'deep learning', 'density', 'design', 'genomic data', 'improved', 'markov model', 'neural network architecture', 'novel', 'novel strategies', 'open source', 'software development', 'statistics', 'success', 'tool']",NIGMS,UNIVERSITY OF MONTANA,R01,2020,287504,-0.032098615991868924
"Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement PROJECT SUMMARY/ABSTRACT Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing. Project Narrative Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing",Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement,10149058,R24MH117179,"['Address', 'Administrative Supplement', 'Archives', 'Artificial Intelligence', 'Award', 'BRAIN initiative', 'Benefits and Risks', 'Consent Forms', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Data Set', 'Ensure', 'Ethics', 'Foundations', 'Funding', 'Future', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Subject Research', 'International', 'Investments', 'Laws', 'Legal', 'Light', 'Machine Learning', 'Magnetic Resonance Imaging', 'Neurosciences', 'Parents', 'Policies', 'Privacy', 'Process', 'Regulation', 'Research', 'Research Subjects', 'Risk', 'Security Measures', 'Series', 'Software Tools', 'Solid', 'Surveys', 'Techniques', 'United States', 'United States National Institutes of Health', 'data archive', 'data privacy', 'data sharing', 'design', 'human subject', 'human subject protection', 'machine learning algorithm', 'neuroethics', 'neuroimaging', 'novel', 'prevent', 'privacy protection', 'research study', 'sharing platform', 'sound', 'stem']",NIMH,STANFORD UNIVERSITY,R24,2020,126592,-0.008744500567480539
"Opening the Black Box of Machine Learning Models Project Summary Biomedical data is vastly increasing in quantity, scope, and generality, expanding opportunities to discover novel biological processes and clinically translatable outcomes. Machine learning (ML), a key technology in modern biology that addresses these changing dynamics, aims to infer meaningful interactions among variables by learning their statistical relationships from data consisting of measurements on variables across samples. Accurate inference of such interactions from big biological data can lead to novel biological discoveries, therapeutic targets, and predictive models for patient outcomes. However, a greatly increased hypothesis space, complex dependencies among variables, and complex “black-box” ML models pose complex, open challenges. To meet these challenges, we have been developing innovative, rigorous, and principled ML techniques to infer reliable, accurate, and interpretable statistical relationships in various kinds of biological network inference problems, pushing the boundaries of both ML and biology. Fundamental limitations of current ML techniques leave many future opportunities to translate inferred statistical relationships into biological knowledge, as exemplified in a standard biomarker discovery problem – an extremely important problem for precision medicine. Biomarker discovery using high-throughput molecular data (e.g., gene expression data) has significantly advanced our knowledge of molecular biology and genetics. The current approach attempts to find a set of features (e.g., gene expression levels) that best predict a phenotype and use the selected features, or molecular markers, to determine the molecular basis for the phenotype. However, the low success rates of replication in independent data and of reaching clinical practice indicate three challenges posed by current ML approach. First, high-dimensionality, hidden variables, and feature correlations create a discrepancy between predictability (i.e., statistical associations) and true biological interactions; we need new feature selection criteria to make the model better explain rather than simply predict phenotypes. Second, complex models (e.g., deep learning or ensemble models) can more accurately describe intricate relationships between genes and phenotypes than simpler, linear models, but they lack interpretability. Third, analyzing observational data without conducting interventional experiments does not prove causal relations. To address these problems, we propose an integrated machine learning methodology for learning interpretable models from data that will: 1) select interpretable features likely to provide meaningful phenotype explanations, 2) make interpretable predictions by estimating the importance of each feature to a prediction, and 3) iteratively validate and refine predictions through interventional experiments. For each challenge, we will develop a generalizable ML framework that focuses on different aspects of model interpretability and will therefore be applicable to any formerly intractable, high-impact healthcare problems. We will also demonstrate the effectiveness of each ML framework for a wide range of topics, from basic science to disease biology to bedside applications. Project Narrative The development of effective computational methods that can extract meaningful and interpretable signals from noisy, big data has become an integral part of biomedical research, which aims to discover novel biological processes and clinically translatable outcomes. The proposed research seeks to radically shift the current paradigm in data-driven discovery from “learning a statistical model that best fits specific training data” to “learning an explainable model” for a wide range of topics, from basic science to disease biology to bedside applications. Successful completion of this project will result in novel biological discoveries, therapeutic targets, predictive models for patient outcomes, and powerful computational frameworks generalizable to critical problems in various diseases.",Opening the Black Box of Machine Learning Models,10020414,R35GM128638,"['Address', 'Basic Science', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Complex', 'Computing Methodologies', 'Data', 'Dependence', 'Development', 'Disease', 'Effectiveness', 'Future', 'Gene Expression', 'Genes', 'Healthcare', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Linear Models', 'Machine Learning', 'Measurement', 'Methodology', 'Modeling', 'Modernization', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Outcome', 'Patient-Focused Outcomes', 'Phenotype', 'Research', 'Sampling', 'Selection Criteria', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Technology', 'Training', 'Translating', 'biomarker discovery', 'clinical practice', 'clinically translatable', 'computer framework', 'deep learning', 'experimental study', 'feature selection', 'high dimensionality', 'innovation', 'inquiry-based learning', 'molecular marker', 'novel', 'precision medicine', 'predictive modeling', 'success', 'therapeutic target']",NIGMS,UNIVERSITY OF WASHINGTON,R35,2020,388750,-0.006671953262419958
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy Project Summary/Abstract Artificial intelligence on genomic/healthcare data that is performed jointly between multiple collaborating institutions relies on a trust model but can accelerate genomic medicine research and facilitate quality improvement. To conduct such machine learning while protecting patient privacy and reducing security risks, we are developing blockchain-based privacy-preserving learning methods in a K99/R00 study supported by the National Human Genome Research Institute (NHGRI). However, our previous design of privacy-preserving learning on private blockchain assumed “semi-honesty” as the underlying adversary assumption. That is, we assume that each participating site is curious yet very careful and honest, such that it would only submit correct predictive models. Nevertheless, in real world this assumption may be too optimistic; the models submitted could be an old one due to network latency or malicious users may try to create fake models, which can in turn lead to bioethical concerns and reduce the incentives for genomic/clinical institutions to participate in the collaborative predictive modeling. Therefore, the capability to detect, assess and prevent “model misconducts” is critical to increase the integrity/reliability of machine learning. To address this issue, we consider the following 3 types of model misconducts: (1) model plagiarism, of which a site becomes a free-rider and just submits a copy of a model from the other sites, trying to hide their own information and inspect models from other sites; (2) model fabrication, of which a site mocks up a model, trying to hide information and disturb the machine learning process; and (3) model falsification, of which a site tweaks its model a bit, trying to just disturb the learning process. For each type of the model misconducts, we are interest in how to detect these misconducts of another site, how to assess the losses of machine learning results due to misconducts, and how to prevent these model misconducts. Our aims include (a) detecting model misconducts using model properties, (b) assessing model misconducts losses via model simulation, and (c) preventing model misconducts based on whole model history. The innovative components to our proposed project include (i) summarizing various types of model misconduct, (ii) developing a complete strategy to handle the model misconduct, and (iii) providing a generalizable approach to mitigate bioethical concerns for collaborative machine learning. Project Narrative Artificial intelligence performed jointly between multiple collaborating institutions can accelerate genomic medicine research and facilitate quality improvement, but relies on a trust model which may be too optimistic in real-world setting. In this project, we plan to develop a comprehensive detection, assessment and prevention mechanism to address the potential bioethical risks brought by misconducts of model plagiarism, fabrication, and falsification. The proposed study can supplement the considerations of model misconducts for our original project of privacy-preserving learning on blockchain.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,10130868,R00HG009680,"['Address', 'Artificial Intelligence', 'Bioethics', 'Budgets', 'Calibration', 'Clinical', 'Data', 'Data Set', 'Detection', 'Digit structure', 'Discrimination', 'Event', 'Genomic medicine', 'Genomics', 'Healthcare', 'Incentives', 'Institution', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pattern', 'Plagiarism', 'Prevention', 'Privacy', 'Privatization', 'Process', 'Property', 'Randomized', 'Recording of previous events', 'Research', 'Risk', 'Security', 'Site', 'Sum', 'Testing', 'Time', 'Trust', 'base', 'blockchain', 'design', 'distributed ledger', 'innovation', 'interest', 'learning strategy', 'models and simulation', 'patient privacy', 'predictive modeling', 'prevent', 'privacy preservation', 'statistics']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,102049,-0.006438084396126224
"Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models Project Summary Amyotrophic lateral sclerosis (ALS) and Frontotemporal Dementia FTD are devastating neurodegenerative disorders that lie on a genetic and mechanistic continuum. ALS is a disease of motor neurons that that is almost uniformly lethal within only 3-5 years of diagnosis. FTD is a heterogeneous, rapidly progressing syndrome that is among the top three causes of presenile dementia. About 10% of ALS cases are caused by dominantly transmitted gene defects. SOD1 and FUS mutations cause aggressive motor neuron pathology while TDP43 mutations cause ALS-FTD. Further, wild type FUS and TDP43 are components of abnormal inclusions in many FTD cases, suggesting a mechanistic link between these disorders. Early phenotypes are of particular interest because these could lead to targeted interventions aimed at the root cause of the disorder that could stem the currently inexorable disease progression. Elucidating such early, potentially shared characteristics of these disorders should be greatly aided by: 1) knock-in animal models expressing familial ALS-FTD genes; 2) sensitive, rigorous and objective behavioral phenotyping methods to analyze and compare models generated in different laboratories. In published work the co-PIs applied their first-generation, machine vision-based automated phenotyping method, ACBM ‘1.0’ (automated continuous behavioral monitoring) to detect and quantify the earliest-observed phenotypes in Tdp43Q331K knock-in mice. This method entails continuous video recording for 5 days to generate >14 million frames/mouse. These videos are then scored by a trained computer vision system. In addition to its sensitivity, objectivity and reproducibility, a major advantage of this method is the ability to acquire and archive video recordings and to analyze the data at sites, including the Cloud, remote from those of acquisition. We will use Google Cloud TPUs supercomputers that have been designed from the ground up to accelerate cutting-edge machine learning workloads, with a special focus on deep learning. We will analyze this data using Bayesian hierarchical spline models that describe the different mouse behaviors along the circadian rhythm. The current proposal has two main goals: 1) Use deep learning to refine and apply a Next Generation ACBM - ‘2.0’ - that will allow for more sensitive, expansive and robust automated behavioral phenotyping of four novel knock-in models along with the well characterized SOD1G93A transgenic mouse. 2) To establish and validate procedures to enable remote acquisition of video recording data with cloud-based analysis. Our vision is to establish sensitive, robust, objective, and open-source machine vision-based behavioral analysis tools that will be widely available to researchers in the field. Since all the computer-annotated video data is standardized in ACBM 2.0 and will be archived, we envision a searchable ‘behavioral database’, that can be freely mined and analyzed. Such tools are critical to accelerate the development of novel and effective therapeutics for ALS-FTD. Narrative ALS and Frontotemporal Dementia (FTD) are devastating, rapidly progressing diseases and current treatments are of limited value. In this proposal a neuroscientist and a computer scientist have teamed up to develop a new machine vision-based method for behavioral analysis novel mouse models of ALS-FTD. The ultimate goal is to reveal early phenotypes in ALS-FTD models that can be used in understanding disease pathology and in the development of new therapeutic targets.",Next generation machine vision for automated behavioral phenotyping of knock-in ALS-FTD mouse models,9979408,R21NS112743,"['Amyotrophic Lateral Sclerosis', 'Animal Model', 'Archives', 'Behavior', 'Behavior monitoring', 'Behavioral', 'Characteristics', 'Circadian Rhythms', 'Computer Vision Systems', 'Computers', 'Data', 'Data Set', 'Databases', 'Defect', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Expression Profiling', 'Familial Amyotrophic Lateral Sclerosis', 'Frontotemporal Dementia', 'Gene Expression', 'Generations', 'Genes', 'Genetic', 'Goals', 'Hour', 'Human', 'Intervention', 'Knock-in', 'Knock-in Mouse', 'Laboratories', 'Lead', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Motor Neuron Disease', 'Motor Neurons', 'Mus', 'Mutation', 'Neurodegenerative Disorders', 'Paralysed', 'Pathology', 'Phenotype', 'Plant Roots', 'Presenile Dementia', 'Procedures', 'Publishing', 'Reproducibility', 'Research', 'Research Personnel', 'Respiratory Paralysis', 'Scientist', 'Site', 'Standardization', 'Syndrome', 'TensorFlow', 'Time', 'Training', 'Transgenic Mice', 'Transgenic Organisms', 'Treatment Efficacy', 'Video Recording', 'Vision', 'Work', 'Workload', 'base', 'behavioral phenotyping', 'cloud based', 'data archive', 'deep learning', 'design', 'frontotemporal lobar dementia-amyotrophic lateral sclerosis', 'interest', 'knockin animal', 'machine vision', 'mouse model', 'new therapeutic target', 'next generation', 'novel', 'open source', 'programs', 'protein TDP-43', 'stem', 'supercomputer', 'superoxide dismutase 1', 'tool']",NINDS,BROWN UNIVERSITY,R21,2020,446875,-0.03150661271965717
"Identifying False HPV-Vaccine Information and Modeling Its Impact on Risk Perceptions PROJECT SUMMARY/ABSTRACT Human papillomavirus (HPV) is the most common sexually transmitted infection in the United States, with over 30,000 new HPV-related-cancers are diagnosed annually. Although HPV vaccines have been approved by the Food and Drug Administration (FDA) since 2006 and recommended for routine vaccination for school-age girls and boys, vaccination rates remain low. One reason that has contributed to low vaccination rates is incorrect “risk perceptions” around HPV vaccines such as the high perceived risks of adverse events or side effects from the HPV vaccine. Incorrect risk perceptions are often rooted in the false information about HPV vaccines that people are exposed to in their daily life, including social media. The impact of social media on health information is substantial. Negative social-media HPV-vaccine information has been found to have an association with low vaccination coverage. Given the negative consequences of false information, there is a need to develop a robust and scalable way to detect false HPV-vaccine information before it propagates and negatively impacts behavior. The overarching goal of the proposed research is to build a model to identify false HPV-vaccine information on Twitter, demonstrate its impact on individual risk perceptions and measure its underlying mechanisms on risk perception formation. We propose a novel approach to leverage machine learning, natural language processing, network analysis, crowdsourcing/expert data annotation, psycholinguistic analysis and statistical modeling to investigate the false HPV-vaccine information collectively (in terms of its detection and propagation patterns) and individually (in terms of its impact and underlying cognitive mechanisms). Our study will first build a computational model to detect false HPV-vaccine information on Twitter. By modeling the domain-specific HPV- vaccine related text content, information-veracity related linguistic features, individual and collective user behaviors, and dissemination patterns, our model will be able to detect false HPV-vaccine information before it gets verified and spreads widely. We will then investigate the impact of false HPV-vaccine information on risk perceptions around HPV vaccination operationalized by natural language processing methods and a developed HPV-vaccine Risk Lexicon. We will further conduct psycholinguistic analysis on the false HPV-vaccine information and use statistical modeling to uncover the underlying mechanism of risk perceptions. Our study will make a critical and timely contribution to identifying the false HPV-vaccine information and its impact, which has the potential to be applied to other health topics. This proposed project will also address the National Cancer Institute priorities in promoting HPV vaccines and combating misinformation in cancer prevention and control. PROJECT NARRATIVE The uptake of human papillomavirus (HPV) vaccine remains low in part because of incorrect perceptions of vaccination risks, which has been linked to the spread of false HPV-vaccine information. The proposed study seeks to build a computational model to detect false HPV-vaccine information on social media (Twitter) and determine its impact on risk perceptions of the HPV vaccine. The findings will provide important contributions to understand the impact of false health information on HPV vaccination behavior and could be expanded to other health topics.",Identifying False HPV-Vaccine Information and Modeling Its Impact on Risk Perceptions,9954963,R21CA237483,"['Address', 'Affect', 'Age', 'Anxiety', 'Attitude', 'Behavior', 'Cancer Control', 'Categories', 'Cognitive', 'Communication', 'Comprehension', 'Computer Models', 'Data', 'Decision Making', 'Detection', 'Diagnosis', 'Electronic cigarette', 'Event', 'Exposure to', 'Fright', 'Goals', 'Harm Reduction', 'Health', 'Human Papilloma Virus Vaccination', 'Human Papilloma Virus Vaccine', 'Human Papilloma Virus-Related Malignant Neoplasm', 'Human Papillomavirus', 'Human papilloma virus infection', 'Individual', 'Information Dissemination', 'Knowledge', 'Lesion', 'Life', 'Linguistics', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Misinformation', 'Modeling', 'National Cancer Institute', 'Natural Language Processing', 'Neural Network Simulation', 'Participant', 'Pathway Analysis', 'Patients', 'Pattern', 'Perception', 'Plant Roots', 'Politics', 'Property', 'Psycholinguistics', 'Psychological reinforcement', 'Research', 'Risk', 'Safety', 'School-Age Population', 'Semantics', 'Sexually Transmitted Diseases', 'Source', 'Statistical Models', 'Text', 'Time', 'Twitter', 'United States', 'United States Food and Drug Administration', 'Vaccination', 'Vaccines', 'Work', 'adverse event risk', 'boys', 'cancer diagnosis', 'cancer prevention', 'combat', 'crowdsourcing', 'deep learning', 'girls', 'high risk', 'information model', 'information processing', 'multilevel analysis', 'news', 'novel strategies', 'premalignant', 'prevent', 'recurrent neural network', 'response', 'risk perception', 'side effect', 'social media', 'theories', 'uptake']",NCI,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R21,2020,211659,-0.015383257464501858
"Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics The human microbiota plays an important role in health and disease, and its therapeutic manipulation is being actively investigated for a wide range of diseases that span every NIH institute. Our microbiota are inherently dynamic, and analyzing these time-dependent properties is key to robustly linking the microbiota to disease, and predicting the effects of therapies targeting the microbiota; indeed, longitudinal microbiome data is being acquired with increasing frequency, and is a major component of many NIH-funded projects. However, there is currently a dearth of computational tools for analyzing microbiome time-series data, which presents several special challenges including high measurement noise, irregular and sparse temporal sampling, and complex dependencies between variables. The objective of this proposal is to introduce new capabilities, improve on, and provide state-of-the-art implementations of tools for analyzing dynamics, or patterns of change in microbiome time-series data. The tools we develop will use Bayesian machine learning methods, which are well-recognized for their strong conceptual and practical advantages, particularly in biomedical domains. Tools will be rigorously tested and validated on synthetic and real human microbiome data, including publicly available datasets and those from collaborators providing 16S rRNA sequencing, metagenomic, and metabolomics data. We propose three specific aims. For Aim 1, we will develop integrated Bayesian machine learning tools for predicting population dynamics of the microbiome and its responses to perturbations. These tools will include a new model that simultaneously learns groups of microbes with similar interaction structure and predicts their behavior over time, and that incorporates prior phylogenetic information. The model will be further improved by incorporating stochastic microbial dynamics and errors in measurements throughout the model. For Aim 2, we will develop Bayesian machine learning tools to predict host status from microbiome dynamics. The tools will learn easily interpretable, human-readable rules that predict host status from microbiome time-series data, and will be further extended to handle a variety of longitudinal study designs. For Aim 3, we will engineer our microbiome dynamics analysis software tools for optimal performance, ease-of- use, maintainability, extensibility, and dissemination to the community. In total, the proposed work will yield a suite of contemporary software tools for analyzing microbiome dynamics, with expected broad use and major impact. The software will allow investigators to answer important scientific and translational questions about the microbiome, including discovering which microbial taxa or their metagenomes are affected over time by perturbations such as changes in diet or invasion by pathogens; predicting the effects of these perturbations over time, including changes in composition or stability of the gut microbiota; and finding temporal signatures in multi-‘omic microbiome data that predict disease risk in the human host. The human microbiota, or collection of micro-organisms living on and within us, plays an important role in health, and when disrupted or abnormal, may contribute to many types of diseases including infections, kidney diseases, bowel diseases, diabetes, heart diseases, arthritis, allergies, brain diseases, and cancer. Sophisticated computer-based tools are needed to make sense of human microbiota data, particularly time- series data, which can yield important insights into how our microbiomes change over time. This work will develop new and improved computer-based tools for analyzing microbiota time-series data, which will be made freely available and will enable scientists to increase our fundamental knowledge about how our microbiota affect us and ultimately to apply this knowledge to prevent and treat human illnesses.",Bayesian Machine Learning Tools for Analyzing Microbiome Dynamics,10015315,R01GM130777,"['16S ribosomal RNA sequencing', 'Affect', 'Algorithms', 'Antibiotics', 'Arthritis', 'Autoimmunity', 'Bayesian learning', 'Behavior', 'Biological Markers', 'Biological Models', 'Brain Diseases', 'Cardiovascular Diseases', 'Childhood', 'Clostridium difficile', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computers', 'Data', 'Data Set', 'Dependence', 'Diabetes Mellitus', 'Diet', 'Disease', 'Engineering', 'Environmental Exposure', 'Frequencies', 'Funding', 'Health', 'Heart Diseases', 'Human', 'Human Microbiome', 'Hypersensitivity', 'Infection', 'Institutes', 'Intervention', 'Intestines', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Learning', 'Link', 'Longitudinal Studies', 'Malignant Neoplasms', 'Measurement', 'Medical', 'Metagenomics', 'Microbe', 'Modeling', 'Names', 'Noise', 'Oligosaccharides', 'Outcome', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Play', 'Population Dynamics', 'Property', 'Pythons', 'Readability', 'Research Design', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Series', 'Shotguns', 'Software Engineering', 'Software Tools', 'Speed', 'Structure', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Work', 'base', 'computerized tools', 'design', 'disorder risk', 'dynamic system', 'gut microbiota', 'human data', 'human microbiota', 'human subject', 'improved', 'insight', 'learning algorithm', 'machine learning method', 'man', 'metabolomics', 'metagenome', 'microbial', 'microbiome', 'microbiome analysis', 'microbiome sequencing', 'microbiota', 'microorganism', 'nervous system disorder', 'novel', 'open source', 'pathogen', 'predictive tools', 'prevent', 'recurrent infection', 'response', 'software development', 'targeted treatment', 'tool']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R01,2020,312939,0.007627504950318887
"Big Data Predictive Phylogenetics with Bayesian Learning Big Data Predictive Phylogenetics with Bayesian Learning Abstract Andrew Holbrook, Ph.D., is a Bayesian statistician with a broad background in applied, theoretical and compu- tational data science. His proposed research Big Data Predictive Phylogenetics with Bayesian Learning tackles viral outbreak forecasting by combining Bayesian phylogenetic modeling with ﬂexible, `self-exciting' stochastic process models. The development and publication of open-source, high-performance computing software for his models will facilitate fast epidemiological ﬁeld response in a big data setting. Dr. Holbrook will apply his method- ology to the reconstruction of the 2015-2016 Zika virus epidemic in the Americas, focusing on identifying key geographical routes of transmission and phylogenetic clades with enhanced infectiousness.  Candidate: Dr. Holbrook is Postdoctoral Scholar at the UCLA Department of Human Genetics. He earned his Ph.D. in Statistics from the Department of Statistics at UC Irvine, during which time he completed his dissertation Geometric Bayes, an investigation into Bayesian modeling and computing on abstract mathematical spaces, and simultaneously participated in scientiﬁc collaborations at the UC Irvine Alzheimer's Disease Research Center. The proposed career development plan will establish Dr. Holbrook as an independent leader in data intensive viral epidemiology by 1) facilitating coursework to build biological domain knowledge, 2) affording Dr. Holbrook the opportunity to lead his own project while remaining under the expert oversight of UCLA Prof. Marc Suchard, M.D., Ph.D., and 3) allowing Dr. Holbrook to continue his focus on quantitative viral epidemiology once he has moved to a faculty commitment.  Mentors: During the ﬁrst three years of the award period, Dr. Holbrook will work closely with Prof. Suchard, continuing their current schedule of weekly meetings. Prof. Suchard is a leading expert in both Bayesian phylo- genetics and high-performance statistical computing; and with his medical background, Prof. Suchard will advise Dr. Holbrook in his expansion of domain knowledge in viral epidemiology. As secondary mentor, Prof. Kristian Andersen, Ph.D., of the Scripps Institute will advise Dr. Holbrook in the impactful application of his statistical and computational methodologies to the 2015-2016 Zika virus epidemic. Dr. Holbrook and Profs. Suchard and Andersen will maintain their collaborations after the postdoctoral period.  Research: Bayesian phylogenetics successfully reconstructs evolutionary histories but fails to predict viral spread. Self-exciting point processes are devoid of biological insight and fail to account for geographic networks of diffusion. Aim 1 addresses deﬁciencies in these two complementary viral epidemiological modeling techniques by innovating a combined model where the phylogenetic and self-excitatory components support each other. Aim 2 makes widespread adoption a reality by publishing open-source, massively parallel computing software suitable for big data analysis. Aim 3 reconstructs the 2015-2016 Zika epidemic, learns key geographical routes of transmission and identiﬁes phylogenetic clades with enhanced infectiousness. Project Narrative Tracking and predicting viral outbreaks remains an open epidemiological problem with deadly consequences. Dr. Holbrook will attack the problem with his Bayesian phylogenetic Hawkes processes, a class of models tailored to simultaneously reconstruct evolutionary histories and predict viral diffusion dynamics. With the mentorship of Profs. Marc Suchard (primary) and Kristian Andersen (secondary), Dr. Holbrook will develop open-source, high-performance computing software and apply his statistical computing methodology to the analysis of the 2015-2016 Zika virus epidemic of the Americas, learning key routes of transmission and identifying phylogenetic clades with enhanced infectiousness.",Big Data Predictive Phylogenetics with Bayesian Learning,10039150,K25AI153816,"['Accounting', 'Address', 'Adoption', 'Air', 'Alzheimer&apos', 's Disease', 'Americas', 'Award', 'Bayesian Modeling', 'Bayesian learning', 'Behavior', 'Big Data', 'Biological', 'Biology', 'Collaborations', 'Complex', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Dangerousness', 'Data', 'Data Analyses', 'Data Science', 'Development', 'Development Plans', 'Diffusion', 'Disease Outbreaks', 'Doctor of Medicine', 'Doctor of Philosophy', 'Earthquakes', 'Epidemic', 'Epidemiologist', 'Epidemiology', 'Evaluation', 'Event', 'Evolution', 'Faculty', 'Failure', 'Free Will', 'Generations', 'Geography', 'Goals', 'Health', 'Herd Immunity', 'High Performance Computing', 'Human Genetics', 'Individual', 'Influenza', 'Institutes', 'Investigation', 'Joints', 'Knowledge', 'Lead', 'Learning', 'Mathematics', 'Medical', 'Mentors', 'Mentorship', 'Methodology', 'Modeling', 'Pattern', 'Performance', 'Phylogenetic Analysis', 'Phylogeny', 'Process', 'Publications', 'Publishing', 'Recording of previous events', 'Research', 'Route', 'Schedule', 'Scientist', 'Ships', 'Speed', 'Statistical Computing', 'Stochastic Processes', 'Structure', 'Techniques', 'Testing', 'Time', 'Travel', 'Viral', 'Viral Epidemiology', 'Viral Physiology', 'Work', 'ZIKA', 'Zika Virus', 'blind', 'career development', 'epidemiological model', 'flexibility', 'innovation', 'insight', 'meetings', 'novel', 'open source', 'parallel computer', 'pathogen', 'reconstruction', 'response', 'statistics', 'transmission process']",NIAID,UNIVERSITY OF CALIFORNIA LOS ANGELES,K25,2020,106467,-0.0019932621683867156
"Deep learning based antibody design using high-throughput affinity testing of synthetic sequences Project Summary We will develop and apply a new high-throughput methodology for rapidly designing and testing antibodies for a myriad of purposes, including cancer and infectious disease immunotherapeutics. We will improve upon current approaches for antibody design by providing time, cost, and humane benefits over immunized animal methods and greatly improving the power of present synthetic methods that use randomized designs. To accomplish this, we will display millions of computationally designed antibody sequences using recently available technology, test the displayed antibodies in a high-throughput format at low cost, and use the resulting test data to train molecular dynamics and machine learning methods to generate new sequences for testing. Based on our test data our computational method will identify sequences that have ideal properties for target binding and therapeutic efficacy. We will accomplish these goals with three specific aims. We will develop a new approach to integrated molecular dynamics and machine learning using control targets and known receptor sequences to refine our methods for receptor generalization and model updating from observed data (Aim 1). We will design an iterative framework intended to enable identification of highly effective antibodies within a minimal number of experiments, in which our methods automatically propose promising antibody sequences to profile in subsequent assays (Aim 2). We will employ rounds of automated synthetic design, affinity test, and model improvement to produce highly target-specific antibodies. (Aim 3). ! Project Narrative We will develop new computational methods that learn from millions of examples to design antibodies that can be used to help cure a wide variety of human diseases such as cancer and viral infection. Previous antibody design approaches used a trial and error approach to find antibodies that worked well. In contrast our mathematical methods will directly produce new antibody designs by learning from large-scale experiments that test antibodies for function against disease targets. !",Deep learning based antibody design using high-throughput affinity testing of synthetic sequences,9878070,R01CA218094,"['Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antigens', 'Architecture', 'Binding', 'Biological Assay', 'Budgets', 'Classification', 'Cloud Computing', 'Communicable Diseases', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Fc Receptor', 'Goals', 'Human', 'Immunize', 'Immunotherapeutic agent', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'Modeling', 'Molecular Machines', 'Oligonucleotides', 'Output', 'Performance', 'Phage Display', 'Property', 'Randomized', 'Research', 'Services', 'Specific qualifier value', 'Specificity', 'Statistical Models', 'Technology', 'Test Result', 'Testing', 'Therapeutic', 'Thinness', 'Time', 'Training', 'Treatment Efficacy', 'Update', 'Virus Diseases', 'Work', 'base', 'cloud based', 'commercialization', 'computing resources', 'cost', 'deep learning', 'design', 'experimental study', 'human disease', 'improved', 'iterative design', 'learning strategy', 'machine learning method', 'mathematical methods', 'molecular dynamics', 'novel', 'novel strategies', 'outcome prediction', 'predictive test', 'receptor']",NCI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,591130,-0.013578602064109997
"A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER) Many promising peripheral neuromodulation techniques have been proposed to treat lower urinary tract (LUT) dysfunction, but our lack of predictive models has forced the community (including the PI’s lab) to explore the vast parameter space of nerve targets, stimulation parameterizations, and electrode designs empirically in animal experiments by trial and error. This type of exploratory experimentation is the only current method of optimizing, personalizing, or discovering novel LUT neuromodulation techniques. Motivated by this clinical need, our long-term goal for this work is to predict the effects of neuromodulation on the LUT. To move toward this goal, we propose to develop a new modeling framework that integrates disparate biophysics models through machine learning, thereby emulating an entire organ system through a process we call Biomechanistic Learning Augmentation of Deep Differential Equation Representations (BLADDER). We will develop and use the general BLADDER framework to create an organ-level model of the normal healthy LUT throughout its filling and voiding cycles, including non-volitional neural reflex control over the bladder and urethra. Our focus on neural reflex control and organ-level scales ensures that, if successful, the BLADDER LUT model will be poised to predict effects of neuromodulation using computational studies, which so far has been impossible due to the complexity of the LUT. The BLADDER framework unites multiple individual mechanistic models (each accounting for a component function of an organ system) by using deep recurrent neural networks (RNN) to learn the appropriate coupling dynamics linking each component model. The combination of mechanistic and machine learning models under a single framework allows us to harness the advantages of both: mechanistic models excel at interpretability but suffer from a lack of scalability (becoming intractable at the level of organ systems), while machine learning models are excellent at scale but lack generalizability and insights for hypothesis generation. The BLADDER framework will scale up mechanistic models to the level of systems physiology by linking tractable model components together using a supervisory RNN, allowing the BLADDER framework to deliver both interpretability and scale. We will draw on existing SPARC datasets in the cat (e.g., Bruns and Gaunt), existing publicly available data in rat, and generate new data in the rat to construct a training dataset for the supervisory RNN. We will further draw from already published small-scale mechanistic models, validated on human and animal data, for the mechanistic components of the BLADDER LUT model. The formal process of identifying these models and datasets, and checking their validity and robustness, will clearly reveal the deficits and strengths in our theoretical and experimental understanding of the LUT in a straightforward and rational way. We will use the 10 Simple Rules to vet mechanistic models for inclusion in the BLADDER LUT model and compile a public inventory for the neurourology community. Major task 1 (Q1-2): Identify available datasets and candidate mechanistic models from published literature. Major deliverables are a public database and a whitepaper detailing the state of the field and prospects for modeling and experimental work. Major Task 2 (Q1-3): Demonstrate proof of concept of BLADDER framework. Major deliverables are a publicly available code linking two LUT component models via supervisory RNN and a report on suitable RNN architectures based on fully described dynamical systems. Major Task 3 (Q3-6): Create a multi-component BLADDER model. Major deliverables are code used to link separate mechanistic LUT models via the supervisory RNN, and an in vivo rat dataset to fill in critical measurables for the machine learning training set. Major Task 4 (Q6-8): Deploy the fully operational BLADDER model of the LUT, including autonomously predicted neural reflex control. Major deliverables are publicly available codes and datasets, and a hypothesis-driven computational experiment to predict simple interventions. n/a",A New Paradigm for Systems Physiology Modeling: Biomechanistic Learning Augmentation with Deep Differential Equation Representations (BLADDER),10206953,OT2OD030524,"['Accounting', 'Animal Experiments', 'Bladder', 'Clinical', 'Code', 'Communities', 'Coupling', 'Data', 'Data Set', 'Databases', 'Differential Equation', 'Electrodes', 'Ensure', 'Equipment and supply inventories', 'Felis catus', 'Functional disorder', 'Generations', 'Goals', 'Individual', 'Intervention', 'Learning', 'Link', 'Literature', 'Lower urinary tract', 'Machine Learning', 'Measurable', 'Methods', 'Modeling', 'Nerve', 'Organ', 'Peripheral', 'Physiology', 'Process', 'Publishing', 'Rattus', 'Reflex control', 'Reporting', 'System', 'Techniques', 'Training', 'Urethra', 'Work', 'animal data', 'base', 'biophysical model', 'body system', 'computer studies', 'design', 'dynamic system', 'experimental study', 'human data', 'in vivo', 'insight', 'neural network architecture', 'neuroregulation', 'novel', 'predictive modeling', 'recurrent neural network', 'relating to nervous system', 'scale up']",OD,FLORIDA INTERNATIONAL UNIVERSITY,OT2,2020,1025141,-0.014263362031072039
"The interaction of myosin and the thin filament: how mutations cause allosteric dysfunction and their connection to genetic cardiomyopathy Project Summary: The long-term goal of this research program is to develop a rigorously experimentally validated all-atom computational model of the cardiac thin filament (CTF) bound to myosin S1 which provides a unique and accessible platform to identify novel, high resolution disease mechanisms linked to Hypertrophic Cardiomyopathy (HCM). In the prior funding period, we refined and extended our existing CTF computational model and successfully employed it to identify unique and clinically relevant allosteric disease mechanisms including HCM mutation-induced changes in myofilament Ca2+ kinetics, mutation-specific molecular causes of differential cardiac remodeling and disease progression. This included an in vivo validation via the development of a novel transgenic mouse model of cTnT-linked dilated cardiomyopathy and a predictive algorithm to determine the pathogenicity of cTnT mutations that out-performed existing computational approaches in a preliminary test. The key to these advances has been the ability of the current model to precisely identify and locate allosteric changes caused by mutations throughout all components of the CTF followed by closely coupled experimental validation and eventual in vivo model correlation. We now propose to significantly expand the biological complexity of the model to include myosin S1, the molecular motor that drives contraction and the second most common genetic cause of HCM. This important and challenging advance will facilitate a deeper understanding of disease pathogenesis by, for the first time, incorporating the role of molecular allosteric mechanisms between myosin S1 and thin filament. This new computational – experimental platform will be used for both mechanistic insight (for example used for the identification of novel myofilament disease targets,) and the development of a comprehensive deep-learning predictive algorithm to assign pathogenicity to both myosin and thin filament HCM mutations. The latter represents the first use of high-resolution structure, dynamics and function to predict HCM disease allele pathogenicity, a central challenge in the clinical management of these complex patients. Both the training and testing components of the deep learning development will utilize data from the highly annotated and curated SHaRe HCM registry thus greatly improving translational power. Two Specific Aims will be pursued: Aim 1 will utilize state of the art rare event simulation methods developed in one of our groups and refinement of existing unstructured domains of the CTF via FRET to establish the new model. Aim 2 will employ an extensive program of computational analysis and subsequent in vitro validation using pathogenic, variants of unknown significance and non- pathogenic HCM alleles derived from SHaRe to provide inputs to the machine learning environment for algorithm development. Novel disease mechanisms for myosin and thin filament HCM that include crosstalk between the two components will also be explored. Elucidation of these mechanisms can be the basis for robust molecular approaches to disease. Precision medicine and “molecular” medicine are concepts that aim to employ a patient’s genetic structure to discern the best medical treatments for disease. Hypertrophic cardiomyopathy is a genetic disease that afflicts 1/500 people. This application translates our knowledge of the molecular level effects of cardiac tissue mutation to disease and will aim to lead to eventual treatment.",The interaction of myosin and the thin filament: how mutations cause allosteric dysfunction and their connection to genetic cardiomyopathy,10071638,R01HL107046,"['Address', 'Alleles', 'Anisotropy', 'Artificial Intelligence', 'Biological', 'Biological Assay', 'Biology', 'Biophysics', 'Breath Tests', 'C-terminal', 'Cardiac', 'Chemistry', 'Clinical Management', 'Complex', 'Computer Analysis', 'Computer Models', 'Contracts', 'Coupled', 'Data', 'Data Set', 'Descriptor', 'Development', 'Differential Scanning Calorimetry', 'Dilated Cardiomyopathy', 'Disease', 'Disease Progression', 'Distant', 'Engineering', 'Enzymes', 'Event', 'Fluorescence Anisotropy', 'Fluorescence Resonance Energy Transfer', 'Functional disorder', 'Funding', 'Generations', 'Genetic', 'Genetic Diseases', 'Genetic Structures', 'Goals', 'Grant', 'Hand', 'Human', 'Hypertrophic Cardiomyopathy', 'In Vitro', 'Individual', 'Induced Mutation', 'Kinetics', 'Knowledge', 'Lead', 'Link', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Microfilaments', 'Modeling', 'Molecular', 'Molecular Conformation', 'Molecular Medicine', 'Molecular Motors', 'Motor', 'Mutation', 'Myosin ATPase', 'Pathogenesis', 'Pathogenicity', 'Patients', 'Perception', 'Physiological', 'Play', 'Protein Conformation', 'Protein Dynamics', 'Proteins', 'Registries', 'Research', 'Resolution', 'Resources', 'Role', 'Sampling', 'Sarcomeres', 'Site', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thick Filament', 'Thin Filament', 'Thinness', 'Time', 'Tissues', 'Training', 'Transgenic Mice', 'Translating', 'Validation', 'Variant', 'Work', 'algorithm development', 'automated analysis', 'base', 'cell motility', 'clinically relevant', 'deep learning', 'educational atmosphere', 'experimental study', 'improved', 'in vivo', 'in vivo Model', 'inherited cardiomyopathy', 'insight', 'machine learning algorithm', 'mouse model', 'neural network', 'next generation', 'novel', 'phosphorescence', 'precision medicine', 'prediction algorithm', 'programs', 'quantum chemistry', 'response', 'simulation', 'stopped-flow fluorescence', 'success', 'variant of unknown significance']",NHLBI,UNIVERSITY OF ARIZONA,R01,2020,585711,-0.03978836575345216
"Training a new generation of computational neuroscientists bridging neurobiology The Training Program in Computational Neuroscience (TPCN) will support integrated undergraduate and graduate training in computational neuroscience at New York University. The program will be hosted by the Center for Neural Science (CNS), with participation of faculty in the Departments of Psychology, Mathematics, and Computer Science, and the Institute of Neuroscience at the School of Medicine. The TPCN will fit well with NYU’s unique strengths and recent developments: (1) NYU is one of a few universities with a critical mass of computational neuroscientists. NYU has had a Sloan-Swartz Center for Theoretical Neuroscience since 1994. In the past three years alone, NYU has hired three computational neuroscientists. (2) CNS established an undergraduate major in neuroscience as early as 1992, and thus has a long track record in undergraduate education, it now has 136 students in the current academic year. (3) Recent faculty hiring in CNS, Psychology, and the School of Medicine has greatly expanded our teaching and research capabilities in the neuroscience of cognitive functions and their impairments associated with mental disorders. (3) As NYU is undertaking a merge of two historically separated neuroscience graduate programs (at CNS and the School of Medicine), this training grant will ensure that computational modeling, which has become indispensible in neuroscience, will be front-and-center in the integrated graduate program. (4) NYU is a major center of Artificial Intelligence and Data Science, with close links to Facebook’s AI Center and the Simons Center for Data Analysis. Our training faculty together with these connections will give our students ample opportunities to acquire machine learning techniques for data analysis and learn about brain-like AI algorithms. The proposed training program will support coherent undergraduate and graduate training in computational neuroscience at NYU. It will have several unique features: (1) Innovative mentorship methods: For example, (a) graduate trainees will mentor undergraduate trainees, (b) faculty will explicitly discuss human factors in academic practice; (c) there will be post-mortems after seminars by outside speakers. (2) Computational psychiatry: We propose new courses and research opportunities that are designed specifically to link cognitive function and the neurobiology of neural circuits. We propose innovative education in the nascent field of Computational Psychiatry, to bring theory and circuit modeling to clinical research in mental health. (3) Broad preparation: We aim to prepare trainees for jobs not only in academia, but also in medical and industry research. To achieve this, we will utilize our strength in machine learning and data science to broaden computational neuroscience training. The Program Directors have complementary strengths and will have complementary roles in the program. Wang will supervise graduate trainees and focus on training in mechanistic/circuit-level side of computational neuroscience as well as computational psychiatry. Ma will supervise undergraduate trainees and focus on the computational/behavioral side. This grant will support training of a new generation of graduate and undergraduate students in  computational neuroscience, which has become increasingly important to meet the challenges of  making discoveries with new data analysis tools and of understanding highly nonlinear complex  neural circuits. A salient component of our program is training in the nascent field  of   Computational  Psychiatry,  bridging  basic  neuroscience  and clinical research on mental  disorders. Therefore, the proposed program has the potential of making  a significant impact on  mental health.",Training a new generation of computational neuroscientists bridging neurobiology,10002209,R90DA043849,"['Academia', 'Algorithms', 'Artificial Intelligence', 'Behavioral', 'Brain', 'Clinical Research', 'Computer Models', 'Data Analyses', 'Data Science', 'Development', 'Education', 'Educational process of instructing', 'Ensure', 'Facebook', 'Faculty', 'Generations', 'Grant', 'Human', 'Impairment', 'Industry', 'Institutes', 'Learning', 'Link', 'Machine Learning', 'Medical', 'Mental Health', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Modeling', 'Neurobiology', 'Neurosciences', 'New York', 'Occupations', 'Preparation', 'Psychiatry', 'Psychology', 'Research', 'Role', 'Science', 'Side', 'Students', 'Supervision', 'Teacher Professional Development', 'Techniques', 'Training', 'Training Programs', 'Universities', 'cognitive function', 'cognitive neuroscience', 'computational neuroscience', 'computer science', 'design', 'innovation', 'mathematical sciences', 'medical schools', 'neural circuit', 'programs', 'relating to nervous system', 'theories', 'undergraduate education', 'undergraduate student']",NIDA,NEW YORK UNIVERSITY,R90,2020,100690,-0.0012055586093287541
"SYMPOSIUM ON COGNITIVE AUDITORY NEUROSCIENCE (SCAN) PROJECT SUMMARY/ABSTRACT In recent years, human cognitive auditory neuroscience has made rapid strides due to advances in human neuroimaging, the advent of innovative machine learning/big data analytic approaches, and a greater mechanistic understanding of cognitive-sensory interactions in animal models. The dynamic landscape of this emergent field necessitates a highly interdisciplinary, human and translation-centric symposium that brings together expertise across academia and industry. This application requests partial funding for the Symposium on Cognitive Auditory Neuroscience (SCAN) to be hosted in Pittsburgh, PA in July 2020 and 2022, as a joint venture between Carnegie Mellon University (CMU) and University of Pittsburgh (Pitt). As a biennial meeting, SCAN aims to become the premiere intellectual and professional venue for current research in the emerging field of human cognitive auditory neuroscience. SCAN will incorporate elements typical to academic conferences (research talks, posters) as well as novel ideas that promote ‘blue sky’ thinking in this rapidly evolving field. SCAN will assiduously and innovatively work towards inclusivity and creating an atmosphere that encourages intellectual and professional engagement from women, underrepresented minorities, and individuals with disabilities. Another critical aim of the SCAN is to foster industry-academic partnerships with an eye towards translation of basic research and fostering career opportunities for trainees. Pittsburgh is uniquely situated to launch SCAN. With an enviable concentration of co-located auditory neuroscience expertise, Pittsburgh is also an intellectual hub for industries/start-ups engaged in in machine learning, natural language processing, and speech recognition. SCAN will leverage these advantages to foster growth and innovation tied to core missions of the National Institutes of Deafness and Communication Disorders. PROJECT NARRATIVE The Symposium on Cognitive Auditory Neuroscience (SCAN) has a strong connection to deafness and communication disorders through its focus on the basic science of human cognitive auditory neuroscience, and its translation. SCAN will establish an intellectual home for dissemination of cutting-edge research in human cognitive auditory neuroscience, support the development of the next generation of scientists, build a vibrant and inclusive community that engages with the grand challenges in the field, and forge new academia-industry partnerships.",SYMPOSIUM ON COGNITIVE AUDITORY NEUROSCIENCE (SCAN),9914387,R13DC018243,"['Academia', 'Acoustics', 'Address', 'Affect', 'Americas', 'Animal Model', 'Atmosphere', 'Attention', 'Auditory', 'Auditory Perception', 'BRAIN initiative', 'Base of the Brain', 'Basic Science', 'Behavioral', 'Big Data Methods', 'Brain', 'Clinical', 'Cognitive', 'Communication', 'Communication impairment', 'Communities', 'Complex', 'Development', 'Disabled Persons', 'Disease', 'Ear', 'Educational workshop', 'Elements', 'Environment', 'Eye', 'Fertilization', 'Fostering', 'Funding', 'Geographic Locations', 'Goals', 'Growth', 'Hearing', 'Home environment', 'Human', 'Industry', 'Influentials', 'Institutes', 'Joint Ventures', 'Learning', 'Life', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Mission', 'Natural Language Processing', 'Neurobiology', 'Neurosciences', 'Neurosciences Research', 'Otolaryngology', 'Participant', 'Perception', 'Peripheral', 'Problem Solving', 'Process', 'Request for Applications', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Seeds', 'Sensory', 'Societies', 'Speech', 'Thinking', 'Training', 'Translational Research', 'Translations', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Woman', 'Work', 'analytical tool', 'base', 'career', 'cognitive neuroscience', 'deafness', 'industry partner', 'innovation', 'interest', 'meetings', 'millisecond', 'neuroimaging', 'new technology', 'next generation', 'novel', 'open data', 'posters', 'pressure', 'relating to nervous system', 'sensory input', 'sound', 'speech recognition', 'symposium', 'virtual reality']",NIDCD,CARNEGIE-MELLON UNIVERSITY,R13,2020,36183,-0.02411105283514922
"Novel machine learning approaches for improving structural discrimination in cryo-electron tomography Project Summary Cellular cryo-electron tomography (Cryo-ET) has made possible the observation of cellular organelles and macromolecular complexes at nanometer resolution with native conformations. The rapid increasing amount of Cryo-ET data available however brings along some major challenges for analysis which we will timely ad- dress in this proposal. We will design novel data-driven machine learning algorithms for improving structural discrimination and resolution. In particular, we have the following speciﬁc aims: (1) We will develop a novel Autoencoder and Iterative region Matching (AIM) algorithm for marker-free alignment of image tilt-series to re- construct tomograms with improved resolution; (2) We will develop a saliency-based auto-picking algorithm for better detecting macromolecular complexes, and combine it with an innovative 2D-to-3D framework to further improve structure detection accuracy; (3) We will design an end-to-end convolutional model for pose-invariant clustering of subtomograms. This model will produce an initial clustering which will be reﬁned by a new subto- mogram averaging algorithm that automatically down-weights subtomograms of noise and little contribution; (4) We will perform experimental evaluations by using previously reported bacterial secretion systems and mito- chondrial ultrastructures datasets to improve the ﬁnal resolution. Implementing algorithms in Aims 1-3, we will develop a user-friendly open-source graphical user interface -tom to directly beneﬁt the scientiﬁc community.  -tom will be systematically compared with existing software including IMOD, EMAN2, and Relion on simulated and benchmark datasets. To facilitate distribution, -tom will be integrated into existing software platforms Sci- pion and TomoMiner. Our data-driven algorithms and software not only will facilitate and accelerate the future use of Cryo-ET, but also can be readily used on analyzing the existing large amounts of Cryo-ET data to im- prove our understanding of the structure, function, and spatial organization of macromolecular complexes in situ. Project Narrative This project will create a system of machine learning algorithms to accelerate and facilitate the use and re-use of the rapidly accumulating Cryo-ET datasets. For easy use, we will develop an open-source GUI -Tom (to be disseminated into the Scipion and TomoMiner software platforms) that streamlines the new approaches from the initial tomogram reconstruction step to the ﬁnal subtomogram averaging step. We will validate the performance of our system by applying it on published Cryo-ET datasets and monitor the improvement of the ﬁnal results.",Novel machine learning approaches for improving structural discrimination in cryo-electron tomography,9973462,R01GM134020,"['3-Dimensional', 'Algorithmic Software', 'Algorithms', 'Back', 'Benchmarking', 'Biological Process', 'Cells', 'Communities', 'Computer Analysis', 'Computer software', 'Cryo-electron tomography', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Discrimination', 'Evaluation', 'Future', 'Gaussian model', 'Group Structure', 'Hour', 'Image', 'In Situ', 'Knowledge', 'Laplacian', 'Literature', 'Machine Learning', 'Macromolecular Complexes', 'Manuals', 'Methods', 'Mitochondria', 'Modeling', 'Molecular Conformation', 'Monitor', 'Neurophysiology - biologic function', 'Noise', 'Organelles', 'Performance', 'Process', 'Publishing', 'Reporting', 'Resolution', 'Series', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Tomogram', 'Weight', 'Work', 'autoencoder', 'automated algorithm', 'base', 'deep learning', 'design', 'falls', 'feature detection', 'graphical user interface', 'improved', 'innovation', 'insight', 'machine learning algorithm', 'nano', 'nanometer resolution', 'novel', 'novel strategies', 'open source', 'particle', 'pi-Mesons', 'programs', 'reconstruction', 'success', 'user-friendly']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,342970,-0.012671999448140403
"Machine Learning for Integrative Modeling of the Immune System in Clinical Settings Machine Learning for Integrative Modeling of the Immune System in Clinical Settings In response to an immunological challenge, immune cells act in concert forming complex and dense networks. A deep understanding of these immune responses is often the first step in developing immune therapies and diagnostic tests. Multivariate modeling algorithms can simultaneously consider all measured aspects of the immune system but requires prohibitively larger cohort sizes as technological advancements increase the number of measurements (a.k.a., “Curse of Dimensionality”). To address this, we propose a series of studies to develop machine learning algorithms for comprehensive profiling of the immune system in clinical settings. Particularly, for analysis of the immune system at a single-cell-level, we will leverage the stochastic nature of clustering algorithms to produce a robust pipeline for prediction of clinical outcomes. Next, we introduce the immunological Elastic-Net (iEN) algorithm, which addresses both the curse of dimensionality and reproducibility by integrating prior immunological knowledge into the models.  The cellular systems that govern immunity act through symbiotic interactions with multiple interconnected biological systems. The simultaneous interrogation of these systems with suitable technologies can reveal otherwise unrecognized crosstalk. In collaboration with several leading laboratories, we have produced multiomics datasets (including analysis the genome, proteome, microbiome, and metabolome) in synchronized groups of patients. Using these coordinated datasets, we will evaluate several algorithms for combining multiple biological modalities while accounting for the intrinsic characteristics of each assay, to reveal biological cross- talk across various systems and increase combined predictive power. Importantly, numerous population- level factors (including medical history, environmental, and socioeconomic factors) significantly impact the immune system and studies focused on homogenous patient populations often lack generalizability to other populations. To address this, we will develop machine learning strategies to integrate population-level factors directly into our immunological data. These models will objectively define subpopulations of patients and enable flexibility in the coefficients of the models (and hence, the importance of the various biological measurements) in each group.  This research program will be executed using data from several biorepositories focused on various diseases. This approach will ensure generalizability of our work to previously unseen datasets and increase the long-term impact of our findings. Throughout the proposal, a major area of focus is the development of visualization and model-reduction strategies that lay the foundation for interpretation of complex models. The machine learning algorithms developed will be readily applicable to a broad range of multiomics and multicohort studies and will be available as open-source software. PROJECT NARRATIVE Recent technological advances have enabled the production of large immune monitoring datasets, providing an opportunity for systems-level efforts to harness the power of the immune system in developing immune therapies and diagnostic tests. In this project, we will develop machine learning algorithms for analysis of the immune system at a single-cell level, in a multiomics setting integrated with various other biological measurements, and subject to adjustments based on population-level factors. This work will provide a strong quantitative bridge between large-scale epidemiologic trends and deep biological profiling to investigate the complex mechanisms that govern the immune system in clinical settings.",Machine Learning for Integrative Modeling of the Immune System in Clinical Settings,10028766,R35GM138353,"['Accounting', 'Address', 'Algorithmic Analysis', 'Algorithms', 'Area', 'Biological', 'Biological Assay', 'Cells', 'Characteristics', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Data', 'Data Set', 'Development', 'Diagnostic tests', 'Dimensions', 'Disease', 'Ensure', 'Environmental Risk Factor', 'Epidemiological trend', 'Foundations', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunologic Monitoring', 'Immunological Models', 'Immunologics', 'Immunotherapy', 'Knowledge', 'Laboratories', 'Machine Learning', 'Measurement', 'Measures', 'Medical History', 'Modality', 'Modeling', 'Nature', 'Patients', 'Population', 'Production', 'Proteome', 'Reproducibility', 'Research', 'Series', 'Socioeconomic Factors', 'System', 'Technology', 'Visualization', 'Work', 'base', 'biobank', 'biological systems', 'cohort', 'flexibility', 'genome analysis', 'learning strategy', 'machine learning algorithm', 'metabolome', 'microbiome', 'multiple omics', 'open source', 'patient population', 'patient subsets', 'predict clinical outcome', 'programs', 'response']",NIGMS,STANFORD UNIVERSITY,R35,2020,382710,-0.012018606408170191
"Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches Project Summary The form (or shape) and function relationship of anatomical structures is a central theme in biology where abnor- mal shape changes are closely tied to pathological functions. Morphometrics has been an indispensable quan- titative tool in medical and biological sciences to study anatomical forms for more than 100 years. Recently, the increased availability of high-resolution in-vivo images of anatomy has led to the development of a new generation of morphometric approaches, called statistical shape modeling (SSM), that take advantage of modern computa- tional techniques to model anatomical shapes and their variability within populations with unprecedented detail. SSM stands to revolutionize morphometric analysis, but its widespread adoption is hindered by a number of sig- niﬁcant challenges, including the complexity of the approaches and their increased computational requirements, relative to traditional morphometrics. Arguably, however, the most important roadblock to more widespread adop- tion is the lack of user-friendly and scalable software tools for a variety of anatomical surfaces that can be readily incorporated into biomedical research labs. The goal of this proposal is thus to address these challenges in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM), which automat- ically constructs optimal statistical landmark-based shape models of ensembles of anatomical shapes without relying on any speciﬁc surface parameterization. The proposed research will provide an automated, general- purpose, and scalable computational solution for constructing shape models of general anatomy. In Aim 1, we will build computational and machine learning algorithms to model anatomies with complex surface topologies (e.g., surface openings and shared boundaries) and highly variable anatomical populations. In Aim 2, we will introduce an end-to-end machine learning approach to extract statistical shape representation directly from im- ages, requiring no parameter tuning, image pre-processing, or user assistance. In Aim 3, we will provide intuitive graphical user interfaces and visualization tools to incorporate user-deﬁned modeling preferences and promote the visual interpretation of shape models. We will also make use of recent advances in cloud computing to enable researchers with limited computational resources and/or large cohorts to build and execute custom SSM work- ﬂows using remote scalable computational resources. Algorithmic developments will be thoroughly evaluated and validated using existing, fully funded, large-scale, and constantly growing databases of CT and MRI images lo- cated on-site. Furthermore, we will develop and disseminate standard workﬂows and domain-speciﬁc use cases for complex anatomies to promote reproducibility. Efforts to develop the proposed technology are aligned with the mission of the National Institute of General Medical Sciences (NIGMS), and its third strategic goal: to bridge biology and quantitative science for better global health through supporting the development of and access to computational research tools for biomedical research. Our long-term goal is to increase the clinical utility and widespread adoption of SSM, and the proposed research will establish the groundwork for achieving this goal. Project Narrative This project will develop general-purpose, scalable, and open-source statistical shape modeling (SSM) tools, which will present unique capabilities for automated anatomy modeling with less user input. The proposed tech- nology will introduce a number of signiﬁcant improvements to current SSM approaches and tools, including the support for challenging modeling problems, inferring shapes directly from images (and hence bypassing the seg- mentation step), parallel optimizations for speed, and new user interfaces that will be much easier and scalable than the current tools. The proposed technology will constitute an indispensable resource for the biomedical and clinical communities that will enable new avenues for biomedical research and clinical investigations, provide new ways to answer biologically related questions, allow new types of questions to be asked, and open the door for the integration of SSM with clinical care.","Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches",9969467,R01AR076120,"['Address', 'Adoption', 'Age', 'Algorithms', 'Anatomic Models', 'Anatomic Surface', 'Anatomy', 'Area', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological Testing', 'Biology', 'Biomedical Research', 'Brain', 'Bypass', 'Cardiology', 'Cessation of life', 'Clinical', 'Clinical Data', 'Cloud Computing', 'Collection', 'Communities', 'Complex', 'Complex Analysis', 'Computational Technique', 'Computer Models', 'Computer software', 'Computers', 'Custom', 'Data', 'Databases', 'Development', 'Disease', 'Felis catus', 'Funding', 'Generations', 'Geometry', 'Goals', 'Human', 'Ice', 'Image', 'Imagery', 'Injury', 'Intuition', 'Laboratory Research', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mathematical Computing', 'Measures', 'Medical', 'Medicine', 'Mission', 'Modeling', 'Modernization', 'Modification', 'Morphogenesis', 'National Institute of General Medical Sciences', 'Occupations', 'Online Systems', 'Organism', 'Orthopedics', 'Pathologic', 'Population', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Science', 'Scientist', 'Shapes', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Speed', 'Statistical Data Interpretation', 'Structure', 'Supervision', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Variant', 'Visual', 'Visualization software', 'Work', 'algorithm development', 'base', 'biomedical resource', 'clinical care', 'clinical investigation', 'clinically relevant', 'cohort', 'computerized tools', 'computing resources', 'deep learning', 'experience', 'flexibility', 'global health', 'graphical user interface', 'image archival system', 'image processing', 'imaging Segmentation', 'in vivo imaging', 'innovation', 'large datasets', 'machine learning algorithm', 'model development', 'multidisciplinary', 'open source', 'particle', 'preference', 'software development', 'tool', 'usability', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,614363,-0.016641245648742406
"Abiotic-Biotic Interfaces for Ophthalmology Symposium ABSTRACT This proposal seeks funding to support a symposium, Abiotic-Biotic Interfaces for Ophthalmology (ABI), which will bring together recognized world experts in clinical, research, vision science, engineering, industrial and pharmaceutical communities as well as junior investigators (i.e., young faculty and those in training) to discuss the current state of ABI, ranging from bioelectronic implantable and wearable devices, to nanoscale scaffolds for stem cell and gene therapies. Given the multidisciplinary nature of this field, it is essential to bring together researchers and clinicians with varying levels of expertise across many domains related to ABI to advance the progress of this novel field, identify challenges of advancement, and develop a strategic action plan to overcome these challenges. The timing to have such a symposium to further the application of implantable and/or wearable bioengineered systems in ophthalmology is now as we focus on precision and personalized medicine and leverage the revolution in deep learning artificial intelligence algorithms. Through symposium talks, sessions, and discussions we will cover the fundamentals and also identify innovative and cutting-edge strategies and methodologies to accelerate the rate of major discoveries and development of novel therapeutics. The specific aims of this symposium are: Specific Aim 1. To bring together both established and junior investigators representing a broad range of disciplines to discuss cutting edge research in this novel field, catalyze the development of cross-disciplinary and translational approaches to advance abiotic-biotic interfaces for ophthalmology, and identify gaps in knowledge and barriers to advancement. We will identify research questions and develop an agenda to guide future research that is consistent with the objectives and interests of NEI. Specific Aim 2. Develop a junior investigator program to motivate a diverse group of students and junior investigators to pursue research careers in vision science and ophthalmologic therapeutic development, who will ultimately submit grant proposals to NEI solicitations and contribute to the scientific literature. Specific Aim 3. Develop a strategic action plan to set priorities for future studies that will encourage inter-agency collaborations (e.g., NEI, NSF, DARPA, etc.). This is critical because often certain engineering tasks are best suited to be supported by NSF or DARPA whereas the biological testing of the engineered systems lends itself to funding from NEI. Hence such inter-agency or cross-agency efforts can help leverage the funding to develop sophisticated abiotic-biotic systems NARRATIVE This meeting is the first on this topic dedicated to the broad use of implantable and/or wearable bioelectronics for ophthalmological applications. It is anticipated that the strategic action plan will significantly impact the field by greatly accelerating the translation of basic science and engineering research findings to stimulate the development of novel treatments and improve clinical practice. Key topics include visual restoration, drug and gene delivery, and sensing intraocular pressure. This meeting will foster training and development of future leaders in this emerging field and promote collaboration and exchange of knowledge and ideas among junior and established investigators.",Abiotic-Biotic Interfaces for Ophthalmology Symposium,10070800,R13EY031988,"['Algorithms', 'Applications Grants', 'Artificial Intelligence', 'Basic Science', 'Biological Testing', 'Biomedical Engineering', 'Cellular Phone', 'Clinical Research', 'Collaborations', 'Communities', 'Computer software', 'Contact Lenses', 'Custom', 'Data', 'Development', 'Devices', 'Diagnosis', 'Discipline', 'Disease', 'Drug Delivery Systems', 'Electronics', 'Engineering', 'Eye', 'Faculty', 'Fostering', 'Funding', 'Future', 'Gene Delivery', 'Glass', 'Industrialization', 'Intraocular lens implant device', 'Knowledge', 'Literature', 'Medicine', 'Methodology', 'Nature', 'Neural Retina', 'Ophthalmology', 'Optics', 'Pharmacologic Substance', 'Physiologic Intraocular Pressure', 'Physiological', 'Research', 'Research Personnel', 'Route', 'Scientific Inquiry', 'Scientist', 'Senior Scientist', 'Students', 'System', 'Time', 'Training', 'Translations', 'Virtual and Augmented reality', 'Visual', 'base', 'career', 'clinical practice', 'deep learning', 'gene therapy', 'implantable device', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'meetings', 'multidisciplinary', 'nanoscale', 'neural network', 'novel', 'novel therapeutics', 'personalized medicine', 'portability', 'precision medicine', 'programs', 'restoration', 'scaffold', 'stem cell therapy', 'symposium', 'therapeutic development', 'translational approach', 'vision science', 'wearable device']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R13,2020,42465,-0.015987629804021204
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10058463,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data reuse', 'data sharing', 'data visualization', 'data warehouse', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,627034,-0.010345167150437635
"Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data Big data is now ubiquitous in every field of modern scientific research. Many contemporary applications, such as the recent national microbiome initiative (NMI), greatly demand highly flexible statistical machine learning methods that can produce both interpretable and reproducible results. Thus, it is of paramount importance to identify crucial causal factors that are responsible for the response from a large number of available covariates, which can be statistically formulated as the false discovery rate (FDR) control in general high-dimensional nonlinear models. Despite the enormous applications of shotgun metagenomic studies, most existing investigations concentrate on the study of bacterial organisms. However, viruses and virus-host interactions play important roles in controlling the functions of the microbial communities. In addition, viruses have been shown to be associated with complex diseases. Yet, investigations into the roles of viruses in human diseases are significantly underdeveloped. The objective of this proposal is to develop mathematically rigorous and computationally efficient approaches to deal with highly complex big data and the applications of these approaches to solve fundamental and important biological and biomedical problems. There are four interrelated aims. In Aim 1, we will theoretically investigate the power of the recently proposed model-free knockoffs (MFK) procedure, which has been theoretically justified to control FDR in arbitrary models and arbitrary dimensions. We will also theoretically justify the robustness of MFK with respect to the misspecification of covariate distribution. These studies will lay the foundations for our developments in other aims. In Aim 2, we will develop deep learning approaches to predict viral contigs with higher accuracy, integrate our new algorithm with MFK to achieve FDR control for virus motif discovery, and investigate the power and robustness of our new procedure. In Aim 3, we will take into account the virus-host motif interactions and adapt our algorithms and theories in Aim 2 for predicting virus-host infectious interaction status. In Aim 4, we will apply the developed methods from the first three aims to analyze the shotgun metagenomics data sets in ExperimentHub to identify viruses and virus-host interactions associated with several diseases at some target FDR level. Both the algorithms and results will be disseminated through the web. The results from this study will be important for metagenomics studies under a variety of environments. Big data is ubiquitous in biological research. Identifying causal factors associated with complex diseases or traits from big data is highly important and challenging. New statistical and computational tools will be developed to control False Discovery Rate (FDR) for molecular sequence data based on the novel model-free knockoffs framework. They will be used to detect sequence motifs for viruses and motif-pairs for virus-host interactions, and to analyze multiple metagenomics data sets related to complex diseases.",Adaptive Reproducible High-Dimensional Nonlinear Inference for Big Biological Data,9923688,R01GM131407,"['Address', 'Algorithms', 'Archaea', 'Attention', 'Bacteria', 'Big Data', 'Biological', 'Bypass', 'Cells', 'Colorectal Cancer', 'Complex', 'Computer software', 'Consult', 'Coupled', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Effectiveness', 'Environment', 'Foundations', 'Frequencies', 'Gaussian model', 'Genes', 'Genetic Materials', 'Genomics', 'Healthcare', 'Human', 'Internet', 'Investigation', 'Joints', 'Length', 'Linear Regressions', 'Literature', 'Liver Cirrhosis', 'Mathematics', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Molecular Sequence Data', 'Mutation', 'Neurosciences', 'Non-Insulin-Dependent Diabetes Mellitus', 'Non-linear Models', 'Obesity', 'Organism', 'Performance', 'Planet Earth', 'Play', 'Procedures', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Sampling Studies', 'Shotguns', 'Social Sciences', 'Testing', 'Theoretical Studies', 'Tissues', 'Training', 'Viral', 'Virus', 'Visualization software', 'Work', 'base', 'biological research', 'computerized tools', 'contig', 'dark matter', 'deep learning', 'deep learning algorithm', 'design', 'flexibility', 'high dimensionality', 'human disease', 'human tissue', 'improved', 'interest', 'learning strategy', 'machine learning method', 'metagenomic sequencing', 'microbial community', 'microbiome', 'microbiome research', 'model design', 'model development', 'new technology', 'novel', 'power analysis', 'response', 'simulation', 'statistical and machine learning', 'theories', 'trait', 'user-friendly', 'virus host interaction', 'virus identification']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,276700,0.004751162392444918
"EVOLVING VIRUS-SPECIFIC sACE2 MIMICS FOR COMPETITIVE INHIBITION OF SARS-CoV-2 PROJECT SUMMARY The rapid spread of the highly-pathogenic, novel SARS-coronavirus 2 (SARS-CoV-2) has caused a global health emergency. Thus, there is a desperate need for effective antiviral therapeutics to counteract this virus. The SARS-CoV-2 virus enters cells using the ACE2 receptor1 which binds the viral spike protein2. In its soluble form, ACE2 (sACE2) has the potential to be used as a stable and non-immunogenic competitive inhibitor to SARS-CoV-2 and is presently being explored in clinical trials3. Due to the potential negative side effects of anti-spike mAbs18, and the fact that ACE2 exhibits other biological roles4–6 including integrin signaling regulation7,8, spike-specific receptor mimics would yield novel therapeutics for SARS-CoV-2 and potentially other highly infectious diseases. This proposal seeks to use machine learning and directed evolution to develop high affinity, yet endogenously-inactive mimics of sACE2 in order to create rapidly implementable therapeutics to combat SARS-CoV-2 and potential corona-like viruses. This approach would allow for the generation of scalable and translatable biologics, and provide a platform to rapidly course-correct for potential mutations that may arise in the future. Utilizing deep-learning with UniRep49, will design and generate sACE2 variants that tightly bind the SARS-CoV2-2 spike protein but do not cross-interact with endogenous targets such as integrins [Aim 1]. Simultaneously, we will perform directed evolution to optimize spike-binding and select against variants that bind endogenous proteins [Aim 2]. Finally, we will identify lead candidates and evaluate the tolerance and immunogenicity of engineered sACE2 variants in mice [Aim 3]. Collectively, this proposal will develop highly-specific ACE2 receptor mimics in order to create novel antivirals with minimal immunogenicity in time to save lives and prevent future outbreaks. 10 Project Narrative There is desperate need for novel, effective, and scalable antiviral medicines to combat the COVID-19 pandemic. The SARS-CoV-2 virus enters human cells by binding to the ACE2 receptor, and a soluble version of that receptor is being explored in clinical trials as a potential “competitive inhibitor”: the virus will bind to the decoy instead of the real receptor. The goal of this project is to combine computational and laboratory evolution to create superior receptor decoys that more tightly bind to the virus but don't affect any other functions of the human body, enabling them to be safely delivered at high levels with minimal side effects.",EVOLVING VIRUS-SPECIFIC sACE2 MIMICS FOR COMPETITIVE INHIBITION OF SARS-CoV-2,10175307,R21AI158169,"['2019-nCoV', 'Affect', 'Affinity', 'Albumins', 'Antiviral Agents', 'Binding', 'Binding Proteins', 'Biological', 'Biological Assay', 'COVID-19 pandemic', 'Cells', 'Clinical', 'Clinical Trials', 'Communicable Diseases', 'Data', 'Directed Molecular Evolution', 'Disease Outbreaks', 'Dose', 'Effectiveness', 'Engineering', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Goals', 'Human', 'Human body', 'Integrin Binding', 'Integrins', 'Laboratories', 'Left', 'Libraries', 'Machine Learning', 'Medicine', 'Mus', 'Mutation', 'Pathogenicity', 'Patients', 'Process', 'Proteins', 'Recombinants', 'Safety', 'Signal Transduction', 'Therapeutic', 'Time', 'Toxic effect', 'Training', 'Variant', 'Viral', 'Virus', 'Virus Diseases', 'Virus Inhibitors', 'base', 'combat', 'cross reactivity', 'deep learning', 'design', 'global health emergency', 'immunogenicity', 'improved', 'in vivo', 'inhibitor/antagonist', 'lead candidate', 'mutant', 'novel', 'novel therapeutics', 'prevent', 'receptor', 'side effect']",NIAID,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2020,399346,-0.02408362060788135
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9847973,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data cleaning', 'data ecosystem', 'data reuse', 'data sharing', 'data standards', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2020,489919,-0.005347690771447512
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,-0.010273784864385424
"Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution Project Summary/Abstract It is hard to overstate the importance of monoclonal antibodies in the life sciences. Antibodies are critical tools in biomedical research and diagnostics (e.g. western blotting, immunoprecipitation, cytometry, biomarker discovery, and histology), are one of the most rapidly growing class of therapeutics, and are the basis for myriad new strategies in cancer therapy, such as checkpoint inhibitors that are revolutionizing treatment. Unfortunately, current methods for the generation of custom antibodies, including animal immunization and phage display, are slow, costly, inaccessible to most researchers, and often unsuccessful. We propose Autonomously EvolvinG Yeast-displayed antibodieS (AEGYS), a system for the continuous and rapid evolution of high-quality antibodies against custom antigens that requires only the simple culturing of yeast cells. We believe this can be achieved by combining cutting-edge generative machine learning algorithms for antibody library design with a new technology for in vivo continuous evolution and a yeast antigen-presenting cell that we will engineer. If successful, AEGYS should have a transformative impact across the whole of biomedicine by turning monoclonal antibody generation into a rapid, scalable, and accessible process where any lab with standard molecular biology capabilities can generate custom antibodies on demand simply by “immunizing” a test tube of yeast cells with an antigen. We anticipate that this democratization of antibody generation will also result in an explosion of crowdsourced antibody sequence data that will train our machine learning algorithms to design better antibody libraries for AEGYS, starting a virtuous cycle. We ourselves will use AEGYS to generate a panel of subtype- and conformation-specific nanobodies against biogenic amine receptors including those that respond to acetylcholine, adrenaline, dopamine, and other neurotransmitters, so that we can understand their role in neurobiology and addiction.! Project Narrative This proposal will provide a system for the scalable continuous evolution and computational design of antibodies against user-selected antigens. Antibodies are critical tools in medical research and are the basis for numerous therapies, but the generation of custom antibodies against new targets is a difficult and specialized task. The system proposed will turn antibody generation into a routine and widely accessible process for researchers in almost any field.","Making antibody generation rapid, scalable, and democratic through machine learning and continuous evolution",10021311,R01CA260415,"['Acetylcholine', 'Affinity', 'Animals', 'Antibodies', 'Antibody Affinity', 'Antibody Formation', 'Antigen Targeting', 'Antigen-Presenting Cells', 'Antigens', 'Architecture', 'Area', 'Back', 'Biogenic Amine Receptors', 'Biological Sciences', 'Biomedical Research', 'Cell Surface Receptors', 'Cells', 'Chemistry', 'Clinic', 'Collection', 'Communities', 'Cultured Cells', 'Custom', 'Cytometry', 'Data', 'Data Set', 'Detergents', 'Diagnostic', 'Directed Molecular Evolution', 'Docking', 'Dopamine', 'Elements', 'Engineering', 'Epidemic', 'Epinephrine', 'Evolution', 'Explosion', 'G-Protein-Coupled Receptors', 'Generations', 'Genes', 'Genetic', 'Histology', 'Human', 'Hybridomas', 'Image', 'Immune checkpoint inhibitor', 'Immune system', 'Immunization', 'Immunize', 'Immunoglobulin Fragments', 'Immunoprecipitation', 'Libraries', 'Machine Learning', 'Medical Research', 'Medicine', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Conformation', 'Monoclonal Antibodies', 'Neuraxis', 'Neurobiology', 'Neurosciences', 'Neurotransmitters', 'Nobel Prize', 'Outcome', 'Pathogen detection', 'Phage Display', 'Pharmaceutical Preparations', 'Pheromone', 'Play', 'Problem Solving', 'Process', 'Production', 'Protein Engineering', 'Proteins', 'Proteome', 'Public Health', 'Reagent', 'Research', 'Research Personnel', 'Role', 'Signal Transduction', 'Specificity', 'Speed', 'Surface', 'System', 'Techniques', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Training', 'Tube', 'Update', 'V(D)J Recombination', 'Western Blotting', 'Yeasts', 'addiction', 'antibody engineering', 'antibody libraries', 'antigen binding', 'base', 'biomarker discovery', 'cancer therapy', 'cost', 'crowdsourcing', 'decision research', 'design', 'empowered', 'experimental study', 'follow-up', 'improved', 'in vivo', 'innovation', 'insight', 'interest', 'machine learning algorithm', 'nanobodies', 'new technology', 'novel', 'receptor', 'response', 'scaffold', 'structural biology', 'tool']",NCI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2020,1690552,-0.016726685382368496
"National Resource for Network Biology (NRNB) OVERALL - PROJECT SUMMARY The mission of the National Resource for Network Biology (NRNB) is to advance the science of biological networks by creating leading-edge bioinformatic methods, software tools and infrastructure, and by engaging the scientific community in a portfolio of collaboration and training opportunities. Much of biomedical research is dependent on knowledge of biological networks of multiple types and scales, including molecular interactions among genes, proteins, metabolites and drugs; cell communication systems; relationships among genotypes and biological and clinical phenotypes; and patient and social networks. NRNB-supported platforms like Cytoscape are among the most widely used software tools in biology, with tens of thousands of active users, enabling researchers to apply network concepts and data to understand biological systems and how they are reprogrammed in disease.  NRNB’s three Technology Research and Development projects introduce innovative concepts with the potential to transform network biology, transitioning it from a static to a dynamic science (TR&D 1); from flat network diagrams to multi-scale hierarchies of biological structure and function (TR&D 2); and from descriptive interaction maps to predictive and interpretable machine learning models (TR&D 3). In previous funding periods our technology projects have produced novel and highly cited approaches, including network-based biomarkers for stratification of disease, data-driven gene ontologies assembled completely from network data, and deep learning models of cell structure and function built using biological networks as a scaffold.  During the next period of support, we introduce dynamic regulatory networks formulated from single-cell transcriptomics and advanced proteomics data (TR&D 1); substantially improved methodology for the study of hierarchical structure and pleiotropy in biological networks (TR&D 2); and procedures for using networks to seed machine learning models of drug response that are both mechanistically interpretable and transferable across biomedical contexts (TR&D 3). These efforts are developed and applied in close collaboration with outside investigators from 19 Driving Biomedical Projects who specialize in experimental generation of network data, disease biology (cancer, neuropsychiatric disorders, diabetes), single-cell developmental biology, and clinical trials. TR&Ds are also bolstered by 7 Technology Partnerships in which NRNB scientists coordinate technology development with leading resource-development groups in gene function prediction, mathematics and algorithm development, and biomedical databases. Beyond these driving collaborations, we continually support a broader portfolio of transient (non-driving) research collaborations; organize and lead international meetings including the popular Network Biology track of the Intelligent Systems for Molecular Biology conference; and deliver a rich set of training opportunities and network analysis protocols. OVERALL - PROJECT NARRATIVE We are all familiar with some of the components of biological systems – DNA, proteins, cells, organs, individuals – but understanding biological systems involves more than just cataloging its component parts. It is critical to understand the many interactions of these parts within systems, and how these systems give rise to biological functions and responses and determine states of health and disease. The National Resource for Network Biology provides the scientific community with a broad platform of computational tools for the study of biological networks and for incorporating network knowledge in biomedical research.",National Resource for Network Biology (NRNB),9937486,P41GM103504,"['Area', 'Automobile Driving', 'Beds', 'Behavior', 'Binding', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biological Process', 'Biological Sciences', 'Biology', 'Biomedical Research', 'Biomedical Technology', 'Cataloging', 'Catalogs', 'Cell Communication', 'Cell model', 'Cell physiology', 'Cells', 'Cellular Structures', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Computer software', 'Conceptions', 'DNA', 'Data', 'Data Set', 'Databases', 'Developmental Cell Biology', 'Diabetes Mellitus', 'Disease', 'Disease stratification', 'Drug Modelings', 'Ecosystem', 'Educational workshop', 'Event', 'Expert Systems', 'Feedback', 'Funding', 'Gene Proteins', 'Generations', 'Genes', 'Genetic Risk', 'Genomics', 'Genotype', 'Goals', 'Health', 'Individual', 'Infrastructure', 'International', 'Knowledge', 'Lead', 'Life', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mentors', 'Methodological Studies', 'Methods', 'Mission', 'Modeling', 'Molecular Biology', 'National Institute of General Medical Sciences', 'Network-based', 'Ontology', 'Organ', 'Pathway Analysis', 'Patients', 'Pharmaceutical Preparations', 'Phase Transition', 'Phenotype', 'Positioning Attribute', 'Procedures', 'Proteins', 'Proteomics', 'Protocols documentation', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Running', 'Science', 'Scientist', 'Seeds', 'Services', 'Social Network', 'Software Tools', 'Structure', 'Students', 'System', 'Technology', 'Testing', 'Tissues', 'Training', 'Visual', 'Visualization', 'Work', 'algorithm development', 'biological systems', 'clinical phenotype', 'cloud storage', 'computational platform', 'computerized tools', 'deep learning', 'gene function', 'genomics cloud', 'improved', 'innovation', 'interoperability', 'lens', 'mathematical algorithm', 'meetings', 'method development', 'multi-scale modeling', 'neuropsychiatric disorder', 'next generation', 'novel', 'pleiotropism', 'prediction algorithm', 'programs', 'protein metabolite', 'response', 'scaffold', 'single cell analysis', 'software infrastructure', 'symposium', 'technology development', 'technology research and development', 'tool', 'training opportunity', 'transcriptome', 'transcriptomics']",NIGMS,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",P41,2020,1995376,-0.014385553327265274
"Learning to learn in structural biology with deep neural networks Project Summary/Abstract  Deep learning is gaining traction across many elds as a powerful tool. In medicine, there have been recent successes in drug design, predicting protein structure, and in functional genomics. These successes have thus far been in areas where there are hundreds of thousands of data points and deep learning in medicine is still limited by lack of large homongeous datasets.  This proposal focuses on applying a new kind of deep learning called meta-learning that mimics the human-like ability to learn from few examples. The PI will establish a sustainable research program on meta-learning by developing benchmark problems and datasets. The PI will further explore meta-learning speci cally on peptide-protein structure and NMR spectra prediction. Due to the imperative need for interpretability when using deep learning in medicine, a strong component will be connecting biophysical modeling with the deep learning models.  The outcome of this work will be a demonstrated new approach to deep learning that can work with little data. The PI will bring these research ideas together to design peptides that can bind to intrinsically disordred proteins, a challenging but important task for curing neurodegenerative diseases. This will be accomplished through meta-learning, molecular simulation, and iterative peptide design. Project Narrative  Deep learning is a technique from arti cial intelligence that has driven many high-pro le break- throughs in recognizing objects in images, translating human languages, and playing games like Chess and Go. Its use is medicine is currently limited by deep learning's need for large amounts of data and its lack of interpretability. This researh plan works towards solving these challenges and applies interpretable deep learning to designing new therapeutics.",Learning to learn in structural biology with deep neural networks,10027477,R35GM137966,"['Area', 'Benchmarking', 'Binding', 'Data', 'Data Set', 'Drug Design', 'Human', 'Image', 'Intelligence', 'Language', 'Learning', 'Medicine', 'Modeling', 'Molecular', 'Neurodegenerative Disorders', 'Outcome', 'Peptides', 'Play', 'Proteins', 'Research', 'Techniques', 'Traction', 'Translating', 'Work', 'biophysical model', 'deep learning', 'deep neural network', 'design', 'functional genomics', 'novel strategies', 'novel therapeutics', 'programs', 'protein structure', 'simulation', 'structural biology', 'success', 'tool']",NIGMS,UNIVERSITY OF ROCHESTER,R35,2020,360287,-0.0386611978971414
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,10136941,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'learning classifier', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2020,746725,-0.021230589961812065
"Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry PROJECT SUMMARY  Cellular interactions with the environment form the basis of health and disease for all organisms. Exposure to nutrients, toxins, and neighboring cells trigger coordinated molecular responses that impact cell function and metabolism in a beneficial, adaptive, or detrimental manner. Although the benefits of multicellularity for the formation of complex tissue structures or the function of entire organ systems has been long appreciated, it has only recently been understood that microbial inhabitants of vertebrates also have a tremendous impact on host cell function and dysfunction. Despite this, an understanding of these interactions has not moved beyond simple associations, and there are virtually no molecular technologies available that adequately define how a complex microbial ecosystem impacts host cell function, or how the host response to microbial colonization affects the bacterial community. This gap in knowledge is striking when one considers the broad and significant impact that microbes have on human health. In this application, we propose to expressly fill this knowledge gap through development of a novel multimodal imaging pipeline that will provide 3-dimensional information on the molecular heterogeneity of microbial communities and the immune response at the host-pathogen interface.  This proposal combines our expertise in immunology, infection biology, mass spectrometry, small animal imaging, machine learning, and computer vision to develop an integrated multimodal visualization method for studying infectious disease. Our unique approach will computationally combine ultra-high speed (~50px/s) MALDI-TOF images, ultra-high mass resolution (>200,000 resolving power) MALDI FTICR IMS, metal imaging by LA-ICP-IMS, high-spatial resolution optical microscopy, and MR imaging using data-driven image fusion. This strategy will enable 3-D molecular images to be generated for thousands of elements, metabolites, lipids, and proteins with an unprecedented combination of chemical specificity and spatial fidelity more than 50x faster than is currently possible. We will use this next-generation imaging capability to (i) define the heterogeneous microbial subpopulations throughout the 3-D volume of a S. aureus community, (ii) uncover the host molecules that form the abscess and accumulate to restrict microbial growth in murine models, and (iii) elucidate molecular markers that differentiate in vivo biofilms at the host-pathogen interface, between abscesses at various stages of progression, and under distinct degrees of nutrient stress. These studies will uncover new targets for therapeutic intervention and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research. PROJECT NARRATIVE This proposal will enable detailed views of the molecular components of infectious disease with unprecedented resolution through the development of a multimodal, 3-dimensional imaging platform. The proposed technologies will improve throughput and molecular specificity, enable automated high-precision and high-accuracy image alignment, and allow for descriptions of molecular signals in 3-D through the fusion of multi-modal imaging data. These studies will uncover targets for therapeutic intervention and antibiotic development and the techniques developed as a result of this proposal will be broadly applicable to all physiologically relevant processes, profoundly impacting biomedical research.",Molecular mapping of microbial communities at the host-pathogen interface by multi-modal 3-dimensional imaging mass spectrometry,9989025,R01AI138581,"['3-Dimensional', 'Abscess', 'Affect', 'Animal Model', 'Animals', 'Anterior nares', 'Antibiotics', 'Antibodies', 'Architecture', 'Awareness', 'Bacteria', 'Bacterial Infections', 'Bacterial Proteins', 'Behavior', 'Biology', 'Biomedical Research', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Cellular Metabolic Process', 'Chemicals', 'Communicable Diseases', 'Communities', 'Complement', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Custom', 'Data', 'Development', 'Diagnosis', 'Differentiation Antigens', 'Dimensions', 'Disease', 'Ecosystem', 'Elements', 'Environment', 'Exposure to', 'Fourier transform ion cyclotron resonance', 'Functional disorder', 'Genus staphylococcus', 'Glean', 'Growth', 'Health', 'Health Promotion', 'Heterogeneity', 'Histology', 'Human', 'Image', 'Imaging technology', 'Immune', 'Immune response', 'Immunology', 'Imprisonment', 'Individual', 'Infection', 'Infectious Diseases Research', 'Integration Host Factors', 'Knowledge', 'Label', 'Lesion', 'Lipids', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Mass Spectrum Analysis', 'Metals', 'Methodology', 'Methods', 'Microbe', 'Microbial Biofilms', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nutrient', 'Optics', 'Organism', 'Pathogenesis', 'Physiological', 'Population', 'Process', 'Proteins', 'Reagent', 'Research', 'Resolution', 'Sampling', 'Signal Transduction', 'Site', 'Source', 'Spatial Distribution', 'Specificity', 'Spectrometry, Mass, Matrix-Assisted Laser Desorption-Ionization', 'Speed', 'Staphylococcus aureus', 'Stress', 'Structure', 'Techniques', 'Technology', 'Therapeutic Intervention', 'Three-Dimensional Imaging', 'Three-dimensional analysis', 'Tissues', 'Toxin', 'Vertebrates', 'Visualization', 'Work', 'animal imaging', 'bacterial community', 'base', 'body system', 'commensal bacteria', 'experimental study', 'host colonization', 'imaging capabilities', 'imaging detection', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'innovation', 'interest', 'microbial', 'microbial colonization', 'microbial community', 'microscopic imaging', 'molecular imaging', 'molecular marker', 'mouse model', 'multimodality', 'neutrophil', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'protein expression', 'response', 'supervised learning', 'targeted treatment', 'virtual']",NIAID,VANDERBILT UNIVERSITY MEDICAL CENTER,R01,2020,612684,-0.023991320238286696
"Evidence Extraction Systems for the Molecular Interaction Literature Burns, Gully A. Abstract  In primary research articles, scientists make claims based on evidence from experiments, and report both the claims and the supporting evidence in the results section of papers. However, biomedical databases de- scribe the claims made by scientists in detail, but rarely provide descriptions of any supporting evidence that a consulting scientist could use to understand why the claims are being made. Currently, the process of curating evidence into databases is manual, time-consuming and expensive; thus, evidence is recorded in papers but not generally captured in database systems. For example, the European Bioinformatics Institute's INTACT database describes how different molecules biochemically interact with each other in detail. They characterize the under- lying experiment providing the evidence of that interaction with only two hierarchical variables: a code denoting the method used to detect the molecular interaction and another code denoting the method used to detect each molecule. In fact, INTACT describes 94 different types of interaction detection method that could be used in conjunction with other experimental methodological processes that can be used in a variety of different ways to reveal different details about the interaction. This crucial information is not being captured in databases. Although experimental evidence is complex, it conforms to certain principles of experimental design: experimentally study- ing a phenomenon typically involves measuring well-chosen dependent variables whilst altering the values of equally well-chosen independent variables. Exploiting these principles has permitted us to devise a preliminary, robust, general-purpose representation for experimental evidence. In this project, We will use this representation to describe the methods and data pertaining to evidence underpinning the interpretive assertions about molecular interactions described by INTACT. A key contribution of our project is that we will develop methods to extract this evidence from scientiﬁc papers automatically (A) by using image processing on a speciﬁc subtype of ﬁgure that is common in molecular biology papers and (B) by using natural language processing to read information from the text used by scientists to describe their results. We will develop these tools for the INTACT repository but package them so that they may then also be used for evidence pertaining to other areas of research in biomedicine. Burns, Gully A. Narrative  Molecular biology databases contain crucial information for the study of human disease (especially cancer), but they omit details of scientiﬁc evidence. Our work will provide detailed accounts of experimental evidence supporting claims pertaining to the study of these diseases. This additional detail may provide scientists with more powerful ways of detecting anomalies and resolving contradictory ﬁndings.",Evidence Extraction Systems for the Molecular Interaction Literature,9983144,R01LM012592,"['Area', 'Binding', 'Biochemical', 'Bioinformatics', 'Biological Assay', 'Burn injury', 'Classification', 'Co-Immunoprecipitations', 'Code', 'Communities', 'Complex', 'Consult', 'Consumption', 'Data', 'Data Reporting', 'Data Set', 'Database Management Systems', 'Databases', 'Detection', 'Disease', 'Engineering', 'European', 'Event', 'Experimental Designs', 'Experimental Models', 'Gel', 'Goals', 'Grain', 'Graph', 'Image', 'Informatics', 'Information Retrieval', 'Institutes', 'Intelligence', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Molecular Weight', 'Names', 'Natural Language Processing', 'Paper', 'Pattern', 'Positioning Attribute', 'Privatization', 'Process', 'Protein Structure Initiative', 'Proteins', 'Protocols documentation', 'Publications', 'Reading', 'Records', 'Reporting', 'Research', 'Scientist', 'Source Code', 'Specific qualifier value', 'Structural Models', 'Structure', 'Surface', 'System', 'Systems Biology', 'Taxonomy', 'Text', 'Time', 'Training', 'Typology', 'Western Blotting', 'Work', 'base', 'data modeling', 'experimental study', 'human disease', 'image processing', 'machine learning method', 'open source', 'optical character recognition', 'protein protein interaction', 'repository', 'software systems', 'structured data', 'text searching', 'tool']",NLM,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,264232,-0.016683818160472035
"Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data Project Abstract/Summary Our interdisciplinary research team will develop algorithms to accelerate the detection of respiratory virus outbreaks at an unprecedented local scale in US cities. We propose to advance outbreak detection by combining machine learning data integration methods and spatial models of disease transmission. The dynamic models that will be developed will provide mechanistic engines for distinguishing typical from atypical disease trends and the optimization methods evaluate the informativeness of data sources to achieve specified public health goals through the rapid evaluation of diverse input data sources. Working with local healthcare and public health leaders, we will translate the algorithms into user-friendly online tools to support preparedness plans and decision-making. Our proposed research is organized around three major aims. In Aim 1, we will apply machine learning and signal processing methods to build systems that track the earliest indicators of emerging outbreaks within seven US cities. We will evaluate non-clinical data reflecting early and mild symptoms as well as clinical data covering underserved communities and geographic and demographic hotspots for viral emergence. In Aim 2, we will develop sub-city scale models reflecting the syndemics of co-circulating respiratory viruses and chronic respiratory diseases (CRD) that can exacerbate viral infections. We will infer viral transmission rates and socio-environmental risk cofactors by fitting the model to respiratory disease data extracted from millions of electronic health records (EHRs) for the last nine years. We will then partner with clinical and EHR experts to translate our models into the first outbreak detection system for severe respiratory viruses that incorporates EHR data on CRDs. Using machine learning techniques, we will further integrate other surveillance, environmental, behavioral and internet predictor data sources to maximize the accuracy, sensitivity, speed and population coverage of our algorithms. In Aim 3, we will develop an open-access Python toolkit to facilitate the integration of next generation data into outbreak surveillance models. This project will produce practical early warning algorithms for detecting emerging viral threats at high spatiotemporal resolution in several US cities, elucidate socio-geographic gaps in current surveillance systems and hotspots for viral emergence, and provide a robust design framework for extrapolating these algorithms to other US cities. Project Narrative We will develop innovative algorithms for detecting emerging respiratory viruses within US cities. To do so, we will model the syndemic dynamics of respiratory viruses and chronic respiratory diseases and apply machine learning to combine geospatial data that track early indicators of emerging threats. Working with local public health and healthcare collaborators, we will translate this research into practical tools for addressing socio- geographic gaps in surveillance and accelerating the detection, prevention and mitigation of severe outbreaks.","Accelerating viral outbreak detection in US cities using mechanistic models, machine learning and diverse geospatial data",9946212,R01AI151176,"['Absenteeism', 'Address', 'African', 'Age', 'Algorithm Design', 'Algorithms', 'Area', 'Articulation', 'Bayesian Method', 'Behavioral', 'Caring', 'Chronic', 'Chronic Disease', 'Cities', 'Climate', 'Clinical', 'Clinical Data', 'Collaborations', 'Communicable Diseases', 'Communities', 'Data', 'Data Sources', 'Decision Making', 'Detection', 'Disease', 'Disease Outbreaks', 'Disease Surveillance', 'Disease model', 'Ebola', 'Electronic Health Record', 'Ensure', 'Epidemic', 'Evaluation', 'Geography', 'Goals', 'Health', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Location', 'Lung diseases', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mexico', 'Modeling', 'Neighborhoods', 'Pollution', 'Population', 'Prevention', 'Public Health', 'Pythons', 'Readiness', 'Reporting', 'Research', 'Resolution', 'Risk', 'Rural', 'Schools', 'Sentinel', 'Series', 'Signal Transduction', 'Social Environment', 'Specific qualifier value', 'Speed', 'Subgroup', 'Surveillance Modeling', 'Symptoms', 'System', 'Techniques', 'Testing', 'Time', 'Translating', 'Uncertainty', 'Validation', 'Viral', 'Virus', 'Virus Diseases', 'Visualization', 'Work', 'World Health Organization', 'austin', 'base', 'cofactor', 'comorbidity', 'dashboard', 'data acquisition', 'data integration', 'design', 'digital', 'disease transmission', 'diverse data', 'epidemiologic data', 'epidemiological model', 'experimental study', 'flexibility', 'global health', 'health care availability', 'health goals', 'high risk', 'high risk population', 'influenza outbreak', 'influenzavirus', 'innovation', 'insight', 'metropolitan', 'next generation', 'novel', 'outcome prediction', 'pandemic disease', 'public health intervention', 'respiratory virus', 'school district', 'signal processing', 'simulation', 'social media', 'sociodemographic group', 'socioeconomics', 'sound', 'spatiotemporal', 'stem', 'tool', 'transmission process', 'trend', 'user-friendly', 'viral transmission']",NIAID,YALE UNIVERSITY,R01,2020,611043,-0.018321297835243062
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9961522,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2020,15000,-0.009969527488571573
"Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1 PROJECT SUMMARY Previous studies showed discrepancies of health and behavior prevalence between American Indians (AI) population and other racial or ethnic groups. Most health surveys have certain limitations when studying AI population due to the small sample sizes for AI population. Data collected by AI Tribal Epidemiology Centers (TECs) provides an excellent opportunity to conduct research for AI population due to sufficient sample size and extensive information. However, most surveys conducted by TECs used non-probability sampling design (e.g. convenient sample) due to its lower cost and increased time efficiency. Non-probability sample may suffer from sampling, coverage and nonresponse errors without further proper adjustments. Such difficulties greatly hampers the analysis of AI population in health and behavior research. Our general hypothesis is that data integration by combining information from non-probability and probability samples can reduce sampling, coverage and nonresponse errors in original non-probability sample. The Goal of this project is to develop an accurate and robust data integration methodology for AI population analysis specifically tailored to health and behavior research. During the past years, we have 1) studied data integration using calibration and parametric modeling approaches; 2) investigated machine learning and propensity score modeling methods in survey sampling and other fields; and 3) assembled an experienced team of multi-disciplinary team of experts. In this project, we propose to capitalize on our expertise and fulfill the following Specific Aims: Aim 1. Develop a data integration approach using machine learning and propensity score modeling We will develop machine learning and propensity score based data integration approaches to combine information from non-probability and probability samples. Compared to existing methods (i.e., Calibration, Parametric approach), our proposed approaches are more robust against the failure of underlying model assumptions. The inference is more general and multi-purpose (e.g. one can estimate most parameters such as means, totals and percentiles). Simulation studies will be performed to compare our proposed methods with other existing methods. A computing package will be built to implement the method in other settings. Aim 2. Evaluate the accuracy and robustness of the proposed method in AI health and behavior research We will use real data to validate the proposed methods in terms of accuracy and robustness to the various data types. The performance will also be assessed by comparing with results from existing data integration methods such as calibration and parametric modeling approaches. The planned study takes advantage of a unique data source and expands the impact of the Indian Health Service (IHS)-funded research. We expect this novel integration method will vertically advance the field by facilitating the analysis based on non-probability sample, which can provide in-depth understanding regarding the AI population health and behavior studies. Project Narrative The overall goal of this R21 project is to develop an accurate, robust and multi-purpose data integration methodology for AI population (non-probability sample) analysis specifically tailored to health and behavior research such as diabetes and smoking. The code implementing the proposed method will be released and is general enough to be applied to AI population studies of other fileds. The success of this study will vertically advance the field by facilitating the AI population analysis, which can provide a better guidance and new insights on the future precision personalized prevention and treatment of certain diseases.",Improving the representativeness of American Indian Tribal Behavioral Risk Factor Surveillance System (TBRFSS) by machine learning and propensity score based data integration approach A1,10063407,R21MD014658,"['Adult', 'Age', 'American', 'American Indians', 'Behavioral', 'Behavioral Risk Factor Surveillance System', 'Calibration', 'Censuses', 'Code', 'Communities', 'Community Surveys', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Sources', 'Diabetes Mellitus', 'Disease', 'Epidemiology', 'Ethnic group', 'Event', 'Failure', 'Funding', 'Future', 'General Population', 'Geographic state', 'Goals', 'Health', 'Health Fairs', 'Health Surveys', 'Health behavior', 'High Prevalence', 'Kansas', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Not Hispanic or Latino', 'Oklahoma', 'Performance', 'Population', 'Population Analysis', 'Population Study', 'Prevalence', 'Probability', 'Probability Samples', 'Publishing', 'Race', 'Research', 'Research Personnel', 'Respondent', 'Risk Factors', 'Sample Size', 'Sampling', 'Smoking', 'Surveys', 'Target Populations', 'Testing', 'Texas', 'Time', 'Tobacco', 'Training', 'United States Indian Health Service', 'Weight', 'Work', 'Youth', 'base', 'behavioral study', 'cigarette smoking', 'cluster computing', 'cost', 'data integration', 'data quality', 'design', 'experience', 'improved', 'individualized prevention', 'innovation', 'insight', 'multidisciplinary', 'novel', 'personalized medicine', 'population health', 'simulation', 'smoking prevalence', 'success', 'therapy development', 'tribal health']",NIMHD,UNIVERSITY OF OKLAHOMA HLTH SCIENCES CTR,R21,2020,115176,-0.006998523938762267
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9874005,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,360227,-0.01651863740108497
"Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios PROJECT SUMMARY The emergence and global expansion of SARS-CoV-2 as a human pathogen over the last four months represents a nearly unprecedented challenge for the infectious disease modelling community. This pandemic has benefitted from huge volumes of data being generated, but the rate of dissemination of these data has often outpaced existing data pipelines. While the last decade has seen significant advances in real-time infectious disease forecasting — spurred by rapid growth in data and computational methods — these methods have primarily focused on seasonal endemic diseases based, are based on historical data, and so do not apply easily to this novel pathogen, or to pandemic scenarios. New methods are needed to leverage the wealth of surveillance data at fine spatial granularity, together with associated information about policy interventions and environmental conditions over space and time, to reason directly about the mechanisms to forecast and understand the transmission dynamics of SARS-CoV-2 transmission. These methods must use sound statistical and epidemiological principles and be flexible and computationally efficient to provide real- time forecasts to guide public health decision-making and respond to changing aspects of this global crisis. The central research activities of this project are (1) to develop scalable, computationally efficient Bayesian hierarchical compartmental models to flexibly respond to state-level public health forecasting needs, and (2) to design models and conduct analyses to draw robust inference about the effectiveness of interventions in impacting the reproductive rate of SARS-CoV-2 infections within the US to build an evidence-base for continued responses to COVID-19 and future pandemics. PUBLIC HEALTH NARRATIVE The SARS-CoV-2 pandemic is an emerging public health crisis. A fundamental challenge is how to turn data into evidence that can inform decision-making about managing resources, improving health outcomes, and controlling further spread of SARS-CoV-2. Real-time forecasting and flexible mechanistic models to understand the disease dynamics can provide policy-makers tools to manage public response. The goal of the proposed research is to adapt existing statistical modeling frameworks and develop new ones for making forecasts of COVID-19 in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: expanding dynamic time-series and machine learning approaches for pandemic scenarios,10150377,R35GM119582,"['2019-nCoV', 'Award', 'Budgets', 'COVID-19', 'Calibration', 'Centers for Disease Control and Prevention (U.S.)', 'Cessation of life', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Contracts', 'Data', 'Data Collection', 'Data Reporting', 'Data Sources', 'Decision Making', 'Dengue', 'Dengue Fever', 'Development', 'Disease', 'Disease Outbreaks', 'Effectiveness of Interventions', 'Endemic Diseases', 'Epidemic', 'Epidemiology', 'Evaluation Methodology', 'Future', 'Goals', 'Grant', 'Health', 'Hospitalization', 'Infection', 'Influenza', 'Influenza prevention', 'Intervention', 'Machine Learning', 'Methods', 'Modeling', 'Natural experiment', 'Outcome', 'Performance', 'Policies', 'Policy Maker', 'Programmed Learning', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Resources', 'Series', 'Social Distance', 'Standardization', 'Statistical Methods', 'Statistical Models', 'Structure', 'Testing', 'Thailand', 'Time', 'Work', 'base', 'data dissemination', 'data pipeline', 'deep learning', 'evidence base', 'flexibility', 'human pathogen', 'improved', 'infectious disease model', 'influenza epidemic', 'machine learning method', 'model design', 'novel', 'pandemic disease', 'pathogen', 'predictive modeling', 'rapid growth', 'real world application', 'reproductive', 'response', 'seasonal influenza', 'sound', 'statistical and machine learning', 'surveillance data', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2020,78507,-0.01581282101647662
"Image-guided robot for high-throughput microinjection of Drosophila embryos PROJECT SUMMARY This proposal is submitted in response to the NIH Development of Animal Models and Related Biological Materials for Research (R21) program. The proposal develops an image-guided robotic platform that performs the automated delivery of molecular genetic tools and non-genetically encoded reagents such as chemical libraries, fluorescent dyes to monitor cellular processes, functionalized magnetic beads, or nanoparticles into thousands of Drosophila embryos in a single experimental session. The proposed work builds on recent engineering innovations in our collaborative group which has developed image-guided robotic systems that can precisely interface with single cells in intact tissue. The two Specific Aims provide for a systematic development of the proposed technologies. AIM 1 first engineers a robotic platform (‘Autoinjector’) that can scan and image Drosophila embryos in arrays of egg laying plates. We will utilize machine learning algorithms for automated detection of embryos, followed by thresholding and morphology analysis to detect embryo centroids and annotate injection sites. In AIM 2, we will utilize microprocessor-controlled fluidic circuits for programmatic delivery of femtoliter to nanoliter volumes of reagents into individual embryos. We will quantify the efficacy of the Autoinjector by comparing the survival, fertility, and transformation rates of transposon or PhiC31-mediated transgenesis to manual microinjection datasets. Finally, we will demonstrate the efficient delivery of sgRNAs and mutagenesis in the presence of Cas9. This project fits very well within the goals of the program by engineering a novel tool for producing and improving animal models. The Autoinjector will accelerate Drosophila research and empower scientists to perform novel experiments and genome-scale functional genomics screens that are currently too inefficient or labor intensive to be conducted on a large scale and may additionally enable other novel future applications. PROJECT NARRATIVE This proposal develops a technology platform that will enable automated microinjection of molecular genetic tools and non-genetically encoded tools such as chemical libraries, fluorescent dyes, functionalized magnetic beads, or nanoparticles, into thousands of Drosophila embryos in a single experimental session. The successful development of this technology will empower Drosophila biologists to perform screens and develop new applications that are currently too inefficient or labor intensive to contemplate and will accelerate research into the function of the nervous system and the molecular and genetic underpinnings of numerous diseases in this important animal model.",Image-guided robot for high-throughput microinjection of Drosophila embryos,9989196,R21OD028214,"['Animal Model', 'Biocompatible Materials', 'Biological Assay', 'Caliber', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Collection', 'Computer Vision Systems', 'Cryopreservation', 'Data Set', 'Detection', 'Development', 'Disease', 'Drosophila genus', 'Drosophila melanogaster', 'Embryo', 'Engineering', 'Expenditure', 'Exploratory/Developmental Grant', 'Fertility', 'Fluorescent Dyes', 'Future', 'Gene Transfer Techniques', 'Genetic', 'Goals', 'Guide RNA', 'Image', 'Individual', 'Injections', 'Investigation', 'Laboratories', 'Liquid substance', 'Location', 'Machine Learning', 'Manuals', 'Mediating', 'Methods', 'Microinjections', 'Microprocessor', 'Microscope', 'Molecular', 'Molecular Biology', 'Molecular Genetics', 'Monitor', 'Morphology', 'Motivation', 'Mutagenesis', 'Needles', 'Nervous System Physiology', 'Performance', 'Process', 'Reagent', 'Research', 'Resources', 'Robot', 'Robotics', 'Scanning', 'Scientist', 'Signaling Molecule', 'Site', 'Space Perception', 'System', 'Technology', 'Tissues', 'Transgenes', 'Transgenic Organisms', 'United States National Institutes of Health', 'Work', 'animal model development', 'automated algorithm', 'base', 'biological research', 'cost', 'egg', 'experience', 'experimental study', 'functional genomics', 'gene product', 'genetic manipulation', 'genome-wide', 'image guided', 'improved', 'innovation', 'machine learning algorithm', 'magnetic beads', 'mutant', 'mutation screening', 'nanolitre', 'nanoparticle', 'novel', 'novel strategies', 'programs', 'response', 'robotic system', 'screening', 'small molecule libraries', 'stem', 'technology development', 'tool']",OD,UNIVERSITY OF MINNESOTA,R21,2020,222618,-0.01568456585551068
"Automated data curation to ensure model credibility in the Vascular Model Repository Three-dimensional anatomic modeling and simulation (3D M&S) in cardiovascular (CV) disease have become a crucial component of treatment planning, medical device design, diagnosis, and FDA approval. Comprehensive, curated 3-D M&S databases are critical to enable grand challenges, and to advance model reduction, shape analysis, and deep learning for clinical application. However, large-scale open data curation involving 3-D M&S present unique challenges; simulations are data intensive, physics-based models are increasingly complex and highly resolved, heterogeneous solvers and data formats are employed by the community, and simulations require significant high-performance computing resources. Manually curating a large open-data repository, while ensuring the contents are verified and credible, is therefore intractable. We aim to overcome these challenges by developing broadly applicable automated curation data science to ensure model credibility and accuracy in 3-D M&S, leveraging our team’s expertise in CV simulation, uncertainty quantification, imaging science, and our existing open data and open source projects. Our team has extensive experience developing and curating open data and software resources. In 2013, we launched the Vascular Model Repository (VMR), providing 120 publicly-available datasets, including medical image data, anatomic vascular models, and blood flow simulation results, spanning numerous vascular anatomies and diseases. The VMR is compatible with SimVascular, the only fully open source platform providing state-of-the-art image-based blood flow modeling and analysis capability to the CV simulation community. We propose that novel curation science will enable the VMR to rapidly intake new data while automatically assessing model credibility, creating a unique resource to foster rigor and reproducibility in the CV disease community with broad application in 3D M&S. To accomplish these goals, we propose three specific aims: 1) Develop and validate automated curation methods to assess credibility of anatomic patient-specific models built from medical image data, 2) Develop and validate automated curation methods to assess credibility of 3D blood flow simulation results, 3) Disseminate the data curation suite and expanded VMR. The proposed research is significant and innovative because it will 1) enable rapid expansion of the repository by limiting curator intervention during data intake, leveraging compatibility with SimVascular, 2) increase model credibility in the CV simulation community, 3) apply novel supervised and unsupervised approaches to evaluate anatomic model fidelity, 4) leverage reduced order models for rapid assessment of complex 3D data. This project assembles a unique team of experts in cardiovascular simulation, the developers of SimVascular and creator of the VMR, a professional software engineer, and radiology technologists. We will build upon our successful track record of launching and supporting open source and open data resources to ensure success. Data curation science for 3D M&S will have direct and broad impacts in other physiologic systems and to ultimately impact clinical care in cardiovascular disease. Cardiovascular anatomic models and blood flow simulations are increasingly used for personalized surgical planning, medical device design, and the FDA approval process. We propose to develop automated data curation science to rapidly assess credibility of anatomic models and 3D simulation data, which present unique challenges for large-scale data curation. Leveraging our open source SimVascular project, the proposed project will enable rapid expansion of the existing Vascular Model Repository while ensuring model credibility and reproducibility to foster innovation in clinical and basic science cardiovascular research.",Automated data curation to ensure model credibility in the Vascular Model Repository,10016840,R01LM013120,"['3-Dimensional', 'Adoption', 'Anatomic Models', 'Anatomy', 'Basic Science', 'Blood Vessels', 'Blood flow', 'Cardiac', 'Cardiovascular Diseases', 'Cardiovascular Models', 'Cardiovascular system', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Databases', 'Diagnosis', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Feedback', 'Fostering', 'Funding', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Incentives', 'Intake', 'Intervention', 'Joints', 'Laws', 'Machine Learning', 'Manuals', 'Maps', 'Mechanics', 'Medical Device Designs', 'Medical Imaging', 'Methods', 'Modeling', 'Musculoskeletal', 'Operative Surgical Procedures', 'Patient risk', 'Patients', 'Physics', 'Physiological', 'Process', 'Publications', 'Radiology Specialty', 'Recording of previous events', 'Reproducibility', 'Research', 'Resolution', 'Resources', 'Risk Assessment', 'Running', 'Science', 'Software Engineering', 'Source Code', 'Supervision', 'System', 'Techniques', 'Time', 'Triage', 'Uncertainty', 'United States National Institutes of Health', 'automated analysis', 'base', 'clinical application', 'clinical care', 'computing resources', 'data curation', 'data format', 'data resource', 'data warehouse', 'deep learning', 'experience', 'gigabyte', 'imaging Segmentation', 'innovation', 'large scale data', 'models and simulation', 'novel', 'online repository', 'open data', 'open source', 'repository', 'respiratory', 'shape analysis', 'simulation', 'software development', 'stem', 'success', 'supercomputer', 'supervised learning', 'three-dimensional modeling', 'treatment planning', 'unsupervised learning', 'web portal']",NLM,STANFORD UNIVERSITY,R01,2020,330502,-0.010604769574916632
"Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach Project Summary Biology is full of stunning examples of emergent behaviors – behaviors that arise from, but cannot be reduced to, the interactions of the constituent parts that make up the system under consideration. These behaviors span the full spectrum of length scales, from the emergence of distinct cell fates (e.g. neurons, muscle, etc.) due to the interactions of genes within cells, to the formation of complex ecological communities arising from the interactions of thousands of species. The overarching goal of my research is to develop new conceptual, theoretical, and computational tools to model such emergent, system-level behaviors in biology. To do so, we utilize an interdisciplinary approach that is grounded in Biological Physics, but draws heavily from Machine Learning, Information Theory, and Theoretical Ecology. Our work is unified and distinguished by our deep commitment to integrating theory with the vast amount of biological data now being generated by modern DNA sequencing-based techniques and quantitative microscopy. An important goal of the proposed research is to find common concepts and tools that transcend traditional biological sub-disciplines and models systems. The proposed research pursues four distinct but conceptually interrelated research directions: (1) understanding how distinct cell fates emerge from bimolecular interactions within mammalian cells (2) investigating how bimolecular networks within cells exploit energy consumption to improve computations, with applications to Synthetic Biology; (3) identifying the ecological principles governing community assembly in microbial communities and developing techniques for synthetically engineering ecological communities; and (4) developing new machine learning algorithms and techniques for biological data analysis. In addition to developing physics-based models for diverse biological phenomena, the proposed research will yield a series of practical important tools and algorithms which we will make publically available including: (1) a new linear-algebra based algorithm for assessing the fidelity of directed differentiation and cellular reprogramming protocols and visualizing reprogramming/differentiation dynamics and (2) improved algorithms for inferring microbial interactions in the human microbiome from high-throughput sequence data. These computational tools will allow scientists to realize the immense therapeutic potential of cellular reprogramming and microbial ecology-based techniques for studying and treating human disease. Project Narrative This project will develop new theoretical and computational tools that will allow scientists to realize the immense therapeutic potential of cellular reprogramming (the conversion of one cell type into another cell type) and microbial ecology-based techniques for studying and treating human diseases.",Modeling Emergent Behaviors in Systems Biology: A Biological Physics Approach,9963304,R35GM119461,"['Algorithms', 'Behavior', 'Biological', 'Biological Models', 'Biological Phenomena', 'Biology', 'Cells', 'Communities', 'Complex', 'Consumption', 'DNA sequencing', 'Data', 'Data Analyses', 'Discipline', 'Ecology', 'Engineering', 'Goals', 'Human Microbiome', 'Information Theory', 'Length', 'Linear Algebra', 'Machine Learning', 'Mammalian Cell', 'Modeling', 'Modernization', 'Muscle', 'Neurons', 'Physics', 'Protocols documentation', 'Quantitative Microscopy', 'Research', 'Scientist', 'Series', 'System', 'Systems Biology', 'Techniques', 'Therapeutic', 'Transcend', 'Work', 'base', 'cell type', 'computerized tools', 'gene interaction', 'human disease', 'improved', 'interdisciplinary approach', 'machine learning algorithm', 'microbial', 'microbial community', 'microorganism interaction', 'synthetic biology', 'theories', 'tool']",NIGMS,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R35,2020,330294,-0.031055592645089218
"Learning Dynamics of Biological Processes from Time Course Omics Datasets Complex biological processes, including organ development, immune response and disease progression, are inherently dynamic. Learning their regulatory architecture requires understanding how components of a large system dynamically interact with each other and give rise to emergent behavior. Recent experimental advances have made ii possible to investigate these biological systems in a data-driven fashion al high temporal resolution, allowing identification of new genes and their regulatory interactions. Longitudinal omics data sets are becoming increasingly common in clinical practice as well. Information on these collections of interacting genes can be integrated to gain systems-level insights into the roles of biological pathways and processes, including progression of diseases. Consequently, developing interpretable methods for learning functional relationships among genes, proteins or metabolites from high-dimensional time series data has become a timely research problem. The nature of these time-course data sets presents exciting opportunities and interesting challenges from a statistical perspective. Typical time-course omics data sets are challenging because of their high-dimensionality and non-linear relationships among system components. To tackle these challenges, one needs sophisticated dimension-reduction techniques that are biologically meaningful, computationally efficient and allow uncertainty quantification. Methods that incorporate prior biological information (e.g., pathway membership, protein-protein interactions) into the data analysis are good candidates for analyzing such high-dimensional systems using small samples. Here, we will develop three core methods to address the above challenges - (Aim 1): an empirical Bayes framework for clustering high-dimensional omics time-course data using prior biological knowledge; (Aim 2): a quantile-based Granger causality framework for learning interactions among genes or metabolites from their lead-lag relationships; and (Aim 3): a decision tree ensemble framework for searching cascades of interactions among genes from their temporal expression profiles. Our interdisciplinary team of statisticians and scientists will analyze time-course omics data from three research projects: (i) innate immune response systems in Drosophila, (ii) developmental process in mouse models, and (ii) longitudinal metabolite profiling of TB patients. These insights will be used to build and validate our methodology, which will be implemented in a publicly available software. This proposal is innovative in its incorporation of prior biological knowledge in the framework of novel dimension reduction techniques for interrogating high-dimensional time-course omics data. This research is significant in that it will impact basic sciences by elucidating data-driven, testable hypotheses on the regulatory architecture of biological processes, and clinical practice by monitoring disease progression and prognosis. n/a",Learning Dynamics of Biological Processes from Time Course Omics Datasets,10021429,R01GM135926,"['Address', 'Algorithms', 'Architecture', 'Basic Science', 'Behavior', 'Biological', 'Biological Process', 'Clinical', 'Collection', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Decision Trees', 'Development', 'Developmental Process', 'Dimensions', 'Disease Progression', 'Drosophila genus', 'Etiology', 'Expression Profiling', 'Gene Proteins', 'Genes', 'Grouping', 'Immune System Diseases', 'Immune response', 'Innate Immune Response', 'Knowledge', 'Lead', 'Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Nature', 'Pathway Analysis', 'Pathway interactions', 'Patients', 'Pattern', 'Process', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Scientist', 'Series', 'Silicon Dioxide', 'Structure', 'System', 'Techniques', 'Time', 'Uncertainty', 'Validation', 'Variant', 'base', 'biological systems', 'clinical practice', 'dynamic system', 'experimental study', 'high dimensionality', 'innovation', 'insight', 'learning algorithm', 'learning strategy', 'mouse model', 'novel', 'open source', 'organ growth', 'outcome forecast', 'protein protein interaction', 'random forest', 'temporal measurement']",NIGMS,CORNELL UNIVERSITY,R01,2020,344345,-0.01288260704072481
"Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry Project Summary The growth and acceptance of wearable devices (e.g., accelerometers) and personal technologies (e.g., smartphones), coupled with larger storage capacities, waterproofing, and more unobtrusive wear locations, has made long-term monitoring of behaviors throughout the 24-hour spectrum more feasible. Wearable devices relevant for human activity (e.g., GENEActiv accelerometer) contain several complementary sensors (accelerometers, gyro, heart- rate monitor etc.) and sample at high rates (e.g., 100Hz for accelerometer). These high-sampling rates and the long duration of capture result in life-log data that truly qualifies as multimodal and big time-series data. The challenges and opportunities involved in fully harvesting these types of data, for widely applicable interventions, suggest that an interdisciplinary approach spanning mathematical sciences, signal processing, and health is needed. Our innovation includes the use of functional-data analysis tools to represent and process the dense time-series data. Functional data analysis is then integrated into machine learning and pattern discovery algorithms for activity classification, prediction of attributes, and discovery of new activity classes. We anticipate that the proposed framework will lead to new insights about human activity and its impact on health outcomes. This interdisciplinary project builds on several research activities of the team. Our past work includes: a) new mathematical developments for computing statistics on time-series data viewed as elements of a function-spaces, b) algorithms for activity recognition that integrate the function-space techniques, and c) data from long-term observational studies of human activity from multimodal sensors. The new work we propose addresses the unique mathematical and computational challenges posed by densely multimodal, long-term, densely-sampled lifelog big-data in a comprehensive framework. The fusion of ideas from human activity modeling, functional-analysis, geometric metrics, and algorithmic machine learning, present unique opportunities for fundamental advancement of the state-of-the-art in objective measurement and quantification of behavioral markers from wearable devices. The proposed approach also brings to fore: a) new mathematical developments of elastic metrics over multi-modal time-series data, b) comparing sequences evolving on different feature manifolds, c) estimation of quasi- periodicities, d) and a new generation of machine-learning and pattern discovery algorithms. The mathematical and algorithmic tools proposed have the potential to significantly advance how wearable data from contemporary devices with high-sampling rates and large storage capabilities are represented, processed, and transformed into accurate inferences about human activity. Wearable devices are becoming more widely adopted in recent years for general health and recreational uses by the broad populace. This research will result in improved algorithms to process the data available from such wearable devices. The long-term goal of the research is to enable personalized home-based physical activity regimens for conditions such as stroke and diabetes. Project Number: 1R01GM135927-01 Title: Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry Project Narrative In this revision application, we seek to submit an equipment supplement to our existing R01 referenced above. As our project progressed, we found that it is important to consider the role of new emerging feature-learning approaches to extract downstream time-series features. To fully develop our approach and conduct additional experiments, we need significant GPU computational resources that will be dedicated to this project.",Dense Life-log Health Analytics from Wearable Sensors using Functional Analysis and Riemannian Geometry,10135658,R01GM135927,"['Accelerometer', 'Address', 'Adopted', 'Algorithmic Analysis', 'Algorithmic Software', 'Algorithms', 'Awareness', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Big Data', 'Cellular Phone', 'Classification', 'Coupled', 'Data', 'Data Analyses', 'Development', 'Devices', 'Diabetes Mellitus', 'Dimensions', 'Elements', 'Equipment', 'Generations', 'Geometry', 'Goals', 'Growth', 'Harvest', 'Health', 'Home environment', 'Hour', 'Human Activities', 'Intervention', 'Learning', 'Life', 'Location', 'Machine Learning', 'Mathematics', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Observational Study', 'Outcome', 'Pattern', 'Periodicity', 'Physical activity', 'Process', 'Regimen', 'Research', 'Research Activity', 'Role', 'Running', 'Sampling', 'Series', 'Statistical Methods', 'Stroke', 'Supervision', 'Techniques', 'Technology', 'Time', 'Time Series Analysis', 'Validation', 'Walking', 'Work', 'analysis pipeline', 'base', 'computing resources', 'density', 'experimental study', 'heart rate monitor', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'machine learning algorithm', 'mathematical algorithm', 'mathematical sciences', 'multimodality', 'preservation', 'sedentary lifestyle', 'sensor', 'signal processing', 'statistics', 'tool', 'wearable device', 'wearable sensor technology']",NIGMS,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R01,2020,69169,-0.06714158365640871
"INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE Project Summary  To understand the many disorders of the brain it is necessary to grapple with its complexity.  Increasingly large and complicated data sets are being collected, but the tools for analyzing and  modeling the data are not yet available. More researchers trained in computational neuroscience are  desperately needed. This project supports graduate and undergraduate training programs in  computational neuroscience (TPCN) at both Carnegie Mellon University (CMU) and the University of  Pittsburgh (Pitt), and a summer school in computational neuroscience for undergraduates, which are  available to students coming from colleges and universities throughout the United States.  The CMU-Pitt TPCN has 16 training faculty in computational neuroscience, 22 training faculty whose  laboratories are primarily experimental, and 20 training faculty whose laboratories are both  computational and experimental. At the graduate level the TPCN offers a PhD program in Neural  Computation (PNC) and joint PhD programs with CMU’s Department of Statistics (PNC-Stat) and its  Machine Learning Department (PNC- MLD), all set within a highly collegial, cross-disciplinary  environment of our Center for the Neural Basis of Cognition (CNBC), which is operated jointly by  CMU and Pitt. The CNBC was established in 1994 to foster interdisciplinary research on the neural  mechanisms of brain function, and now comprises 145 faculty having appointments in 22 departments.  At the undergraduate level a substantial pool of local students is supplemented during the summer  by a cohort of students from across the country. During this renewal funding period the project is  strengthening the role of statistics and machine learning throughout the training programs; (2) revising the summer undergraduate program by creating a didactic two-week “boot camp” at the  beginning, which includes a 20-lecture overview of computational neuroscience; (3) creating online  materials, in conjunction with the boot camp, that will serve not only our own students but also  the greater world of training in computational neuroscience; and (4) enhancing our minority  recruitment by (a) taking advantage of the boot camp and online materials, as well as making  promotional visits to targeted campuses, and (b) creating and running a one-year “bridge” program  to better prepare under-represented minorities for PhD programs.  TPCN trainees work in vertically integrated, cross-disciplinary research teams. Graduate students  take a year- long course in computational neuroscience that bridges modeling and modern statistical  machine learning approaches to neuroscience. To ensure their competency in core neuroscience  principles they also take courses in cognitive neuroscience, neurophysiology, and systems  neuroscience. They then pursue depth in a relevant quantitative discipline, such as computer  science, engineering, mathematics, or statistics. Graduate students have extended experience in at  least one experimental laboratory, and they take part in journal clubs and seminars within the  large Pittsburgh neuroscience community. Year-long undergraduates take courses in mathematics,  computer programming, statistics, and neuroscience; they take an additional course in neuroscience  or psychology and a course in computational neuroscience; and they complete a year-long research  project. In addition, they complete the TPCN summer program. Undergraduate trainees in the summer  program go through the boot camp on topics in computational neuroscience, including tutorials in  Matlab, statistical methods, fundamentals of differential equations, and ideas of neural coding;  they then complete a research project under careful guidance. All trainees will receive training in  responsible conduct of research. Across 5 years of funding, the TPCN supports 20 NRSA graduate  students, 10 non-NRSA graduate students, 30 undergraduate year-long fellows, and 60 undergraduate  summer fellows. Project Narrative  Research in neuroscience is crucial for attacking the causes of neurological and mental health  disorders. If the field of neuroscience is to continue its rapid advance, neuroscientists must use,  understand, and develop new technologies, acquire and analyze ever larger data sets, and grapple  more directly with the complexity of neurobiological systems. The primary goal of these training  programs will be to help train a new generation of interdisciplinary neuroscientists with strong  quantitative skills.",INTERDISCIPLINARY TRAINING IN COMPUTATIONAL NEUROSCIENCE,10004013,R90DA023426,"['Appointment', 'Brain', 'Brain Diseases', 'Code', 'Cognition', 'Communities', 'Competence', 'Country', 'Data Set', 'Differential Equation', 'Discipline', 'Doctor of Philosophy', 'Educational Status', 'Engineering', 'Ensure', 'Environment', 'Faculty', 'Fostering', 'Funding', 'Interdisciplinary Study', 'Joints', 'Journals', 'Laboratories', 'Machine Learning', 'Mathematics', 'Minority Recruitment', 'Modeling', 'Modernization', 'National Research Service Awards', 'Neurosciences', 'Psychology', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Running', 'Schools', 'Statistical Methods', 'Students', 'System', 'Teacher Professional Development', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States', 'Universities', 'Visit', 'Work', 'bridge program', 'cognitive neuroscience', 'cohort', 'college', 'computational neuroscience', 'computer program', 'computer science', 'data modeling', 'experience', 'graduate student', 'lectures', 'neuromechanism', 'neurophysiology', 'programs', 'relating to nervous system', 'responsible research conduct', 'statistical and machine learning', 'statistics', 'summer program', 'tool', 'undergraduate student']",NIDA,CARNEGIE-MELLON UNIVERSITY,R90,2020,239423,-0.0020936381406555563
"Nathan Shock Center of Excellence in Basic Biology of Aging OVERALL PROJECT SUMMARY This application is for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington and affiliated institutions. This Center has over the past 25 years provided key resources in support of investigators who study the biology of aging. This application continues a theme that emphasizes outreach and service to the broadest community of investigators in the gerosciences. Of proximal relevance is the characterization of aging-related phenotypes of longevity and healthspan. As our Center services must be easily accessible to outside users, our Longevity and Healthspan Core (Core E) focuses on invertebrate assays, many of them novel. Two other Resources Cores focus on the high dimensional assessments that are closely related to aging phenotypes: Protein Phenotypes of Aging (Core C) and Metabolite Phenotypes of Aging (Core D). Sophisticated computational and bioinformatic tools for data analysis and optimal insight are provided by the Artificial Intelligence and Bioinformatics Core F. Each of these four Resource Cores is led by highly respected experts in that field, including Michael MacCoss and Judit Villen (Core C), Daniel Promislow (Core D), Matt Kaeberlein and Maitreya Dunham (Core E) and Su-In Lee (Core F). Each will push the envelope of appropriate technologies, developing new state-of-the art approaches for assessments that are the most applicable to gerontology and making them accessible to the national aging community. The Research Development Core (Core B) will continue to support pilot and junior faculty studies, with a firm focus on outreach of service to the national geroscience constituency. The Administrative and Program Enrichment Core (Core A) supports administrative management, an external advisory panel, courses, and data sharing and dissemination. Core A’s program of seminars and symposia will continue a focus on sponsorship and organization of national courses, meetings and pre-meetings, as well as workshops in the fields allied to our Resource Core Services. In coordination with other Nathan Shock Centers, we will support a new Geropathology Research initiative. UW NATHAN SHOCK CENTER OVERALL - PROJECT NARRATIVE We apply for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington, which has for 25 years provided key resources supporting investigators who study the biology of aging. The overarching goal of this Center is to have a positive impact on the field by accelerating research discovery and providing research support for investigators nationally and internationally, particularly junior investigators in the process of building their own research programs. We will accomplish this goal through six cores that function synergistically together: four Resource Cores with particular expertise in protein (Core C) and metabolite (Core D) phenotypes of aging, invertebrate longevity and healthspan phenotypes (Core E) and artificial intelligence and bioinformatics (Core F), along with a Research Development Core (Core B) that supports external pilot projects and junior faculty studies, and an Administrative and Program Enrichment Core (Core A) that supports administrative management, an external advisory panel, sponsorship and organization of national meetings and pre-meetings, courses, workshops and seminars, and, in coordination with other Nathan Shock Centers, a Geropathology Research initiative.",Nathan Shock Center of Excellence in Basic Biology of Aging,10042617,P30AG013280,"['Aging', 'Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'Biology of Aging', 'Collaborations', 'Communication', 'Communities', 'Consult', 'Data', 'Data Analyses', 'Development', 'Educational workshop', 'Environment', 'Experimental Designs', 'Faculty', 'Genes', 'Genetic study', 'Gerontology', 'Geroscience', 'Goals', 'Growth', 'Informatics', 'Institution', 'International', 'Invertebrates', 'Leadership', 'Longevity', 'Methodology', 'Methods', 'Microfluidics', 'Molecular Genetics', 'Office of Administrative Management', 'Pathway interactions', 'Phenotype', 'Philosophy', 'Pilot Projects', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Research', 'Research Activity', 'Research Personnel', 'Research Support', 'Resources', 'Robotics', 'Services', 'Shock', 'Statistical Data Interpretation', 'Technology', 'Transcript', 'Universities', 'Variant', 'Washington', 'bioinformatics tool', 'career development', 'cell age', 'computerized tools', 'data dissemination', 'data sharing', 'healthspan', 'high dimensionality', 'insight', 'meetings', 'metabolomics', 'multiple omics', 'novel', 'outreach', 'outreach services', 'programs', 'protein metabolite', 'research and development', 'symposium', 'tool', 'trait']",NIA,UNIVERSITY OF WASHINGTON,P30,2020,962037,-0.006300242820672399
"An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics Immune-repertoire sequence, which consists of an individual's millions of unique antibody and T-cell receptor (TCR) genes, encodes a dynamic and highly personalized record of an individual's state of health. Our long- term goal is to develop the computational models and tools necessary to read this record, to one day be able diagnose diverse infections, autoimmune diseases, cancers, and other conditions directly from repertoire se- quence. The key problem is how to find patterns of specific diseases in repertoire sequence, when repertoires are so complex. Our hypothesis is that a combination of bottom-up (sequence-level) and top-down (systems- level) modeling can reveal these patterns, by encoding repertoires as simple but highly informative models that can be used to build highly sensitive and specific disease classifiers. In preliminary studies, we introduced two new modeling approaches for this purpose: (i) statistical biophysics (bottom-up) and (ii) functional diversity (top-down), and showed their ability to elucidate patterns related to vaccination status (97% accuracy), viral infection, and aging. Building on these studies, we will test our hypothesis through two specific aims: (1) We will develop models and classifiers based on the bottom-up approach, statistical biophysics; and (2) we will de- velop the top-down approach, functional diversity, to improve these classifiers. To achieve these aims, we will use our extensive collection of public immune-repertoire datasets, beginning with 391 antibody and TCR da- tasets we have characterized previously. Our team has deep and complementary expertise in developing computational tools for finding patterns in immune repertoires (Dr. Arnaout) and in the mathematics that under- lie these tools (Dr. Altschul), with additional advice available as needed regarding machine learning (Dr. AlQuraishi). This proposal is highly innovative for how our two new approaches address previous issues in the field. (i) Statistical biophysics uses a powerful machine-learning method called maximum-entropy modeling (MaxEnt), improving on past work by tailoring MaxEnt to learn patterns encoded in the biophysical properties (e.g. size and charge) of the amino acids that make up antibodies/TCRs; these properties ultimately determine what targets antibodies/TCRs can bind, and therefore which sequences are present in different diseases. (ii) Functional diversity fills a key gap in how immunological diversity has been measured thus far, by factoring in whether different antibodies/TCRs are likely to bind the same target. This proposal is highly significant for (i) developing an efficient, accurate, generative, and interpretable machine-learning method for finding diagnostic patterns in repertoire sequence; (ii) applying a robust mathematical framework to the measurement of immuno- logical diversity; (iii) impacting clinical diagnostics; and (iv) adding a valuable new tool for integrative/big-data medicine. The expected outcome of this proposal is an integrated pair of robust and well validated new tools/models for classifying specific disease exposures directly from repertoire sequence. This proposal in- cludes plans to make these tools widely available, to maximize their positive impact across medicine. The proposed research is relevant to public health because B cells/antibodies and T cells play vital roles across such a vast range of health conditions, from infection, to autoimmunity, to cancer, that the ability to de- code what they are doing would be an important step forward for diagnosing these conditions. The proposed research is relevant to the NIH's mission of fostering fundamental creative discoveries, innovative research strategies, and their applications as a basis for ultimately protecting and improving health, specifically relating to the diagnosis of human diseases.",An Integrated Multilevel Modeling Framework for Repertoire-Based Diagnostics,10050030,R01AI148747,"['Address', 'Affect', 'Aging', 'Amino Acid Motifs', 'Amino Acids', 'Antibodies', 'Autoimmune Diseases', 'Autoimmunity', 'B-Lymphocytes', 'Base Sequence', 'Big Data', 'Binding', 'Biophysics', 'Characteristics', 'Charge', 'Classification', 'Clinical', 'Code', 'Collection', 'Complex', 'Computer Models', 'Data Set', 'Dependence', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Entropy', 'Fostering', 'Gene Frequency', 'Genes', 'Goals', 'Health', 'Human', 'Immune', 'Immunology', 'Individual', 'Infection', 'Influenza vaccination', 'Intuition', 'Learning', 'Letters', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Pattern', 'Performance', 'Persons', 'Physics', 'Play', 'Population Heterogeneity', 'Privatization', 'Property', 'Public Health', 'Reading', 'Reporting', 'Research', 'Role', 'Sample Size', 'Sampling', 'Sampling Errors', 'Signs and Symptoms', 'Speed', 'Statistical Study', 'System', 'T-Cell Receptor', 'T-Cell Receptor Genes', 'T-Lymphocyte', 'Testing', 'United States National Institutes of Health', 'Vaccination', 'Virus Diseases', 'Work', 'base', 'biophysical properties', 'clinical diagnostics', 'computerized tools', 'diagnostic accuracy', 'human disease', 'immunological diversity', 'improved', 'information model', 'innovation', 'machine learning method', 'multidisciplinary', 'multilevel analysis', 'novel', 'novel strategies', 'tool']",NIAID,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,535171,-0.0077776380263136205
"Multi-Study Integer Programming Methods for Human Voltammery Project Summary/Abstract  The development of treatments for addiction requires the characterization of neural mechanisms underlying reward. Studying reward in humans requires assays that can detect changes in neurotransmitter levels with high chemical specificity. Recently, fast-scan cyclic voltammetry (FSCV) has been implemented in humans to measure dopamine with high temporal and spatial resolution. This technological achievement was enabled in large part through the novel application of machine learning methods. FSCV relies on statistical tools since FSCV records an electrochemical response which must be converted into concentration estimates via a statistical model. The validity of the scientific conclusions from human FSCV studies therefore depends heavily on the reliability of these statistical models to generate accurate dopamine concentration estimates.  In human FSCV, models are fit on in vitro training sets as making in vivo training sets in humans is infeasible. Producing accurate estimates thus requires that models trained on in vitro training sets generalize to in vivo brain recordings. Combining data from multiple training sets is the standard approach human FSCV researchers have employed to improve model generalizability. This proposal extends work that shows that multi-study machine learning methods improve dopamine concentration estimates by combining training sets from different electrodes such that the resulting average signal (“cyclic voltammogram” or CV) is similar to the average CV of the electrode used in the brain. However, this approach relies on random resampling. This is problematic because the randomness limits the extent to which estimate accuracy can be improved and the slow speed of the resampling approach precludes the generation of estimates during data collection, which is critical to experiment success.  This proposal details the development of methods that leverage mixed integer programming to optimally generate training sets that combine data from multiple electrodes. By generating training sets that are specifically tailored to the electrode used for brain measurements, one can vastly improve dopamine concentration estimate accuracy. The speed of the integer programming methods will enable the use of this approach during data collection. This work will include validation of the methods on in vitro data as well as on data from published in vivo and slice experiments in rodents. By applying methods to published optogenetic experiments, one can compare estimates from the proposed methods and from standard methods. The asymptotic properties of the proposed methods will be characterized analytically assuming a linear mixed effects model and empirically through application of the methods to data simulated under this model.  This work will be conducted at the highly collaborative and innovative Harvard School of Public Health. The fellowship will support growth in statistical, computing and collaborative skills, and prepare the trainee for a productive career as a biostatistics professor who develops methods for neuroscience and addiction research. Project Narrative  Fast-scan cyclic voltammetry in humans offers an invaluable tool to study the neural mechanisms underlying reward by allowing for sub-second detection of dopamine during cognitive-behavioral tasks. However, conducting voltammetry in humans presents distinct statistical challenges that must be overcome to ensure optimal dopamine concentration estimates. We propose novel statistical methods that use mixed integer optimization and extend preliminary work that shows multi-study machine learning methods substantially improve dopamine concentration estimate accuracy.",Multi-Study Integer Programming Methods for Human Voltammery,10067624,F31DA052153,"['Achievement', 'Address', 'Algorithms', 'Behavioral', 'Biological Assay', 'Biometry', 'Brain', 'Cells', 'Chemicals', 'Cognitive', 'Complex Mixtures', 'Computer software', 'Data', 'Data Collection', 'Data Set', 'Detection', 'Development', 'Dopamine', 'Electrodes', 'Ensure', 'Fellowship', 'Generations', 'Goals', 'Grant', 'Growth', 'Human', 'In Vitro', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Neurosciences', 'Neurotransmitters', 'Nucleus Accumbens', 'Performance', 'Periodicity', 'Property', 'Public Health Schools', 'Publications', 'Publishing', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Rewards', 'Rodent', 'Scanning', 'Scheme', 'Signal Transduction', 'Slice', 'Specificity', 'Speed', 'Statistical Computing', 'Statistical Methods', 'Statistical Models', 'Techniques', 'Training', 'Validation', 'Work', 'addiction', 'algorithm training', 'career', 'effective therapy', 'experimental study', 'improved', 'in vivo', 'innovation', 'insight', 'machine learning method', 'method development', 'multiple data sources', 'neuromechanism', 'novel', 'optogenetics', 'predictive modeling', 'professor', 'relating to nervous system', 'response', 'skills', 'success', 'therapy development', 'tool']",NIDA,HARVARD SCHOOL OF PUBLIC HEALTH,F31,2020,37235,-0.010333314841885882
"Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions PROJECT SUMMARY The research interests of my group are rooted in explorations of new and useful conceptual models to improve the control and prediction of noncovalent interactions. Our research involves the use of a variety of computational quantum chemical tools, applications of density functional theory (DFT), cheminformatics, and machine-learning methods. A premise of our research is that aromaticity may be used to modulate many types of noncovalent interactions (such as hydrogen bonding, π-stacking, anion-π interactions). The reciprocal relationship we find, between “aromaticity” in molecules and the strengths of “noncovalent interactions,” is surprising especially since they are typically considered as largely separate ideas in chemistry. The innovation of this research is that it will enable use of intuitive “back-of-the-envelope” electron-counting rules (such as the 4n+2πe Hückel rule for aromaticity) to make predictions of experimental outcomes regarding the impact of noncovalent interactions. A five-year goal is to realize the use of our conceptual models in real synthetic examples prepared by our experimental collaborators. My research vision is to bridge discoveries of innovative concepts to their practical impacts for biomedical and biomolecular research. PROJECT NARRATIVE This research proposal includes four projects that are jointly motivated by the challenge to control and predict noncovalent interactions in organic and biomolecular systems. The proposed work involves applications of a variety of computational quantum chemical tools and synergistic investigations with experimental collaborators. We seek to identify new and useful concepts to guide experimental designs of novel “non-natural” molecular systems (e.g., receptors, biosensors, and hydrogels) that have potential biomedical applications.",Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions,10016376,R35GM133548,"['Anions', 'Back', 'Biosensor', 'Chemicals', 'Chemistry', 'Electrons', 'Experimental Designs', 'Goals', 'Hydrogels', 'Hydrogen Bonding', 'Intuition', 'Investigation', 'Modeling', 'Molecular', 'Outcome', 'Plant Roots', 'Research', 'Research Project Summaries', 'Research Proposals', 'System', 'Vision', 'Work', 'cheminformatics', 'density', 'improved', 'innovation', 'interest', 'machine learning method', 'novel', 'quantum computing', 'receptor', 'theories', 'tool']",NIGMS,UNIVERSITY OF HOUSTON,R35,2020,377200,-0.011886449679356216
"High-Performance Compute Cluster for Comprehensive Cancer and Infectious Diseases Research Project Summary/Abstract Fred Hutch respectfully requests funds to upgrade the current 523-node, 3328-core high performance computing (HPC) cluster, which was created in 2004 and expanded in 2006, 2010 (S10 funds), 2013, 2015 (S10 funds), and 2018. 456 end of life nodes (1824 cores) will be replaced and 144 nodes (3456 cores) will be added, creating a new 211-node, 4996-core system with an overall 50% increase in core count, 60% increase in processing power, and more than 100% increase in memory capacity over the old system. The expanded capacity will enable deep and efficient analysis of our research studies and accommodate 20% annual growth in computing intensive research, much of which is not possible on the current cluster. The core user group for the new HPC cluster consists of at least 37 NIH-funded research groups participating in this proposal, however as much as 85 groups use the cluster regularly. Their biomedical research is aimed at eradicating cancer and other diseases and dependent on computationally intensive technical approaches such as development of novel statistical analysis or machine learning methods, for example for assessing immune correlates to facilitate vaccine development, analyzing large scale clinical trials or to develop software tools for the analysis of large-scale immunological datasets, DNA and RNA sequencing, modeling prostate cancer outcomes, studies of the human microbiome, modeling of cancers, mRNA, miRNA, and structural variant detection, structural biology with Cryo- EM, modeling of infectious agents and pandemics, computational modeling, prediction and design of macromolecular structures and interactions, identifying drivers of neoplasia and an international consortium improving colorectal cancer detection using GWAS, whole genome sequencing and genome-wide gene- environment (GxE) studies as well as research in diabetes, mhealth and cardiovascular diseases. Several of the Major Users at Fred Hutch are currently experiencing substantial delays in accomplishing their work using the current cluster. Others have projects that cannot be done at all on the existing instrument. (see Research Projects section for details). The Scientific Computing department (SciComp) has operated the current HPC cluster for more than 10 years and has a staff with a combined experience of over 150 years. The proposed new HPC cluster will be installed in available space in a Fred Hutch datacenter. The expanded cluster will address both immediate and future needs of our user community, supporting NIH-funded research at Fred Hutch. Funded research at our Center will greatly benefit from the increased data-processing capacity and improved performance of the requested HPC cluster, including applications of machine learning to the study of clinical trial efficacy, comparison of immune system receptors to identify responses to specific pathogens/diseases, modeling of carcinogenesis. Besides multiple infectious diseases Fred Hutch researches all types of cancer and our computationally intensive investigators tend to focus on colon, prostate, brain, Barrett’s esophagus, lung and liquid tumors such as leukemia. Project Narrative (Public Health Relevance) We are requesting an expansion of our high-performance computing (HPC) cluster to provide capacity for the growing computational needs in a broad range of biomedical research studies at our Center. Access to fast and reliable computational power is critically important for analyzing the exploding amounts of data produced by large-scale clinical and epidemiological studies, as well as scientific instruments such as genome sequencers and electron microscopes. The prevention, detection, and treatment of cancer, HIV, and other life-threatening diseases are major areas of NIH-funded research at our Center that will greatly benefit from the improved performance of the requested HPC cluster, including large-scale clinical and epidemiologic studies of various cancers, modeling of vaccine efficacy, and infectious disease transmission and proteomic-based biomarker discovery.",High-Performance Compute Cluster for Comprehensive Cancer and Infectious Diseases Research,9940345,S10OD028685,"['Address', 'Barrett Esophagus', 'Biomedical Research', 'Brain', 'Cancer Model', 'Cardiovascular Diseases', 'Clinical Trials', 'Colon', 'Communicable Diseases', 'Communities', 'Computer Models', 'Cryoelectron Microscopy', 'DNA sequencing', 'Data Set', 'Development', 'Diabetes Mellitus', 'Disease', 'Disease model', 'Environment', 'Funding', 'Future', 'Genes', 'Growth', 'High Performance Computing', 'Human Microbiome', 'Immune', 'Immune system', 'Immunologics', 'Infectious Agent', 'Infectious Diseases Research', 'International', 'Liquid substance', 'Lung', 'Machine Learning', 'Malignant Neoplasms', 'Memory', 'Messenger RNA', 'MicroRNAs', 'Modeling', 'Molecular Structure', 'Neoplasms', 'Performance', 'Prostate', 'Prostate Cancer Outcomes Study', 'Research', 'Research Personnel', 'Research Project Grants', 'Software Tools', 'Statistical Data Interpretation', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'cancer type', 'carcinogenesis', 'cluster computing', 'colorectal cancer screening', 'computerized data processing', 'design', 'efficacy clinical trial', 'end of life', 'experience', 'genome sequencing', 'genome wide association study', 'genome-wide', 'improved', 'instrument', 'leukemia', 'mHealth', 'machine learning method', 'microbiome research', 'novel', 'pandemic disease', 'pathogen', 'prostate cancer model', 'receptor', 'research study', 'response', 'scientific computing', 'software development', 'structural biology', 'transcriptome sequencing', 'tumor', 'vaccine development', 'variant detection', 'whole genome']",OD,FRED HUTCHINSON CANCER RESEARCH CENTER,S10,2020,2000000,-0.03958421223556241
"Multimodality imaging-driven multifidelity modeling of aortic dissection PROJECT SUMMARY. Aortic dissections are responsible for significant morbidity and mortality in young and old individuals alike. Whereas type A (ascending aorta) dissections are treated aggressively via surgery, type B (descending thoracic aorta) dissections are often monitored for long periods to determine the best treatment. These lesions can cease to propagate (i.e., stabilize or heal) or they can propagate further and either turn inward and connect again with the true lumen to form a re-entry tear or turn outward and result in rupture in the case of an compromised adventitia. Notwithstanding the importance of these later events, there is a pressing need to understand better the early processes that initiate the dissection and drive its initial propagation as well as to determine whether the presence of intramural thrombus is protective or not against early or continued propagation. Over the past 5 years our collaborative team has developed numerous new multimodality imaging techniques, biomechanical testing methods, and computational modeling approaches across multiple scales that uniquely positions us to understand better the process of early aortic dissection and the possible roles played by early intramural thrombus development. In this project, we propose to use nine complementary mouse models to gain broad understanding of the bio-chemo-mechanical processes that lead to aortic dissection and to introduce a new machine learning based multifidelity modeling approach to develop predictive probabilistic multiscale models of dissection. These models will be informed, trained, and validated via data obtained from a combination of unique in vitro biomechanical phenotyping experiments (wherein we can, for the first time, quantify the initial delamination process under well-controlled conditions and regional material properties thereafter) and novel multimodality imaging of delamination / dissection both in vitro and in vivo. We will consider, for example, the roles of different elastic lamellar geometries; we will assess separate roles of focal proteolytic activation and pooling of highly negatively charged mucoid material, which can degrade or swell the wall respectively; and we will model and assess the effects of early thrombus deposition within a false lumen. We submit that our new probabilistic paradigm, based on statistical autoregressive schemes and enabled by machine learning tools, could be transformative and lead to a paradigm shift in disease prediction where historical data, animal experiments, and limited clinical input (e.g., multiomics) can be used synergistically for robust prognosis and thus interventional planning. Our work is also expected to lead naturally to an eventual better understanding of the chronic processes associated with dissection via predictive models that are aided by the expected “revolution of resolution” in diagnostic imaging. PUBLIC HEALTH RELEVANCE Mounting evidence reveals that thoracic aortic dissections – which afflict young and old individuals alike – are responsible for even greater disability and death than long thought. We will use a unique combination of multiple mouse models, advanced medical imaging, and novel computational models to elucidate the mechanisms responsible for the initiation of a dissection and reasons for the extreme biological variability that characterizes these lethal lesions.",Multimodality imaging-driven multifidelity modeling of aortic dissection,9981804,U01HL142518,"['Acute', 'Address', 'Animal Experiments', 'Animal Model', 'Aorta', 'Aortic Rupture', 'Arteries', 'Attention', 'Biological', 'Biomechanics', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Blunt Trauma', 'Carotid Arteries', 'Categories', 'Cervical', 'Cessation of life', 'Charge', 'Chest', 'Child', 'Chronic', 'Clinical', 'Coagulation Process', 'Collaborations', 'Communities', 'Computer Models', 'Coupling', 'Data', 'Defect', 'Deposition', 'Development', 'Diagnostic Imaging', 'Dilatation - action', 'Disease', 'Dissection', 'Elderly', 'Event', 'Foundations', 'Geometry', 'Glycosaminoglycans', 'Goals', 'Heritability', 'Human', 'Hypertension', 'Image', 'Imaging Techniques', 'In Vitro', 'Individual', 'Infusion procedures', 'Intervention', 'Knowledge', 'Lead', 'Lesion', 'Long-Term Effects', 'Machine Learning', 'Mechanics', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Motivation', 'Multimodal Imaging', 'Operative Surgical Procedures', 'Optical Coherence Tomography', 'Outcome', 'Phase', 'Phenotype', 'Platelet aggregation', 'Play', 'Positioning Attribute', 'Prevention', 'Process', 'Property', 'Research', 'Resolution', 'Risk Factors', 'Role', 'Rupture', 'Scheme', 'Site', 'Solid', 'Statistical Models', 'Testing', 'Thoracic aorta', 'Thrombus', 'Time', 'Training', 'Tunica Adventitia', 'Ultrasonography', 'Uncertainty', 'Video Microscopy', 'Work', 'ascending aorta', 'base', 'digital imaging', 'disability', 'experimental study', 'healing', 'hemodynamics', 'improved', 'in silico', 'in vivo', 'insight', 'intracranial artery', 'microSPECT', 'mortality', 'mouse model', 'mucoid', 'multi-scale modeling', 'multiple omics', 'normotensive', 'novel', 'novel strategies', 'outcome forecast', 'particle', 'predictive modeling', 'public health relevance', 'spatiotemporal', 'supervised learning', 'tool', 'virtual', 'young adult']",NHLBI,YALE UNIVERSITY,U01,2020,601275,-0.005582933117973528
"Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches PROJECT SUMMARY The past decade of biomedical research has borne witness to rapid growth in data and computational methods. A fundamental challenge for the scientific community in the 21st century is learning how to turn this deluge of data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. The emerging field of real-time infectious disease forecasting is a prime example of a research area with great potential for leveraging modern analytical methods to maximize the impact on public health. Infectious diseases exact an enormous toll on global health each year. Improved real- time forecasts of infectious disease outbreaks can inform targeted intervention and prevention strategies, such as increased healthcare staffing or vector control measures. However we currently have a limited understanding of the best ways to integrate these types of forecasts into real-time public health decision- making. The central research activities of this project are (1) to develop and validate a suite of robust, real-time statistical prediction models for infectious diseases, (2) we will develop and evaluate an ensemble time-series prediction methodology for integrating multiple prediction models into a single forecast, and (3) to develop a collaborative platform for dissemination and evaluation of predictions by different research teams. Additionally, we will develop a suite of open-source educational modules to train researchers and public health officials in developing, validating, and implementing time-series forecasting, with a focus on real-time infectious disease applications. PUBLIC HEALTH NARRATIVE A fundamental challenge for the scientific community in the 21st century is learning how to turn data into evidence that can inform decision-making about improving health and preventing illness at the individual and population levels. Real-time infectious disease forecasting is a prime example of a field with great potential for leveraging modern analytical methods to maximize the impact public health. The goal of the proposed research is to develop statistical modeling frameworks for making forecasts of infectious diseases in real-time and integrating these forecasts into public health decision making.",Statistical methods for real-time forecasts of infectious disease: dynamic time-series and machine learning approaches,10002249,R35GM119582,"['Area', 'Biomedical Research', 'Communicable Diseases', 'Communities', 'Computing Methodologies', 'Data', 'Decision Making', 'Disease Outbreaks', 'Evaluation', 'Goals', 'Health', 'Healthcare', 'Individual', 'Intervention', 'Learning', 'Learning Module', 'Machine Learning', 'Measures', 'Methodology', 'Modernization', 'Population', 'Prevention strategy', 'Public Health', 'Research', 'Research Activity', 'Research Personnel', 'Series', 'Statistical Methods', 'Statistical Models', 'Time', 'Training', 'analytical method', 'global health', 'improved', 'infectious disease model', 'open source', 'predictive modeling', 'prevent', 'rapid growth', 'vector control']",NIGMS,UNIVERSITY OF MASSACHUSETTS AMHERST,R35,2020,593966,-0.020803768594724898
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,10017004,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'clinical center', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'mHealth', 'machine learning method', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2020,725375,-0.02790796069246297
"S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450 This application is requesting funds to purchase the Aperio™ GT-450 digital pathology slide scanner from Leica Biosystems. The requested instrumentation will be located in the Pathology and Biobanking Core of the Lester and Sue Smith Breast Center at Baylor College of Medicine (BCM). The predominant use of the Aperio scanner will be research-based whole slide imaging (WSI) and analysis of patient specimens, patient-derived xenograft (PDX) cancer models, and pre-clinical investigations on various animal- and cell-line model systems. All user projects have large sample cohorts that require high throughput, high-resolution scanning and image analysis. High capacity and improved scanning with dynamic focusing makes the GT-450 microscope scanner well-suited and the most cost-effective for use in the proposed projects. An underlying theme in the studies selected for Aperio scanner-supported services integrates novel biomarker and molecular pathway discovery with spatial morphological characterization, a necessary process to investigate heterogeneity in disease states. This instrument leverages high-throughput scanning capability with open-source, fully customizable machine-learning analytics to meet the evolving needs of investigators at Baylor College of Medicine, in particular faculty groups studying mechanisms of cancer cell dynamics and the development of new therapeutic targets. Expansion of systems biology and precision medicine research is an essential component of the college’s strategic roadmap. The Aperio GT-450 is critically needed as we modernize our laboratory offerings and capabilities; the acquisition of this digital scanner will strengthen existing research programs underway and establish new, collaborative research opportunities and directions within Baylor College of Medicine and surrounding institutions. To address the growing demand for integrating quantitative spatial assessment of biomarkers with molecular pathway discovery, we request funding for the Leica Aperio GT-450 digital microscope scanner. This instrument will facilitate research that seeks to better understand molecular mechanisms of tumorigenesis in the context of its spatial environment and will be critical for the development of clinically correlative biomarkers for next generation precision medicine research initiatives at Baylor College of Medicine.",S10 Shared Instrument Grant - Leica Aperio Digital Scanner GT450,9940426,S10OD028671,"['Animals', 'Biological Models', 'Breast', 'Cancer Model', 'Cell Line', 'Development', 'Disease', 'Faculty', 'Funding', 'Grant', 'Heterogeneity', 'Image Analysis', 'Institution', 'Laboratories', 'Machine Learning', 'Medicine', 'Microscope', 'Modernization', 'Molecular', 'Morphology', 'Pathology', 'Pathway interactions', 'Patients', 'Process', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scanning', 'Services', 'Slide', 'Specimen', 'Systems Biology', 'Xenograft procedure', 'base', 'biobank', 'cancer cell', 'clinical investigation', 'cohort', 'college', 'cost effective', 'digital', 'digital pathology', 'improved', 'instrument', 'instrumentation', 'new therapeutic target', 'novel marker', 'open source', 'pre-clinical', 'precision medicine', 'programs', 'whole slide imaging']",OD,BAYLOR COLLEGE OF MEDICINE,S10,2020,477043,-0.021867029513523473
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,-0.007746985602389721
"Development of a Universal Influenza Vaccine ABSTRACT Influenza virus (flu) ranks highest in disease burden of all infectious diseases as measured in disability-adjusted life years. Seasonal epidemics cause 200,000-500,000 worldwide deaths annually. The total economic burden of seasonal flu is estimated to range from approximately $26B to $87B each year in the US in terms of direct medical expenses and lost work and productivity. Additionally, at least six known flu pandemics have become global human catastrophes, most notably the Spanish Flu pandemic of 1918, which killed 3-5% of the world’s population. Any reduction in the infection rate, transmission, and severity of flu infection would greatly reduce our healthcare expenditures and improve the quality of life for millions of people every year. The current vaccines are formulated annually based on predictions of which circulating flu strains may be prevalent in a given season. The effectiveness of these vaccines varies from year to year based on the circulation of unexpected antigenic variants and other factors. Vaccine design is complicated the by the multiplicity of flu strains, each with rapidly-evolving dominant antigen epitopes (“decoy” epitopes) that largely stimulate strain- restricted immunity. One strategy for rational antigen design, termed Immune Refocusing Technology (IRT), involves introducing mutations that reduce the immunogenicity of these decoy epitopes thus shifting the immune response to target more widely-conserved subdominant epitopes. BMI has previously applied this IRT approach with some notable successes to other viral antigens (e.g. HRV and the RSV F protein), and we now focus on the major flu surface antigen glycoprotein HA using H1, H3, and B vaccine strains as parental antigens. The anticipated effort to design a suitably modified antigen would ordinarily involve a protracted process of trial-and-error testing of many potential candidates. However, we have recently developed the ANATOPE automated B cell epitope prediction software package with algorithm parameters tuned using methods in artificial intelligence. Our algorithm identifies epitopes with a significantly higher success rate than previously available prediction programs. This breakthrough allows us to assign immunogenicity “strength” scores to particular antigen surface patches and will further guide and accelerate the design of mutant antigens that refocus the immune response to cross-strain conserved epitopes. In this application, we propose to engineer and test the immunogenicity of rationally-designed HA antigens containing mutations that both 1) dampen the immunogenicity of dominant strain-restricted decoy epitopes and 2) enhance the immunogenicity of conserved subdominant epitopes associated with broadly neutralizing antibodies. Follow- up studies will assess the rationally-designed antigens in a ferret challenge study and prepare the approach for translation into humans as a universal vaccine that does not require annual reformulation. NARRATIVE Influenza is among the most important pathogens in terms of negative impact upon human health and healthcare expense. The current seasonal vaccines have a mixed record in terms of preventing illness and death. Development of improved vaccines is complicated by the rapid antigenic evolution of circulating viruses and the strain-restricted protections developed by our immune systems. In this proposal, we combine two novel technologies to develop universal influenza vaccines. The first, the Immune Refocusing Technology, is used to alter antibody binding sites, epitopes, such that the immune system can produce a broadened, cross-strain protective response. The second, a computational B cell epitope analysis program called ANATOPE, is used to guide the rational design of antigenic mutants bearing amino acid substitutions that stimulate improved immune responses. This project will focus on improving the breadth of protection stimulated by the three major components of the seasonal vaccine to reduce the need for annual reformulations. If successful, follow-up studies will include additional analysis in alternative animal models, a more comprehensive analysis of T cell immune responses, and preparation for advancement into IND-enabling studies.",Development of a Universal Influenza Vaccine,10080771,R43AI152652,"['Algorithms', 'Amino Acid Substitution', 'Animal Model', 'Animal Testing Alternatives', 'Antibodies', 'Antibody Binding Sites', 'Antibody Formation', 'Antibody Response', 'Antigens', 'Antiviral Agents', 'Artificial Intelligence', 'B-Lymphocyte Epitopes', 'Baculoviruses', 'Binding Sites', 'Biological Assay', 'Blood Circulation', 'California', 'Cells', 'Cellular Immunity', 'Cessation of life', 'Communicable Diseases', 'Computational algorithm', 'Computer Analysis', 'Computer software', 'Cryoelectron Microscopy', 'Development', 'Distant', 'Economic Burden', 'Engineering', 'Epidemic', 'Epitopes', 'Evolution', 'Ferrets', 'Follow-Up Studies', 'Future', 'Glycoproteins', 'Goals', 'Ha antigen', 'Health', 'Health Expenditures', 'Healthcare', 'Hemagglutination', 'Hemagglutinin', 'Hong Kong', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunity', 'Immunization', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Insecta', 'Manuals', 'Measures', 'Medical', 'Methods', 'Modeling', 'Modification', 'Mus', 'Mutation', 'Pattern Recognition', 'Population', 'Preparation', 'Process', 'Productivity', 'Proteins', 'Quality of life', 'Recombinants', 'Seasons', 'Sequence Analysis', 'Series', 'Serological', 'Severities', 'Singapore', 'Site', 'Spanish flu', 'Statistical Data Interpretation', 'Structure', 'Surface', 'Surface Antigens', 'Switzerland', 'T-Lymphocyte', 'Technology', 'Testing', 'Texas', 'Translations', 'Vaccine Design', 'Vaccines', 'Validation', 'Variant', 'Viral Antibodies', 'Viral Antigens', 'Viral Physiology', 'Virus', 'Work', 'antiviral immunity', 'base', 'burden of illness', 'cross reactivity', 'design', 'disability-adjusted life years', 'flu', 'immunogenicity', 'improved', 'in silico', 'indexing', 'infection rate', 'influenza virus vaccine', 'influenzavirus', 'mutant', 'neutralizing antibody', 'new technology', 'novel', 'pandemic influenza', 'pathogen', 'prevent', 'programs', 'response', 'seasonal influenza', 'success', 'transmission process', 'universal influenza vaccine', 'universal vaccine', 'vaccine effectiveness']",NIAID,"BIOLOGICAL MIMETICS, INC.",R43,2020,299218,-0.004477712698312321
"Multifidelity and multiscale modeling of the spleen function in sickle cell disease with in vitro, ex vivo and in vivo validations Project Summary The spleen plays a key role in the human immune system but also clears senescent red blood cells (RBC) from the circulation and those altered by acquired or inherited diseases. In patients with sickle cell disease (SCD), the spleen is one of the first targets of pathogenic processes and a potential protector against major complications. Under hypoxic conditions, mutated sickle hemoglobin (HbS) polymerizes to fibers which increase both the stiffness and adhesion of RBC. Splenic filtration of altered RBC prone to sickling (a process that cannot be directly observed in human subjects) contributes to anemia and likely triggers acute splenic sequestration crises (ASSC). On the other hand, it potentially prevents complications associated with intravascular sickling. Self- amplified blockade of vessels with sickled RBCs is indeed a hallmark of vaso-occlusive crises, acute chest syndrome, and acute hepatic crises, that severely impact the life quality and expectancy of patients with SCD. We propose to formulate and validate a new predictive modeling framework for how the spleen filters altered RBC in SCD by synergistically integrating in silico, in vitro, ex vivo and in vivo data using multifidelity-based neural networks (NN). This will deliver predictive models that can continuously learn when new data become available, a paradigm shift in biomedical modeling. We will develop multiscale/multifidelity computational models (and corresponding NN implementations) that link sub-cellular, cellular, and vessel level phenomena spanning across four orders of magnitude in spatio-temporal scales. This scale coupling will be accomplished using a molecular dynamics/dissipative particle dynamics (MD/DPD) framework. We will validate these predictive computational models by data from in vitro and ex vivo experiments, and RBC quantitative features collected in SCD patients. Specifically, we will use three new spleen-on-a-chip microfluidic devices with oxygen control and the unique human spleen perfusion setup of our foreign partner, with the following aims: Aim 1: Develop and validate a splenic inter-endothelial slit filtration model; Aim 2: Develop new models of RBC macrophage adhesion and of phagocytosis in the spleen; Aim 3: Perform Spleen-on-a-Chip experiments and validation; Aim 4: Validate the predictive framework based on RBC samples from patients. Realization of our four Specific Aims will significantly increase our understanding of the complex pathogenic and protective roles of the spleen in SCD. Feeding our new multifidelity neural networks with morphological and functional measures of RBC circulating in SCD patients will lead to models for residual spleen function in SCD, which should help predict the risk of acute splenic sequestration crises, and guide optimal timing for Stem Cell Transplantation or Gene Therapy. The new paradigm in using deep learning tools to integrate data from different sources will be applicable to modeling many other blood diseases. Project Narrative In patients with sickle cell disease (SCD), the spleen is the target of early pathogenic processes and a potential protector against major complications. We will formulate and validate predictive multiscale models for red blood cell (RBC) filtration by the spleen based on new deep learning neural networks fed with data from simulations, experiments using new spleen-mimetic microfluidics, and RBC quantitative features from ex vivo perfusion of human spleens and from SCD patients. These models will be used to predict acute splenic sequestrations crises and guide treatment decisions in patients with SCD.","Multifidelity and multiscale modeling of the spleen function in sickle cell disease with in vitro, ex vivo and in vivo validations",10052044,R01HL154150,"['Accounting', 'Acute', 'Adhesions', 'Adhesiveness', 'Adhesives', 'Anemia', 'Biomechanics', 'Blood Circulation', 'Blood specimen', 'Cell Communication', 'Cell Shape', 'Cell model', 'Clinical', 'Collaborations', 'Complement', 'Complex', 'Complication', 'Computer Models', 'Coupling', 'Data', 'Decision Making', 'Devices', 'Endothelium', 'Erythrocytes', 'Expectancy', 'Fiber', 'Filtration', 'Goals', 'Hematological Disease', 'Hepatic', 'Hereditary Disease', 'Hereditary Spherocytosis', 'Human', 'Hypoxia', 'Immune system', 'In Vitro', 'Lead', 'Learning', 'Life', 'Link', 'Malaria', 'Measures', 'Mechanics', 'Medical', 'Microfluidic Microchips', 'Microfluidics', 'Modeling', 'Molecular', 'Morphology', 'Mutate', 'Organ', 'Output', 'Oxygen', 'Paper', 'Paris, France', 'Pathogenicity', 'Patients', 'Perfusion', 'Phagocytosis', 'Physicians', 'Play', 'Polymers', 'Process', 'Proteins', 'Quality of life', 'Residual state', 'Risk', 'Role', 'Sampling', 'Shapes', 'Sickle Cell', 'Sickle Cell Anemia', 'Sickle Hemoglobin', 'Source', 'Spleen', 'Stem cell transplant', 'Surface', 'System', 'Training', 'Validation', 'acute chest syndrome', 'base', 'biophysical properties', 'cohort', 'deep learning', 'deep neural network', 'design', 'ex vivo perfusion', 'experimental study', 'feeding', 'gene therapy', 'high dimensionality', 'human subject', 'improved', 'in silico', 'in vivo', 'learning progression', 'macrophage', 'mimetics', 'molecular dynamics', 'mouse model', 'multi-scale modeling', 'neural network', 'outcome forecast', 'particle', 'predictive modeling', 'prevent', 'retention rate', 'senescence', 'sickling', 'simulation', 'spatiotemporal', 'tool', 'vaso-occlusive crisis']",NHLBI,BROWN UNIVERSITY,R01,2020,705243,-0.036755859064966784
"Reactome: An Open Knowledgebase of Human Pathways Project Summary  We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated, open access biomolecular pathway database that can be freely used and redistributed by all members of the biological research community. It is used by clinicians, geneti- cists, genomics researchers, and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomic studies, and by systems biologists building predictive models of normal and disease variant pathways.  Our curators, PhD-level scientists with backgrounds in cell and molecular biology work closely with in- dependent investigators within the community to assemble machine-readable descriptions of human biological pathways. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated Reactome pathways currently cover 8930 protein- coding genes (44% of the translated portion of the genome) and ~150 RNA genes. We also offer a network of reliable ‘functional interactions’ (FIs) predicted by a conservative machine-learning approach, which covers an additional 3300 genes, for a combined coverage of roughly 60% of the known genome.  Over the next five years, we will: (1) curate new macromolecular entities, clinically significant protein sequence variants and isoforms, and drug-like molecules, and the complexes these entities form, into new reac- tions; (2) supplement normal pathways with alternative pathways targeted to significant diseases and devel- opmental biology; (3) expand and automate our tools for curation, management and community annotation; (4) integrate pathway modeling technologies using probabilistic graphical models and Boolean networks for pathway and network perturbation studies; (5) develop additional compelling software interfaces directed at both computational and lab biologist users; and (6) and improve outreach to bioinformaticians, molecular bi- ologists and clinical researchers. Project Narrative  Reactome represents one of a very small number of open access curated biological pathway databases. Its authoritative and detailed content has directly and indirectly supported basic and translational research studies with over-representation analysis and network-building tools to discover patterns in high-throughput data. The Reactome database and web site enable scientists, clinicians, researchers, students, and educators to find, organize, and utilize biological information to support data visualization, integration and analysis.",Reactome: An Open Knowledgebase of Human Pathways,9888390,U41HG003751,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Animal Model', 'Applications Grants', 'Back', 'Basic Science', 'Biological', 'Cellular biology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Developmental Biology', 'Disease', 'Doctor of Philosophy', 'Ensure', 'Event', 'Funding', 'Genes', 'Genome', 'Genomics', 'Human', 'Knowledge', 'Literature', 'Machine Learning', 'Mining', 'Modeling', 'Molecular', 'Molecular Biology', 'Pathway interactions', 'Pattern', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'RNA', 'Reaction', 'Readability', 'Research Personnel', 'Scientist', 'Students', 'System', 'Technology', 'Translating', 'Translational Research', 'Variant', 'Work', 'biological research', 'clinically significant', 'data visualization', 'experimental study', 'improved', 'knowledge base', 'member', 'novel', 'outreach', 'predictive modeling', 'research study', 'tool', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2020,1932440,-0.012678116277285309
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9992596,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,20899,0.013572157472697763
"Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications Summary: Viruses are ubiquitous in almost every ecological environment including the human body, water, soil, etc. They play important roles in the normal function of human microbiome. Many viruses have been shown to be associated with human diseases. However, our understanding of the roles of viruses in ecological communities is very limited. Recent technological and computational advances make it possible to have a deep understanding of the roles of viruses in public health and the environment. Metagenomics studies from various environments including the human microbiome projects (HMP), global ocean, and the earth microbiome projects have generated large amounts of short read data. Viruses are present in most of these metagenomic data sets and their hosts are unknown. In this proposal, the investigators will develop computational approaches for the identification of viral sequences from metagenomic data sets and for the study of virus-host interactions. For the identification of viral sequences from metagenomics samples, novel statistical measures using word patterns will first be developed. Second, a unified naïve Bayesian integrative approach by combining information from word patterns, gene directionality, and gene annotation will be studied. Third, the identified viral sequences from metagenomes will be further assembled to construct complete viral genomes using a novel binning approach to be developed by the investigators. Finally, the remaining reads will be assigned to the corresponding bins. For the study of virus- host interactions, computational methods to estimate the reliability of virus-host interactions from high-throughput experiments will first be developed. Then machine learning approaches will be developed to predict viruses infecting certain hosts. Finally, a network logistic regression approach will be developed to predict virus-host interactions. These computational approaches for the identification of viral sequences and for predicting virus-host interactions will be applied to a public liver cirrhosis and a unique metagenomics data set to understand how metagenomes change with health status, identify viruses and virus-host interactions associated with disease status and accurately predict disease status using bacteria, viruses and virus-host interactions. The developed computational methods will also be used to analyze metageomic data from various locations based on the TARA ocean data and a unique time series data to understand how environmental factors affect virus abundance and virus-host interactions. Some of the predictions will be experimentally validated. Software derived from the proposal will be developed and freely distributed to the scientific community. Project Narrative Viruses are abundant in many environments and are important to public health. New statistical and computational tools will be developed for the identification of viral sequences from metagenomics samples and for the prediction of virus-host interactions. These tools will be used to analyze microbial data sets related to liver cirrhosis and travelers’ diarrhea as well as marine metagenomics data sets from various geographic locations and time series.",Computational Studies of Virus-host Interactions Using Metagenomics Data and Applications,9899262,R01GM120624,"['Affect', 'Bacteria', 'Biological', 'Body Water', 'Cells', 'Communities', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Disease', 'Environment', 'Environment and Public Health', 'Environmental Risk Factor', 'Functional disorder', 'Genes', 'Genome', 'Geographic Locations', 'Health', 'Health Status', 'Human', 'Human Microbiome', 'Human body', 'Liver Cirrhosis', 'Location', 'Logistic Regressions', 'Machine Learning', 'Measures', 'Metagenomics', 'Methods', 'Microbe', 'Network-based', 'Oceans', 'Organism', 'Pattern', 'Planet Earth', 'Play', 'Policies', 'Public Health', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Series', 'Soil', 'Technology', 'Time', 'Traveler&apos', 's diarrhea', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Visualization software', 'base', 'computer studies', 'computerized tools', 'contig', 'design', 'experimental study', 'gut metagenome', 'human disease', 'interest', 'metagenome', 'microbial', 'microbial community', 'microbiome', 'novel', 'particle', 'statistics', 'tool', 'user-friendly', 'virus host interaction']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2020,430738,0.013572157472697763
"Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center The “clinical high risk” (CHR) for psychosis syndrome is an antecedent period characterized by attenuated psychotic symptoms that are marked by subtle deviations from normal development in thinking, motivation, affect, behavior, and a decline in functioning. Early intervention in this CHR population is critical to prevent psychosis onset as well as other adverse outcomes. However, the presentation of symptoms and subsequent course is highly variable, and there is a paucity of biomarkers to guide treatment development. Thus, to improve predictive models that are clinically relevant, several issues need to be addressed: 1) focusing on outcomes beyond psychosis; 2) taking into account heterogeneity in samples and outcomes; and 3) integrating data sets with a broad array of variables using innovative algorithms to overcome variability across studies. To address these challenges, the proposed “Psychosis Risk Evaluation Data Integration and Computational Technologies: Data Processing, Analysis, and Coordination Center” (PREDICT-DPACC) brings together a multidisciplinary team of highly experienced researchers with proven capabilities in all aspects of large-scale studies, CHR studies, as well as computational expertise. The ultimate goal is to identify new CHR biomarkers, and CHR subtypes that will enhance future clinical trials. To do so, the PREDICT-DPACC will 1) aggregate extant CHR- related data sets from legacy datasets; 2) provide collaborative management, direction, data processing and coordination for new U01 multisite network(s); and 3) develop and apply advanced algorithms to identify biomarkers that predict outcomes, and to stratify CHR into subtypes based on outcome trajectories, first from the extant data and then refined and applied to the new data. The PREDICT-DPACC team has the broad, comprehensive, and robust infrastructure that is sufficiently flexible to accommodate the inclusion of multiple data types and to optimally address the needs of the CHR U01 network(s). Carefully selected extant data will be rapidly obtained, processed, and uploaded to the NIMH Data Archive (NDA). Proposed analysis methods are powerful and robust, leveraging the expertise and experience of computer scientist developers, and experienced clinical researchers. The U01 network(s) will be coordinated by a team that is experienced in managing large studies, familiar with the needs of such studies, flexible, and is knowledgeable in all aspects of CHR studies, including measures, outcomes, biomarkers, and cohorts. Upon meeting the goals of this U24, and the supported U01 network(s), the expected outcomes of the PREDICT-DPACC will be new predictive biomarkers for CHR outcomes, new definitions of CHR subtypes that are clinically useful, and new curated and comprehensive CHR datasets (extant and new) as well as processing tools and prediction algorithms that are shared with the research community through the NIMH Data Archive. NARRATIVE The “Clinical High Risk” (CHR) for psychosis syndrome in young people represents an opportune window for early intervention to prevent the onset of psychosis and other disorders, and to forestall disability; however, clinical heterogeneity and the paucity of biomarkers have hampered the development of effective intervention. To address these challenges, working with NIMH and key stakeholders, we will harmonize and aggregate existing “legacy” CHR data, and guide and coordinate the collection of new data across a network of sites, to develop biomarker algorithms that can predict individual trajectories for diverse outcomes. This proposal leverages a multidisciplinary team with broad and CHR-specific experience in large-scale multisite and multimodal studies (including clinical trials), along with expertise in data type-specific processing, coordination, analysis, and computational analyses (e.g., machine and deep learning tools from artificial intelligence, and advanced statistical approaches), ethics, community outreach, and data dissemination, all of which will ensure the success of this project.","Psychosis Risk Evaluation, Data Integration and Computational Technologies (PREDICT): Data Processing, Analysis, and Coordination Center",10092398,U24MH124629,"['Address', 'Adolescent', 'Affect', 'Algorithms', 'Anxiety Disorders', 'Artificial Intelligence', 'Attenuated', 'Behavior', 'Big Data', 'Biological Markers', 'Child', 'Clinical', 'Clinical Trials', 'Collection', 'Common Data Element', 'Communities', 'Community Outreach', 'Computer Analysis', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease remission', 'Early Intervention', 'Early identification', 'Enrollment', 'Ensure', 'Ethics', 'Evaluation', 'FAIR principles', 'Follow-Up Studies', 'Funding', 'Future', 'Goals', 'Heterogeneity', 'Human Resources', 'Impaired cognition', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Intervention', 'Lead', 'Leadership', 'Longterm Follow-up', 'Machine Learning', 'Measures', 'Mental disorders', 'Meta-Analysis', 'Methods', 'Monitor', 'Moods', 'Motivation', 'National Institute of Mental Health', 'Online Systems', 'Outcome', 'Output', 'Perception', 'Procedures', 'Process', 'Protocols documentation', 'Psychotic Disorders', 'Quality Control', 'Recovery', 'Research', 'Research Personnel', 'Risk', 'Risk stratification', 'Safety', 'Sampling', 'Scientist', 'Secure', 'Site', 'Social Functioning', 'Standardization', 'Substance Use Disorder', 'Suggestion', 'Symptoms', 'Technology', 'Thinking', 'Time', 'Training', 'Transact', 'United States', 'Validation', 'Visualization software', 'adverse outcome', 'analytical tool', 'attenuated psychosis syndrome', 'base', 'bioinformatics infrastructure', 'candidate marker', 'clinical heterogeneity', 'clinical risk', 'clinical subtypes', 'clinically relevant', 'cloud based', 'cohort', 'computerized data processing', 'data acquisition', 'data archive', 'data dictionary', 'data dissemination', 'data harmonization', 'data infrastructure', 'data integration', 'data tools', 'deep learning', 'demographics', 'design', 'disability', 'effective intervention', 'experience', 'flexibility', 'functional decline', 'functional disability', 'high risk', 'high risk population', 'improved', 'inclusion criteria', 'innovation', 'meetings', 'member', 'multidisciplinary', 'multimodal data', 'multimodality', 'multiple data types', 'outcome prediction', 'persistent symptom', 'prediction algorithm', 'predictive marker', 'predictive modeling', 'prevent', 'prospective', 'psychotic symptoms', 'quality assurance', 'recruit', 'research study', 'resilience', 'response', 'success', 'therapy development', 'tool', 'working group']",NIMH,BRIGHAM AND WOMEN'S HOSPITAL,U24,2020,3935239,-0.017883467859146397
"Mapping host-microbe-metabolite interactions in 3D to find diet-derived enhancers of immunity Project Summary Antimicrobial resistance is an increasing problem, and current drug pipelines are not keeping pace with the rise of antimicrobial resistance. An alternative strategy is to boost host immunity. An often overlooked side-effect of the vitamin and mineral supplementation projects of the 1940s is that these supplements greatly reduced infectious disease burden. Recent work has shown that further gains may be possible, especially in adding phytochemicals back to highly processed diets typically consumed in the United States. However, we lack a fundamental understanding of how these components are processed by the microbiome, and how diet-derived molecules, microbiome and host immune system work together to resist infectious disease. A key barrier preventing us from making these discoveries is that each individual assay (microbiome, host gene expression metabolome, dietary compounds) is expensive and highly multivariate. Three key insights that enable the current project are the miniaturization of DNA and RNA sequencing assays on advanced nanoliter- scale liquid handling robots, greatly reducing the cost, the combination of untargeted and targeted mass spectrometry on the same samples in high throughput to enable discovery of a much greater chemical space, and the ability to use explicitly spatial maps on multiple scales to integrate the dataset throughout the body and enable both visual analytics and deep learning approaches based on spatial data. These breakthroughs will provide a fundamentally new understanding of how dietary metabolites promote disease resistance, and will allow us to develop a new infrastructure to integrate results from many investigators in different laboratories studying various aspects of these systems. Additionally, the results will allow us to choose biomaterials and biomarkers in human subjects that provide maximum information about internal nutritional and immune status. Results will be tested against the NHANES and American Gut cohorts. The results of this project will therefore be: 3D maps of mouse models showing how the microbiome, diet, and host gene expression produce immunity; an infrastructure for creating and sharing these maps; and a preliminary test of whether the results extend to large human populations. Project Narrative The public health relevance of this proposal is that it will provide an infrastructure for analyzing impacts of dietary components and microbiomes throughout the body. The analysis of these maps will help us identify dietary components that improve resistance to infectious disease.",Mapping host-microbe-metabolite interactions in 3D to find diet-derived enhancers of immunity,10015205,DP1AT010885,"['3-Dimensional', 'American', 'Antimicrobial Resistance', 'Back', 'Biocompatible Materials', 'Biological Assay', 'Biological Markers', 'Chemicals', 'Communicable Diseases', 'Consumption', 'DNA sequencing', 'Data', 'Data Set', 'Diet', 'Dietary Component', 'Disease Resistance', 'Enhancers', 'Gene Expression', 'Human', 'Immune system', 'Immunity', 'Individual', 'Infrastructure', 'Laboratory Study', 'Liquid substance', 'Maps', 'Mass Spectrum Analysis', 'Metabolite Interaction', 'Microbe', 'Minerals', 'Miniaturization', 'National Health and Nutrition Examination Survey', 'Nutritional status', 'Pharmaceutical Preparations', 'Phytochemical', 'Population', 'Process', 'Research Personnel', 'Resistance', 'Robot', 'Sampling', 'Supplementation', 'System', 'Testing', 'United States', 'Visual', 'Vitamins', 'Work', 'base', 'burden of illness', 'cohort', 'cost', 'deep learning', 'human subject', 'immunological status', 'improved', 'insight', 'metabolome', 'microbiome', 'mouse model', 'nanolitre scale', 'prevent', 'public health relevance', 'side effect', 'transcriptome sequencing']",NCCIH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",DP1,2020,885000,-0.006749430494026114
"Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity PROJECT SUMMARY Reliable and real-time municipality-level predictive modeling and forecasts of infectious disease activity have the potential to transform the way public health decision-makers design interventions such as information campaigns, preemptive/reactive vaccinations, and vector control, in the presence of health threats across the world. While the links between disease activity and factors such as: human mobility, climate and environmental factors, socio-economic determinants, and social media activity have long been known in the epidemic literature, few efforts have focused on the evident need of developing an open-source platform capable of leveraging multiple data sources, factors, and disparate modeling methodologies, across a large and heterogeneous nation to monitor and forecast disease transmission, over four geographic scales (nation, state, city, and municipal). The overall goal of this project is to develop such a platform. Our long-term goal is to investigate effective ways to incorporate the findings from multiple disparate studies on disease dynamics around the globe with local and global factors such as weather conditions, socio- economic status, satellite imagery and online human behavior, to develop an operational, robust, and real- time data-driven disease forecasting platform. The objective of this grant is to leverage the expertise of three complementary scientific research teams and a wealth of information from a diverse array of data sources to build a modeling platform capable of combining information to produce real-time short term disease forecasts at the local level. As part of this, we will evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales--nation, state, city, and municipality--using Brazil as a test case. Additionally, we will use machine learning and mechanistic models to understand disease dynamics at multiple spatial scales, across a heterogeneous country such as Brazil. Our specific aims will (1) Assess the utility of individual data streams and modeling techniques for disease forecasting; (2) Fuse modeling techniques and data streams to improve accuracy and robustness at the four spatial scales; (3) Characterize the basic computational infrastructure necessary to build an operational disease forecasting platform; and (4) Validate our approach in a real-world setting. This contribution is significant because It will advance our scientific knowledge on the accuracy and limitations of disparate data streams and multiple modeling approaches when used to forecast disease transmission. Our efforts will help produce operational and systematic disease forecasts at a local level (city- and municipality-level). Moreover, we aim at building a new open-source computational platform for the epidemiological community to use as a knowledge discovery tool. Finally, we aim at developing this platform under the guidance of a Subject Matter Expert (SME) panel comprising of WHO, CDC, academics, and local and federal stakeholders within Brazil. The proposed approach is innovative because few efforts have focused on developing an open-source computational platform capable of combining disparate data sources and drivers, across a heterogeneous and large nation, into multiple modeling approaches to monitor and forecast disease transmission, over multiple geographic scales.. In addition, we propose to investigate how to best combine modeling approaches that have, to this date, been developed and interpreted independently, namely, traditional epidemiological mechanistic models and novel machine-learning predictive models, in order to produce accurate and robust real-time disease activity estimates and forecasts. Project Narrative The proposed research is of crucial importance to public health surveillance and preparedness communities because it seeks to identify effective ways to utilize previously disconnected results, that have pointed out links between disease spread and factors such as socio-economic status, local weather conditions, human mobility, social media activity, to build an open-source and data driven, modeling platform capable of extracting and disseminating information from disparate data sources, and complementary modeling approaches, to (1) Evaluate the predictive power of disparate data streams and modeling approaches to monitor and forecast disease at multiple geographic scales: nation, state, city, and municipality; (2) Fuse complementary modeling approaches that have been developed independently and oftentimes not used in conjunction; (3) produce real- time and short term forecasts of disease activity in multiple geographic scales across a heterogeneous and large nation like Brazil.",Development of an Open-Source and Data-Driven Modeling Platform to Monitor and Forecast Disease Activity,10000112,R01GM130668,"['Area', 'Assimilations', 'Beds', 'Behavior', 'Brazil', 'Burn injury', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Climate', 'Communicable Diseases', 'Communities', 'Complement', 'Country', 'Data', 'Data Set', 'Data Sources', 'Dengue', 'Developing Countries', 'Development', 'Disease', 'Disease Outbreaks', 'Economics', 'Elements', 'Environment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Geography', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'High Performance Computing', 'Human', 'Imagery', 'Individual', 'Influenza', 'Influenza B Virus', 'Institution', 'Internet', 'Knowledge', 'Knowledge Discovery', 'Lead', 'Link', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Municipalities', 'Population Surveillance', 'Process', 'Public Health', 'Readiness', 'Research', 'Socioeconomic Status', 'Techniques', 'Testing', 'Time', 'Twitter', 'Vaccination', 'Vector-transmitted infectious disease', 'Water', 'Weather', 'Work', 'ZIKA', 'base', 'chikungunya', 'climate variability', 'computational platform', 'computer infrastructure', 'data infrastructure', 'data modeling', 'data streams', 'digital', 'disease transmission', 'economic determinant', 'experience', 'flu', 'genomic data', 'heterogenous data', 'improved', 'innovation', 'mathematical methods', 'multiple data sources', 'novel', 'open data', 'open source', 'pathogen', 'pathogen genomics', 'predictive modeling', 'social', 'social media', 'sociodemographics', 'socioeconomics', 'spreading factor', 'therapy design', 'time use', 'tool', 'transmission process', 'trend', 'vector control', 'vector-borne']",NIGMS,BOSTON CHILDREN'S HOSPITAL,R01,2020,365601,-0.003015586397542168
"A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth This project provides a data science framework and a toolbox of best practices for systematic and reproducible data-driven methods for validating and deriving RDoC constructs with relevance to psychopathology. Despite recent advances in methods for data-driven constructs, results are often hard to reproduce using samples from other studies. There is a lack of systematic statistical methods and analytical design for enhancing reproducibility. To fill this gap, we will develop a data science framework, including novel scalable algorithms and software, to derive and validate RDoC constructs. Although the proposed methods will generally apply to all RDoC domains and constructs, we focus specifically on furthering understanding of the RDoC domains of cognitive control (CC) and attention (ATT) constructs implicated in attention deficit disorder (ADHD) and obsessive-compulsive disorder (OCD). Our application will use multi-modal neuroimaging, behavioral, and clinical/self-report data from large, nationally representative samples from the on Adolescent Brain Cognitive Development (ABCD) study and multiple local clinical samples with ADHD and OCD. Specifically, using the baseline ABCD samples, in aim 1, we will apply and develop methods to assess and validate the current configuration of RDoC for CC and ATT using confirmatory latent variable modeling. We will implement and develop new unsupervised learning methods to construct new computational-driven, brain-based domains from multi-modal image data. In Aim 2, We will introduce network analysis (via Gaussian graphical models) to characterize heterogeneity in the interrelationship of RDoC measurements due to observed characteristics (i.e., age and sex). We will further model the heterogeneity of the population due to unobserved characteristics by introducing the data-driven precision phenotypes, which are the subgroup of participants with similar RDoC dimensions. We propose a Hierarchical Bayesian Generative Model and scalable algorithm for simultaneous dimension reduction and identify precision phenotypes. The model also serves as a tool to transfer information from the community sample ABCD to local clinical enriched studies. In aim 3, we will utilize the follow-up samples from ABCD and local clinical enriched data sets to validate the results from Aims 1 and 2 and assess the clinical utility of the precision phenotypes in predicting psychological development in follow-up time. Our project will provide a suite of analytical tools to validate existing RDoC constructs and derive new, reproducible constructs by accounting for various sources of heterogeneity. To advance the understanding of psychopathology using dimensional constructs of measurements from multiple units of analysis, we propose reproducible statistical framework for validating and deriving RDoC constructs with relevance to psychopathology. We will use multi-modal neuroimaging, behavioral and clinical/self-report data from multiple samples to develop this framework. The design of our study consists of analyzing large, nationally representative samples, validating the results in local clinically enriched samples, and transfer information from the large community samples to local clinical samples.",A Data Science Framework for Empirically Evaluating and Deriving Reproducible and Transferrable RDoC Constructs in Youth,10058921,R01MH124106,"['11 year old', 'Accounting', 'Adolescent', 'Age', 'Algorithmic Software', 'Algorithms', 'Attention', 'Attention Deficit Disorder', 'Base of the Brain', 'Behavioral', 'Brain', 'Characteristics', 'Child', 'Chronology', 'Clinical', 'Clinical Data', 'Communities', 'Data', 'Data Reporting', 'Data Science', 'Data Set', 'Development', 'Dimensions', 'Ensure', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Goals', 'Heterogeneity', 'Image', 'Knowledge', 'Learning', 'Link', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Obsessive-Compulsive Disorder', 'Participant', 'Pathway Analysis', 'Patient Self-Report', 'Phenotype', 'Population Heterogeneity', 'Prediction of Response to Therapy', 'Psychological Transfer', 'Psychopathology', 'Reproducibility', 'Reproducibility of Results', 'Research Domain Criteria', 'Sampling', 'Source', 'Statistical Methods', 'Structure', 'Subgroup', 'Symptoms', 'Time', 'Variant', 'Youth', 'age effect', 'analytical tool', 'autoencoder', 'base', 'biological sex', 'cognitive control', 'cognitive development', 'deep learning', 'design', 'follow up assessment', 'follow-up', 'high dimensionality', 'independent component analysis', 'insight', 'learning algorithm', 'learning strategy', 'machine learning algorithm', 'multimodality', 'network models', 'neuroimaging', 'novel', 'psychologic', 'response', 'sex', 'tool', 'unsupervised learning']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2020,710101,-0.013551467791536797
"Meta-analysis in human brain mapping This is the competing renewal of R01MH074457-13, which sustains the BrainMap Project (www.brainmap.org). The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data. To date, the BrainMap Project has designed and populated two coordinate-based databases: 1) a task-activation repository (TA DB); and, 2) a voxel-based morphometry repository (VBM DB). The TA DB contains >17,200 experiments, collectively representing > 78,000 subjects and > 110 task- activation paradigms. The VBM DB contains > 3,100 experiments, collectively representing > 81,000 subjects with > 80 psychiatric, neurologic and developmental disorders with ICD-10 coding. The BrainMap Project has created, optimized and validated an integrated pipeline of multi-platform (Javascript), open-access tools to curate (Scribe), filter and retrieve (Sleuth), analyze (GingerALE), visualize (Mango) and interpret analysis output (BrainMap meta-data plugins for Mango). Several network-modeling approaches have been applied to BrainMap data -- MACM, independent components analysis (ICA), graph theory modeling (GTM), author-topic modeling (ATM), structural equation modeling (SEM), and connectivity-based parcellation (CBP) – but none are yet pipeline components. Utilization of these CBMA resources is substantial: BrainMap software, data and meta-data have been used in > 825 peer-reviewed publications. Of these, > 350 were published within the current funding period (April 2015-March 2019; brainmap.org/pubs). In this competing renewal, four tool- development aims are proposed, each of which extends this high-impact research resource. Aim 1. Database Expansion. BrainMap data repositories will be expanded. Aim 2. Meta-analytic Network Modeling. Network modeling will be added to the BrainMap pipeline. Aim 3. Large-Scale Simulations, Comparisons and Validations. Data simulations, characterizations and validations will be performed. Aim 4. Meta-data Inferential tools. Tools for mining BrainMap’s location-linked meta-data will be expanded. Data Sharing Plan. BrainMap data, meta-data, pipeline tools, and templates created by whole-database modeling (e.g., ICA and ATM network masks) are shared at BrainMap.org. Of all new data entries, more than half are contributed by BrainMap users, i.e., community data sharing via BrainMap.org. For community-coded entries, the BrainMap team provides curation and quality control. Comprehensive database images (database dumps) are available to tool developers through Collaborative Use Agreements. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data  sets, metadata, computational tools, and related resources that enable coordinate-­based meta-­analyses  (CBMA), meta-­analytic connectivity modeling (MACM), meta-­data informed interpretation (“decoding”) of  imaging results, and meta-­analytic priors for mining (including machine learning) primary (per-­subject)  neuroimaging data.    ",Meta-analysis in human brain mapping,10056029,R56MH074457,"['Agreement', 'Area', 'Brain', 'Brain Mapping', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Set', 'Databases', 'Disease', 'Educational workshop', 'Equation', 'Functional disorder', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Institution', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Java', 'Link', 'Location', 'Machine Learning', 'Mango - dietary', 'Masks', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Output', 'Peer Review', 'Plug-in', 'Publications', 'Publishing', 'Quality Control', 'Research Domain Criteria', 'Resources', 'Rest', 'Site', 'Software Framework', 'Specificity', 'Structure', 'Training', 'Universities', 'Validation', 'base', 'candidate marker', 'computerized tools', 'data pipeline', 'data sharing', 'data warehouse', 'design', 'developmental disease', 'experimental study', 'graph theory', 'independent component analysis', 'interest', 'large scale simulation', 'morphometry', 'nervous system disorder', 'network architecture', 'network models', 'neuroimaging', 'neuropsychiatric disorder', 'repository', 'simulation', 'tool', 'tool development']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R56,2020,543396,-0.013555133527620063
"Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design PROJECT SUMMARY/ABSTRACT Molecular simulation is a powerful tool to predict the properties of biomolecules, interpret biophysical experiments, and design small molecules or biomolecules with therapeutic utility. However, a number of obstacles have impeded the development of quantitative, cloud-scale research workﬂows involving biomolecular simulation. Two main ob- stacles are the insufﬁcient accuracy of current atomistic models for biomolecules and small molecule therapeutics and the lack of interoperability in simulation toolchains used in both academic and industrial biomolecular research. Our original R01, “Open Data-driven Infrastructure for Building Biomolecular Force Fields for Predictive Bio- physics and Drug Design,” seeks to solve the ﬁrst problem. It helps fund our effort, the Open Force Field Initiative (https://openforceﬁeld.org) to develop open, extensible, and shared software and data infrastructure, implementing statistically robust methods of parameterizing force ﬁelds and choosing new force ﬁelds in a statistically sound manner. This work is designed to create not just a new generation of force ﬁelds, but an open technology to continue advancing force ﬁeld science. However, even with improved molecular models, putting together complete workﬂows of biomolecular simulations involves interfacing substantial numbers of different tools. However the majority of the existing molecular simulation workﬂows are mutually incompatible, with differing representations of the molecular models. The Open Force Field Initiative effort already includes the development of molecular data structures that we can ex- port into existing molecular simulation tools. We propose to extend the existing scope of our R01 to create an extensible common molecular simulation representation and translators to and from this representation. Such a set of tools will immediately make it signiﬁcantly easier to combine the disparate workﬂows developed for different sets of molecular simulation tools. Researchers will be able to set up and build the biophysical simulations using their usual tools, but run and analyze them with currently incompatible tools, enabling better matching of computational resources and methods to problems. It will help avoid trapping in a single software framework, and enable combinations of functionalities previously impossible without substantial developer time and effort. We will (Aim 1) work with partners to generalize our modular, extensible object model for representing parameterized biomolecular systems in a manner that accommodates the force ﬁeld terms currently supported by most popular biomolecular simulation packages. We will engineer it to be extensible to advanced interaction forms, such as polarizability and other multibody terms, and machine learning models for intermolecular forces. We will (Aim 2): enable easy conversion between components of molecular simulation workﬂows by allowing other molecular simulation packages to easily store their representations in this data model, developing converters that can import/export this object model to multiple popular ﬁle formats, focusing initially on OpenMM, AMBER, CHARMM, and GROMACS. We will demonstrate the utility of this interface in cloud-ready workﬂows. PROJECT NARRATIVE Scientists use computer simulations of proteins, DNA, and RNA, at atomic detail, to learn how these molecules of life carry out their functions and to design new medications. We aim to greatly increase the utility of all of these simulations by improving the accuracy of the formulas they use to compute the forces acting between atoms. This supplement will make it much easier for molecular simulation workﬂows to interoperate with each other in large-scale workﬂows.",Open Data-driven Infrastructure for Building Biomolecular Force Field for Predictive Biophysics and Drug Design,10166314,R01GM132386,"['Affinity', 'Binding', 'Biophysics', 'COVID-19', 'Collaborations', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'DNA', 'Development', 'Drug Design', 'Ecosystem', 'Engineering', 'Funding', 'Generations', 'Human', 'Individual', 'Industrialization', 'Infrastructure', 'Language', 'Learning', 'Libraries', 'Life', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Motion', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Problem Solving', 'Property', 'Proteins', 'Pythons', 'RNA', 'Readability', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Science', 'Scientist', 'Software Framework', 'System', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'Writing', 'biomaterial interface', 'computing resources', 'data infrastructure', 'data modeling', 'design', 'experimental study', 'file format', 'improved', 'interoperability', 'molecular modeling', 'open data', 'simulation', 'small molecule', 'small molecule therapeutics', 'software infrastructure', 'sound', 'structured data', 'tool']",NIGMS,UNIVERSITY OF COLORADO,R01,2020,225000,-0.0063594307219873355
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9858390,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data infrastructure', 'data integration', 'data standards', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'large scale data', 'member', 'mouse genome', 'multiple data types', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2020,2000000,-0.029924869771353186
"Improving predictive capacity of models for universal influenza vaccine development The need for improved and more universally protective influenza vaccines is well recognized. Central to efforts towards improvements is the development of animal models more predictive of the human response to immunization and/or infection. Indeed, this need has been highlighted by the NIAID Strategic Plan for a Universal Influenza Vaccine. While animal models may never be able to fully predict the human response, understanding their full strengths and weaknesses and identifying the optimal models for different purposes is a significant public health need and is the scientific premise behind our proposed objectives. These objectives, which are built upon our extensive use of influenza animal models, are to optimize animal modeling of immunologic imprinting, to improve vaccine efficacy testing, and to identify immune correlates of protection and boosting immune responses. Our overall goal is to provide superior preclinical models to support universal influenza vaccine development. We will achieve this goal through three complementary and interrelated specific aims, 1) optimal modeling of human serologic responses to repeat influenza antigen exposure in animal models; 2) improving the quantitative nature of the ferret influenza challenge model; and 3) defining serologic correlates of influenza virus induced clinical symptoms. Our ability to conduct these aims is supported through our participation in, and collaboration with, a recently NIAID-funded human infant cohort, the DIVINCI study. We will mirror the influenza antigen exposures of a selection of these infants in three animal models and compare immunologic data sets to identify which most accurately reflects the human response (Aim 1). This marriage of human and animal data sets and samples offers an innovative way forward and will provide a unique set of differentially primed animals with which to determine immune correlates of novel physiologic parameters of infection and immune responses (Aim 2) using original machine learning algorithms (Aim 3). Current models for influenza vaccine development suffer from poor predictability of the human response. Through an innovative combination of approaches that use data from longitudinal human cohorts, we intend to greatly enhance the reliability and value of these models in universal influenza vaccine testing.",Improving predictive capacity of models for universal influenza vaccine development,9950635,R01AI150745,"['Address', 'Algorithms', 'Animal Model', 'Animals', 'Antigens', 'Antiviral Agents', 'Area', 'Cavia', 'Clinical', 'Clinical Research', 'Collaborations', 'Data', 'Data Set', 'Development', 'Dose', 'Ensure', 'Exposure to', 'Family suidae', 'Ferrets', 'Funding', 'Future', 'Goals', 'Hamsters', 'Human', 'Immune', 'Immune response', 'Immunity', 'Immunization', 'Immunological Models', 'Immunologics', 'Individual', 'Infant', 'Infection', 'Influenza', 'Influenza A Virus, H3N2 Subtype', 'Information Systems', 'Lasso', 'Lung', 'Marriage', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Mus', 'National Institute of Allergy and Infectious Disease', 'Natural History', 'Nature', 'Organ', 'Outcome', 'Output', 'Pathogenesis', 'Performance', 'Physiological', 'Population', 'Pre-Clinical Model', 'Predictive Value', 'Primates', 'Property', 'Public Health', 'Reagent', 'Recording of previous events', 'Research', 'Sampling', 'Seasons', 'Serological', 'Strategic Planning', 'Symptoms', 'System', 'Telemetry', 'Time', 'United States', 'Upper respiratory tract', 'Vaccinated', 'Vaccine Production', 'Vaccines', 'Virulence', 'Virus Replication', 'Whole Body Plethysmography', 'animal data', 'animal model development', 'cohort', 'efficacy testing', 'flexibility', 'human data', 'human model', 'imprint', 'improved', 'influenza virus vaccine', 'influenzavirus', 'innovation', 'machine learning algorithm', 'mouse development', 'multitask', 'next generation', 'novel', 'preclinical evaluation', 'product development', 'research and development', 'response', 'safety study', 'tool', 'transmission process', 'universal influenza vaccine', 'universal vaccine', 'vaccine candidate', 'vaccine development', 'vaccine effectiveness', 'vaccine efficacy', 'vaccine evaluation']",NIAID,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,R01,2020,723996,0.009580114200826698
"Influenza host specific glycan motif identification through systems biology Project Summary Influenza A viruses (IAVs) have caused large losses of life around the world and continue to present a great public health challenge. IAVs can cause infections in birds, sea mammals, lower mammals (e.g., pigs, dogs, and horses), and humans. Previous studies have demonstrated that the structures of the carbohydrate receptors determine influenza host and tissue tropisms. Thus, it is necessary to understand the receptor- binding properties for IAVs and monitor changes to them, especially for IAVs at the animal–human interface. However, this understanding is hampered by our lack of detailed knowledge of IAV glycan substructures; most of our knowledge is limited to SA2,3Gal-like and SA2,6Gal-like structures. The goals of this project are to develop and validate a machine learning method to identify host-specific glycan substructures for IAVs by using glycan array data and to identify and validate the glycan motifs associated with the host tropisms of IAVs, including those for zoonotic IAVs. The study will focus on natural hosts of IAVs: humans, swine, canines, equines, and various avian species, including common domestic poultry species and wild bird species. We expect to identify structural determinants for receptor binding with human-, swine-, canine-, and avian-origin IAVs. Such knowledge will help us understand the factors that contribute to influenza infection and transmission and thereby facilitate development of an effective influenza vaccine to prevent virus infection and block virus transmission. This knowledge will also help us develop rapid assays for monitoring emerging influenza threats at the animal–human interface. We also expect to develop a computational method for identifying glycan motifs associated with influenza host tropisms; this method will be able to be adapted to determine functional glycan motifs for other proteins, lectins, antibodies, antisera, and microorganisms, including those of other infectious pathogens, by using glycan arrays. Project Narrative This project will develop and apply a computational tool to identify glycan motifs associated with influenza host tropisms, and the derived knowledge will help us develop effective strategies for influenza prevention and control.",Influenza host specific glycan motif identification through systems biology,9895377,R21AI144433,"['Affect', 'Animals', 'Antibodies', 'Area', 'Avian Influenza A Virus', 'Binding', 'Binding Sites', 'Biological', 'Biological Assay', 'Biosensor', 'Birds', 'Canis familiaris', 'Code', 'Complex', 'Computing Methodologies', 'Data', 'Data Set', 'Development', 'Domestic Fowls', 'Equus caballus', 'Family suidae', 'Goals', 'Growth', 'Hemagglutinin', 'Human', 'Immune Sera', 'Infection', 'Influenza', 'Influenza A virus', 'Influenza prevention', 'Knowledge', 'Learning', 'Lectin', 'Life', 'Link', 'Mammals', 'Methods', 'Molecular', 'Monitor', 'Mutation', 'National Institute of Allergy and Infectious Disease', 'Natural History', 'Pathogenesis', 'Performance', 'Play', 'Polysaccharides', 'Property', 'Proteins', 'Public Health', 'Research', 'Resources', 'Risk', 'Role', 'Sea', 'Sialic Acids', 'Signal Transduction', 'Slide', 'Strategic Planning', 'Streptavidin', 'Structure', 'Systems Biology', 'Technology', 'Testing', 'Tissues', 'Trees', 'Tropism', 'United States National Institutes of Health', 'Validation', 'Variant', 'Virus Diseases', 'Zoonoses', 'base', 'carbohydrate receptor', 'carbohydrate structure', 'computerized tools', 'design', 'experimental study', 'flu transmission', 'improved', 'in silico', 'influenza virus vaccine', 'influenzavirus', 'large datasets', 'learning strategy', 'machine learning method', 'microorganism', 'multi-task learning', 'multitask', 'novel', 'pathogen', 'prevent', 'protein profiling', 'receptor', 'receptor binding', 'respiratory', 'response', 'swine influenza', 'tissue tropism', 'tool', 'transmission process', 'universal influenza vaccine', 'viral transmission', 'wild bird']",NIAID,UNIVERSITY OF MISSOURI-COLUMBIA,R21,2020,239725,-0.005758025272311615
"Coordinating Research on Emerging Arboviral Threats Encoing the Neotropics (CREATE-NEO) Project Summary In recent decades, Central and South America have experienced spillover of endemic arthropod-borne viruses (arboviruses) from wildlife reservoirs into humans, exchange and recombination of emerging arboviruses within the region, resurgence of arboviruses previously controlled by vaccination or vector control, introduction and spread of novel arboviruses, and exportation of viruses to other regions. Furthermore, there is great concern that newly-introduced Zika virus may spill back into an enzootic transmission cycle in the Americas. Central and South America encompass enormous vertebrate and invertebrate biodiversity, and these species harbor a broad range of arboviruses whose risk of spillover and spread in humans is presently unknown. Increases in the rates of global travel, invasion of novel vector species, urban expansion, deforestation, and global climate change all elevate the risk of further arbovirus emergence, as does the breakdown of public health structures in Venezuela.  The Coordinating Research on Emerging Arboviral Threats Encompassing the Neotropics (CREATE- NEO) project will provide a network of surveillance sites in the neotropics coupled to cutting-edge modeling approaches in order to anticipate and counter emerging arboviruses. Aim 1 will identify novel and known arboviruses as well as the host-vector networks that sustain transmission of these viruses within the neotropics, map the spatial distribution of these transmission networks, and characterize virus transmission dynamics within these networks. To do so, we will collect mosquitoes and other vectors as well as non-human primates and other vertebrate hosts at multiple sites in areas of high and varied biodiversity in Panama and Brazil and screen these samples for known and novel arboviruses. These data will then be analyzed using niche modeling, machine learning to predict undiscovered hosts and vectors, and dynamical transmission models. Aim 2 will focus on prospective and retrospective analysis of human infection and disease. To do so, we will leverage ongoing human clinical cohorts at multiple sites in Brazil and Panama. We will extend and expand these cohorts, with a particular focus on the immune-mediated interactions among multiple arboviruses at sites of hyperendemicity. We will also develop novel diagnostics to capture known and novel arboviruses and model the impact of human and non-human primate movement on spillover and spillback of target arboviruses.  Data and models generated via these two aims will forewarn local, regional and global public health agencies of arboviruses within Central and South America that pose particularly high risk of spillover, emergence into transmission among humans, and/or international spread. Moreover CREATE-NEO will build local capacity to predict, detect and respond to emerging arboviruses at their point of origin, thereby maximizing the potential to avert full-blown emergence. Project Narrative Arthropod-borne viruses, such as dengue, Zika and Mayaro, are emerging at an accelerating rate in Central and South America. The Coordinating Research on Emerging Arboviral Threats Encompassing the Neotropics (CREATE-NEO) project will provide a nimble and flexible network of surveillance sites in Central and South America coupled to cutting-edge modeling approaches in order to anticipate and counter these threats to public health.",Coordinating Research on Emerging Arboviral Threats Encoing the Neotropics (CREATE-NEO),9968994,U01AI151807,"['Acute', 'Address', 'Age', 'Americas', 'Animals', 'Arbovirus Infections', 'Arboviruses', 'Area', 'Back', 'Biodiversity', 'Biological', 'Blood Circulation', 'Brazil', 'Central America', 'Chikungunya virus', 'Cities', 'Clinical', 'Cohort Studies', 'Collection', 'Coupled', 'Culicidae', 'Data', 'Deforestation', 'Dengue', 'Detection', 'Diagnostic', 'Disease', 'Emergency Situation', 'Enzyme-Linked Immunosorbent Assay', 'Frequencies', 'Genetic Recombination', 'Genetic Variation', 'Habitats', 'Human', 'Immune', 'Infection', 'International', 'Invertebrates', 'Machine Learning', 'Malaria', 'Maps', 'Measles', 'Mediating', 'Modeling', 'Movement', 'Panama', 'Public Health', 'Research', 'Risk', 'Route', 'Sampling', 'Site', 'South America', 'Spatial Distribution', 'Structure', 'Technology', 'Testing', 'Time', 'Travel', 'Vaccination', 'Vector-transmitted infectious disease', 'Venezuela', 'Virus', 'West Nile virus', 'Yellow Fever', 'ZIKA', 'Zika Virus', 'Zoonoses', 'chikungunya', 'climate change', 'cohort', 'design', 'enzootic', 'experience', 'flexibility', 'high risk', 'insight', 'nanobodies', 'nonhuman primate', 'novel', 'novel diagnostics', 'pathogen', 'prospective', 'seroconversion', 'surveillance network', 'transmission process', 'vector', 'vector control', 'viral transmission']",NIAID,UNIVERSITY OF TEXAS MED BR GALVESTON,U01,2020,1615615,-0.0074544242757331285
"Modeling Homeostasis of Human Blood Metabolites PROJECT SUMMARY  Metabolite levels in human blood are regulated by a relatively strict system of homeostatic control. Previous investigations of homeostasis have taken a number of approaches, and models of glucose and a few other metabolites have been developed, typically focused on a single organ. However, while potentially extremely useful, an accurate and quantitative model of blood metabolite levels under homeostasis does not currently exist.  It is well known that numerous demographic and clinical factors such as gender, age, BMI, smoking, etc., as well as pre-analytical factors and many diseases, significantly affect the levels of blood metabolites. Numerous studies in the field of metabolomics have attempted to account for the effects of many such factors. However, efforts to quantify these effects and validate them across different studies have so far been challenging, and resulted in consistent failures to validate discovered putative biomarkers. The challenges to integrate metabolite profiles with clinical and demographic factors are complicated by the high dimensionality of the data and the numerous correlations among the metabolites. Traditional statistical methods are incapable of accounting for these factors, and hence, investigations suffer from a high false discovery rate (FDR).  To overcome these challenges, we propose to develop quantitative statistical models of blood metabolite levels in healthy adults, and thereby produce a predictive model of homeostasis. Our preliminary work indicates that we can predict metabolite levels with much reduced variance using the reproducibly measured levels of a large pool of blood metabolites and clinical and demographic variables. We propose to develop sophisticated models of homeostasis based on advanced statistical methods and evaluate their predictive performance across different sample sets and metabolite classes.  The proposed project has four main Aims: (1) Obtain broad-based metabolomics data on blood samples collected from geographically distinct sites to explore the effects of a range of confounding effects on metabolite levels. (2) Model individual or biologically related groups of metabolite levels using multivariate statistical approaches to determine the contribution of clinical/demographic and pre-analytical variables and their predictability across collection site. (3) Investigate the interactions between metabolites and clinical/demographic variables using machine learning approaches to identify stable metabolites and key interactions. (4) Provide the community with user-friendly software packages for the prediction of blood metabolite levels under homeostasis.  An overall model of the metabolite concentrations in blood will be highly useful for a number of applications that include a better understanding of systems biology at the whole organism level, and ultimately improved risk prediction, disease diagnosis, treatment monitoring and outcomes analysis. PROJECT NARRATIVE A quantitative model of blood homeostasis based on predicting the normal levels of small molecules in the blood can help identify diseases or other stresses that cause changes to those levels. The proposed statistical methods that will be used to develop this homeostasis model have the potential to efficiently identify more reliable disease markers and to more accurately predict disease risk.",Modeling Homeostasis of Human Blood Metabolites,9995722,R01GM131491,"['Accounting', 'Address', 'Adult', 'Affect', 'Age', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biometry', 'Blood', 'Blood specimen', 'Clinical', 'Collection', 'Communities', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Demographic Factors', 'Development', 'Dimensions', 'Disease', 'Disease Marker', 'Evaluation', 'Failure', 'Gases', 'Gender', 'Geographic Locations', 'Geography', 'Glucose', 'Homeostasis', 'Human', 'Individual', 'Investigation', 'Ions', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Metabolic', 'Metabolic Pathway', 'Metabolite Interaction', 'Modeling', 'Monitor', 'NMR Spectroscopy', 'Organ', 'Outcome', 'Performance', 'Risk', 'Sampling', 'Site', 'Smoking', 'Source', 'Statistical Methods', 'Statistical Models', 'Stress', 'Supervision', 'System', 'Systems Biology', 'Technology', 'Temperature', 'Time', 'Training', 'Validation', 'Whole Organism', 'Work', 'base', 'clinical effect', 'cohort', 'computerized tools', 'data quality', 'disease diagnosis', 'disorder risk', 'improved', 'interoperability', 'metabolome', 'metabolomics', 'multidimensional data', 'predictive modeling', 'sample collection', 'small molecule', 'software development', 'user friendly software', 'user-friendly', 'validation studies']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,395738,-0.011016304314594055
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,0.0008686811611427508
"Dissecting host-pathogen interactions through the lens of genomics Summary: Dissecting host-pathogen interactions through the lens of genomics Current investigation of mechanisms underlying many diseases relies on the acquisition of multi-dimensional genomics data. The utility of these data is, however, offset by the lag in development of tools and models to fully interrogate them. In the context of infectious diseases, such data contains molecular information including gene transcription, regulation, and variations from both the infecting pathogen and the host cell, providing a snapshot of the host and pathogen interactions (HPIs). These HPIs determine infection outcomes. For instance, when a pathogen evades, or evolves resistance to defensive host immunity via a multifaceted HPI, it can result in persisting infection, chronic inflammation, malignant transformation, and/or elevated mortality. Recent successes in overcoming immune-evasion of infected tumor cells with checkpoint inhibitors exemplifies the clinical gains that can be made by identifying and specifically targeting essential mechanisms of HPIs. Hence, precisely identifying new mode(s) of HPIs is critical for development of effective and personalized interventions. The molecular mechanisms of HPIs underpinning disease can be identified from genomics data. For example, information on whether a transcription factor (TF) regulates genes from either host or pathogen, or both, can be captured by chromatin immunoprecipitation (ChIP) sequencing of infected host cells. This means that integrative analysis of genome-scale data can provide a platform for large-scale and unbiased detection of often multi- dimensional and novel facets of HPIs in host cells. However, there is a lack of data mining tools and models to extract such information. More importantly, the available analysis tools typically focus on data from either the host or the pathogen and not on the interactions occurring between the two, excluding us from investigating the full HPI spectrum. Thus, novel methods to determine HPIs by simultaneously modeling both host and pathogen data are critical for understanding key cellular mechanisms and developing treatment strategies. My lab specializes in developing computational models to construct HPI maps and to experimentally validate them. As proof-of-principle, we produced a comprehensive HPI map from sequencing samples from large numbers of tumors caused by Epstein–Barr virus. This map delivered unprecedented insights, identifying novel viral integrations, mutations linked to viral reactivation and providing molecular classification of tumors expected to yield individualized cancer therapy. Therefore, my lab is uniquely positioned to uncover mechanistic insights from HPIs. Our program seeks to develop new models and machine learning tools to construct HPI maps in several diseases by focusing on the following major questions: 1) how do expression, integration, and mutational landscapes of host and pathogen affect pathogenesis of disease?; 2) what is the nature of physical HPIs and cross-regulation by major host and pathogen factors that modulate gene expression, such as TFs and RNA binding proteins?; 3) how do HPIs define molecular subtypes to guide personalized treatments? We expect to identify novel HPIs and provide systems-level understanding of mechanisms critical to cell biology. Narrative Understanding how host cells and pathogens interact is key to developing new and individualized therapeutics. Here, we will develop novel computational tools and models to analyze existing and newly generated high throughput data and construct multi-dimensional host pathogen interaction maps. These maps will provide detailed mechanisms underpinning multi-faceted interactions occurring between host and pathogen and will delineate molecular subtypes that can be utilized for novel and personalized treatment options.",Dissecting host-pathogen interactions through the lens of genomics,10028454,R35GM138283,"['Affect', 'Cells', 'Cellular biology', 'ChIP-seq', 'Chronic', 'Clinical', 'Communicable Diseases', 'Computer Models', 'Data', 'Detection', 'Development', 'Disease', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Genomics', 'Human Herpesvirus 4', 'Immune Evasion', 'Immune checkpoint inhibitor', 'Immunity', 'Infection', 'Inflammation', 'Investigation', 'Link', 'Machine Learning', 'Malignant - descriptor', 'Maps', 'Methods', 'Modeling', 'Molecular', 'Molecular Classification of Tumors', 'Mutation', 'Nature', 'Outcome', 'Pathogenesis', 'Positioning Attribute', 'RNA-Binding Proteins', 'Regulation', 'Resistance', 'Sampling', 'System', 'Therapeutic', 'Transcriptional Regulation', 'Variant', 'Viral', 'Virus Integration', 'computerized tools', 'data mining', 'effective intervention', 'genome analysis', 'genome-wide', 'genomic data', 'insight', 'lens', 'molecular subtypes', 'mortality', 'neoplastic cell', 'novel', 'pathogen', 'personalized cancer therapy', 'personalized intervention', 'personalized medicine', 'programs', 'success', 'tool', 'tool development', 'transcription factor', 'treatment strategy', 'tumor']",NIGMS,PURDUE UNIVERSITY,R35,2020,381824,-0.015947257790626383
"Detection and characterization of critical under-immunized hotspots Detection and characterization of critical under-immunized hotspots  Emergence of undervaccinated geographical clusters for diseases like measles has become a national concern. A number of measles outbreaks have occurred in recent months, despite high MMR coverage in the United States ( 95%). Such undervaccinated clusters can act as reservoirs of infection that can transmit the disease to a wider population, magnifying their importance far beyond what their absolute numbers might indicate. The existence and growth of such undervaccinated clusters is often known to public health agencies and health provider networks, but they typically do not have enough resources to target people in each such cluster, to attempt to improve the vaccination rate. Preliminary results show that not all undervaccinated clusters are “equal” in terms of their potential for causing a big outbreak (referred to as its “criticality”), and the rate of undervaccination in a cluster does not necessarily correlate with its criticality.  However, there are no existing methods to estimate the potential risk of such clusters, and to identify the most “critical” ones. Some of the key reasons are: (i) purely data-driven spatial statistics methods rely only on immunization coverage, which does not give any indication of the risk of an outbreak; and (ii) current causal epidemic models need to be combined with detailed incidence data, which has not been easily available.  This proposal brings together a systems science approach, combining agent-based stochastic epidemic models, and techniques from machine learning, high performance computing, data mining, and spatial statistics, along with novel public and private datasets on immunization and incidence, to develop a novel methodology for identifying critical clusters, through the following tasks: (i) Identify spatial clusters with signiﬁcantly low immunization rates, or strong anti-vaccine sentiment; (ii) Develop an agent based model for the spread of measles that incorporates detailed immunization data, and is calibrated using a novel source of incidence data; (iii) Develop methods to ﬁnd and characterize critical spatial clusters, with respect to different metrics, which capture both epidemic and economic burden, and order underimmunized clusters based on their criticality; and (iv) Use the methodology to evaluate interventions in terms of their effect on criticality. A highly interdisciplinary team involving two universities, a health care delivery organization and a state department of Health, will work together to develop this methodology. Characterization of such clusters will enable public health departments and policy makers in targeted surveillance of their regions and a more efﬁcient allocation of resources. Project Narrative  This project will develop a new methodology to quantify the potential risks of under-vaccinated spatial clusters for highly infectious diseases. It will rank the clusters based on their economic and epidemic burden which will enable public health ofﬁcials in targeted surveillance and interventions, to mitigate their risk.",Detection and characterization of critical under-immunized hotspots,9887876,R01GM109718,"['Affect', 'Bayesian Method', 'Behavioral Model', 'California', 'Characteristics', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computing Methodologies', 'Country', 'Data', 'Data Set', 'Detection', 'Disease', 'Disease Clusterings', 'Disease Outbreaks', 'Disease model', 'Economic Burden', 'Economics', 'Epidemic', 'Epidemiology', 'Exhibits', 'Funding', 'Geography', 'Growth', 'Health', 'Health Personnel', 'Herd Immunity', 'High Performance Computing', 'Immunization', 'Immunize', 'Incidence', 'Individual', 'Infection', 'Intervention', 'Machine Learning', 'Measles', 'Measles-Mumps-Rubella Vaccine', 'Medical', 'Methodology', 'Methods', 'Minnesota', 'Modeling', 'New Jersey', 'New York', 'Oregon', 'Outcome', 'Pathway interactions', 'Policies', 'Policy Maker', 'Population', 'Population Analysis', 'Privatization', 'Public Health', 'Records', 'Registries', 'Resolution', 'Resource Allocation', 'Resources', 'Risk', 'Scanning', 'Schools', 'Science', 'Source', 'System', 'Systems Analysis', 'Techniques', 'Time', 'Uncertainty', 'United States', 'Universities', 'Vaccinated', 'Vaccination', 'Vaccines', 'Washington', 'Work', 'base', 'data mining', 'demographics', 'diverse data', 'economic cost', 'economic outcome', 'health care delivery', 'health disparity', 'health organization', 'improved', 'interest', 'novel', 'novel strategies', 'population based', 'provider networks', 'public health intervention', 'social', 'social media', 'spatiotemporal', 'statistics', 'tool', 'transmission process']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,324178,-0.014227452126244515
"Combining chemical and computational tools for predictive models of microbiome communities ABSTRACT The gut microbiome has a tremendous impact on health and disease, actively contributing to obesity, diabetes, inflammatory bowel disease, cardiovascular diseases, and several poorly understood neurological disorders. We do not yet have the necessary tools to precisely probe these microbial communities, though such tools could unlock extensive benefits to human health. Elucidating the contributions of individual species or consortia of bacteria would provide a rational basis for understanding microbiota-controlled disease and lead to novel therapies. To carry out the fundamental research planned in this proposal, we will tackle three major problems: First, we will build the first set of molecular tools that effectively and precisely modulate the microbiome bacteria; second, we will analyze the multiscale dynamics of microbial communities; and third, we will construct an ingestible biosensor for real-time monitoring of microbiome populations. Although antibiotics and fecal transplants can reconfigure microbial consortia, they do not precisely target individual bacteria. Conversely, antimicrobial peptides (AMPs) have evolved to selectively attack pathogenic bacteria but do not target microbiome bacteria, constituting desirable scaffolds for molecular engineering and potential sources of microbiome-targeting agents. We will develop a new computational peptide design methodology, based on classical and hybrid-quantum mechanical molecular dynamics (MD) simulations, to create a groundbreaking assessment of the dynamical and emergent properties of AMPs. Chemical synthesis and large-scale screening will confirm predicted selectivity against microbiome species, and a machine learning workflow will connect sequences of individual peptides to their dynamics and activity. We will then apply the synthetic AMPs to interrogate the human microbiome by selectively removing species during bacterial consortia experiments, to be carried out in bioreactors, under regular or anaerobic conditions. We will pair our experiments with whole-cell metabolic network models, providing a systems biology perspective to the analysis of inter-species interactions. An integrated ingestible biosensing device will be developed to monitor the microbiome by electrochemically sensing unique biomarkers from gut microbes. This will provide the first real-time measurements of microbiome composition and will be integrated to our bioreactors for testing, to ultimately be used for in vivo tests. This work will build the first set of molecular and computational tools for microbiome engineering and will lay the foundation to address critical gaps in our understanding of the gut micro-environment, and of the contributions of gut bacteria to the etiology of disease. Grounded in our demonstrated expertise in synthetic biology, computer science, microbiology, and electrical engineering, this project will provide a computational- experimental framework for developing a peptide encyclopedia for the gut microbiome, in line with NIH's public health mission and goals. PROJECT NARRATIVE  The gut microbiome plays roles in nutrition, immunity, metabolism, and several poorly understood neurological disorders. Suitable tools, however, do not yet exist for engineering the microbial communities that constitute the human microbiome. The proposed research introduces the first molecular tools to precisely understand the functions of microbiome communities in our health and disease in order to then delineate therapeutic interventions for diseases mediated by the gut microbiota, thereby addressing NIH's public health mission.",Combining chemical and computational tools for predictive models of microbiome communities,10029354,R35GM138201,"['Address', 'Anaerobic Bacteria', 'Antibiotics', 'Bacteria', 'Biochemical Pathway', 'Biological Markers', 'Bioreactors', 'Biosensing Techniques', 'Biosensor', 'Cardiovascular Diseases', 'Cells', 'Chemicals', 'Communities', 'Devices', 'Diabetes Mellitus', 'Disease', 'Electrical Engineering', 'Encyclopedias', 'Engineering', 'Etiology', 'Foundations', 'Goals', 'Health', 'Human', 'Human Microbiome', 'Hybrids', 'Immunity', 'Individual', 'Inflammatory Bowel Diseases', 'Lead', 'Machine Learning', 'Mechanics', 'Mediating', 'Metabolism', 'Methodology', 'Microbiology', 'Mission', 'Molecular', 'Molecular Computations', 'Monitor', 'Obesity', 'Peptides', 'Play', 'Population', 'Property', 'Public Health', 'Research', 'Role', 'Source', 'Systems Biology', 'Testing', 'Therapeutic Intervention', 'United States National Institutes of Health', 'Work', 'antimicrobial peptide', 'base', 'chemical synthesis', 'computer science', 'computerized tools', 'design', 'experimental study', 'fecal transplantation', 'fundamental research', 'gut bacteria', 'gut microbes', 'gut microbiome', 'gut microbiota', 'in vivo evaluation', 'microbial community', 'microbiome', 'microbiome composition', 'microbiota', 'molecular dynamics', 'nervous system disorder', 'network models', 'novel therapeutics', 'nutrition', 'pathogenic bacteria', 'predictive modeling', 'quantum', 'real time monitoring', 'scaffold', 'screening', 'synthetic biology', 'targeted agent', 'temporal measurement', 'tool']",NIGMS,UNIVERSITY OF PENNSYLVANIA,R35,2020,342713,0.012354874975251938
"Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases 7. Project Summary/Abstract There is an urgent need to support research that generates high-quality evidence to inform clinical decision making. Cluster randomized trials (CRTs) achieve the highest standard of evidence for the evaluation of community-level effectiveness of intervention strategies against infectious diseases. However, there is a need to develop new methods to improve the design and analysis of CRTs because unique and complicated analytical challenges arise in such settings. One such issue relates to the intraclass correlation coefficient (ICC), the degree to which individuals within a community are more similar to one another than to individuals in other communities. Design and analysis of CRTs must take into account the ICC. Lack of accurate information on the ICC jeopardizes the power of CRTs, leads to suboptimal choices of analysis methods and complicates the interpretation of study results. However, reliable information on the ICC is difficult to obtain. A robust and efficient approach for estimating ICCs is based on the second-order generalizing estimating equations. However, its use has been limited by considerable computational burden and poor convergence rates associated with the existing algorithms solving these equations. The first aim addresses these computational challenges. Missing data are ubiquitous and can lead to bias and loss of efficiency. The second aim proposes to develop novel robust and efficient methods for estimating ICCs in the presence of informative missing data. For infectious diseases, the underlying contact/transmission networks give rise to complicated correlation structure. The third aim is to develop network and epidemic models to project the ICC. User-friendly software will be developed to facilitate the implementation of new methods. An immediate application of the proposed methods is their application to the Botswana Combination Prevention Project to improve the estimation of intervention effect and to generate reliable ICC estimates for designing future CRTs in the same population. The proposed methods can be applied to other ongoing and future CRTs, and more broadly, to longitudinal studies and agreement studies where ICCs are also of great interest. The proposed research is significant, because success in addressing these issues will improve the ability to design efficient and well-powered CRTs and the precision in estimating the effects of intervention strategies. Innovation lies in the development of improved computing algorithms adapting approaches from deep learning, the use of semiparametric efficiency theory, and the integration of network modeling, epidemic modeling and statistical inference. The results of the proposed research will benefit both ongoing and future CRTs, permit more efficient use of the resources, and ultimately expedite the control of infectious diseases. 8. Project Narrative The proposed research is relevant to public health because improved methodologies for the design and analysis of cluster randomized trials will benefit both ongoing and future studies, permit more efficient use of the resources, and ultimately improve public health response intended to control the spread of infectious diseases. Thus, the proposed research is relevant to the part of NIAID’s mission that pertains to conducting and supporting research to prevent infectious diseases and to respond to emerging public health threats.",Network modeling and robust estimation of the intraclass correlation coefficient to inform the design and analysis of cluster randomized trials for infectious diseases,10011756,R01AI136947,"['AIDS prevention', 'Accounting', 'Address', 'Affect', 'Agreement', 'Algorithms', 'Americas', 'Area', 'Attention', 'Behavior Therapy', 'Botswana', 'Characteristics', 'Clinical', 'Clinical Trials', 'Cluster Analysis', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Complex', 'Contracts', 'Data', 'Dependence', 'Development', 'Disease', 'Disease Outbreaks', 'Ebola', 'Effectiveness of Interventions', 'Epidemic', 'Equation', 'Evaluation', 'Future', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Institute of Medicine (U.S.)', 'Intervention', 'Intervention Studies', 'Knowledge', 'Lead', 'Longitudinal Studies', 'Measures', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Institute of Allergy and Infectious Disease', 'Nosocomial Infections', 'Population', 'Prevention', 'Prevention strategy', 'Probability', 'Public Health', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Support', 'Resources', 'Role', 'Running', 'Science', 'Societies', 'Structure', 'System', 'United States National Institutes of Health', 'Work', 'adverse outcome', 'base', 'clinical decision-making', 'collaboratory', 'deep learning', 'design', 'effectiveness evaluation', 'experience', 'high standard', 'improved', 'innovation', 'insight', 'interest', 'intervention effect', 'mathematical model', 'network models', 'novel', 'prevent', 'response', 'semiparametric', 'success', 'systems research', 'theories', 'transmission process', 'user friendly software']",NIAID,"HARVARD PILGRIM HEALTH CARE, INC.",R01,2020,247413,0.0066395078112832595
"ShapeWorks in the Cloud Project Summary This application is submitted in response to NOT-OD-20-073 as an administrative supplement to the parent award R01AR076120 titled: ""Anatomy Directly from Imagery: General-purpose, Scalable, and Open-source Machine Learning Approaches."" The form (or shape) of anatomies is the clinical language that describes abnormal mor- phologies tied to pathologic functions. Quantifying such subtle morphological shape changes requires parsing the anatomy into a quantitative description that is consistent across the population in question. For more than 100 years, morphometrics has been an indispensable quantitative tool in medical and biological sciences to study anatomical forms. But its representation capacity is limited to linear distances, angles, and areas. Sta- tistical shape modeling (SSM) is the computational extension of classical morphometric techniques to analyze more detailed representations of complex anatomy and their variability within populations The parent award ad- dresses existing roadblocks for the widespread adoption of SSM computational tools in the context of a ﬂexible and general SSM approach termed particle-based shape modeling (PSM) and its associated suite of open-source software tools, ShapeWorks. ShapeWorks enables learning population-level shape representation via automatic dense placement of homologous landmarks on image segmentations of general anatomy with arbitrary topology. The utility of ShapeWorks has been demonstrated in a range of biomedical applications. ShapeWorks has the potential to transform the way researchers approach studies of anatomical forms, but its widespread applicability and impact to medicine and biology are hindered by computational barriers that most existing shape modeling packages face. The goal of this supplement award is to provide supplemental support for Aim 3 of the parent award to leverage best practices in software development and advances in cloud computing to enable researchers with limited computational resources and/or large-scale cohorts to build and execute custom SSM workﬂows us- ing remote scalable computational resources. To achieve this goal, we have developed a plan to enhance the design, implementation, and cloud-readiness of ShapeWorks and augmented our scientiﬁc team to add senior, experienced software engineers/developers who have extensive experience in professional programming, code refactoring, and scientiﬁc computing. This award will provide our team with the support necessary to (Aim 1) de- sign ShapeWorks as a collection of modular and reusable services, (Aim 2) decouple ShapeWorks services from explicitly encoded data sources, and (Aim 3) refactor ShapeWorks to scale efﬁciently on the cloud. All software development will be performed in adherence to software engineering practices and design principles, including coding style, documentation, and version control. The proposed efforts will be released as open-source software in a manner consistent with the principles of reproducible research and the practices of open science. Our long- term goal is to make ShapeWorks a standard tool for shape analyses in medicine, and the work proposed herein in addition to the parent award will establish the groundwork for achieving this goal. Project Narrative ShapeWorks is a free, open-source software tool that uses a ﬂexible method for automated construction of sta- tistical landmark-based shape models of ensembles of anatomical shapes. The impact and scientiﬁc value of ShapeWorks have been recognized in a range of applications, including psychology, biological phenotyping, car- diology, and orthopedics. If funded, this supplement will provide support to revise, refactor, and redeploy Shape- Works to take advantage of new cloud computing paradigms, to be robust, sustainable, scalable, and accessible to a broader community, and to address the growing need for shape modeling tools to handle large collections of clinical data and to obtain sufﬁcient statistical power for large shape studies.",ShapeWorks in the Cloud,10166337,R01AR076120,"['Address', 'Adherence', 'Administrative Supplement', 'Adoption', 'Anatomy', 'Applied Research', 'Architecture', 'Area', 'Award', 'Biological', 'Biological Sciences', 'Biology', 'Cardiology', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Cloud Computing', 'Cloud Service', 'Code', 'Collection', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computer Models', 'Computer software', 'Computers', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Databases', 'Disabled Persons', 'Documentation', 'Environment', 'Face', 'Funding', 'Goals', 'Image', 'Imagery', 'Language', 'Learning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methods', 'Modeling', 'Morphology', 'Occupations', 'Online Systems', 'Orthopedics', 'Parents', 'Pathologic', 'Phenotype', 'Population', 'Privatization', 'Psychology', 'Readiness', 'Reproducibility', 'Research', 'Research Personnel', 'Running', 'Scientist', 'Services', 'Shapes', 'Software Design', 'Software Engineering', 'Software Tools', 'Source Code', 'Speed', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'base', 'cohort', 'computational platform', 'computerized tools', 'computing resources', 'data management', 'design', 'experience', 'flexibility', 'imaging Segmentation', 'improved', 'innovation', 'large datasets', 'model development', 'open data', 'open source', 'particle', 'response', 'scientific computing', 'shape analysis', 'software development', 'statistics', 'tool', 'user-friendly']",NIAMS,UNIVERSITY OF UTAH,R01,2020,210000,-0.007086536293528787
"Nathan Shock Center for Excellence in Basic Biology of Aging OVERALL—PROJECT SUMMARY Healthspan is a complex trait, influenced by many interacting polymorphic alleles and environmental factors that may accelerate or delay aging, reduce or increase disease risk, and/or promote extended lifespan. Thus, assessing the role of genetic variation in aging requires an experimental strategy capable of modeling the genetic and biological complexity of human populations while allowing for efficient identification and validation of candidate genes. With this proposal, the JAX NSC seeks support to further develop and disseminate the next generation of genetic, phenotyping, and information resources necessary to enable a systems-wide approach to understanding healthy aging. Over the past 15 years, The JAX NSC has transformed aging research both at JAX and across the geroscience community, providing central resources to support investigators that have resulted in 26 peer-reviewed publications in the last funding period. The Center has developed nascent regional and national resources for aging research, including aging mouse resources and tissues that support our numerous collaborations and external researchers. All JAX NSC data and tools are publicly disseminated on the Mouse Phenome Database and the JAX NSC website, thus ensuring that the resources generated and expertise acquired through the Center is readily available to the aging research community. In this renewal, we will advance towards our goal by providing unique resources, tools, and support to geroscience investigators while leveraging JAX's unparalleled expertise in the large-scale identification and functional validation of complex polygenic traits in mice. We will do this by providing effective Center administration and enhancing the utility of JAX NSC resources throughout the aging community (Aim 1); expanding the research focus on aging, healthspan and age-related diseases through a robust Research Development Core (Aim 2); increasing the diversity of mouse resources available for aging research, including a new study to, for the first time, investigate the effect of genetic variation on cellular senescence and treatment with senolytic drugs (Aim 3); strengthening the data and computational and support available to the aging community (Aim 4); expanding the use of machine learning technologies in interpretation of aging pathologies (Aim 5). The Center will be led by a highly experienced team of Principal Investigators and Core Leaders who, with oversight from an External Advisory Board, will provide effective management to facilitate the goals and objectives of the Center. The Center will leverage unparalleled institutional resources, facilities and expertise of The Jackson Laboratory, a globally renowned institution for mouse genetics research, to enhance its goals and the utility of the resources it generates for the aging research community. OVERALL—PUBLIC HEALTH RELEVANCE Human aging is influenced by genetic factors, whereby differences in longevity as well as changes in health and disease risk with time are linked to variation in individuals' genetic codes. The Jackson Laboratory Nathan Shock Center will develop resources to encourage the use of a wider range of mouse models in aging research. Resources—including aged mouse models that mirror human genetic variation, metabolic and microbiome data, and methods to reveal genetic factors tied to human aging—will be available to the scientific community, accelerating research to understand and ultimately prolong healthy human aging.",Nathan Shock Center for Excellence in Basic Biology of Aging,10045024,P30AG038070,"['Advisory Committees', 'Aging', 'Alleles', 'Animals', 'Biological', 'Biology of Aging', 'Candidate Disease Gene', 'Cell Aging', 'Collaborations', 'Communities', 'Complex', 'Computer Assisted', 'Data', 'Data Analyses', 'Databases', 'Disease', 'Educational workshop', 'Ensure', 'Environmental Risk Factor', 'Funding', 'Genes', 'Genetic', 'Genetic Code', 'Genetic Models', 'Genetic Research', 'Genetic Variation', 'Geroscience', 'Goals', 'Health', 'Heart', 'Histologic', 'Human', 'Human Genetics', 'Image Analysis', 'Inbred Strain', 'Individual', 'Information Resources', 'Institution', 'Joints', 'Laboratories', 'Leadership', 'Link', 'Liver', 'Longevity', 'Lung', 'Machine Learning', 'Maps', 'Mentorship', 'Metabolic', 'Methods', 'Mus', 'Pathology', 'Peer Review', 'Pharmaceutical Preparations', 'Phenotype', 'Physiological', 'Pilot Projects', 'Polygenic Traits', 'Population', 'Principal Investigator', 'Process', 'Protocols documentation', 'Publications', 'Quality Control', 'Research', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Shock', 'Statistical Methods', 'Structure', 'System', 'Technology', 'The Jackson Laboratory', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Validation', 'Variant', 'Visit', 'age related', 'aged', 'animal tissue', 'behavioral phenotyping', 'candidate validation', 'career development', 'data dissemination', 'data management', 'data tools', 'disorder risk', 'experience', 'healthspan', 'healthy aging', 'insight', 'microbiome', 'mouse genetics', 'mouse model', 'next generation', 'novel', 'open source', 'phenome', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'senescence', 'symposium', 'tool', 'trait', 'user-friendly', 'web site']",NIA,JACKSON LABORATORY,P30,2020,1069526,-0.029305414213624575
"Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine A fundamental challenge in precision medicine is to understand the patterns of differentiation between individuals. To address this challenge, we propose to go beyond the traditional `one disease--one model' view of bioinformatics and pursue a new view built upon personalized patient models that facilitates precision medicine by leveraging both commonalities within a patient cohort as well as signatures unique to every individual patient. With the emergence of large-scale databases such as The Cancer Genome Atlas (TCGA), the International Cancer Genome Consortium (ICGC), and the Gene Expression Omnibus (GEO), which collect multi-omic data on many different diseases, a new “pan-omics” and “pan-disease” paradigm has emerged to jointly analyze all patients in a disease cohort while accounting for patient-specific effects. An example of this is the recently released Pan-Cancer Atlas. At the same time, next generation statistical tools to accurately and rigorously draw the necessary inferences are lacking. In this project we propose a series of mathematically rigorous, statistically sound, and computationally feasible approaches to infer sample-specific models, providing a more complete view of heterogeneous datasets. By bringing together ideas from the machine learning, statistics, and mathematical optimization communities, we provide a rigorous framework for precision medicine via sample-specific statistical models. Crucially, we propose to analyze this framework and prove strong theoretical guarantees under weak assumptions--this dramatically distinguishes our framework from much of the existing literature. Towards these goals, we propose the following aims: Aim 1: Discovery of new molecular profiles with sample-specific statistical models. We propose a general framework for inferring sample-specific models with low-rank structure based on the novel concept of distance-matching. This allows us to infer statistical models at the level of a single patient without overfitting, and is general enough to be applied for prediction, classification, and network inference as well as a variety of diseases and phenotypes. Aim 2: Multimodal approaches to personalized diagnosis--contextually interpretable models for actionable clinical decision support. In order to translate these models into practice, we propose a novel interpretable predictive model that supports complex, multimodal data types such as images and text combined with high-level interpretable features such as SNP data, gender, age, etc. This framework simultaneously boosts the accuracy of clinical predictions by exploiting sample heterogeneity while providing human-digestable explanations for the predictions being made. Aim 3: Next-generation precision medicine--algorithms and software for personalized estimation. To put our models into practical use, we will develop new algorithms for interpretable prediction of personalized clinical outcomes and visualization of personalized statistical models. All of our tools will be combined into a user-friendly software package called PrecisionX that will be freely available to researchers and clinicians everywhere. RELEVANCE (See instructions): Personalization with data is a critical challenge whenever decisions must be made at scale, and has applications that go beyond precision medicine; businesses, educational institutions, and financial institutions are among the many players that have acknowledged a stake in this complex problem. We expect the proposed work to provide a rigorous foundation for personalization with large and high-dimensional datasets, finding use throughout the broader scientific community as well as with industry and educational institutions. Alongside our collaboration with Pitt/UPMC, we will work with physicians and data scientists for practical feedback as well as provide training in the methods developed. n/a",Sample-specific Models for Molecular Portraits of Diseases in Precision Medicine,10133782,R01GM140467,"['Accounting', 'Address', 'Age', 'Algorithmic Software', 'Algorithms', 'Atlases', 'Bioinformatics', 'Businesses', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Scientist', 'Data Set', 'Disease', 'Feedback', 'Foundations', 'Gender', 'Gene Expression', 'Goals', 'Heterogeneity', 'Human', 'Image', 'Individual', 'Industry', 'Institution', 'Instruction', 'International', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modeling', 'Molecular Profiling', 'Multiomic Data', 'Outcome', 'Patients', 'Pattern', 'Physicians', 'Portraits', 'Research Personnel', 'Sampling', 'Series', 'Statistical Models', 'Structure', 'Text', 'The Cancer Genome Atlas', 'Time', 'Training', 'Translating', 'Visualization', 'Work', 'base', 'cancer genome', 'clinical decision support', 'clinically actionable', 'cohort', 'disease phenotype', 'heterogenous data', 'high dimensionality', 'individual patient', 'large-scale database', 'molecular modeling', 'multimodal data', 'multimodality', 'next generation', 'novel', 'personalized diagnostics', 'personalized predictions', 'precision medicine', 'predictive modeling', 'sound', 'statistics', 'tool', 'user friendly software']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2020,305566,-0.013757694754724078
"Computational and Statistical Framework to Model Tissue Shape and Mechanics PROJECT SUMMARY  The morphologic and mechanical characteristics of a tissue are fundamental to understanding the development, homeostasis, and pathology of the human body. During the previous period of funding, we developed statistical shape modeling (SSM) methods and applied these to the study of structural hip disease. We also developed the initial framework to integrate SSM with finite element (FE) analysis to enable the study of shape and mechanics together. If incorporated into clinical practice, SSM and FE analysis could identify features of the anatomy likely responsible for injury, remodeling, or repair. Geometry needed for SSM and FE models is typically generated by segmentation of volumetric imaging data. This step can be painstakingly slow, error prone, and cost prohibitive, which hampers clinical application of these computational techniques. We have created a deep machine learning algorithm ‘DeepSSM’ that uses a convolutional neural network to establish the correspondence model directly from unsegmented images. In Aim 1 we will apply DepSSM to improve clinical understanding of structural hip disease by characterizing differences in anatomy between symptomatic and asymptomatic individuals; these morphometric comparisons will identify anatomic features most telling of disease, thereby guiding improvements in diagnosis. Computational advancements have simplified the process to generate patient-specific FE models, enabling clinically focused research. However, there is no framework to collectively visualize, compare, and interpret (i.e., post-process) results from multiple FE models. Currently, inter-subject comparisons require oversimplifications such as averaging results over subjectively defined regions. In Aim 2 we will develop new post-processing methods to collectively visualize, interpret and statistically analyze FE results across multiple subjects and study groups. We will map FE results to synthetic anatomies representing statistically meaningful distributions using the correspondence model. Statistical parametric mapping will be applied to preserve anatomic detail through statistical testing. We will use our published FE models of hip joint mechanics as the test system. Finally, volumetric images provide a wealth of information that is delivered to physicians in a familiar format. Yet, tools are not available to interpret model data with clinical findings from volumetric images. In Aim 3, we will develop methods that evaluate relationships between shape, mechanics, and clinical findings gleaned from imaging through integrated statistical tests and semi-automatic medical image annotation tools that utilize standard ontologies. Quantitative CT and MRI images of the hip, which estimate bone density and cartilage ultrastructure, respectively, will be evaluated as test datasets. To impart broad impact, we will disseminate our methods to the community as open source software that will call core functionality provided by existing, open source software that has a large user base (FEBio, ShapeWorks). PROJECT NARRATIVE The proposed technology will provide the methodologies necessary to increase the clinical acceptance and applicability of computer models. These models measure three-dimensional tissue shape and estimate tissue mechanics, providing information that cannot be measured conventionally. We will implement these methods into software that can be used by the public free-of-charge.",Computational and Statistical Framework to Model Tissue Shape and Mechanics,9972694,R01EB016701,"['3-Dimensional', 'Adoption', 'Algorithms', 'Anatomy', 'Architecture', 'Bone Density', 'Cardiology', 'Cartilage', 'Characteristics', 'Charge', 'Clinical', 'Communities', 'Computational Technique', 'Computer Models', 'Computer software', 'Computing Methodologies', 'Data', 'Data Set', 'Deformity', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Finite Element Analysis', 'Foundations', 'Funding', 'Geometry', 'Glean', 'Grooming', 'Hip Joint', 'Hip region structure', 'Homeostasis', 'Human Pathology', 'Human body', 'Image', 'Individual', 'Injury', 'Intuition', 'Libraries', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Measures', 'Mechanics', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Morphology', 'Neurology', 'Ontology', 'Orthopedics', 'Pathology', 'Patient imaging', 'Patients', 'Performance', 'Physicians', 'Procedures', 'Process', 'Publishing', 'Quantitative Evaluations', 'Research', 'Resources', 'Scheme', 'Shapes', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Tissue Model', 'Tissues', 'Training', 'Validation', 'X-Ray Computed Tomography', 'annotation  system', 'base', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data modeling', 'disease diagnosis', 'improved', 'in vivo', 'machine learning algorithm', 'novel', 'open source', 'predictive modeling', 'preservation', 'relating to nervous system', 'repaired', 'shape analysis', 'simulation', 'three-dimensional modeling', 'tool']",NIBIB,UNIVERSITY OF UTAH,R01,2020,563658,-0.01575338606289402
"Pan-vaccine Analysis to Test the Impact of Cytomegalovirus on Vaccine Efficacy PROJECT SUMMARY/ABSTRACT Cytomegalovirus (CMV) infects around 50% of the US population. Even though the CMV exists in a latent state in healthy individuals, it profoundly shapes the immune system. Recent studies suggest that the CMV infection alters the immune response to influenza vaccine. However, the exact effect of CMV on the efficacy of the influenza vaccine remains controversial. In addition, how CMV shapes the immune responses toward other vaccines are unknown. We hypothesize that latent CMV infection induces critical changes in the immune system, which alters the efficacy of multiple types of vaccines. The ImmPort database currently hosts 133 vaccine studies, covering 21 types of vaccines, creating an unprecedented opportunity for us to test our hypothesis. We will perform a comprehensive meta-analysis to test the relationship between CMV and vaccine efficacies, and will use state-of-art statistical models (e.g., Dynamic Bayesian Network) to identify the mechanism by which CMV alters the vaccine response. Leveraging the group's expertise in computational immunology and rich datasets on ImmPort, we will address the following aims. Aim1: Test the effect of CMV on influenza vaccine outcome. We will perform a meta-analysis of 60 influenza studies available on ImmPort to test the impact of CMV. We will quantify and standardize the efficacy of influenza vaccine across studies, which are measured by hemagglutinin inhibition (HAI) assays before and after the vaccination. We will also determine the CMV infection status in subjects, either directly from serological tests or indirectly from immune- phenotyping data using cutting-edge machine learning tools. We will then test if CMV increases the response to influenza vaccine by analyzing data from all studies in a unified statistical framework while taking the heterogeneity between studies into account. Aim2: Bayesian network analysis of influenza vaccine response. We will harmonize multimodal immune-phenotyping data from the influenza vaccine studies, including transcriptomics data, cytometry data, and cytokine measurements. We will use state-of-art network analysis methods (e.g., Dynamic Bayesian network) to model the interplay between the immune components over time. Using the Bayesian network, we will investigate the mechanism by which CMV shapes the outcome of influenza vaccination. Aim3: Explore the effect of CMV infection on other vaccines. We will extend our analysis to vaccines other than influenza vaccine, (e.g., West Nile, Hepatitis B, yellow fever, malaria, and Tuberculosis). We will quantify the vaccine efficacy using assays specific to the vaccine type, such as Controlled Human Malaria Infection (CHMI) for the malaria vaccine and Plaque Reduction Neutralization Test for the yellow fever vaccine. We will perform separate network analyses to characterize the relationship between CMV and the immune response of individual vaccines. We will then perform joint analysis across vaccine types to identify the common impact of CMV across vaccine types. PROJECT NARRATIVE Infectious diseases remain an urgent problem, resulting in an estimated 3 million deaths worldwide and more than 110,000 deaths in the United States annually, but the efficacy of vaccines against many infectious diseases remains suboptimal, including malaria, influenza, and dengue. To improve the vaccines, it is crucial to understand the factors that affect the immune response toward vaccines. In this study, we investigate how cytomegalovirus affects the efficacy of multiple vaccines, providing valuable information for improving the design of vaccines.",Pan-vaccine Analysis to Test the Impact of Cytomegalovirus on Vaccine Efficacy,10026284,UH2AI153016,"['Address', 'Affect', 'Bayesian Analysis', 'Bayesian Modeling', 'Bayesian Network', 'Biological Assay', 'Cells', 'Cessation of life', 'Communicable Diseases', 'Communities', 'Computer Models', 'Cytomegalovirus', 'Cytomegalovirus Infections', 'Cytomegalovirus Vaccines', 'Cytometry', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dengue', 'Diagnosis', 'Disease', 'Goals', 'Hemagglutinin', 'Hepatitis', 'Hepatitis B', 'Herpesviridae', 'Heterogeneity', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunology', 'Individual', 'Influenza', 'Influenza vaccination', 'Joints', 'Knowledge', 'Machine Learning', 'Malaria', 'Malaria Vaccines', 'Measurement', 'Measures', 'Meta-Analysis', 'Methods', 'Modeling', 'Neutralization Tests', 'Outcome', 'Outcome Measure', 'Pathway Analysis', 'Population', 'Research', 'Research Personnel', 'Resources', 'Serologic tests', 'Shapes', 'Standardization', 'Statistical Models', 'Testing', 'Time', 'Tuberculosis', 'United States', 'Vaccination', 'Vaccine Design', 'Vaccines', 'Validation', 'Virus', 'Virus Latency', 'West Nile virus', 'Yellow Fever', 'Yellow Fever Vaccine', 'cytokine', 'design', 'improved', 'individual response', 'influenza virus vaccine', 'malaria infection', 'multimodality', 'network models', 'pathogen', 'phenotypic data', 'response', 'tool', 'transcriptomics', 'vaccine efficacy', 'vaccine response', 'vaccine trial']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",UH2,2020,242126,-0.0681983877172339
"Effects of Host Metabolic Variation on Antibiotic Susceptibility ﻿    DESCRIPTION (provided by applicant): Host variation affects the pathogenicity of, susceptibility to, and recovery from infectious diseases. Elucidating how the host environment alters antibiotic susceptibility is therefore a critical step towards the long-term goal of realizig precision medicine for the clinical management of infectious diseases. The overall objective of this project is to identify the metabolic pathways participating in antibiotic susceptibility and t determine how host metabolism may affect antibiotic treatments at the site of infection. The working hypothesis is that metabolic processes can function as bacterial control mechanisms for antibiotic susceptibility and that the host metabolome can act on these processes to affect the outcome of antibiotic treatment. This hypothesis will be tested in three specific aims: (1) identif metabolic pathways involved in antibiotic susceptibility (K99 phase); (2) characterize changes in the host metabolome elicited by antibiotic administration (K99 phase); (3) evaluate effects of the host metabolome on antibiotic killing (R00 phase). During the mentored K99 phase, bactericidal antibiotics will be counterscreened with various metabolites to identify metabolic perturbations that can affect antibiotic susceptibility. Metabolic pathways contributing to antibiotic lethality ill be identified by combining this data with metabolic modeling and machine learning. Additionally, plasma and peritoneal fluid will be sampled and metabolomically profiled from a mouse peritoneal infection model, with and without antibiotic treatment. These profiles will be used to determine if antibiotics can alter the host metabolism in ways that may affect antibiotic susceptibility at the site of infection. During the independent R00 phase, the effects of host metabolic variation on antibiotic killing will be systematically tested by quantifying antibiotic susceptibility in synthesized media defined by metabolomic profiles from published and measured human and mouse plasma samples. A better understanding of how the host metabolic environment participates in antibiotic treatment fits NIH's public health mission and has direct implications for the clinical management of infectious diseases. Work from the proposed studies will form a quantitative framework for directly evaluating how host metabolism may affect antibiotic treatment outcomes and guide improved antibiotic stewardship in clinical practice. Although the applicant has significant expertise in systems biology, this award will provide the applicant research training to gain new experimental skills and an opportunity for continued career training and mentorship from an advisory committee comprised of international leaders in systems biology, metabolomics, chemical biology and infectious diseases. The support and training provided by this award and by the advisory committee will provide the applicant tools and expertise critical to his future independent research program. PUBLIC HEALTH RELEVANCE (provided by applicant): Host variation alters the clinical response to antibiotic treatment for infectious disease, but the pathways underlying differences in patient outcome have not yet all been elucidated. The proposed research is relevant to public health because it is the first systematic investigation of how host metabolites may act on bacterial pathogen metabolism and alter antibiotic susceptibility. This work is relevant to NIH's public health mission by providing a quantitative framework for establishing precision medicine for the treatment of infectious diseases.",Effects of Host Metabolic Variation on Antibiotic Susceptibility,10088499,R00GM118907,"['Adjuvant', 'Advisory Committees', 'Affect', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotic susceptibility', 'Antibiotics', 'Award', 'Bacteria', 'Biochemical Pathway', 'Biology', 'C57BL/6 Mouse', 'Carbon', 'Cells', 'Cessation of life', 'Chemicals', 'Clinical', 'Clinical Management', 'Clinical Treatment', 'Communicable Diseases', 'Culture Media', 'Data', 'Development', 'Dimensions', 'Disease Resistance', 'Environment', 'Escherichia coli', 'Formulation', 'Future', 'Germ-Free', 'Goals', 'Growth', 'Human', 'Immunity', 'In Vitro', 'Infection', 'Innovative Therapy', 'International', 'Investigation', 'Knock-out', 'Knowledge', 'Light', 'Liquid substance', 'Machine Learning', 'Measures', 'Mentors', 'Mentorship', 'Metabolic', 'Metabolic Pathway', 'Metabolism', 'Mission', 'Modeling', 'Mus', 'Nitrogen', 'Outcome', 'Pathogenicity', 'Pathway interactions', 'Patient-Focused Outcomes', 'Peritoneal', 'Peritoneal Fluid', 'Peritoneal lavage', 'Phase', 'Phosphorus', 'Plasma', 'Predisposition', 'Process', 'Public Health', 'Publishing', 'Recovery', 'Research', 'Research Training', 'Sampling', 'Serum', 'Site', 'Source', 'Sulfur', 'Supplementation', 'System', 'Systems Biology', 'Testing', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Variant', 'Virulence', 'Work', 'bacterial metabolism', 'bactericide', 'base', 'biological adaptation to stress', 'career', 'clinical practice', 'cohort', 'commensal microbes', 'counterscreen', 'desensitization', 'genome-wide', 'improved', 'in vivo', 'infectious disease treatment', 'insight', 'metabolome', 'metabolomics', 'microbial', 'novel therapeutics', 'overexpression', 'pathogen', 'pathogenic bacteria', 'precision medicine', 'programs', 'public health relevance', 'respiratory', 'response', 'skills', 'tool', 'treatment strategy']",NIGMS,RBHS-NEW JERSEY MEDICAL SCHOOL,R00,2020,249000,-0.00782387673091026
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,10019388,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2020,381282,-0.0034030235252328167
"Systems biology frameworks to unravel mechanisms driving complex disorders Project Summary/Abstract This application proposes a training program to integrate the PI, Dr. Varadan's previous research efforts in informatics and machine learning into investigations pertaining to the etiology and progression of Barrett's Esophagus, a gastrointestinal disorder of significant public health interest. Much of Dr. Varadan's previous research has involved developing intelligent algorithms and informatics approaches to decode the interconnections within complex biological systems, with only a basic understanding of the clinical needs and complexities involved in translational research. The proposed project would provide a broad and in-depth mentored experience focused on clinical and biological aspects of Barrett's Esophagus, as well as added knowledge in the use of preclinical model systems to investigate biological mechanisms. The overall goal is to expand the PI's experience and training in the design and conduct of translational studies focused on gastrointestinal (GI) diseases. This objective will be achieved through a combination of didactic and research activities conducted under an exceptional mentoring team of translational researchers at Case Western Reserve University, spanning achievements across clinical management of GI disorders, molecular genetics and inflammatory processes associated with diseases of the gut. Accordingly, this proposal leverages Dr. Varadan's computational background to address an urgent and unmet need within the biomedical research community to develop reliable analytic approaches that can quantify signaling network activities in individual biological samples by integrating multi-omics measurements. We recently conceived a systems biology computational framework, InFlo, which integrates molecular profiling data to decode the functional states of cellular/molecular processes underpinning complex human diseases. Barrett's esophagus is one such complex disease gaining increasing importance to public health, as it is the known precursor to the deadly cancer, esophageal adenocarcinoma. Given that the mechanisms underlying the etiology and pathogenesis of Barrett's Esophagus remain elusive, a major objective of this proposal is to employ the InFlo framework combined molecular profiles derived from primary tissue cohorts, in vitro and in vivo model systems to establish the molecular roadmap of BE pathogenesis and disease recurrence, thus elucidating unifying mechanisms underlying this disease. This systems biology approach would enable the development of evidence-based, diagnostic/prognostic biomarkers for Barrett's esophagus and inform preventive strategies within at-risk populations. Project Narrative This proposal details a novel systems biology approach to enable seamless integration of patient molecular data to decipher the mechanisms underlying complex human diseases. Using this novel integrative analytics approach, we propose to resolve the molecular basis for the development and recurrence of Barrett's Esophagus, a disease with significant public health importance, since it is a known precursor to a lethal esophageal cancer and the mechanisms underpinning this disease remain largely unknown. The findings from our proposed research will enable the development of new diagnostic and prognostic biomarkers and will also inform preventive strategies in high-risk patient populations.",Systems biology frameworks to unravel mechanisms driving complex disorders,9888378,K25DK115904,"['3-Dimensional', 'Ablation', 'Achievement', 'Address', 'Automobile Driving', 'Award', 'Barrett Esophagus', 'Biological', 'Biological Models', 'Biomedical Research', 'Candidate Disease Gene', 'Cell Culture Techniques', 'Clinical', 'Clinical Management', 'Columnar Epithelium', 'Communities', 'Competence', 'Complex', 'DNA Methylation', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease model', 'Electrical Engineering', 'Ephrins', 'Epithelial', 'Epithelium', 'Esophageal Adenocarcinoma', 'Esophageal Tissue', 'Esophagitis', 'Esophagus', 'Etiology', 'Event', 'Exhibits', 'Follow-Up Studies', 'Gastrointestinal Diseases', 'Gene Expression', 'Gland', 'Goals', 'Human', 'In Vitro', 'Individual', 'Inflammatory', 'Informatics', 'Injury', 'Interleukin-1 beta', 'Investigation', 'Knowledge', 'Lesion', 'Machine Learning', 'Malignant - descriptor', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Maps', 'Measurement', 'Mentors', 'Modeling', 'Molecular', 'Molecular Abnormality', 'Molecular Analysis', 'Molecular Genetics', 'Molecular Profiling', 'Mucous Membrane', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Phenotype', 'Populations at Risk', 'Pre-Clinical Model', 'Prevention strategy', 'Process', 'Prognostic Marker', 'Proliferating', 'Proteins', 'Public Health', 'Recurrence', 'Research', 'Research Activity', 'Risk', 'Risk Factors', 'Sampling', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Specificity', 'Squamous Epithelium', 'Stomach', 'System', 'Systems Analysis', 'Systems Biology', 'Techniques', 'Testing', 'Time', 'Tissue Sample', 'Tissues', 'Training', 'Training Programs', 'Transgenic Mice', 'Translational Research', 'Universities', 'Validation', 'base', 'candidate identification', 'candidate marker', 'career', 'cohort', 'complex biological systems', 'computer framework', 'design', 'diagnostic biomarker', 'evidence base', 'experience', 'genetic manipulation', 'genome-wide', 'high risk', 'human disease', 'in vivo Model', 'injury and repair', 'intelligent algorithm', 'interest', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel diagnostics', 'patient population', 'prevent', 'resistance mechanism', 'standard of care', 'stem cells', 'success', 'therapeutic target', 'transcriptome', 'transcriptomics', 'translational scientist', 'translational study']",NIDDK,CASE WESTERN RESERVE UNIVERSITY,K25,2020,171720,0.000807322072237718
"Combinatorial matrix-mimetic recombinant proteins as engineered nerve guidance conduits ABSTRACT Over 500,000 Americans suffer from peripheral nerve injury (PNI), and despite surgical interventions, most suffer permanent loss of motor function and sensation. Current clinical options for long nerve gap PNI include naturally- derived grafts, which provide native matrix cues to regenerate neurons but suffer from very limited supply and batch-to-batch variability, or synthetic nerve guidance conduits (NGCs), which are easy to manufacture but often fail due to lack of regenerative cues. The main challenge with using any NGC for treatment of PNI is the immense trade-off between providing the complex matrix cues necessary for optimal nerve regeneration while providing a conduit that is readily available, reproducible, and easily fabricated. To overcome this challenge, we propose an entirely new type of biomaterial: a computationally optimized, protein-engineered recombinant NGC (rNGC). This rNGC combines the reliability of synthetic NGCs with the presentation of multiple regenerative matrix cues of natural NGCs. Because current understanding of cell-matrix interactions is insufficient to enable to direct design of a fully functional rNGC, we hypothesize that the use of machine learning, computational optimization methods will allow identification of an rNGC that promotes nerve regeneration similar to the current gold standard autograft. We utilize a family of protein-engineered, elastin-like proteins (ELPs) that are reproducible, with predictable, consistent material properties, and fully chemically defined for streamlined FDA approval. Due to ELPs’ modular design, they have biomechanical (i.e. matrix stiffness) and biochemical (i.e. cell-adhesive ligand) properties that are independently tunable over a broad range. While numerous studies detail the effects of individual biomechanical or biochemical matrix cues on neurite outgrowth using single-variable approaches, their combinatorial effects have been largely unexplored as insufficient knowledge exists to make accurate predictions of their interactions a priori. This fundamentally prohibits the direct design of combinatorial matrix cues. We hypothesize that optimized presentation of biomechanical and biochemical cues will create a microenvironment that better mimics the native ECM milieu, resulting in synergistic ligand cross-talk to improve nerve regeneration. In Aim 1, we use computational optimization methods to identify the combination of ligand identities, ligand concentrations, and matrix stiffness that best enhances neurite outgrowth. We will develop and characterize a library of ELP variants with distinct cell-adhesive ligands derived from native ECM, and assess their ability to support neurite outgrowth from rat dorsal root ganglia (DRG). In Aim 2, we will validate our in vitro optimization results in a preclinical, rat sciatic nerve injury model. A core-shell, ELP-based rNGC with an inner core matrix of the optimized ELP formulation from Aim 1 will be fabricated and evaluated for its ability to enhance therapeutic outcome. Controls include reversed nerve autograft, hollow silicone conduit, and non-optimized ELP- based rNGC. This study would represent the first use of computational optimization methods to design a reproducible, reliable, recombinant biomaterial with multiple regenerative matrix cues. PROJECT NARRATIVE The main challenge with using nerve guidance conduits (NGCs) to bridge long peripheral nerve gap injuries is the immense trade-off between providing the complex matrix cues necessary for optimal nerve regeneration while providing a conduit that is readily available, reproducible, and easily fabricated. To address this challenge, here we utilize (1) computational optimization methods to identify the optimal biochemical and biomechanical matrix cues for nerve regeneration, and (2) advanced protein-engineering strategies to incorporate these cues into a recombinant NGC (rNGC). Our rNGC combines the reliability of synthetic NGCs with the matrix cues of naturally-derived NGCs to make an affordable, off-the-shelf rNGC that promotes nerve regeneration.",Combinatorial matrix-mimetic recombinant proteins as engineered nerve guidance conduits,9872885,R21NS114549,"['Address', 'Adhesives', 'Allografting', 'American', 'Amino Acids', 'Autologous Transplantation', 'Axon', 'Behavioral', 'Biochemical', 'Biocompatible Materials', 'Biomechanics', 'Blood Vessels', 'Cell Surface Receptors', 'Cells', 'Chemicals', 'Cholinergic Receptors', 'Chronic', 'Clinical', 'Collagen', 'Complex', 'Cues', 'Data', 'Elastin', 'Electron Microscopy', 'Encapsulated', 'Engineering', 'Esthesia', 'Extracellular Matrix', 'Fibronectins', 'Formulation', 'Gold', 'Immunofluorescence Immunologic', 'Immunohistochemistry', 'In Vitro', 'Individual', 'Injury', 'Knowledge', 'Label', 'Laminin', 'Libraries', 'Ligands', 'Machine Learning', 'Mass Spectrum Analysis', 'Methods', 'Modeling', 'Modulus', 'Motor', 'Motor Neurons', 'Muscle', 'Natural regeneration', 'Nerve', 'Nerve Regeneration', 'Neurites', 'Neuroglia', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Peripheral Nerves', 'Peripheral nerve injury', 'Process', 'Property', 'Protein Engineering', 'Protein Family', 'Proteins', 'Rattus', 'Recombinant Proteins', 'Recombinants', 'Recovery of Function', 'Reporting', 'Reproducibility', 'Signal Pathway', 'Signal Transduction', 'Silicones', 'Spinal', 'Spinal Ganglia', 'Stains', 'Synaptophysin', 'Tenascin', 'Tissues', 'Tolonium chloride', 'Variant', 'Walking', 'alpha Bungarotoxin', 'base', 'combinatorial', 'comparative', 'design', 'exhaustion', 'experimental study', 'gel electrophoresis', 'improved', 'in vivo evaluation', 'mimetics', 'motor control', 'myelination', 'nerve autograft', 'nerve gap', 'nerve injury', 'nerve supply', 'postsynaptic', 'pre-clinical', 'regenerative', 'sciatic nerve', 'therapy outcome', 'tomato lectin', 'transcriptional coactivator p75']",NINDS,STANFORD UNIVERSITY,R21,2020,436238,-0.03196533169466641
"Fluomics: The Next Generation Influenza A virus is a major human respiratory pathogen, and available vaccines and antivirals are of limited efficacy. In order to identify novel targets for therapeutic intervention during influenza virus infection, we have assembled an interdisciplinary team that uses a highly integrated systems level approach to identify and validate key genes/networks involved in virus pathogenesis. The overarching theme of our multidisciplinary proposal “FluOMICS: The NEXT Generation” is to obtain multiple OMICS-based systems level measurements and integrate them using modeling approaches and machine learning algorithms to identify and validate 1) host-virus networks that modulate influenza A virus disease severity, 2) biomarkers in blood that reflect the activation states of these networks and 3) novel host targets for therapeutic interventions. Our underlying main hypothesis is that host networks involved in viral replication and early host responses regulate disease outcomes and represent targets for therapeutic intervention. The proposed studies leverage on our previous collaborations that generated global datasets and models that predict severity of disease caused by three influenza virus strains with different levels of virulence. While our previous studies gave many novel insights in influenza pathogenesis, they likely provide a narrow window on the determinants of disease severity in humans. Thus, it is necessary expand beyond the specific virus strains that were used to study pathogenesis, and explore a broader context of viral and host perturbations linked to clinical outcomes. In order to identify clinically relevant networks involved in influenza virus pathogenesis we propose to integrate into predictive and comprehensive models global responses during influenza virus infection in three systems 1) human blood from a human cohort of patients with documented influenza virus infection and diverse clinical outcomes (Project 1); 2) mouse blood and tissues from experimentally infected animals under a variety of conditions and perturbations resulting in diverse disease outcomes (Project 1) and 3) relevant primary human cells (Project 2). Samples will be processed and send to the Technology Core for global transcriptomics, proteomics and metabolomics analysis. OMICS data sets will be integrated and compared by the Modeling Core to generate network models of disease, uncover blood biomarkers and identify key drivers as targets for therapeutic intervention. Predicted network regulators will be used as a source for subsequent iterative rounds of perturbations to refine existing and to identify new network disease models. Data and models will be managed and disseminated by the Data Management and Bioinformatics Core. We expect that these studies will uncover and validate novel pathogenic networks, blood biomarkers associated with them, and specific therapeutic targets to revert pathogenic networks. In summary, our modeling approaches will find correlates and associations between diverse experimental systems that will help us define human blood biomarkers, and link them to in vivo and ex vivo signatures for both companion diagnostics and personalized therapies. We propose a systematic approach (FluOMICS) to generate predictive models of influenza virus pathogenesis which will a) allow us to identify biomarkers for predicting disease outcome, and b) provide avenues to explore for new, host-directed, therapeutic interventions.",Fluomics: The Next Generation,10132152,U19AI135972,"['Animals', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Biological Markers', 'Blood', 'CRISPR screen', 'Cells', 'Clinical', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Complementary DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Disease', 'Disease Outcome', 'Disease model', 'Environmental Risk Factor', 'Experimental Models', 'Genes', 'Genetic Polymorphism', 'Human', 'Immune response', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A virus', 'Integration Host Factors', 'Intervention', 'Link', 'Lung', 'Lung diseases', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Profiling', 'Mus', 'Network-based', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Post-Translational Protein Processing', 'Predisposition', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Sampling', 'Severities', 'Severity of illness', 'Small Interfering RNA', 'Source', 'System', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic Intervention', 'Tissues', 'Vaccines', 'Viral', 'Viral Pathogenesis', 'Virulence', 'Virus', 'Virus Diseases', 'Virus Replication', 'base', 'clinically relevant', 'cohort', 'companion diagnostics', 'data integration', 'data management', 'early detection biomarkers', 'epigenome', 'epigenomics', 'functional genomics', 'human pathogen', 'in vivo', 'influenza virus strain', 'influenzavirus', 'insight', 'machine learning algorithm', 'metabolome', 'metabolomics', 'mouse model', 'multidisciplinary', 'multiple omics', 'network models', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'personalized medicine', 'predictive marker', 'predictive modeling', 'programs', 'protein protein interaction', 'resilience', 'respiratory', 'response', 'specific biomarkers', 'targeted treatment', 'therapeutic target', 'transcriptome', 'transcriptomics']",NIAID,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U19,2020,635916,0.007618075548388424
"Fluomics: The Next Generation Influenza A virus is a major human respiratory pathogen, and available vaccines and antivirals are of limited efficacy. In order to identify novel targets for therapeutic intervention during influenza virus infection, we have assembled an interdisciplinary team that uses a highly integrated systems level approach to identify and validate key genes/networks involved in virus pathogenesis. The overarching theme of our multidisciplinary proposal “FluOMICS: The NEXT Generation” is to obtain multiple OMICS-based systems level measurements and integrate them using modeling approaches and machine learning algorithms to identify and validate 1) host-virus networks that modulate influenza A virus disease severity, 2) biomarkers in blood that reflect the activation states of these networks and 3) novel host targets for therapeutic interventions. Our underlying main hypothesis is that host networks involved in viral replication and early host responses regulate disease outcomes and represent targets for therapeutic intervention. The proposed studies leverage on our previous collaborations that generated global datasets and models that predict severity of disease caused by three influenza virus strains with different levels of virulence. While our previous studies gave many novel insights in influenza pathogenesis, they likely provide a narrow window on the determinants of disease severity in humans. Thus, it is necessary expand beyond the specific virus strains that were used to study pathogenesis, and explore a broader context of viral and host perturbations linked to clinical outcomes. In order to identify clinically relevant networks involved in influenza virus pathogenesis we propose to integrate into predictive and comprehensive models global responses during influenza virus infection in three systems 1) human blood from a human cohort of patients with documented influenza virus infection and diverse clinical outcomes (Project 1); 2) mouse blood and tissues from experimentally infected animals under a variety of conditions and perturbations resulting in diverse disease outcomes (Project 1) and 3) relevant primary human cells (Project 2). Samples will be processed and send to the Technology Core for global transcriptomics, proteomics and metabolomics analysis. OMICS data sets will be integrated and compared by the Modeling Core to generate network models of disease, uncover blood biomarkers and identify key drivers as targets for therapeutic intervention. Predicted network regulators will be used as a source for subsequent iterative rounds of perturbations to refine existing and to identify new network disease models. Data and models will be managed and disseminated by the Data Management and Bioinformatics Core. We expect that these studies will uncover and validate novel pathogenic networks, blood biomarkers associated with them, and specific therapeutic targets to revert pathogenic networks. In summary, our modeling approaches will find correlates and associations between diverse experimental systems that will help us define human blood biomarkers, and link them to in vivo and ex vivo signatures for both companion diagnostics and personalized therapies. We propose a systematic approach (FluOMICS) to generate predictive models of influenza virus pathogenesis which will a) allow us to identify biomarkers for predicting disease outcome, and b) provide avenues to explore for new, host-directed, therapeutic interventions.",Fluomics: The Next Generation,9843974,U19AI135972,"['Animals', 'Antiviral Agents', 'Benchmarking', 'Bioinformatics', 'Biological Markers', 'Blood', 'CRISPR screen', 'Cells', 'Clinical', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Complementary DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Disease', 'Disease Outcome', 'Disease model', 'Environmental Risk Factor', 'Experimental Models', 'Genes', 'Genetic Polymorphism', 'Human', 'Immune response', 'Infection', 'Influenza', 'Influenza A Virus, H1N1 Subtype', 'Influenza A virus', 'Integration Host Factors', 'Intervention', 'Link', 'Lung', 'Lung diseases', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Molecular Profiling', 'Mus', 'Network-based', 'Outcome', 'Pathogenesis', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Post-Translational Protein Processing', 'Predisposition', 'Process', 'Proteins', 'Proteome', 'Proteomics', 'Sampling', 'Severities', 'Severity of illness', 'Small Interfering RNA', 'Source', 'System', 'Systems Biology', 'Technology', 'Testing', 'Therapeutic Intervention', 'Tissues', 'Vaccines', 'Viral', 'Viral Pathogenesis', 'Virulence', 'Virus', 'Virus Diseases', 'Virus Replication', 'base', 'clinically relevant', 'cohort', 'companion diagnostics', 'data integration', 'data management', 'early detection biomarkers', 'epigenome', 'epigenomics', 'functional genomics', 'human pathogen', 'in vivo', 'influenza virus strain', 'influenzavirus', 'insight', 'machine learning algorithm', 'metabolome', 'metabolomics', 'mouse model', 'multidisciplinary', 'multiple omics', 'network models', 'new therapeutic target', 'next generation', 'novel', 'pathogen', 'personalized medicine', 'predictive marker', 'predictive modeling', 'programs', 'protein protein interaction', 'resilience', 'respiratory', 'response', 'specific biomarkers', 'targeted treatment', 'therapeutic target', 'transcriptome', 'transcriptomics']",NIAID,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U19,2020,2648396,0.007618075548388424
"Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity   Project Summary    For something as complex and multifaceted as bacterial antibiotic resistance (AR), our drug evaluation  paradigm is strikingly narrow and homogenous: MIC/MBC testing in standardized bacteriologic media. We  have shown that this drug evaluation paradigm is inadequate, even misleading, as changes in the media  conditions of the procedure lead to dramatically different results. A more holistic definition of antibiotic therapy  that centers on understanding antibiotic activity in synergy with host innate immune factors such as cationic  antimicrobial peptides (AMPs), serum and phagocytic cells (e.g. neutrophils) reveals therapeutic options  unrecognized in standard testing. The proposed U01 program represents a groundbreaking approach to use  systems biology approaches and inform more effective antibiotic utilization in the context of host innate  immunity. We propose to: 1) build an iterative systems biology workflow that integrates multiple experimental  and computational approaches to give a comprehensive assessment of AR; and 2) apply this workflow to high  priority pathogens to systematically elucidate AR mechanisms and their condition­dependency. The iterative  workflow includes: (i) omics and physiological data generation.  Clinically isolated strains of the selected  pathogens will be grown under conventional testing (bacteriologic media) and more physiologic conditions  (tissue culture media, serum, and in presence of AMPs and neutrophils) to probe for advantageous gain of  activity.  The omics data types collected are: DNA resequencing, RNAseq, and metabolomics.  (ii)  Bioinformatics and data modeling analysis involves three approaches: big data analysis for data set  dimensionality and coarse grained variable dependencies assessment, genome­scale modeling for  mechanistic elucidation and analysis, and machine learning that uses AR­related metadata to classify the  overall biological functions. This analysis will lead to understanding of AR mechanisms.  (iii) Multi­scale  validation from animal models, to laboratory evolution, to cytology, to gene expression alteration, to structural  protein analysis of putative targets. The validation thus ranges from host behavior to atomistic detail of  ligand­target interactions. The iterative loop then closes, comparing computational prediction to experimental  outcomes. False­negative and false­positive predictions are then algorithmically analyzed by a hypothesis  generating family of algorithms that then makes suggestions about what conditions to use in the next iteration  of the loop.  The pathogens that we will focus on are methicillin­resistant ​Staphylococcus aureus ​(MRSA), the  carbapenem­resistant Enterobacteriaceae (CRE) Klebsiella ​pneumoniae ​and ​Acinetobacter baumannii,​ and  Pseudomonas aeruginosa​. The team of investigators has made the foundational observations and led the  development of the technologies on which the iterative workflow is based. A multi­ and genome­scale methods  of systems biology fulfills requirements of RFA­AI­14­064 to which it responds.              Narrative    The current evaluation of antibiotic drug candidates in drug discovery and in clinical medicine is conducted in  laboratory media that ignores the actual physiologic conditions in the host and the host immune system.  We  have discovered potent antimicrobial activities of existing antibiotics against highly “drug­resistant superbugs”  that are currently ignored but revealed in synergy with the human immune system. This program proposes a  holistic and comprehensive systems biology approach to systematically discover novel treatment opportunities  and underlying mechanisms using a novel iterative data generation, analysis, and modeling workflow.       ",Systems Biology Approach to Redefine Susceptibility Testing and Treatment of MDR Pathogens in the Context of Host Immunity,9933789,U01AI124316,"['Acinetobacter baumannii', 'Algorithmic Analysis', 'Algorithms', 'Animal Model', 'Animals', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Antimicrobial Cationic Peptides', 'Bacterial Antibiotic Resistance', 'Bacteriology', 'Behavior', 'Big Data', 'Bioinformatics', 'Biological Assay', 'Biological Process', 'Biological Products', 'Biology', 'Clinical', 'Clinical Medicine', 'Collection', 'Complex', 'Computing Methodologies', 'Culture Media', 'Cytology', 'DNA Resequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Drug Evaluation', 'Drug resistance', 'Effectiveness', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Future', 'Gene Expression Alteration', 'Generations', 'Genome', 'Grain', 'Growth', 'Human', 'Immune', 'Immune system', 'Immunity', 'Immunologic Factors', 'Infection', 'Integration Host Factors', 'Klebsiella pneumoniae', 'Knowledge', 'Laboratories', 'Lead', 'Ligand Binding', 'Ligands', 'Machine Learning', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular Biology', 'Molecular Profiling', 'Mus', 'Natural Immunity', 'Network-based', 'Organism', 'Outcome', 'Output', 'Participant', 'Phagocytes', 'Pharmaceutical Preparations', 'Pharmacodynamics', 'Physiological', 'Predisposition', 'Procedures', 'Process', 'Protein Analysis', 'Pseudomonas aeruginosa', 'Research Personnel', 'Resistance', 'Resistance development', 'Risk Assessment', 'Series', 'Serum', 'Standardization', 'Statistical Data Interpretation', 'Structural Protein', 'Structure', 'Suggestion', 'Superbug', 'Systems Biology', 'Testing', 'Therapeutic', 'Update', 'Validation', 'antimicrobial', 'antimicrobial peptide', 'bacterial genetics', 'base', 'carbapenem-resistant Enterobacteriaceae', 'data modeling', 'design', 'drug candidate', 'drug discovery', 'experience', 'genome-wide', 'human tissue', 'improved', 'in silico', 'in vivo', 'macromolecule', 'metabolomics', 'methicillin resistant Staphylococcus aureus', 'microbial', 'microbial host', 'multi-drug resistant pathogen', 'multidrug-resistant Pseudomonas aeruginosa', 'neutrophil', 'novel', 'pathogen', 'priority pathogen', 'product development', 'programs', 'reconstruction', 'resistance mechanism', 'response', 'screening', 'synergism', 'technology development', 'tissue culture', 'tool', 'transcriptome sequencing', 'transcriptomics', 'treatment response']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U01,2020,1806219,-0.00880788892583872
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an extensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spatial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high-resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",10148333,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human BioMolecular Atlas Program', 'Human body', 'Image', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Visualization', 'Work', 'base', 'cell type', 'data exploration', 'data standards', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2020,1000000,-0.023643332899245056
"Successful Clinical Response In Pneumonia Therapy (SCRIPT) Systems Biology Center Modified Project Summary/Abstract Section  This innovative integrated systems biology application seeks to delineate the complex host/pathogen interactions occurring at the alveolar level that lead to unsuccessful response to therapy in serious pneumonia, including serious viral pneumonia. To achieve this objective, we will leverage our unique access to alveolar fluid collected as part of routine clinical care in mechanically ventilated patients with suspected pneumonia in our medical intensive care unit. Bronchoalveolar lavage fluid will be obtained serially from well characterized mechanically ventilated patients with serious viral or Pseudomonas or Acinetobacter bacterial pneumonias. Both of these CDC-designated serious hazard level bacterial pathogens have clinical failure rates as high as 50%. A robust clinical definition will allow comparison of both host and pathogen signatures associated with failure of therapy vs. success. These clinical specimens and extensive patient phenomics will anchor two mutually supportive and iterative research projects. Project One will deploy robust tools for flow sorting macrophage and lymphocyte subset populations, isolating RNA from these populations, and performing transcriptomic and epigenomic analysis to compare successful and unsuccessful host response. Project Two will focus on both specific pathogen genomic profiles associated with unsuccessful outcome and the differential microbiome response. Specific pathogen genomic profiles identified will be tested for causality in a unique humanized alveolar macrophage mouse model by the Technology Core. Changes in microbiome communities will be comprehensively assessed by shotgun deep sequencing to detect bacteriophage, other virus, and fungal DNA, in addition to bacterial. A Technical Core will perform cell sorting of BAL macrophage and lymphocyte subsets, RNA sequencing, and whole genome methylation, as well as perform the mouse pneumonia model studies. A Data Management and Bioinformatics Core will develop tools to reduce the dimensionality of these large comprehensive datasets, including the clinical phenomics, and provide them to the Modeling Core. The Modeling Core will then use an ecosystem-based approach to this complex adaptive system combined with unique machine learning tools and neural networks to generate biomarkers of host, pathogen and/or microbiome patterns predictive of successful pneumonia outcome. Predictive biomarkers developed in the Modeling Core will then be validated in a prospective confirmatory cohort of patients in whom analogous data will be generated. Biomarkers will also be tested for causality the mouse model. The Administrative Core will perform the outward- facing role of education and outreach to the community and sponsor, as well as regularly exchanging datasets, analytic tools, and specimens with NIH-sponsored/approved repository sites. Modified Public Health Relevance Section.   The Successful Clinical Response In Pneumonia Treatment (SCRIPT) systems biology center seeks to delineate the complex host/pathogen interactions occurring at the alveolar level that lead to unsuccessful response to therapy in serious pneumonia, including severe viral pneumonia. We will leverage our unique access to alveolar fluid collected as part of routine clinical care in mechanically ventilated patients to generate clinical phenomic, transcriptomic, epigenomic and metagenomic data that describe the host response, pathogen characteristics and microbiome of the alveolar space during pneumonia. We will then integrate this comprehensive phenotypic data into an ecosystem-based model to generate predictive biomarkers of pneumonia outcome for subsequent validation in a second cohort and tested for causality in a humanized alveolar macrophage mouse model.",Successful Clinical Response In Pneumonia Therapy (SCRIPT) Systems Biology Center,9843971,U19AI135964,"['Acinetobacter', 'Address', 'Alveolar', 'Alveolar Macrophages', 'Alveolus', 'Animal Model', 'Antibiotic Therapy', 'Bacteriophages', 'Bioinformatics', 'Biological Markers', 'Bronchoalveolar Lavage', 'Bronchoalveolar Lavage Fluid', 'Cause of Death', 'Cell Separation', 'Cells', 'Centers for Disease Control and Prevention (U.S.)', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Microbiology', 'Clinical Trials', 'Collection', 'Communities', 'Complex', 'DNA Viruses', 'DNA sequencing', 'Data', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Ecosystem', 'Education and Outreach', 'Equilibrium', 'Etiology', 'Failure', 'Fungal DNA', 'Generations', 'Genes', 'Genetic', 'Goals', 'Human', 'Immune response', 'Infection', 'Intensive Care Units', 'Lead', 'Link', 'Liquid substance', 'Lymphocyte Subset', 'Lymphoid Cell', 'Machine Learning', 'Mechanics', 'Medical', 'Metagenomics', 'Methylation', 'Modeling', 'Multiomic Data', 'Myeloid Cells', 'Nosocomial Infections', 'Nosocomial pneumonia', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Pneumonia', 'Population', 'Preparation', 'Prospective cohort', 'Protocols documentation', 'Pseudomonas', 'Pseudomonas aeruginosa', 'Pseudomonas aeruginosa infection', 'RNA', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Secondary to', 'Shotguns', 'Site', 'Sorting - Cell Movement', 'Specimen', 'Standardization', 'Stream', 'System', 'Systems Biology', 'Technology', 'Testing', 'Treatment Failure', 'United States National Institutes of Health', 'Validation', 'Virulence', 'analytical tool', 'base', 'clinical care', 'cohort', 'data management', 'data pipeline', 'deep sequencing', 'epigenomics', 'experience', 'genomic data', 'genomic profiles', 'hazard', 'humanized mouse', 'improved', 'innovation', 'macrophage', 'microbiome', 'microbiome composition', 'mouse model', 'multiple omics', 'neural network', 'new therapeutic target', 'novel', 'pathogen', 'pathogen genomics', 'phenomics', 'phenotypic data', 'pneumonia model', 'predictive marker', 'predictive tools', 'prospective', 'repository', 'response', 'success', 'tool', 'transcriptome sequencing', 'transcriptomics', 'viral DNA', 'whole genome']",NIAID,NORTHWESTERN UNIVERSITY AT CHICAGO,U19,2020,2400000,0.0011924095750153085
"Carolina Population Center PROJECT SUMMARY/ABSTRACT The Carolina Population Center requests infrastructure support that will advance population dynamics research at CPC by increasing research impact, innovation, and productivity, supporting the development of junior scientists, and reducing the administrative burden on scientists. Infrastructure support will advance science in three primary research areas: Sexuality, Reproduction, Fertility, and Families; Population, Health, and the Environment; and Inequality, Mobility, Disparities, and Well-Being. Much of the research at CPC draws on large publicly available longitudinal data sets that our faculty have designed and collected, including the National Longitudinal Study of Adolescent to Adult Health, the China Health and Nutrition Survey, newer surveys associated with the Transfer Project, and the Study of the Tsunami Aftermath and Recovery, all of which will continue to be important in work related to our primary research areas over the next five years. These projects embody several themes that have guided research at CPC since the Center's inception. These themes, which will continue to shape our work, are the importance of life course processes and longitudinal data, multi-level processes and measurement of context, interventions and natural experiments as means of learning about causal processes, and the relevance of sociodemographic variables such as age, gender, race- ethnicity, and socioeconomic status for disparities in health and well-being. By embedding these themes, our projects provide data that enable us to address barriers that otherwise impede progress in the population sciences generally, and in our primary research areas in particular. We request support for three cores which in combination will provide an institutional infrastructure that will push populations dynamics research forward by empowering CPC faculty to tackle challenging questions using state of the art measurement techniques and methods. The Administrative Core plans activities that maintain a stimulating intellectual community, streamlines administrative processes so that scientists can focus on research, coordinates activities of the Cores so that services are offered efficiently, and communicates information about research and data more broadly. The Development Core supports early stage investigators and other faculty with exciting new ideas through multiple mechanisms: workshops, access to technical expertise in measurement, and seed grants. The Research Services Core enables scientists to address complex and important population research issues by providing access to state-of-the-art research tools and professional support for programming, survey development, and analysis. NARRATIVE This project will provide infrastructure support for a cutting edge program of research on population dynamics at the Carolina Population Center. Research at the Center will analyze state-of-the art data to address fundamental questions regarding fertility, adolescent health, and links between the environment and health. Special attention will be paid to factors creating health disparities.",Carolina Population Center,10005569,P2CHD050924,"['Address', 'Adolescent', 'Adopted', 'Adult', 'Age', 'Applications Grants', 'Area', 'Attention', 'Biological Markers', 'China', 'Cognitive', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Vision Systems', 'Creativeness', 'Data', 'Data Collection', 'Development', 'Diffuse', 'Educational workshop', 'Environment', 'Ethnic Origin', 'Extramural Activities', 'Faculty', 'Family', 'Fertility', 'Fostering', 'Funding', 'Gender', 'Genetic', 'Grant', 'Hand', 'Health', 'Health Surveys', 'Home environment', 'Inequality', 'Infrastructure', 'Intervention', 'Journals', 'Learning', 'Life Cycle Stages', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mainstreaming', 'Measurement', 'Mentors', 'Methods', 'Natural experiment', 'Nutrition Surveys', 'Personal Satisfaction', 'Phase', 'Policy Making', 'Population', 'Population Dynamics', 'Population Research', 'Population Sciences', 'Postdoctoral Fellow', 'Process', 'Production', 'Productivity', 'Publishing', 'Race', 'Recovery', 'Reproduction', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Schools', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Seeds', 'Services', 'Sexuality', 'Shapes', 'Socioeconomic Status', 'Structure', 'Students', 'Surveys', 'Talents', 'Teacher Professional Development', 'Technical Expertise', 'Techniques', 'Training Programs', 'Tsunami', 'Universities', 'Work', 'adolescent health', 'career', 'collaborative environment', 'cost', 'data access', 'design', 'empowered', 'experience', 'faculty support', 'health disparity', 'innovation', 'interdisciplinary collaboration', 'longitudinal dataset', 'novel strategies', 'population health', 'privacy protection', 'programs', 'research and development', 'response', 'sociodemographic variables', 'success', 'tool']",NICHD,UNIV OF NORTH CAROLINA CHAPEL HILL,P2C,2020,774402,-0.014688071959518215
"FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control PROJECT SUMMARY In this proposal we plan to contribute addressing the above foundational and operational challenges by advancing the science of influenza modeling and contributing novel methods and data sources that will increase the accuracy and availability of seasonal and pandemic influenza models. To address these challenges, we plan to build on the unique mechanistic spatially structured modeling approaches developed by our consortium, that includes stochastic metapopulation models and fully developed agent-based models nested together in our global epidemic and mobility modeling (GLEAM) approach. The objective of this project is to generate novel and actionable scientific insights from dynamic transmission models of influenza transmission that effectively integrate key socio-demographic indicators of the focus population, as well as a wide spectrum of pharmaceutical and non-pharmaceutical interventions. Our proposed work in specific aim 1 (A1) will leverage our global modeling (from the global to local scale) framework that can be used to explore the multi-year impact of influenza vaccination, antiviral prophylaxis/treatment, and community mitigation during influenza seasons and pandemics. Our specific aim 2 (A2) will focus on using high quality data to model heterogeneous transmission drivers and novel contact pattern stratifications that will allow us to guide mitigation strategies and prioritization for interventions. In our Aim 3 (A3) we will use artificial intelligence approaches to identify interventions that are particularly synergistic and well-suited to particular epidemic scenarios, for seasonal and pandemic influenza. Our overarching goal is to provide a modeling portfolio with flexible and innovative mathematical and computational approaches. We aim to address several questions commonly asked about seasonal and pandemic influenza and match these with analytical methods and outbreak projections. The modeling and data developed in this project can help facilitate and justify transparent public health decisions, while contributing to the definition of standard methods for model selection and validation. Finally, our influenza modeling platform can also benefit the broader network of modeling teams and can be used to improve result sharing and harmonization of modeling approaches. The objective of this proposal is to advance the science of modeling and contribute novel methods and data analytics tools that will increase the understanding of seasonal and pandemic influenza in the context of the network of modeling teams coordinated by the CDC. To address these challenges, we plan to develop a novel global modeling framework, contribute new data and methods for improve the accuracy and validation of flu modeling approaches, and evolve successful methodologies to advance the analysis of layered intervention with artificial Intelligence.",FluMod - Center for the Multiscale Modeling of Pandemic and seasonal Flu Prevention and Control,10071782,U01IP001137,[' '],NCIRD,NORTHEASTERN UNIVERSITY,U01,2020,371721,0.009088820038738543
"PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry Project Summary Paralleling the growth of neuroscience research, there has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. Such sharing, evaluation, and integration are necessary if computational modeling efforts are to be useful not only in generating reliable and accurate accounts of how brain subsystems operate, but also of how they interact to give rise to higher cognitive functions, and how disruptions of such interactions may give rise to disturbances of mental function observed in psychiatric and neurological disorders. This proposal seeks to meet this need by developing PsyNeuLink: an open source, Python-based software environment that makes it easy to create new models, import and/or re-implement existing ones, integrate these within a single software environment that will facilitate head-to-head comparison of comparable models, the assembly of complementary models into system-level models, and serve as a common repository for the documentation and dissemination of such models for both research and didactic purposes (i.e., publication, education, etc.). These goals will be pursued under two Specific Aims: 1) Extend the scope of modeling efforts that PsyNeuLink can accommodate by: i) enhancing its application programmer interface (API) used to add new components and interfaces to statistical analysis tools and other modeling environments (such as PyTorch, Emergent and ACT-R; ii) enriching its Library by adding PsyNeuLink implementations of influential models of neural subsystems; and iii) developing a publicly available workbook of simulation exercises as both an introduction to PsyNeuLink and for use in Cognitive Neuroscience and Computational Psychiatry curricula. 2) Accelerate PsyNeuLink by developing a custom compiler that preserves its simplicity and flexibility, while dramatically increasing its speed, to make it suitable for simulation of large and complex system-level models, and for parameter estimation, model fitting, and model comparison. This project will exploit the power and accelerating use of Python, and modern just-in-time compilation methods to develop a tool designed specifically for the needs of systems-level Cognitive Neuroscience and Computational Psychiatry. This promises to open up new opportunities for research at the systems-level — a level of analysis that is crucial both for understanding how human mental function emerges from the interplay among neural subsystems, and how disturbances of individual neural subsystems impact this interplay, disruptions of which are almost certainly a critical factor in neurologic and psychiatric disorders. Project Narrative Paralleling the growth of neuroscience research has been an explosion in the development of computationally explicit models of the functions of core brain subsystems. Unfortunately, however, there has not been a commensurate development of the tools needed to share, validate, and compare such models, or integrate them into models of system-level function. This proposed project seeks to address this need by developing a standard software platform for the construction, documentation, sharing, and integration of computational models of brain function, that promises to accelerate the study of how system-level interactions give rise to mental function and, critically, the kinds of disruptions of such system-level interactions produced by disturbances of individual subsystems — disruptions that are sure to be a complex but critical factor in neurological and psychiatric disorders.",PsyNeuLink:  A Block Modeling Environment for Cognitive Neuroscience and Computational Psychiatry,9976610,R21MH117548,"['Acceleration', 'Address', 'Architecture', 'Attention', 'Basal Ganglia', 'Biological', 'Biological Models', 'Brain', 'Complement', 'Complex', 'Computer Models', 'Computer software', 'Custom', 'Data', 'Development', 'Documentation', 'Education', 'Educational Curriculum', 'Environment', 'Episodic memory', 'Evaluation', 'Exercise', 'Explosion', 'Foundations', 'Goals', 'Grain', 'Growth', 'Hippocampus (Brain)', 'Human', 'Individual', 'Influentials', 'Libraries', 'Literature', 'Maintenance', 'Manuals', 'Mental disorders', 'Methods', 'Modeling', 'Modernization', 'Neurosciences Research', 'Perceptual learning', 'Play', 'Prefrontal Cortex', 'Procedures', 'Psychiatry', 'Publications', 'Publishing', 'Pythons', 'Research', 'Role', 'Seeds', 'Short-Term Memory', 'Speed', 'Statistical Data Interpretation', 'System', 'Time', 'Writing', 'application programming interface', 'base', 'cognitive function', 'cognitive neuroscience', 'cognitive process', 'deep learning', 'deep neural network', 'design', 'flexibility', 'head-to-head comparison', 'improved', 'learning network', 'memory encoding', 'memory retrieval', 'mental function', 'nervous system disorder', 'neural model', 'open source', 'parallelization', 'preservation', 'programs', 'relating to nervous system', 'repository', 'simulation', 'tool', 'tool development']",NIMH,PRINCETON UNIVERSITY,R21,2020,197545,-0.017040406928221615
"Rapid and Precise Molecular Pathway Modelling of the SARS-CoV-1 and SARS-CoV-2 Infection Cycle with Human Host Protein and Therapeutic Interactions Project Summary The Reactome Knowledgebase is a widely used and internationally recognized expert-curated, open-source resource of a broad array of human biological processes and their disease counterparts, coupled to powerful tools for data analysis and display, and integrated with diverse community genomics resources. The work proposed here will add molecular annotations of the COVID-19 infection process mediated by the SARS-CoV-2 coronavirus, interactions between viral components and human host proteins that mediate the severity of viral infection, and the effects of therapeutics and drug-like compounds on both viral and host proteins. The resulting SARS-CoV-2 pathway annotations will provide a framework for pathway- and network-based data analysis and visualization, which will be critical for the interpretation of numerous COVID-19 studies now and in the future. In collaboration with a team of community experts in virology, drug design, and infectious disease, we will assemble information in two stages. First, a draft annotation will associate relevant SARS-CoV-1 and SARS-CoV- 2 viral and host cell proteins with each stage of the infection process and the host response to it. These annotations will be immediately useful for identifying additional relevant interacting proteins, for assessing possible effects of variation in the host or viral proteins on specific steps of viral infection, and for identifying possible drug targets. In the second stage, the SARS-CoV-2 map will be annotated more extensively to fill in molecular details of each step in these processes and to highlight differences in the processes mediated by SARS- CoV-2 virus and related coronaviruses. This annotation process will continue for the duration of the project to incorporate newly validated molecular details as they are uncovered by the research community. All the data, code and tools developed by this project will be open source and open. Project Narrative Using long-established Reactome Knowledgebase standards for authorship, curation and peer-review of molecular pathway data from published and unpublished sources, we will annotate the molecular details of the processes by which SARS-CoV-2 and SARS-CoV-1 coronaviruses infect human cells. This will include the interactions among viral and host proteins, the expression of viral proteins likely to trigger innate and adaptive immunity, and an annotation of the molecular effects of drugs on these processes. Our strategy will allow rapid revision and extension of annotations as data accumulate, will support close integration with other community resources, and will generate a valuable resource for community analysis of experimental data sets relevant to COVID-19 disease.",Rapid and Precise Molecular Pathway Modelling of the SARS-CoV-1 and SARS-CoV-2 Infection Cycle with Human Host Protein and Therapeutic Interactions,10165320,U41HG003751,"['2019-nCoV', 'Authorship', 'Biological Process', 'COVID-19', 'Cells', 'Code', 'Collaborations', 'Communicable Diseases', 'Communities', 'Computer software', 'Consensus', 'Coronavirus', 'Coupled', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Disease', 'Disease Pathway', 'Drug Design', 'Drug Targeting', 'Ensure', 'Future', 'Genomics', 'Human', 'Immune response', 'Infection', 'Innate Immune Response', 'Institutes', 'International', 'Link', 'Literature', 'Manuals', 'Maps', 'Mediating', 'Mining', 'Modeling', 'Molecular', 'Natural Immunity', 'Natural Language Processing', 'Network-based', 'Paper', 'Pathogenicity', 'Pathway interactions', 'Peer Review', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Proteome', 'Publishing', 'Reaction', 'Research', 'Resources', 'SARS coronavirus', 'Severities', 'Source', 'System', 'Systems Biology', 'Therapeutic', 'Therapeutic Effect', 'Therapeutic Monoclonal Antibodies', 'Variant', 'Viral', 'Viral Proteins', 'Virus', 'Virus Diseases', 'Work', 'adaptive immunity', 'data integration', 'data structure', 'data tools', 'data visualization', 'experimental analysis', 'knowledge base', 'open source', 'protein expression', 'small molecule', 'structured data', 'tool', 'virology']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2020,310292,-0.01684884113945806
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),10019333,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip Prosthesis', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee Prosthesis', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'clinical center', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2020,629384,-0.008696463578333513
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9962426,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Pooling', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'multiple data types', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,323659,-0.023372206811977856
"Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes PROJECT SUMMARY Overview: We will extend and develop implementations of foundational methods for analyzing populations of attributed connectomes. Our toolbox will enable brain scientists to (1) infer latent structure from individual connectomes, (2) identify meaningful clusters among populations of connectomes, and (3) detect relationships between connectomes and multivariate phenotypes. The methods we develop and extend will naturally overcome the challenges inherent in connectomics: high-dimensional non-Euclidean data with multi-level nonlinear interactions. Our implementations will comply with the highest open-source standards by: providing extensive online documentation and extended tutorials, hosting workshops to demonstrate our tools on an annual basis, and merging our implementations into commonly used packages such as scikit-learn [1], scipy [2], and networkx [3]. All of the code we develop is open source. We strive to ensure that our code is shared in accordance with the strictest guiding principles. We chose to implement these algorithms in Python due to its wide adoption in the neuroscience and data science fields. In particular, many other neuroscience tools applicable to connectomics, including NetworkX DiPy, mindboggle, nilearn, and nipy, are also implemented in Python. This will enable researchers to chain our analysis tools onto pre-existing pipelines for data preprocessing and visualization. Nonetheless, we feel that sharing our code in our own public repositories is insufficient for global reach. We have also begun reaching out to developers of the leading data science packages in python, including scipy, sklearn, networkx, scikit-image, and DiPy. For each of those packages, we have informal approval to begin integrating algorithms that we have developed. Those packages are collectively used by >220,000 other packages, so merging our algorithms into those packages will significantly extend our global reach. All researchers investigating connectomics, including all the authors of the 24,000 papers that mention the word “connectome”, will be able to apply state-of-the-art statistical theory and methods to their data. Currently, we have about 150 open source software projects on our NeuroData GitHub organization. Collectively, these projects get about 2,000 downloads and >11,000 views per month. As we incorporate additional functionality as described in this proposal, we expect far more researchers across disciplines and sectors will utilize our software. 20 ​ ​​ ​ ​​ Project Narrative Connectomes are an increasingly important modality for characterizing the structure of the brain, to complement behavior, genetics, and physiology. We and others have developed foundational statistical theory and methods over the last decade for the analysis of networks, networks with edge, vertex, and other attributes, and populations thereof, with preliminary implementations of those tools that we leverage in our laboratory for various application papers. In this project, we will extend our package, called graspy, to be of professional quality, implementing key functionality to include (1) estimating latent structure from attributed connectomes, (2) identifying meaningful clusters among populations of connectomes, and (3) detecting relationships between connectomes and multivariate phenotypes, such as behavior, genetics, and physiology. 18",Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes,10012519,RF1MH123233,"['Adoption', 'Algorithms', 'Behavioral Genetics', 'Brain', 'Code', 'Coin', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Development', 'Discipline', 'Documentation', 'Educational workshop', 'Ensure', 'Foundations', 'Funding', 'Genes', 'Human', 'Image', 'Individual', 'Journals', 'Laboratories', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modality', 'Modernization', 'Motivation', 'Neurosciences', 'Paper', 'Pathway Analysis', 'Phenotype', 'Physiology', 'Population', 'Population Analysis', 'Population Study', 'Property', 'PubMed', 'Publishing', 'Pythons', 'Research Personnel', 'Scientist', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Study', 'Structure', 'Telecommunications', 'Testing', 'Visualization', 'Work', 'brain research', 'connectome', 'data pipeline', 'design', 'high dimensionality', 'high standard', 'open source', 'public repository', 'software development', 'theories', 'tool', 'user-friendly']",NIMH,JOHNS HOPKINS UNIVERSITY,RF1,2020,1246005,-0.012519303984975085
"Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence Abstract Enzyme functionality is a critical component of all life systems. Whereas advances in experimental methodology have enabled a better understanding of factors that control enzyme function, critical components of the reaction space such as highly unstable intermediates and transition states are best accessed for evaluation through computational simulations. Similarly, computational methodology continues to provide a key resource for probing excited-state processes such as bioluminescence. Combined ab initio quantum mechanical molecular mechanical (ai-QM/MM) simulations are, in principle, the preferred choice in the modeling of both processes. But ai-QM/MM modeling of enzymatic reactions is now severely limited by its computational cost, where a direct ai-QM/MM free energy simulation of an enzymatic reaction can take 500,000 or more CPU hours. Meanwhile, ai-QM/MM modeling of firefly bioluminescence is also hindered by the computational accuracy, where it has yet to produce quantitatively correct predictions for the bioluminescence spectral shift with site-directed mutagenesis. The goal of this proposal is to accelerate ai-QM/MM simulations of enzymatic reaction free energy and to improve the quality of ai-QM/MM-simulated bioluminescence spectra, so that ai-QM/MM simulations can be routinely performed by experimental groups. This will be achieved via a) using a lower-level (semi-empirical QM/MM) Hamiltonian for sampling; b) an enhancement to the similarity between the two Hamiltonians by calibrating the low-level Hamiltonian using the reaction pathway force matching approach, in conjunction with several other methods. The expected outcomes of this collaborative effort include: a) advanced methodologies for accelerated reaction free energy simulations and accurate bioluminescence spectra predictions, which will be released through multiple software platforms; b) a fundamental understanding of reactions such as Kemp elimination and polymerase-eta catalyzed DNA replication; c) a deeper insight into the role of macromolecular environment in the modulation of enzyme catalytic activities or bioluminescence wavelengths, which can further enhance our capability of designing new enzymes and bioluminescence probes. Narrative This project aims to develop quantum-mechanics-based computational methods to more quickly model enzymatic reactions and more accurately model bioluminescence spectra. It will lead to reliable and efficient computational tools for use by the general scientific community. It will facilitate the probe of enzymatic reaction mechanisms and the computer-aided design of new bioluminescence probes.",Multiscale Modeling of Enzymatic Reactions and Firefly Bioluminescence,10021018,R01GM135392,"['Adopted', 'Biochemical Reaction', 'Bioluminescence', 'Calibration', 'Communities', 'Computer Simulation', 'Computer software', 'Computer-Aided Design', 'Computing Methodologies', 'DNA biosynthesis', 'DNA-Directed DNA Polymerase', 'Electrostatics', 'Environment', 'Enzymes', 'Evaluation', 'Fireflies', 'Free Energy', 'Freedom', 'Generations', 'Goals', 'Hour', 'Ions', 'Life', 'Machine Learning', 'Mechanics', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Multienzyme Complexes', 'Outcome', 'Pathway interactions', 'Polymerase', 'Process', 'Protocols documentation', 'Quantum Mechanics', 'Reaction', 'Resources', 'Role', 'Sampling', 'Site-Directed Mutagenesis', 'System', 'Temperature', 'Thermodynamics', 'Time', 'base', 'computerized tools', 'cost', 'design', 'experimental group', 'improved', 'innovation', 'insight', 'multi-scale modeling', 'mutant', 'quantum', 'simulation', 'theories']",NIGMS,UNIVERSITY OF OKLAHOMA NORMAN,R01,2020,255238,-0.004044851289175831
"University of Buffalo Clinical and Translational Science Institute The Buffalo Translational Consortium (BTC), which includes the University at Buffalo (UB) health sciences schools, the major healthcare institutions in our region, four key research institutes and five influential community partners, have embarked on a comprehensive strategic plan to build a strong foundation for clinical and translational research in response to our community needs. Buffalo is the second most populous city in New York State and has a rich cultural history. The proportion of underrepresented minorities in Buffalo in 2018 (50%) parallels that projected for the US in 2050, making Buffalo a microcosm of what the US will look like in 30 years. A similar proportion of our population experiences health disparities. The vision for our CTSA hub is to perform innovative research across the translational spectrum to improve the health of our community and the nation. We will develop, test and share novel approaches to engage difficult-to-engage populations and reduce health disparities in our community, which represents a “population of the future”. Guided by our vision, the CTSA has catalyzed a transformation of our environment since our CTSA was first funded in August 2015 with remarkable growth in clinical and translational research. Further, in just the past year, the UB medical school has moved into a spectacular new building and our clinical partner, Kaleida Health, the largest healthcare system in the region, opened the new Oishei Children’s Hospital, both on the Buffalo Niagara Medical Campus and connected to the Clinical and Translational Research Center devoted entirely to clinical and translational research that opened in 2012. This rapid and continuing trajectory of growth in healthcare and research in the region has resulted in a new 21st century Academic Health Center with healthcare, medical education and clinical and translational research on one campus in the heart of Buffalo, creating a foundation to enhance the impact of our CTSA even further. While launching our CTSA, we have prioritized participation in the national consortium through hosting and testing Innovation Labs as a team science tool, working with multiple hubs on initiatives to solve translational research barriers and sharing tools that we have developed with the CTSA consortium, including novel health informatics tools. Our CTSA has five ambitious but achievable aims, including: 1) Accelerate innovative translational research with teams that engage communities, regional stakeholders and the national consortium; 2) Train an excellent, diverse workforce to advance translation of discoveries; 3) Enhance inclusion of special populations across the lifespan and difficult-to-engage populations; 4) Streamline clinical research processes focusing on quality and efficiency with emphasis on multisite studies; 5) Develop, test and share biomedical informatics tools to integrate data from multiple sources to speed translation. Guided by our vision to perform research to improve the health of our community and the nation, we will continue our momentum to expand translational research, train our diverse workforce, streamline processes, engage our community, and actively contribute to the national consortium. The University at Buffalo Clinical and Translational Science Institute (CTSI) is the coordinating center of the Buffalo Translational Consortium, which includes the region's premier research, educational and clinical institutions with influential community partners. The vision of the CTSI is to perform innovative clinical and translational research to reduce health disparities and improve the health of our community and the nation. We engage our community as research partners to create a shared environment to bring discoveries in the laboratory, clinic and community to benefit individual and public health.",University of Buffalo Clinical and Translational Science Institute,10053435,UL1TR001412,"['Achievement', 'Address', 'Adopted', 'African American', 'Buffaloes', 'Center for Translational Science Activities', 'Cities', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Health', 'County', 'Coupled', 'Cultural Backgrounds', 'Data', 'Diverse Workforce', 'Ensure', 'Environment', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Healthcare', 'Healthcare Systems', 'Heart', 'Image', 'Imaging technology', 'Individual', 'Influentials', 'Informatics', 'Institutes', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Life Expectancy', 'Longevity', 'Medical', 'Medical Education', 'Medical center', 'Methods', 'Natural Language Processing', 'New York', 'Outcomes Research', 'Participant', 'Pediatric Hospitals', 'Phenotype', 'Population', 'Poverty', 'Process', 'Program Development', 'Prospective Studies', 'Public Health', 'Public Health Informatics', 'Recording of previous events', 'Recruitment Activity', 'Refugees', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Schools', 'Science', 'Sensitivity and Specificity', 'Site', 'Speed', 'Strategic Planning', 'System', 'Testing', 'Training', 'Translational Research', 'Translations', 'Underrepresented Minority', 'Universities', 'Vision', 'Work', 'Workforce Development', 'base', 'biomedical informatics', 'clinical center', 'clinical data warehouse', 'community partnership', 'data sharing', 'education research', 'experience', 'health care disparity', 'health disparity', 'imaging genetics', 'improved', 'informatics tool', 'innovation', 'interoperability', 'medical schools', 'multidisciplinary', 'multiple data sources', 'named group', 'novel', 'novel strategies', 'recruit', 'response', 'sharing platform', 'skills', 'social health determinants', 'structured data', 'tool', 'translational impact', 'translational pipeline', 'translational scientist', 'unstructured data']",NCATS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,UL1,2020,4118079,-0.018124477669165147
"Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics Abstract Support is requested for a Keystone Symposia conference entitled Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics, organized by Drs. Jose M. Lora and Timothy K. Lu. The conference will be held in Breckenridge, Colorado from March 29- April 1, 2019. Synthetic Biology tools and principles have matured tremendously over the last decade and have reached extraordinary levels of sophistication, both in eukaryotic and prokaryotic systems. Synthetic biology as a therapeutic modality is starting to enter multiple clinical studies and has the potential to have a significant impact on medicine across a wide range of diseases (e.g., metabolic, immune-mediated, cancer, and neurologic diseases). This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine. While there are conferences that capture synthetic biology in only a few talks mixed in among other various topics, there is a paucity of conferences focused on synthetic biology as drugs to treat disease. However, due to the rapid pace of fundamental scientific advances along with an expanding number of biotechnology companies and emerging clinical studies with synthetic biology at their core, this conference will be highly relevant for a wide audience of scientists both from academia and industry. In addition, other meetings in this field have a highly technology-driven focus on synthetic biology techniques with relatively little attention given to biological and medical context. Ultimately, this Keystone Symposia conference should inspire researchers from diverse backgrounds to discuss synthetic biology via many new angles. PROJECT NARRATIVE Over the past two decades, tremendous advances have been made in the use of biological parts to engineer systems that can effectively direct living cells for a vast variety of purposes (a.k.a. synthetic biology). Synthetic biology is being used to construct more effective therapies in diseases such as cancer, but there are remaining obstacles to the clinical translation of these therapies. This Keystone Symposia conference will delve into the field of synthetic biology with a special emphasis on its applications to medicine.",Synthetic Biology: At the Crossroads of Genetic Engineering and Human Therapeutics,9913772,R13EB029305,"['Academia', 'Address', 'Area', 'Attention', 'Biological', 'Biomedical Research', 'Biotechnology', 'Cells', 'Clinical Research', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Collaborations', 'Colorado', 'Computers', 'Disease', 'Educational workshop', 'Engineering', 'Future', 'Genetic Engineering', 'Genetic Screening', 'Human', 'Immune', 'Industrialization', 'Industry', 'Knowledge', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Medical', 'Medicine', 'Metabolic', 'Methodology', 'Modality', 'Neurologic', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Postdoctoral Fellow', 'Preventive', 'Process', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Scientific Advances and Accomplishments', 'Scientist', 'System', 'Techniques', 'Technology', 'Therapeutic', 'Work', 'clinical application', 'clinical practice', 'clinical translation', 'combinatorial', 'design', 'effective therapy', 'graduate student', 'meetings', 'nervous system disorder', 'next generation', 'novel diagnostics', 'posters', 'symposium', 'synthetic biology', 'targeted treatment', 'tool']",NIBIB,KEYSTONE SYMPOSIA,R13,2020,10000,-0.006075313658583239
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,-0.008628856178850553
"South Carolina Clinical & Translational Research Institute (SCTR) PROJECT SUMMARY – SCTR INSTITUTE Parent Award UL1-TR001450 Since 2009, the South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina (SC) by creating a Learning Health System that supports high- quality clinical and translational research (CTR) and fosters collaboration and innovation. Headquartered at the Medical University of South Carolina (MUSC), SCTR has engaged stakeholders and created statewide partnerships to improve care and address social determinants of health across SC. However, greater than 75% of SC is rural, and all 46 counties contain areas designated as medically underserved, so health disparities remain an issue. Over the next five years, SCTR will strengthen its outreach to these medically underserved areas through collaboration with the Clemson University Health Extension Program and the MUSC Telehealth Center of Excellence. With a focus on implementation and dissemination as well as discovery, we will develop and demonstrate innovative technologies and outreach to improve the health of our stakeholders. We will build on prior successes and introduce innovative approaches to expand CTR across SC through the following aims: Aim 1. Extend and enhance high-quality, innovative, flexible curricula and training experiences for all levels of the CTR workforce, with particular emphasis on enhancing workforce heterogeneity and team science. Aim 2. Engage a diverse group of stakeholders as active partners in CTR to address health care priorities while enhancing the scientific knowledge base about collaboration and engagement. Aim 3. Promote greater inclusion across the full translational spectrum of research by engaging investigators from many disciplines and patient populations from diverse demographic backgrounds and geographic areas. Aim 4. Develop, demonstrate and disseminate innovative methods and processes to address barriers and accelerate the translation of research discoveries to improvements in human health that can be generalized to a variety of practice settings. Aim 5. Enhance the conduct of translational research through the development of secure and innovative informatics and digital health solutions, tools and methodologies that affect every aspect of CTR. SCTR’s vision is to be a major force in facilitating the translation of innovative science into practice to address the health priorities of the citizens of SC and beyond. To achieve this vision, SCTR’s mission is to catalyze the development of methods and technologies that lead to more efficient translation of biomedical discoveries into interventions that improve individual and public health. SCTR will serve as the statewide academic home for CTR, one that is well-integrated with SC’s healthcare systems and provides essential support for innovative, efficient, multidisciplinary research and research training. We will work within SCTR, with our partners across SC and with the CTSA Consortium to realize this vision. PROJECT NARRATIVE The South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina by creating a Learning Health System characterized by strong training and infrastructure resources that stimulate collaboration and innovation as a means to accelerate the translation of biomedical research discoveries into human health improvements. A major focus of this application is to strengthen SCTR’s statewide collaborations through innovative partnerships and initiatives with an emphasis on rural and medically underserved communities where significant health disparities exist. We will develop, demonstrate and disseminate innovative ways to train a diverse workforce and address barriers to translational research, and we will continue to collaborate across the CTSA Consortium to maximize impact and improve the health of the nation.",South Carolina Clinical & Translational Research Institute (SCTR),10241088,UL1TR001450,"['2019-nCoV', 'Address', 'Administrative Supplement', 'Affect', 'Area', 'Award', 'Biomedical Research', 'COVID-19', 'COVID-19 pandemic', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials Network', 'Clinical and Translational Science Awards', 'Clinical effectiveness', 'Collaborations', 'Communities', 'County', 'Databases', 'Development', 'Discipline', 'Diverse Workforce', 'Educational Curriculum', 'Electronic Health Record', 'Emergency Situation', 'Environment', 'Fast Healthcare Interoperability Resources', 'Fostering', 'Geographic Locations', 'Health', 'Health Priorities', 'Health Sciences', 'Health system', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Home environment', 'Human', 'Individual', 'Informatics', 'Information Retrieval', 'Interdisciplinary Study', 'Intervention', 'Lead', 'Learning', 'Link', 'Medical', 'Medically Underserved Area', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Natural Language Processing', 'Outcome', 'Parents', 'Patients', 'Phenotype', 'Population', 'Population Heterogeneity', 'Process', 'Public Health', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Rural', 'Science', 'Secure', 'South Carolina', 'Support System', 'Technology', 'Terminology', 'Text', 'Training', 'Training and Infrastructure', 'Translating', 'Translational Research', 'Translations', 'Universities', 'Vision', 'Work', 'base', 'biomedical informatics', 'cohort', 'coronavirus disease', 'cost effective', 'data access', 'data enclave', 'data interoperability', 'data modeling', 'data sharing', 'digital', 'expectation', 'experience', 'flexibility', 'health disparity', 'improved', 'innovation', 'innovative technologies', 'interest', 'knowledge base', 'medically underserved', 'member', 'method development', 'outreach', 'parent grant', 'patient population', 'practice setting', 'programs', 'response', 'rural underserved', 'social health determinants', 'success', 'synergism', 'telehealth', 'tool', 'translational pipeline', 'web services']",NCATS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,UL1,2020,100000,-0.017473703317959735
"Eliminating the human factor from stereotaxic surgeries Project Summary: The main goal of this research project is to develop a new line of new stereotaxic devices for small animal research that outperforms existing devices in terms of accuracy, reproducibility, and ease of use. Advancing a tool such as an electrode, injection pipette or optical fiber through a small hole in the cranium, sometimes over long distances, and placing it precisely in a particular brain area, often much less than one millimeter in diameter, is a significant experimental challenge. Any time an investigator misses the target brain area and the experiment fails as a result, a significant amount of work is lost, additional animals get sacrificed, materials are wasted, and the pace of scientific discovery has been slowed. Even in cases when experiments succeed, they can be difficult to reproduce because many research groups rely on their most experienced lab members and their “special touch” to perform these procedures – thereby adding an element of non- quantitativeness to the procedures, effectively making the experiment less reproducible. We propose to develop a novel stereotaxic apparatus which will overcome many of these shortcomings. Our device features a radically different mechanical design which is natively compatible with both traditional and novel in-vivo techniques. We propose to combine computer 3D vision and robotics for automatic and software guided adjustments of the animal's skull. Landmarks are measured with 3D vision, based on structured illumination at a level of accuracy that has not been accomplished by any of the existing devices. This information will guide a robotic platform to position the animal for the experiment. Finally, we propose to develop an open software platform for neuronavigation that will allow investigators to use the platform with any small animal species they desire to use. Brain atlas systems for neuronavigation can either be downloaded from a cloud based site, or produced de-novo by the investigator by preparing a single set of MRI and CT scans from one sample animal. Our device will help make stereotaxic procedures more accurate and less dependent on human input and thereby increase the repeatability of experiments within a laboratory as well as the reproducibility of experiments across laboratories. Narrative: The main goal of this research project is to develop a new line of new stereotaxic devices for small animal research that outperforms existing devices in terms of accuracy, reproducibility, and ease of use. These devices will help make stereotaxic procedures less dependent on human input and thereby increase the repeatability of experiments within a laboratory as well as the reproducibility of experiments across laboratories. Most importantly, they will help reduce or eliminate failed experiments due to mistargeted interventions, thereby accelerating the pace of scientific discovery.",Eliminating the human factor from stereotaxic surgeries,10080673,R41NS119079,"['3-Dimensional', 'Animal Experimentation', 'Animal Experiments', 'Animals', 'Area', 'Atlases', 'Base of the Brain', 'Brain', 'Caliber', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Databases', 'Devices', 'Dorsal', 'Electrodes', 'Elements', 'Ensure', 'Frustration', 'Goals', 'Human', 'Image', 'Injections', 'Intervention', 'Laboratories', 'Lighting', 'Location', 'MRI Scans', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Mechanics', 'Monitor', 'Neuronavigation', 'Operative Surgical Procedures', 'Persons', 'Positioning Attribute', 'Procedures', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Robotics', 'Sampling', 'Savings', 'Scanning', 'Side', 'Site', 'Speed', 'Structure', 'Surgical sutures', 'System', 'Techniques', 'Technology', 'Time', 'Touch sensation', 'Translations', 'Vision', 'Work', 'X-Ray Computed Tomography', 'age group', 'base', 'bone', 'bone imaging', 'brain tissue', 'cloud based', 'cost effective', 'cranium', 'design', 'experimental study', 'genetic strain', 'hexapod', 'in vivo', 'laboratory experience', 'member', 'millimeter', 'novel', 'operation', 'optical fiber', 'programs', 'prototype', 'soft tissue', 'software development', 'tool', 'virtual', 'wasting']",NINDS,POPNEURON LTD.,R41,2020,251960,-0.025631566911645212
"Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS Network models are used to investigate the spread of HIV/AIDS, but rather than assuming that the members of a population of interest are fully mixed, the network approach enables individual-level specification of contact patterns by considering the structure of connections among the members of the population. By representing individuals as nodes and contacts between pairs of individuals as edges, this network depiction enables identification of individuals who drive the epidemic, allows for accurate assessment of study power in cluster- randomized trials, and makes it possible to evaluate the impact of interventions on the individuals themselves, their partners, and the broader network. There are currently two major mathematical paradigms to the modeling of networks: the statistical approach and the mechanistic approach. In the statistical approach, one specifies a model that states the likelihood of observing a given network, whereas in the mechanistic approach one specifies a set of domain-specific mechanistic rules at the level of individual nodes, the actors in the network, that are used to evolve the network over time. Given that mechanistic models directly model individual-level behaviors – modification of which is the foundation of most prevention measures – they are a natural fit for infectious diseases. Another attractive feature of mechanistic models is their scalability as they can be implemented for networks consisting of thousands or even millions of nodes, making it possible to simulate population-wide implementation of interventions. Lack of statistical methods for calibrating these models to empirical data has however impeded their use in real-world settings, a limitation that stems from the fact that there are typically no closed-form likelihood functions available for these models due the exponential increase in the number of ways, as a function of network size, of arriving at a given observed network. We propose to overcome this gap by advancing inferential and model selection methods for mechanistic network models, and by developing a framework for investigating their similarities with statistical network models. We base our approach on approximate Bayesian computation (ABC), a family of methods developed specifically for settings where likelihood functions are intractable or unavailable. Our specific aims are the following. Aim 1: To develop a statistically principled framework for estimating parameter values and their uncertainty for mechanistic network models. Aim 2: To develop a statistically principled method for model choice between two competing mechanistic network models and estimating the uncertainty surrounding this choice. Aim 3: To establish a framework for mapping mechanistic network models to statistical models. We also propose to implement these methods in open source software, using a combination of Python and C/C++, to facilitate their dissemination and adoption. We believe that the research proposed here can help harness mechanistic network models – and with that leverage some of the insights developed in the network science community over the past decade and more – to help eradicate this disease. PROJECT NARRATIVE Network models are used to gain a more precise understanding of human behavioral factors associated with the spread of HIV/AIDS in order to develop more effective interventions to halt the epidemic. There are two main mathematical paradigms for modeling networks, the statistical approach and the mechanistic approach, and given that the latter directly models individual-level behaviors – modification of which is the foundation of most prevention measures – mechanistic models are a natural fit for infectious diseases. Lack of statistical methods for calibrating these models to empirical data has so far impeded their use in real-world settings, and we therefore propose to develop parameter inference and model selection methods for mechanistic network models in order to endow the biomedical community with these powerful tools.",Bridging Statistical Inference and Mechanistic Network Models for HIV/AIDS,9970407,R01AI138901,"['AIDS prevention', 'AIDS/HIV problem', 'Adoption', 'Automobile Driving', 'Bayesian Analysis', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Biological', 'Cluster randomized trial', 'Communicable Diseases', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Development', 'Dimensions', 'Disease', 'Epidemic', 'Ethics', 'Evaluation', 'Evolution', 'Family', 'Foundations', 'Goals', 'HIV', 'Health Sciences', 'Human', 'Individual', 'Infection', 'Intervention', 'Learning', 'Likelihood Functions', 'Logistics', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Modeling', 'Pattern', 'Physics', 'Population', 'Prevention Measures', 'Prevention strategy', 'Probability', 'Process', 'Property', 'Public Health', 'Pythons', 'Research', 'Research Personnel', 'SET Domain', 'Science', 'Specific qualifier value', 'Statistical Methods', 'Statistical Models', 'Structure', 'Time', 'Uncertainty', 'base', 'effective intervention', 'high dimensionality', 'indexing', 'innovation', 'insight', 'interest', 'member', 'network models', 'open source', 'pandemic disease', 'pathogen', 'pre-exposure prophylaxis', 'simulation', 'statistics', 'stem', 'tool', 'treatment adherence', 'treatment strategy']",NIAID,HARVARD SCHOOL OF PUBLIC HEALTH,R01,2020,453846,-0.01267720700458476
"Model-guided design of next-generation bacterial therapeutics to treat cardiovascular disease PROJECT SUMMARY/ABSTRACT It is becoming increasingly evident that the composition and metabolites produced by the human gut microbiome influence the progression of cardiovascular diseases. While we are continuing to discover important associations between the gut microbiome and human physiology and diseases, we lack the tools and methodology to precisely manipulate gut microbiota to benefit human health. We propose to develop computational models and optimization frameworks to predict community dynamics and functions and design interventions to shift the gut microbiome to desired states. We will design novel bacterial therapeutics that operate autonomously in the mammalian gastrointestinal tract to steer the microbiome towards healthy states. These next-generation bacterial therapeutics will sense important gut microbiome metabolites, process information, and deliver species- specific antimicrobial proteins to reshape the dynamics and functions of this ecosystem. The performance of these bacterial therapeutics will be characterized in vitro using synthetic human gut microbiome communities and in gnotobiotic mouse models of cardiovascular disease. Model-guided microbiome engineering has the potential to transform human medicine and is becoming increasingly important as scientists continue to discover connections between the microbiome and human health and disease. PROJECT NARRATIVE Recent studies have shown close connections between the human gut microbiome and cardiovascular diseases (CVDs), which are the leading cause of death worldwide. While engineering of the gut microbiome holds tremendous potential as a novel therapeutic strategy for CVD, we currently lack the tools and methodology required to design interventions that precisely shift the structure and function of the gut microbiome. The major goals of our project are to (1) develop computational modeling techniques to design perturbations that can steer the microbiome to desired states and (2) design next-generation bacterial therapeutics that sense major gut microbiome-produced metabolites and deliver selective antimicrobials to shift microbiome states to ameliorate CVD.",Model-guided design of next-generation bacterial therapeutics to treat cardiovascular disease,10044931,R01EB030340,"['Address', 'Bacteriophages', 'Bacteroides', 'Behavior', 'Biosensor', 'Butyrates', 'Cardiovascular Diseases', 'Cause of Death', 'Communities', 'Complex', 'Computer Models', 'Data', 'Development', 'Disease', 'Ecosystem', 'Engineering', 'Equilibrium', 'Escherichia coli', 'Feedback', 'Gastrointestinal tract structure', 'Genetic Transcription', 'Germ-Free', 'Gnotobiotic', 'Goals', 'Health', 'Human', 'In Vitro', 'Intervention', 'Intestines', 'Lead', 'Machine Learning', 'Mediating', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Organism', 'Performance', 'Phenotype', 'Physiology', 'Probiotics', 'Process', 'Production', 'Protein Engineering', 'Proteins', 'Scientist', 'Stimulus', 'Structure', 'System', 'Techniques', 'Therapeutic', 'Time', 'Work', 'antimicrobial', 'computer framework', 'design', 'dynamic system', 'gut microbiome', 'gut microbiota', 'human disease', 'lysin', 'member', 'microbial', 'microbial community', 'microbiome', 'microbiome alteration', 'mouse model', 'next generation', 'novel', 'novel therapeutic intervention', 'novel therapeutics', 'personalized medicine', 'predictive modeling', 'prototype', 'real time monitoring', 'response', 'sensor', 'therapy design', 'tool', 'trimethylamine']",NIBIB,UNIVERSITY OF WISCONSIN-MADISON,R01,2020,697498,0.007325311912901886
"Tufts Clinical and Translational Science Institute (N3C Supplement) PROJECT SUMMARY Tufts Clinical and Translational Science Institute (Tufts CTSI) is based on the conviction that authentic involvement of the entire spectrum of clinical and translational research (CTR) is critical to fulfilling the promise of biomedical science for meeting the public's needs. This includes not only from translation from bench to bedside (T1 translation), but also, crucially for having health impact, translation into effective clinical practice (T2), care delivery and public health (T3), and health policy (T4). Advances on all of these fronts is increasingly dependent on making effective use of scientific data from multiple domains. The COVID-19 global emergency presents both an immediate challenge and an opportunity to progress on important data sharing aims emphasized by NIH. In response, NCATS and the Centers for Translational Science Award (CTSA) hubs, several HHS agencies, and other partnering organizations have committed to developing a next-generation repository for clinical data related to COVID-19, the National COVID Cohort Collaborative (N3C), as a means of accelerating global research into the disease and aiding the development of diagnostics, therapeutics, and effective vaccines. The N3C initiative's goal of improving the efficiency and accessibility of analyses with clinical data is consistent with the primary informatics objectives of Tufts CTSI, which am to reduce barriers to the integration of healthcare and research by providing innovative systems, data repositories, and analytical tools, and by enabling greater exchange and collaboration through interoperability, standardization, and resource sharing. In- line with shared objectives, in this supplement we seek to contribute to the N3C initiative as a data provider and thought partner through the following specific aims: (1) continue to play an important role providing tools and resources for N3C's analytics platform; and (2) ensure Tufts CTSI's Informatics Program has sufficient staff and technical resources to continue to provide COVID-specific patient data from our hub to the N3C repository. PROJECT NARRATIVE An integrated, continuously updated data repository and a platform of putting powerful analytics capabilities at the disposal of the scientific community can generate insights into COVID-19 and accelerate the development of effective treatments and vaccines to counter the disease. In this project, Tufts Clinical and Translational Science Institute plans to contribute to this goal by providing carefully structured clinical data and innovative informatics tools to the National COVID Cohort Collaborative (N3C), an initiative demonstrating a novel approach for collaborative pandemic data sharing.",Tufts Clinical and Translational Science Institute (N3C Supplement),10172199,UL1TR002544,"['Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communities', 'Data', 'Development', 'Diagnostic', 'Disease', 'Emergency Situation', 'Ensure', 'Environment', 'Funding', 'Goals', 'Health', 'Health Care Research', 'Health Policy', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Mission', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Play', 'Provider', 'Public Health', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Risk', 'Role', 'Science', 'Secure', 'Speed', 'Standardization', 'Structure', 'System', 'Time', 'Translational Research', 'Translations', 'Treatment Efficacy', 'United States National Institutes of Health', 'Update', 'Vaccines', 'analytical tool', 'base', 'bench to bedside', 'care delivery', 'clinical data warehouse', 'clinical decision support', 'clinical practice', 'cohort', 'collaborative approach', 'convict', 'coronavirus disease', 'data integration', 'data sharing', 'data warehouse', 'effective therapy', 'health management', 'improved', 'informatics tool', 'innovation', 'insight', 'interoperability', 'meetings', 'next generation', 'novel strategies', 'pandemic disease', 'programs', 'repository', 'response', 'social determinants', 'support tools', 'tool']",NCATS,TUFTS UNIVERSITY BOSTON,UL1,2020,100000,-0.015771772575352495
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,-0.022302676124036816
"AUGS/DUKE UrogynCREST program PROJECT SUMMARY Health Services Research (HSR) and predictive analytics are rapidly growing fields and will have enormous implications for women’s health research in pelvic floor disorders (PFDs). The AUGS/DUKE Urogynecology Clinical Research Educational Scientist Training (UrogynCREST) program will prepare participants to recognize the critical role that data play in delivering high quality health care. It brings together expertise in health service and women’s health research, medical informatics and prediction modeling. This program will target Urogynecology Faculty at the Assistant Professor level who seek successful careers in health services research (HSR) and analytics. Participants will obtain skills through a combination of didactic and interactive coursework; hands-on manipulation of data through extraction, cleaning, and analysis; and project-based one on one mentoring. The UrogynCREST program will be an interactive, hands-on educational program with centralized activities organized and delivered by distance through a popular on-line learning platform called Sakai, with educational software designed to support teaching, research and collaboration. A diverse faculty with expertise in data sciences teaches courses and the advanced methodology required to perform HSR. Yearly in-person meetings at the annual American Urogynecologic Society meeting enhance networking and the development of partnerships between participants from various institutions, as well as, interactions with the mentors and other HSR in the field. The program’s strategy allows national leaders with particular skills in the field to provide their knowledge to the participants and help mentor them through development of a relevant research question and identification of an appropriate and existing database(s) to address the question. With the guidance of a dedicated statistician and analyst programmer, participants will learn and perform the necessary computer programming needed to extract, clean and analyze these data. Participants whose projects involve the development of prediction models in the form of scores, nomograms or other tools will learn how to build and validate such tools in the existing project. Each participant’s project will culminate in the completion of a submitted manuscript to a peer- reviewed journal or study proposal and publicly available tools when relevant. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and resources for invigorating data discovery and tools for investigations in HSR specifically addressing (PFDs). PROJECT NARRATIVE: The AUGS/DUKE UrogynCREST program will prepare participants to recognize the critical role that data play in delivering high quality health care for pelvic floor disorders. It will add structure to the health data science education for Assistant Professor Level Faculty in Urogynecology by bringing together expertise in health service and women’s health research, medical informatics, and prediction modeling. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and mentorship for invigorating data discovery and tools for investigations in health service research specifically addressing pelvic floor disorders.",AUGS/DUKE UrogynCREST program,9918946,R25HD094667,"['Address', 'Age', 'American', 'Area', 'Caring', 'Clinical Research', 'Collaborations', 'Communities', 'Connective Tissue', 'Data', 'Data Discovery', 'Data Science', 'Databases', 'Development', 'E-learning', 'Educational process of instructing', 'Faculty', 'Fecal Incontinence', 'Fostering', 'Future', 'Goals', 'Health Services', 'Health Services Research', 'Healthcare', 'Infrastructure', 'Institution', 'Instruction', 'Investigation', 'Journals', 'Knowledge', 'Lead', 'Learning', 'Manuscripts', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modernization', 'Muscle', 'Nomograms', 'Participant', 'Peer Review', 'Pelvic Floor Disorders', 'Pelvis', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Public Health', 'Research', 'Resources', 'Role', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Shapes', 'Societies', 'Software Design', 'Structure', 'Techniques', 'Testing', 'Training Programs', 'Urinary Incontinence', 'Woman', 'Women&apos', 's Health', 'base', 'career', 'clinical decision-making', 'clinical development', 'computer program', 'computer science', 'data tools', 'design', 'health care quality', 'health data', 'improved', 'injured', 'innovation', 'meetings', 'pelvic organ prolapse', 'predictive modeling', 'professor', 'programs', 'recruit', 'science education', 'skills', 'social', 'statistical and machine learning', 'tool']",NICHD,DUKE UNIVERSITY,R25,2020,153800,-0.014486999503101937
"Consortium for Immunotherapeutics against Emerging Viral Threats SUMMARY: OVERALL  This proposal, Consortium for Immunotherapeutics Against Emerging Viral Diseases, addresses a critical gap in the biodefense portfolio by building an academic-industry partnership to advance effective, fully human, antibody-based immunotherapeutics against three major families of emerging/re-emerging viruses: Lassa virus, Ebola and other Filoviruses, and mosquito-transmitted Alphaviruses that threaten millions worldwide. This program follows directly from our significant body of preliminary data (the largest available for these families of viruses), therapeutics in hand, multidisciplinary expertise, and demonstrated collaborative success. Included in the proposed CETR portfolio are: (1) the only available immunotherapeutics against endemic Lassa virus, with reversal of late-stage disease and complete survival in infected non-human primates, (2) novel Ebola and pan- ebolavirus therapeutics that also completely protect non-human primates from disease, and that were built by the paradigm-shifting and comprehensive analysis of a global consortium, and (3) much needed, first-in-class therapeutics against the re-emerging alphaviruses that have tremendous epidemic potential in the United States and around the globe. These multidisciplinary studies, founded upon pioneering structural biology of the antigen targets, include innovations such as agnostic, high-throughput Fc profiling and optimization, coupled with Fv evolution to enhance potency and developability, as well as a sophisticated statistical and computational analysis core to evaluate thresholds and correlates of protection across the major families of pathogens. Together, we aim to understand what findings represent general rules and what data are specific to each virus family. We also aim to provide streamlined systems for antibody choice and optimization that do not yet exist, and to build a broadly applicable platform for mAb discovery and delivery against any novel pathogen as they emerge. The recent resurgence of Lassa, the epidemic nature of Ebola virus and other re-emerging filoviruses, as well as the major population at risk by global movement of mosquito-borne alphaviruses together demonstrate the tremendous global need for immunotherapeutics developed and advanced by this program. NARRATIVE Three major families of emerging viruses (Lassa and other arenaviruses, Ebola and other filoviruses, and mosquito-borne alphaviruses) threaten human health worldwide, but lack approved therapeutics or vaccines. The proposed multidisciplinary consortium, an academic-industry partnership, will advance safe and effective, fully human, monoclonal antibody therapies against these viruses, using candidate therapies that confer complete protection in non-human primates as our starting point. Our collaborative databases, multivariate analyses and innovative antibody optimization strategies will establish platforms for discovery and delivery of much-needed treatments against these and other infectious diseases.",Consortium for Immunotherapeutics against Emerging Viral Threats,9924443,U19AI142790,"['Address', 'Alphavirus', 'Antibodies', 'Antigen Targeting', 'Arenavirus', 'Arthritogenic', 'Biological Assay', 'Communicable Diseases', 'Computer Analysis', 'Computer Models', 'Computing Methodologies', 'Coupled', 'Culicidae', 'Data', 'Databases', 'Developed Countries', 'Developing Countries', 'Disease', 'Ebola', 'Ebola virus', 'Epidemic', 'Evolution', 'Family', 'Filovirus', 'Fostering', 'Goals', 'Hand', 'Health', 'Human', 'Immune', 'Immunotherapeutic agent', 'Lassa virus', 'Machine Learning', 'Mathematics', 'Mediating', 'Monoclonal Antibodies', 'Monoclonal Antibody Therapy', 'Movement', 'Multivariate Analysis', 'Nature', 'Populations at Risk', 'Primate Diseases', 'Reagent', 'Research Project Grants', 'Resources', 'Statistical Data Interpretation', 'System', 'Talents', 'Testing', 'Therapeutic', 'Therapeutic Monoclonal Antibodies', 'Translating', 'Translations', 'United States', 'Vaccines', 'Viral', 'Virus', 'Virus Diseases', 'base', 'biodefense', 'chikungunya', 'clinical development', 'design', 'experience', 'human monoclonal antibodies', 'improved', 'industry partner', 'innovation', 'insight', 'mosquito-borne', 'multidisciplinary', 'nonhuman primate', 'novel', 'pandemic disease', 'pathogen', 'programs', 'research study', 'structural biology', 'success', 'synergism', 'tool']",NIAID,LA JOLLA INSTITUTE FOR IMMUNOLOGY,U19,2020,7143424,0.0001349757359138521
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),10007746,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Infrastructure', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data de-identification', 'data harmonization', 'data management', 'data portal', 'data privacy', 'data sharing', 'data tools', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'preservation', 'social', 'success', 'theories', 'therapy design']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2020,103799,-0.0039192263057880065
"Precision Medicine in Sarcoidosis ABSTRACT Sarcoidosis is a systemic inflammatory disease of unknown etiology characterized by non-caseating granulomas in affected organs, primarily in the lungs. Approximately 30% of patients with sarcoidosis progress to debilitating disease; however, the drivers of susceptibility or resilience to disease remain poorly understood. An inflammatory response to an undefined antigen is postulated as the etiology of granuloma formation, and the pathogenesis has been suggested to involve gene-pathogen interaction, yet analysis of single genes or microbes has not proven applicable to diagnosis of all forms of sarcoidosis. Indeed, rather than a single organism, the disease may represent an interaction between the community of organisms that comprise the lung microbiome (community of organisms that live in and on us) and the host immune response. We propose that understanding the microbiome/host interaction will suggest strategies for precision medicine approaches to sarcoidosis. This proposal addresses this significant gap by investigating interactions between the lung microbiome, host immune and clinical responses in sarcoidosis using multiomics approaches – a critically innovative strategy. Our preliminary data support our novel hypotheses. First, we identified distinct lung microbiomes that differentiated patients with sarcoidosis versus controls. Second, our results identified biomarkers of disease severity that were associated with decreased lung function. Third, a recurrent analytic theme that emerged, regardless of the type of -omic analysis, was that sarcoidosis is characterized by pathways related to apoptosis and autophagy, which is consistent with our observation of decreased abundance of peripheral lymphocytes and functional immune anergy. These data led us to our Overall Hypothesis: Lung microbiome and host immune interactions characterized by apoptosis and autophagy pathways influence sarcoidosis clinical course. This hypothesis will be tested by an observational prospective and validation study of sarcoidosis patients at 5 time points to facilitate time series analyses. Aims 1 and 2 focus on lung microbiome or host immune responses, respectively, in relation to clinical course of sarcoidosis. Using these data in Aim 3, predictive models will be constructed based on integrated data of metagenomic and host-immune interactions. The novelty and significance of our multiomics strategy is to construct models for precision medicine therapies to harness bioinformatic strategies into focused, patient-specific approaches. The long-term significance of this study is to define pathways for sarcoidosis progression or resolution, and to develop database of these findings to further develop more precise, testable, models. PROJECT NARRATIVE Sarcoidosis is a disease of unknown etiology that predominately affects the lung and may affect other organs. We propose to construct a model to predict sarcoidosis progression or resolution by identifying microbial and immune interactions. We postulate that these models will be helpful in designing therapeutic options.",Precision Medicine in Sarcoidosis,9851921,R01HL138628,"['Address', 'Affect', 'Antigens', 'Apoptosis', 'Apoptotic', 'Autophagocytosis', 'Bioinformatics', 'Biological Markers', 'Biological Process', 'Blood', 'Bronchoalveolar Lavage', 'Clinical', 'Communities', 'Data', 'Databases', 'Diagnosis', 'Disease', 'Disease Progression', 'Disease model', 'Etiology', 'Feces', 'Genes', 'Granuloma', 'Immune', 'Immune response', 'Immunity', 'Inflammatory', 'Inflammatory Response', 'Lung', 'Lymphocyte', 'Messenger RNA', 'Metagenomics', 'Methods', 'MicroRNAs', 'Microbe', 'Modeling', 'Organ', 'Organism', 'Pathogenesis', 'Pathway interactions', 'Patients', 'Peripheral', 'Phenotype', 'Predisposition', 'Prospective Studies', 'Recurrence', 'Resolution', 'Respiratory physiology', 'Sarcoidosis', 'Severity of illness', 'Taxonomy', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Time Series Analysis', 'Tissue-Specific Gene Expression', 'anergy', 'base', 'clinical predictors', 'clinically relevant', 'cytokine', 'deep sequencing', 'design', 'host microbiome', 'indexing', 'inflammatory marker', 'innovation', 'lung microbiome', 'machine learning algorithm', 'metagenome', 'metagenomic sequencing', 'microbial', 'microbiome', 'multiple omics', 'novel', 'novel strategies', 'outcome forecast', 'pathogen', 'precision medicine', 'predictive modeling', 'resilience', 'response', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'validation studies']",NHLBI,UNIVERSITY OF ILLINOIS AT CHICAGO,R01,2020,914660,-0.020226994388651734
"MUFA-SIRT1 signaling as a central node regulating healthspan PROJECT SUMMARY Macronutrients serve a multitude of roles beyond provision of energy, with numerous nutrients and/or their downstream metabolites acting as signaling molecules to coordinate cellular metabolism and function. Indeed, numerous nutrient sensing pathways (e.g. mTOR, AMPK and sirtuins) have evolved allowing us to respond to specific nutrients/metabolites, which in turn impacts healthspan. Sirtuins are largely thought to be driven by redox, whereby high levels of NAD, a cofactor in the sirtuin reaction and indicator of low energy charge, drives sirtuin-catalyzed deacylation of target proteins. SIRT1, the most-studied sirtuin, is a key nutrient sensing node that regulates a plethora of cellular functions to promote lifespan extension and healthy aging. As a result, there is immense interest in the use of SIRT1 activating compounds (STACs) to prevent or treat a wide range of aging-related disease. The links between dietary macronutrients, nutrient sensing and healthspan have historically focused upon caloric or protein restriction with limited attention given to dietary lipids. However, a small and growing body of literature has linked monounsaturated fatty acids (MUFAs) to improved healthspan. In addition to positive effects on lifespan and healthy aging in model organisms, dietary MUFAs have been linked to wide-ranging health benefits in epidemiological studies and, since they are a primary constituent of olive oil, thought to contribute to the benefits of the Mediterranean Diet. Despite these studies, little is known about the biological underpinnings through which MUFAs elicit their beneficial health effects. We have previously shown that lipid droplet catabolism (i.e. lipolysis) increases SIRT1 and downstream PGC-1a/PPAR- a signaling as a means to increase mitochondrial biogenesis and function during times of nutrient deprivation. Our preliminary data show for the first time that MUFAs released specifically from lipolysis are trafficked to the nucleus where they allosterically activate SIRT1 towards select acetylated peptide substrates. This discovery makes MUFAs the first-known endogenous allosteric activators of SIRT1. Moreover, we show that MUFAs activate SIRT1 through a similar mechanism to resveratrol suggesting that MUFA signaling may modulate the response to exogenous SIRT1 activators. Based on these preliminary data, the objective of this application is to further characterize the role of MUFAs as endogenous SIRT1 activators. We hypothesize that MUFAs selectively activate SIRT1 to modulate the response to numerous dietary interventions known to impact healthspan. To test our objective, we propose the following aims: Aim 1: To define how MUFAs modulate SIRT1 substrate selectivity. Aim 2: To characterize the SIRT1-dependent effects of MUFAs/olive oil on healthspan. Aim 3: To determine the contribution of MUFAs in mediating the response to STACs or caloric restriction. Upon completion of the proposes studies, we will have further expanded our understanding of SIRT1 biology allowing for refined approaches to activate SIRT1 to promote healthy aging. NARRATIVE The proposed studies will advance our understanding into the underlying biology linking dietary factors to healthspan. The data gleaned from these studies will help refine therapeutic or nutritional avenues to modulate lifespan and aging-related diseases resulting in a direct, positive impact on human health.",MUFA-SIRT1 signaling as a central node regulating healthspan,10092409,R01AG069768,"['Aging', 'Animal Model', 'Animals', 'Attention', 'Biogenesis', 'Biological', 'Biology', 'Caloric Restriction', 'Catabolism', 'Cell Nucleus', 'Cell physiology', 'Charge', 'Clinical Trials', 'Data', 'Deacetylation', 'Development', 'Diet', 'Dietary Factors', 'Dietary Fats', 'Dietary Intervention', 'Disease', 'Dose', 'FRAP1 gene', 'Fasting', 'Glean', 'Gold', 'Health', 'Health Benefit', 'Human', 'Link', 'Lipids', 'Lipolysis', 'Literature', 'Longevity', 'Machine Learning', 'Macronutrients Nutrition', 'Maps', 'Mediating', 'Mediterranean Diet', 'Metabolism', 'Mitochondria', 'Modeling', 'Monounsaturated Fatty Acids', 'Mus', 'Nutrient', 'Nutritional', 'Oils', 'Olive oil preparation', 'Olives - dietary', 'Outcome', 'Oxidation-Reduction', 'PPAR alpha', 'Pathway interactions', 'Peptides', 'Pharmacologic Substance', 'Proteins', 'Proteomics', 'Reaction', 'Research', 'Resveratrol', 'Role', 'SIRT1 gene', 'Signal Transduction', 'Signaling Molecule', 'Sirtuins', 'Source', 'Testing', 'Therapeutic', 'Time', 'Work', 'analog', 'base', 'cofactor', 'deacylation', 'detection of nutrient', 'epidemiology study', 'healthspan', 'healthy aging', 'improved', 'innovation', 'interest', 'middle age', 'mutant mouse model', 'novel', 'nutrient deprivation', 'polyphenol', 'prevent', 'red wine', 'response']",NIA,UNIVERSITY OF MINNESOTA,R01,2020,315700,-0.021490395463581127
"Modeling the Incompleteness and Biases of Health Data Modeling the Incompleteness and Biases of Health Data Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. Existing efforts for missing health data imputation often focus on only cross-sectional correlation (e.g., correlation across subjects or across variables) but neglect autocorrelation (e.g., correlation across time points). Moreover, they often focus on modeling incompleteness but neglect the biases in health data. Modeling both the incompleteness and bias may contribute to better understanding of health data and better support clinical decision making. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets. Aim 1 introduces the MICA framework to jointly consider cross-sectional correlation and auto-correlation. In Aim 2, we will augment MICA to be bias-aware (hence BAMICA) to account for biases stemmed from multiple roots such as healthcare process and use them as features in imputing missing health data. This augmentation is achieved by a novel recurrent neural network architecture that keeps track of both evolution of health data variables and bias factors. In Aim 3, we will supplement unstructured clinical notes to structured health data for modeling incompleteness and biases using a novel architecture of graph neural network on top of memory network. We will apply graph neural networks to process clinical notes in order to learn proper representations as input to the memory networks for imputation and downstream predictive modeling tasks. Depending on the clinical problem and data availability, not all modules may be needed. Thus our proposed BAMICA framework is designed to be flexible and consists of selectable modules to meet some or all of the above needs. In summary, our proposal bridges a key knowledge gap in jointly modeling incompleteness and biases in health data and utilizes unstructured clinical notes to supplement and augment such modeling in order to better support predictive modeling and clinical decision making. We will demonstrate generalizability by experimenting on four large clinical and cohort study datasets, and by scaling up to the eMERGE network spanning 11 institutions nationwide. We will disseminate the open-source framework. The principled and flexible framework generated by this project will bring significant methodological advancement and have a direct impact on enhancing discovery from health data. Researchers are increasingly working to “mine” health data to derive new medical knowledge. Unlike experimental data that are collected per a research protocol, the primary role of clinical data is to help clinicians care for patients, so the procedures for its collection are not often systematic. Thus, missing and/or biased data can hinder medical knowledge discovery and data mining efforts. We propose a novel framework of Bias-Aware Missing data Imputation with Cross-sectional correlation and Autocorrelation (BAMICA), and leverage clinical notes to better inform the methods that will otherwise rely on structured health data only. In addition to evaluating its imputation accuracy, we will apply the proposed framework to assist in downstream tasks such as predictive modeling for multiple outcomes across a diverse range of clinical and cohort study datasets.",Modeling the Incompleteness and Biases of Health Data,9941499,R01LM013337,"['Adoption', 'Algorithms', 'Architecture', 'Awareness', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cohort Studies', 'Collection', 'Communities', 'Computer software', 'Critical Care', 'Data', 'Data Collection', 'Data Set', 'Dependence', 'Derivation procedure', 'Development', 'Diagnostic', 'Diagnostic tests', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Evolution', 'Functional disorder', 'General Hospitals', 'Goals', 'Graph', 'Health', 'Healthcare', 'Healthcare Systems', 'Hospitals', 'Hour', 'Individual', 'Inpatients', 'Institution', 'Intuition', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Learning', 'Measurement', 'Medical', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Plant Roots', 'Procedures', 'Process', 'Protocols documentation', 'Regimen', 'Research', 'Research Personnel', 'Resources', 'Role', 'Schedule', 'Structure', 'Symptoms', 'System', 'Test Result', 'Testing', 'Time', 'Training', 'Validation', 'clinical decision support', 'clinical decision-making', 'data mining', 'data quality', 'design', 'experimental study', 'flexibility', 'health care service utilization', 'health data', 'improved', 'lifetime risk', 'machine learning algorithm', 'neglect', 'neural network', 'neural network architecture', 'novel', 'open source', 'patient population', 'personalized diagnostics', 'personalized therapeutic', 'predictive modeling', 'recurrent neural network', 'scale up', 'social health determinants', 'stem', 'structured data', 'text searching', 'tool', 'trait']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2020,348397,-0.018170857479842893
"Estimating Mediation Effects in Prevention Studies The purpose of this competing continuation grant proposal is to develop, evaluate and apply  methodological and statistical procedures to investigate how prevention programs change outcome  variables. These mediation analyses assess the link between program effects on the constructs targeted  by a prevention program and effects on the outcome. As noted by many researchers and federal  agencies, mediation analyses identify the most effective program components and increase  understanding of the underlying mechanisms leading to changing outcome variables. Information from  mediation analysis can make interventions more powerful, more efficient, and shorter. The P. I. of this grant received a one-year NIDA small grant and four multi-year grants to develop and evaluate mediation  analysis in prevention research. This work led to many publications and innovations. The proposed  five-year continuation focuses on the further development and refinement of exciting new mediation  analysis statistical developments. Four statistical topics represent next steps in this research and include  analytical and simulation research as well as applications to etiological and prevention data. The work expands on our development of causal mediation and Bayesian mediation methods that hold great promise for mediation analysis. In Study 1, practical causal mediation and Bayesian mediation analyses  for research designs are developed and evaluated. This approach will clarify methods and develop  approaches for dealing with violation of testable and untestable assumptions. Study 2 investigates  important measurement issues for the investigation of mediation. This work will focus on methods to identify critical facets of mediating variables, approaches to understanding whether mediators and  outcomes are redundant, and develop methods for studies with big data. Study 3 continues the development and evaluation of new longitudinal mediation methods for ecological momentary assessment data and other studies with massive data collection. These new methods promise to more accurately model change over time for both individuals and groups of individuals. Study 4 develops methods to  uncover subgroups in mediation analysis including causal mediation methods, multilevel models, and new  approaches based on residuals for identifying individuals for whom mediating processes differ in  effectiveness from other individuals. For each study, we will investigate unique issues with mediation analysis of prevention data including methods for small N and also massive data collection (big data), the RcErLitEicVaANl rCoEle(Soeef imnsetruacstiounrse):ment for mediating mechanisms, and the application of the growing literature on  causal methods and Bayesian methods. Study 5 applies new statistical methods to data from several NIH  The project further develops a method, statistical mediation analysis, that extracts more information from  funded prevention studies providing important feedback about the usefulness of the methods. Study 6  research. Mediation analysis explains how and why prevention and treatments are successful. Mediation  disseminates new information about mediation analysis through our website and other media, by  analysis improves prevention and treatment so that their effects are greater and even cost less. communication with researchers, and publications from the project. n/a",Estimating Mediation Effects in Prevention Studies,9851457,R37DA009757,"['Address', 'Alcohol or Other Drugs use', 'Applications Grants', 'Bayesian Method', 'Behavioral Mechanisms', 'Big Data', 'Biological Models', 'Communication', 'Complex', 'Consultations', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Ecological momentary assessment', 'Educational workshop', 'Effectiveness', 'Etiology', 'Evaluation', 'Feedback', 'Funding', 'Grant', 'Individual', 'Individual Differences', 'Intervention', 'Investigation', 'Link', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mediating', 'Mediation', 'Mediator of activation protein', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'National Institute of Drug Abuse', 'Outcome', 'Persons', 'Prevention', 'Prevention Research', 'Prevention program', 'Principal Investigator', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Randomized', 'Recommendation', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Residual state', 'Statistical Data Interpretation', 'Statistical Methods', 'Subgroup', 'Testing', 'Time', 'Translating', 'United States National Institutes of Health', 'Work', 'base', 'computer program', 'cost', 'data space', 'design', 'dynamic system', 'improved', 'innovation', 'interest', 'longitudinal design', 'model design', 'multilevel analysis', 'novel strategies', 'programs', 'simulation', 'successful intervention', 'theories', 'therapy design', 'tool', 'treatment research', 'web site']",NIDA,ARIZONA STATE UNIVERSITY-TEMPE CAMPUS,R37,2020,382893,-0.01116312443728728
"Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs PROJECT ABSTRACT Above-knee amputees often struggle to perform the varying activities of daily life with conventional prostheses. Emerging powered knee-ankle prostheses have motors that can restore normative biomechanics, but these devices are limited to a small set of pre-defined activities that must be tuned to the user by technical experts over several hours. The overall goal of this project is to model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning. The universal use of different task-specific controllers in current powered legs is a direct consequence of the prevailing paradigm for viewing human locomotion as a discrete set of activities. There is a fundamental gap in knowledge about how to analyze, model, and control continuously varying locomotion, which greatly limits the adaptability and agility of powered prostheses. The central hypothesis of this project is that continuously varying activities can be represented by a single mathematical model based on measureable physical quantities called task variables. The proposed project will be scientifically significant to understanding how humans continuously adapt to varying activities and environments, technologically significant to the design of agile, user-synchronized powered prosthetic legs, and clinically significant to the adoption of powered knee-ankle prostheses for improved community ambulation. The proposed model of human locomotion will enable new prosthetic strategies for controlling and adapting to the environment, which aligns with the missions of the NICHD/NCMRR Devices and Technology Development program area and the NIBIB Mathematical Modeling, Simulation, and Analysis program. The innovation of this work is encompassed in 1) a continuous paradigm for variable locomotor activities that challenges the existing discrete paradigm, 2) a unified task control methodology that drastically improves the agility of powered prosthetic legs, and 3) a partially automated tuning process that significantly reduces the time and technical expertise required to configure powered knee- ankle prostheses. This continuous task paradigm will provide new methods and models for studying human locomotion across tasks and task transitions. This innovation will address a key roadblock in control technology that currently restricts powered legs to a small set of activities that do not generalize well across users. The adaptability of the proposed control paradigm across users and activities will transform the prosthetics field with a new generation of “plug-and-play” powered legs for community ambulation. PROJECT NARRATIVE The proposed research is relevant to public health because the clinical application of variable-activity powered prosthetic legs can significantly improve community mobility and therefore quality of life for nearly a million American amputees. Recently developed powered knee-ankle prostheses are limited to a small set of pre- defined activities that require several hours of expert tuning for each user. This project will model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning, which aligns with the missions of the Devices and Technology Development program area of the NICHD National Center for Medical Rehabilitation Research and the Mathematical Modeling, Simulation, and Analysis program of the NIBIB.",Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs,9925236,R01HD094772,"['Address', 'Adoption', 'American', 'Amputees', 'Ankle', 'Area', 'Artificial Leg', 'Biomechanics', 'Clinical', 'Communities', 'Data', 'Degree program', 'Device or Instrument Development', 'Devices', 'Doctor of Philosophy', 'Electrical Engineering', 'Environment', 'Gait', 'Gait speed', 'Generations', 'Goals', 'Hand', 'Home environment', 'Hour', 'Human', 'Human body', 'Joints', 'Knee', 'Knowledge', 'Lead', 'Leg', 'Life', 'Locomotion', 'Lower Extremity', 'Machine Learning', 'Mathematical Model Simulation', 'Measurable', 'Measures', 'Mechanics', 'Medical center', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motor', 'Motor Activity', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'Orthotic Devices', 'Outcome', 'Phase', 'Play', 'Process', 'Program Development', 'Prosthesis', 'Public Health', 'Quality of life', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Speed', 'Spinal cord injury', 'Stroke', 'Study models', 'System', 'Technical Expertise', 'Technology', 'Time', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'clinical application', 'clinically significant', 'design', 'exoskeleton', 'experience', 'human data', 'human model', 'improved', 'innovation', 'kinematics', 'mathematical model', 'multidisciplinary', 'powered prosthesis', 'programs', 'prosthesis control', 'rehabilitation research', 'robot control', 'sensor', 'success', 'technology development', 'temporal measurement', 'trend']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,446707,-0.013265080302053707
"Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine The overall goal of this project is to develop a safe, broadly effective, and affordable vaccine to prevent group A streptococcal infections. Antibodies against the N-terminal hypervariable region (HVR) of surface M (Emm) proteins of GAS are opsonic and are associated with protection against infection. Immunity has classically been described as “type-specific”, leading to the assumption that natural immunity confers protection against only one of the more than 200 different emm types of GAS. We now have new information that calls into question this classic view and serves as the basis for an entirely different approach to GAS vaccine design and development. A recent comprehensive sequence analysis of M proteins from a global collection of 175 emm types of GAS resulted in a new emm cluster typing system that classified 96.2% of all contemporary GAS isolates into 48 emm clusters containing structurally and functionally related M proteins. Moreover, 117 emm types contained in 16 clusters accounted for 94.4% of GAS infections in the world. Indeed, preclinical studies indicated that a multivalent vaccine containing N-terminal peptides from 30 prevalent M types cross-opsonized a significant number of non-vaccine emm types of GAS that co-localized in clusters with vaccine emm types. The frequency of cross-opsonic antibodies, combined with the emm cluster data, prompted us to conclude that there is a need for a paradigm shift away from the concept of “type-specific” immunity against GAS infections to one of “cluster-specific” immunity. Our overall hypothesis is that immunity to GAS infections is the result of both type-specific and cross-reactive antibodies against the N-terminal regions of M proteins and that a new approach employing computational predictions of peptide structures will result in a multivalent vaccine that will induce broadly protective immunity in populations throughout the world. Our preliminary results indicate the feasibility of using structure-based design to predict the antigenic relatedness of M peptides within a cluster. The specific aims of this proposal are to: 1) Apply computational structure-based design in an iterative process with immunological data from Aim 2 to predict the minimal number of M peptide sequences that are most representative of the structural and physicochemical properties of the peptides in one emm cluster containing 17 GAS emm types, 2) determine the cross-reactive immunogenicity of the selected peptides with all seventeen emm types of GAS in the cluster, and apply the results to refine the computational design predictions in Aim 1, 3) apply the refined computational parameters from Aims 1 and 2 to analyze the remaining epidemiologically important emm clusters, select a comprehensive panel of peptides representing all emm types, construct four multivalent recombinant vaccine proteins, and assess potential cross-protective immunogenicity using in vitro bactericidal assays against all 117 emm types of GAS, and 4) determine the protective immunogenicity of the final multivalent vaccine in unique transgenic mice expressing human C4BP and factor H that will be immunized and then challenged with multiple emm types of GAS. The world needs an effective, safe and affordable vaccine to prevent group A streptococcal (GAS) infections. Although most GAS infections are mild, there are more than 18 million people with a chronic complication of a severe GAS disease worldwide, over 15 million of whom have rheumatic heart disease, another 2 million cases of severe disease occur each year and a total of 517,000 deaths annually are estimated to be due to this organism. Vaccine prevention of even a fraction of these life-threatening diseases could have a significant impact on the health of people around the world.",Structure-Based Design of a Broadly Protective Group A Streptococcal Vaccine,9965720,R01AI132117,"['Animals', 'Antibodies', 'Bacteria', 'Base Sequence', 'Binding', 'Biological Assay', 'Cell surface', 'Cells', 'Cessation of life', 'Chronic', 'Collection', 'Complement Factor H', 'Complementarity Determining Regions', 'Complication', 'Computer Analysis', 'Data', 'Development', 'Disease', 'Ensure', 'Enzyme-Linked Immunosorbent Assay', 'Epidemiology', 'Epitopes', 'Frequencies', 'Goals', 'Health', 'Human', 'Immune', 'Immune Sera', 'Immunity', 'Immunize', 'Immunologics', 'In Vitro', 'Infection', 'Life', 'Link', 'Machine Learning', 'Modeling', 'Mus', 'N-terminal', 'Natural Immunity', 'Organism', 'Oryctolagus cuniculus', 'Peptide Vaccines', 'Peptide antibodies', 'Peptides', 'Population', 'Prevention', 'Process', 'Property', 'Proteins', 'Recombinant Vaccines', 'Recombinants', 'Rheumatic Heart Disease', 'Sequence Analysis', 'Streptococcal Infections', 'Streptococcal Vaccines', 'Streptococcus pyogenes', 'Structure', 'Surface', 'System', 'Testing', 'Transgenic Mice', 'Vaccine Antigen', 'Vaccine Design', 'Vaccines', 'bactericide', 'base', 'cross reactivity', 'design', 'experimental study', 'flexibility', 'hybrid protein', 'immunogenic', 'immunogenicity', 'innovation', 'molecular dynamics', 'multiple myeloma M Protein', 'novel', 'novel strategies', 'peptide structure', 'preclinical study', 'prevent', 'protein aminoacid sequence', 'protein structure', 'retinal S antigen peptide M', 'synthetic peptide', 'tool', 'vaccine development', 'vaccine evaluation']",NIAID,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2020,741796,-0.007978375344881931
"Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions Project Summary/Abstract DNA sequencing has spawned the “microbiome revolution” -- thousands of microbes and a dizzying number of microbial interactions that are associated with human health and disease. Unfortunately, most species in the microbiome are known only by a (partial) genome. The limited phenotypic data on newly discovered bacteria reveal species that behave unlike any of our model organisms. While genome-scale modeling plays an important role in understanding the microbiome, the paucity of phenotypic data for most species prevents detailed simulation of the microbial communities that affect our health. This project will develop an automated system for profiling, synthesizing, and modeling microbial communities. The center of our approach is Deep Phenotyping, an automated robotic platform that performs complex growth experiments on demand. Data from Deep Phenotyping will be used to train metabolic and statistical models of the oral pathogens Streptococcus mutans and Candida albicans to predict conditions that keep both microbes in a nonpathogenic state. Project Narrative The microbiome revolution has uncovered thousands of species of bacteria with roles in health and disease. This project automates the identification of interactions between environments, genes, and the microbes that live in and around us. Understanding these interactions is a critical step in re-engineering the microbiome to improve human health.","Automated, model-guided phenotyping to identify metabolite/gene/microbe interactions",9852330,R21EB027396,"['Affect', 'Animal Model', 'Antibiotics', 'Area', 'Bacteria', 'Biochemical Pathway', 'Bioinformatics', 'Biological', 'Biological Assay', 'Candida albicans', 'Carbon', 'Coculture Techniques', 'Collaborations', 'Combinatorics', 'Communities', 'Complex', 'Computer software', 'DNA sequencing', 'Data', 'Development', 'Disease', 'Engineering', 'Environment', 'Environmental Risk Factor', 'Exposure to', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Growth', 'Health', 'Human', 'Human Microbiome', 'Hybrids', 'Image', 'Individual', 'Knock-out', 'Knowledge', 'Link', 'Liquid substance', 'Machine Learning', 'Maps', 'Metabolic', 'Methods', 'Microbe', 'Microbiology', 'Modeling', 'Oral', 'Oral candidiasis', 'Pathway interactions', 'Phenotype', 'Play', 'Positioning Attribute', 'Regulator Genes', 'Research Personnel', 'Robotics', 'Role', 'Shapes', 'Source', 'Statistical Models', 'Streptococcus mutans', 'Structure', 'System', 'Technology', 'Time', 'Training', 'Work', 'base', 'combinatorial', 'cost', 'design', 'dysbiosis', 'experience', 'experimental study', 'fitness', 'fungus', 'genome-wide', 'improved', 'instrumentation', 'large datasets', 'metabolomics', 'microbial', 'microbial community', 'microbiome', 'microorganism interaction', 'network models', 'open source', 'oral pathogen', 'pathogenic fungus', 'phenotypic data', 'preference', 'prevent', 'screening', 'simulation', 'transcription factor', 'transcriptome sequencing', 'transposon sequencing']",NIBIB,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,R21,2020,178949,-0.00882375188880822
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9969443,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data standards', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'large datasets', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'public repository', 'repository', 'research and development', 'software development', 'software infrastructure', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,158388,-0.00973322338599481
"Pro-inflammatory activation of human macrophages regulated by lncRNAs Project Summary Macrophage activation promotes major inflammatory disorders, including arterial diseases. Its underlying mechanisms, however, remain obscure. The present study will establish a systems approach, involving computational prediction analyses, multi-omics, network science, and in vitro and in vivo validation, to discover long noncoding RNA (lncRNA)-mediated mechanisms for pro-inflammatory activation of macrophages and arterial disease. In Specific Aim 1, we will involve omics studies of human macrophages to identify lncRNAs and their interacting proteins and develop computational analyses to predict human lncRNAs that regulate macrophage activation. Specific Aim 2 will examine the functionality of candidate lncRNAs in macrophage activation in vitro and in vivo. The findings from the study will help to identify new mechanisms for macrophage activation and may provide molecular bases for new therapies. Project Narrative Inflammation plays a key role in coronary artery disease and other major vascular diseases, global health threats. Even with potent risk modifiers, e.g., statins, many patients still suffer vascular events. Long noncoding RNAs (lncRNAs) regulate various biological processes. We aim to discover lncRNAs that promotes vascular inflammation. The potential outcomes will offer new targets for much needed therapies for vascular diseases.",Pro-inflammatory activation of human macrophages regulated by lncRNAs,9973174,R01HL149302,"['Address', 'Biological', 'Biological Process', 'Biology', 'Blood', 'Blood Vessels', 'Cells', 'Communities', 'Complex', 'Computational Biology', 'Computer Analysis', 'Coronary Arteriosclerosis', 'Data', 'Development', 'Discipline', 'Disease', 'Drug usage', 'Endotoxemia', 'Event', 'Gene Expression Profiling', 'Goals', 'Hematopoietic Stem Cell Transplantation', 'Heterogeneity', 'Human', 'In Vitro', 'Inflammation', 'Inflammatory', 'Laboratories', 'Lesion', 'Leukocytes', 'Life', 'Link', 'Machine Learning', 'Macrophage Activation', 'Mechanics', 'Mediating', 'Methods', 'Molecular', 'Myocardial', 'NF-kappa B', 'Network-based', 'Outcome', 'Pathway Analysis', 'Patients', 'Plasma', 'Play', 'Protein Analysis', 'Proteins', 'Proteomics', 'RNA', 'Reporting', 'Residual state', 'Risk', 'Risk Factors', 'Role', 'Science', 'Signal Transduction', 'Small Interfering RNA', 'Splenocyte', 'System', 'Systems Biology', 'Tissues', 'Untranslated RNA', 'Validation', 'Vascular Diseases', 'arterial lesion', 'base', 'cytokine', 'experimental study', 'femoral artery', 'gain of function', 'global health', 'human disease', 'humanized mouse', 'in vivo', 'injured', 'loss of function', 'macrophage', 'modifiable risk', 'mouse model', 'multiple omics', 'network models', 'novel', 'novel therapeutics', 'overexpression', 'protein protein interaction', 'single cell analysis', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'vascular inflammation']",NHLBI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2020,726322,-0.013103874768953293
"Dietary modulation of gut microbiome and host gene expression across human evolution and the emergence of modern human disease Evidence suggests that lifestyle changes, concordant with the adoption of agriculture and industrialization, have impacted the emergence of the so-called diseases of modern civilization in humans (e.g. metabolic disorders, cardiovascular disease etc.). The incidence of these diseases in contemporary, industrialized populations is believed to be associated with a lack of adaptation of our genomes to the rapid dietary and lifestyle changes that occurred across human evolution. However the usefulness and resolution of this evolutionary model of disease are limited. Moreover, although the dietary and genetic markers of human evolution have been studied, we still lack understanding on how the microbiome, our second genome, has interacted with nutritional and host- genomic axes to confer increased disease risk in modern humans. Preliminary data by our group show that dietary shifts significantly modulate the gut microbiome and metabolome of wild primates, our closest evolutionary relatives. Additionally, we have identified gut microbiome markers only found in populations representing Paleolithic lifestyles (hunter-gatherers) and distinguishing them from traditional agriculturalists and industrialized populations. Thus, given 1) the potential role of diet in human evolution, 2) the critical impact of the gut microbiome on the nutritional and immune landscape of mammals, and 3) the existence of gut microbiome patterns exclusive of hunter-gatherers, we hypothesize that the emergence of metabolic disease in modern humans was significantly mediated by interactions between diet, the gut microbiome and the human genome across evolution. These issues are still unexplored. Thus, in Aim 1 of this proposal we will use a multi- OMIC approach (gut metabolomics, metagenomics and transcriptomics of the host colonic tissue) to identify metabolic and genetic markers that emerged and/or were lost when humans transitioned from hunter-gatherer to agricultural and industrialized lifestyles, and in humans affected by metabolic disease phenotypes. In Aim 2, we will use integrated meta-OMICs and network theory approaches to predict metabolic disease phenotypes, from hunter-gatherers to, populations in transition to agriculture to modern populations at risk. This system-level study will broaden our understanding of the extrinsic (environmental/nutritional) and intrinsic factors (genetic/metabolic) impacting the evolution of modern human disease. Additionally, the evolutionary approach proposed will shed light on potentially novel diet and microbe-based translational strategies to mitigate the incidence of metabolic disease in contemporary human populations. PROJECT NARRATIVE The high incidence of metabolic disorders (disorders of glucose, lipid and energy metabolism) is a significant public health threat in industrialized populations, affecting up to 25% of the adult population. Despite extensive work on characterizing the nutritional and genetic backgrounds of common metabolic disorders, we still have limited understanding as to how these factors interact with each other and with the gut microbiome, our second genome. This proposal interrogates the evolutionary baseline of modern human disease by exploring associations between nutritional, (host)genetic and microbiome markers in hunter-gatherers, traditional agriculturalists and industrialized human populations susceptible to metabolic disorders. The implementation of an evolutionary, system-level model improves our understanding of modern human disease, and, validates existing and novel dietary interventions to lessen their incidence in industrialized societies.",Dietary modulation of gut microbiome and host gene expression across human evolution and the emergence of modern human disease,9892997,R01DK112381,"['Adoption', 'Adult', 'Affect', 'African', 'Agriculture', 'Americas', 'C-reactive protein', 'Cardiovascular Diseases', 'Civilization', 'Data', 'Diet', 'Dietary Intervention', 'Disease', 'Disease model', 'Energy Metabolism', 'Evolution', 'Expression Profiling', 'Feces', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genome', 'Genomics', 'Glucose Metabolism Disorders', 'Glycosylated hemoglobin A', 'Goals', 'High Density Lipoproteins', 'Human', 'Human Genome', 'Immune', 'Incidence', 'Industrialization', 'Inflammation', 'Inflammatory', 'Intrinsic factor', 'Jamaica', 'Life Style', 'Light', 'Mammals', 'Measures', 'Mediating', 'Metabolic', 'Metabolic Diseases', 'Metabolic Marker', 'Metabolic syndrome', 'Metagenomics', 'Microbe', 'Modeling', 'Modernization', 'Nutrient', 'Nutritional', 'Pattern', 'Population', 'Populations at Risk', 'Primates', 'Public Health', 'Resolution', 'Risk Factors', 'Role', 'Sampling', 'Societies', 'Structure', 'System', 'TNF gene', 'TNFRSF1A gene', 'Therapeutic Intervention', 'Tissue-Specific Gene Expression', 'Tissues', 'Triglycerides', 'Trinidad', 'Variant', 'Work', 'base', 'cohort', 'density', 'disease phenotype', 'disorder risk', 'fecal metabolome', 'glycemic control', 'gut microbiome', 'human disease', 'improved', 'lipid metabolism', 'machine learning method', 'metabolome', 'metabolomics', 'metagenome', 'microbial', 'microbial host', 'microbiome', 'microbiome composition', 'multiple omics', 'novel', 'nutrition related genetics', 'theories', 'transcriptome sequencing', 'transcriptomics', 'translational approach']",NIDDK,"J. CRAIG VENTER INSTITUTE, INC.",R01,2020,313952,-0.02157019210066562
"Computational and Experimental Resources for Virome Analysis in Inflammatory Bowel Disease (CERVAID) SUMMARY  While the role of the bacterial microbiome in human health and disease is well established, few studies have evaluated the contribution of the virome. Recently, we demonstrated that alterations in the enteric virome in adulthood are associated with diseases such as inflammatory bowel disease (IBD) and AIDS. In a cross- sectional comparison of IBD cases and household controls, a significant expansion of the Caudovirales, an order of phages, and anelloviruses, a family of eukaryotic DNA viruses, was observed. Advancing understanding of the IBD virome beyond this finding is limited by: (1) A lack of well defined longitudinal human cohorts to enable discovery of temporal associations of the virome with health and disease; (2) The challenge of viral “dark matter”. Dark matter refers to the typically >50% of the sequences present in purified virus preparations cannot be classified due to a lack of statistically significant alignment to known reference sequences. Thus, current virome studies effectively assess < 50% of the virome, thereby compromising our ability to detect important associations between the virome and disease; (3) Inadequate experimental systems to manipulate the virome. Although sequencing has identified many novel eukaryotic viruses, there are only cell culture systems for a limited number of viruses; moreover, there are no small animal infection models for newly described viruses. In addition while a tremendous diversity of phage has been identified, only a tiny fraction have known hosts and an even smaller fraction has been cultured. Thus, there are significant barriers that must be overcome to be able to experimentally test the impact of either eukaryotic viruses or phages in murine IBD models. These barriers to understanding the role of the IBD virome will be addressed as follows: Aim 1 will define the enteric virome and bacterial microbiome in a longitudinal cohort of IBD patients and household controls and identify virome associations with IBD. In Aim 2 novel computational tools to identify and characterize viruses present in enteric viromes will be developed, including approaches to classify dark matter. In Aim 3 novel experimental systems for functional virome analysis, including novel cultures of both eukaryotic viruses and phages as well as animal infection models, will be developed with the end goal of evaluating causal roles for the viruses and phage in existing muring IBD models. Together, these Aims will not only address significant barriers in understanding of IBD, but will provide a wealth of tangible computational and experimental resources to advance the general field of virome studies.   NARRATIVE The overall goal of this project is to develop novel computational and experimental tools to address challenges in understanding the role of viruses in inflammatory bowel disease. These tools will facilitate studies of not only inflammatory bowel disease, but also broader studies of the relationship of the human virome to other diseases.  ",Computational and Experimental Resources for Virome Analysis in Inflammatory Bowel Disease (CERVAID),10019521,RC2DK116713,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adult', 'Animal Model', 'Animals', 'Bacteria', 'Bacterial Infections', 'Bacteriophages', 'Bioinformatics', 'Biology', 'Caudovirales', 'Cell Culture System', 'Cell Culture Techniques', 'Chronic', 'Classification', 'Clinical', 'Communities', 'Computer Analysis', 'Computer software', 'DNA Viruses', 'Databases', 'Defect', 'Development', 'Digestive System Disorders', 'Disease', 'Disease model', 'Enteral', 'Family', 'Feces', 'Genome', 'Gnotobiotic', 'Goals', 'Health', 'High-Throughput Nucleotide Sequencing', 'Household', 'Human', 'Immune system', 'Infection', 'Inflammatory Bowel Diseases', 'Longitudinal cohort', 'Machine Learning', 'Metabolic Diseases', 'Metadata', 'Metagenomics', 'Modeling', 'Mus', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Norovirus', 'Open Reading Frames', 'Parasitic infection', 'Pathogenesis', 'Pathogenicity', 'Pathology', 'Patients', 'Play', 'Population', 'Preparation', 'Resource Development', 'Resources', 'Ribosomal RNA', 'Role', 'System', 'Taxonomy', 'Testing', 'Time', 'Viral', 'Viral Genes', 'Viral Genome', 'Virus', 'Virus Diseases', 'bacterial resistance', 'bacteriome', 'base', 'case control', 'cohort', 'computerized tools', 'contig', 'dark matter', 'early childhood', 'experimental analysis', 'genome annotation', 'gut microbiome', 'human virome', 'improved', 'metagenome', 'microbial', 'molecular sequence database', 'multidisciplinary', 'novel', 'phenomenological models', 'protein function', 'software development', 'stool sample', 'tool', 'virology', 'virome']",NIDDK,WASHINGTON UNIVERSITY,RC2,2020,1832307,-0.0008730562971066025
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10050843,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk stratification', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,759918,-0.02959053748291742
"Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration PROJECT SUMMARY The cell nucleus is a heterogeneous organelle that consists of nuclear bodies such as nuclear lamina, speckles, nucleoli and PML bodies. These structures continuously tether and tug chromatin at the small and large scales to synergistically orchestrate dynamic functions in distinct spatio-temporal compartments. A major obstacle to the production of navigable 4D reference maps and relating structure to function in the nucleus remains understanding how these different scales of organization influence each other. In particular, we have a poor understanding of the large-scale genome organization. Growing evidence suggests that such nuclear compartmentalization is causally connected with vital genome functions in human health and disease. However, the principles of this nuclear compartmentalization, its dynamics during changes in cell conditions, and its functional relevance are poorly understood. One lesson from Phase 1 4DN was the huge gap in throughput between imaging methods, that directly measure large-scale multi-landmark relationships, and genomic methods, that aim for whole genome high-resolution maps but are indirect measurements and provide limited information about large-scale compartments. For this 4DN UM1 Center application, we propose to meet these needs through the following Aims: (1) Generate multi-modal imaging and genomic datasets to reveal the structure, dynamics, and function of nuclear compartmentalization; (2) Develop and apply computational tools for data-driven genome structure modeling and integrative analysis of nuclear compartmentalization; (3) Develop an integrative analysis and visualization platform with navigable 4D reference maps of nuclear organization. The combined datasets and results of our proposed approaches will advance our understanding of nuclear compartmentalization, the interwoven connections among different nuclear components, and their functional significance. Our new integrative analysis tools and data-driven predictive models will produce more complete nuclear organization reference maps that integrate large-scale chromosome structure data from live and super-resolution microscopy with multi-modal genomic data including smaller scale chromatin interaction maps and predict functional relationships and dynamic responses. Our navigable reference maps will be publicly accessible through an analysis platform that provides interactive visualization of multiple data types, thus enabling investigators with diverse expertise to simultaneously explore their own data and related datasets/tools and promoting collaborations that will open new horizons into the role of the 4D nucleome in human health and disease. PROJECT NARRATIVE The proposed research is relevant to public health because it will enhance our understanding of nuclear genome organization and functions that are increasingly being linked to health and disease. Because we develop tools to disseminate this information and enable others to work with our data and their own data, we will also bring nuclear architecture to bear on a broad range of ongoing health related research. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration,10156141,UM1HG011593,"['Address', 'Architecture', 'Atlases', 'Binding', 'Biochemical', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Chromatin', 'Chromatin Loop', 'Chromatin Structure', 'Chromosome Structures', 'Chromosomes', 'Collaborations', 'Communities', 'Complement', 'Computing Methodologies', 'Cytology', 'DNA Replication Timing', 'Data', 'Data Set', 'Development', 'Disease', 'Formulation', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Interphase Chromosome', 'Intuition', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Methods', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nuclear', 'Nuclear Lamina', 'Nuclear Structure', 'Organelles', 'Outcome', 'Output', 'Phase', 'Population', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Structural Models', 'Structure', 'Technology', 'Three-Dimensional Imaging', 'United States National Institutes of Health', 'Ursidae Family', 'Validation', 'Variant', 'Visualization', 'Work', 'base', 'cell cycle genetics', 'cell type', 'computer framework', 'computerized tools', 'data exploration', 'data integration', 'data tools', 'experimental study', 'genome-wide', 'genomic data', 'histone modification', 'imaging modality', 'improved', 'insight', 'machine learning algorithm', 'mental function', 'multimodal data', 'multimodality', 'multiple data types', 'multiscale data', 'predictive modeling', 'response', 'spatiotemporal', 'structured data', 'tool', 'transcription factor', 'transcriptome sequencing', 'user-friendly', 'whole genome']",NHGRI,CARNEGIE-MELLON UNIVERSITY,UM1,2020,2075409,-0.019475305434123433
"Mechanisms of mechano-chemical rupture of blood clots and thrombi Mechanisms of mechano-chemical rupture of blood clots and thrombi Prashant K. Purohit, John L. Bassani, Valeri Barsegov and John W. Weisel The goal of this proposal is to explore and understand the fracture toughness of blood clots and thrombi, thus providing a mechanistic basis for life-threatening thrombotic embolization. A combination of experiments, theoretical modeling and computer simulations will reveal how mechanical stresses (due to blood flow) in synergy with enzymatic lysis induce structural damage from the molecular to continuum scales and affect the propensity of a clot to embolize. The specific aims of this proposal are: (1) Measure and model fracture toughness of fibrin gels in quasi-static conditions, (2) Investigate rate dependent dissipative effects on toughness of fibrin gels, and (3) Study the effects of blood cells, prothrombotic blood composition, and fibrinolysis on rupture of blood clots. In Specific Aim (SA) 1, we will measure toughness of fibrin clots and provide a structural basis for rupture at the micron and nanometer scales. In SA2, we will delve into the thermodynamics and rate-dependence of the fracture of fibrin gels, including fluid flow through pores and fluid drag on fibrin fibers to capture how energy dissipation increases toughness. In the translational SA3, we will investigate toughness of physiologically relevant clots with effects of platelets, red blood cells, and neutrophils in the absence and presence of the physiological fibrinolytic activator (tPA). We will also study the rupture of clots made from the blood of venous thromboembolism patients to explore the effects of (pro)thrombotic alterations of blood composition on clot mechanical stability. Our preliminary studies show that i) the toughness of cross-linked fibrin gels is in the range of those for synthetic hydrogels, ii) the addition of tPA to a crack tip reduces the loads for crack growth, iii) fibers are aligned and broken along the tensile direction at the crack tip, and iv) crack propagation results from the rupture of covalent and non-covalent bonds. We also developed v) dynamic force spectroscopy in silico to mechanically test fibrin fibers and fibrin networks using pulling simulations and vi) atomic stress approach to map the stress-strain fields using the output from simulations. We will use continuum and finite element models of swellable biopolymer hydrogels, and statistical mechanical models for the forced unfolding of fibrin molecules. We will employ multiscale computational modeling based on Molecular Dynamics simulations of atomic structures of fibrin fibers, and Langevin simulations of fibrin networks accelerated on Graphics Processing Units. The proposed experiments cover the whole gamut of macroscopic tensile tests, shear rheometry, electron microscopy and confocal microscopy to visualize and quantitate the structural alterations of ruptured blood clots. Our experiments and modeling will help us to understand the mechanisms of thrombotic embolization and will address the clinically important question: why is there a strong association between clot structure/mechanical properties and cardiovascular diseases? The new knowledge will also help to design new hydrogel-based biomaterials that are currently at the forefront of research in mechanics, materials science and bioengineering. Project Narrative The research objective of this proposal is to measure, model and predict the mechanisms of mechano-chemical rupture of blood clots and thrombi at the molecular and continuum length scales. Our experiments and modeling will help to understand the mechanisms of embolization and will address the clinically important question: why is there a strong correlation between clot structure/mechanical properties and cardiovascular disease? The new knowledge will also help to design new hydrogel-based biomaterials that are currently at the forefront of research in mechanics, materials science and bioengineering.",Mechanisms of mechano-chemical rupture of blood clots and thrombi,9970812,R01HL148227,"['Address', 'Affect', 'Biocompatible Materials', 'Biological', 'Biomedical Engineering', 'Biopolymers', 'Blood', 'Blood Cells', 'Blood Platelets', 'Blood coagulation', 'Blood flow', 'Cardiovascular Diseases', 'Cause of Death', 'Chemicals', 'Clinical', 'Clinical Medicine', 'Coagulation Process', 'Complex', 'Computer Models', 'Computer Simulation', 'Confocal Microscopy', 'Cytolysis', 'Dependence', 'Diagnosis', 'Disease', 'Electron Microscopy', 'Elements', 'Enzymes', 'Erythrocytes', 'Evolution', 'Fiber', 'Fibrin', 'Fibrinogen', 'Fibrinolysis', 'Fracture', 'Frustration', 'Gel', 'Glean', 'Goals', 'Growth', 'Hydrogels', 'Knowledge', 'Laws', 'Length', 'Life', 'Link', 'Liquid substance', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Mechanical Stress', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Molecular Structure', 'Output', 'Patients', 'Physicians', 'Physiological', 'Plasma', 'Predisposition', 'Prevention', 'Process', 'Property', 'Prophylactic treatment', 'Proteins', 'Research', 'Research Proposals', 'Resistance', 'Resources', 'Rupture', 'Specimen', 'Spectrum Analysis', 'Stress', 'Structural Models', 'Structural defect', 'Structure', 'Testing', 'Theoretical Studies', 'Theoretical model', 'Therapeutic Embolization', 'Thermodynamics', 'Thick', 'Thrombin', 'Thromboembolism', 'Thrombosis', 'Thrombus', 'Traction', 'Work', 'base', 'crosslink', 'density', 'design', 'disability', 'experimental study', 'fiber cell', 'fluid flow', 'in silico', 'in vivo', 'insight', 'instrumentation', 'interdisciplinary approach', 'materials science', 'mechanical properties', 'models and simulation', 'molecular dynamics', 'molecular scale', 'multi-scale modeling', 'nanoscale', 'neutrophil', 'novel strategies', 'predictive modeling', 'prevent', 'response', 'simulation', 'synergism', 'theories', 'tool', 'venous thromboembolism', 'viscoelasticity']",NHLBI,UNIVERSITY OF PENNSYLVANIA,R01,2020,656886,-0.011942421534418917
"Mathematical ecology models of host-microbiota interaction in auto microbiota transplants (auto-FMT) Project Summary Mathematical ecology models of host-microbiota interaction  in auto microbiota transplants (auto-FMT) We aim to develop mathematical models for the rational design of microbiota transplants that can restore compositional diversity and function to the damaged microbiota of antibiotic-treated patients. We will focus on hospitalized cancer patients receiving allogeneic hematopoietic stem cell transplants (allo-HSCT). Allo-HSCT is a potentially curative cancer treatment that compromises the immune system, and requires that patients receive massive antibiotic treatments to prevent and treat life-threatening infections. We will build on a vast clinical database, in vitro experiments in bioreactors and in vivo experiments with mice to develop dynamic mathematical models that describe how antibiotics cause changes in the microbial composition, and how that can impact the recovery of the host's immune system after allo-HSCT. The model expands approaches pioneered by our team—the Generalized Lotka Volterra Ecological Regression (GLOVER) and agent-based models—towards a model that can assist in the development of microbiota therapies for patients undergoing allo-HSCT.  In aim 1 we will use data from a unique clinical resource available at the Memorial Sloan Kettering Cancer center—a sample bank obtained from >1,500 allo-HSCT patients (including microbiome 16S rRNA and shotgun sequencing) and extensive clinical metadata (including time series of complete blood counts and time and doses of all drugs given while the patients are hospitalized); we will also use data from a first-of-its-kind controlled randomized trial of autologous fecal microbiota transplant (auto-FMT) undergoing in allo-HSCT patients. We will use these unique resources to parameterize our models and investigate how the microbiota composition influences the recovery of the host immune system. In aim 2 we will validate the microbial component of our mathematical model using experimental data from anaerobic laboratory reactors that recreate—in vitro—the human microbiota dynamics during antibiotic treatment and auto-FMT in the absence of a living host. In aim 3 we will develop mouse models to investigate those same microbiota dynamics experimentally but now in the context of a living host.  The data obtained from these clinical studies, in vitro experiments and in vivo models will refine our mathematical models in close cycles of simulation and quantitative experimentation. Our ultimate goal is to develop models that can define optimal microbial cocktails and reconstitute the perturbed microbiota of allo- HSCT patients. In the process we hope to uncover general principles of microbiota ecology for future therapies in other patient populations whose microbiota is damaged by antibiotic treatments. Project Narrative Mathematical ecology models of host-microbiota interaction  in auto microbiota transplants (auto-FMT) The intestinal microbiota is an ecosystem with thousands of bacterial species that interact with each other and with their living host. We seek to develop and validate new mathematical models of host- microbiota ecology—using clinical data from hospitalized patients, in vitro experiments with bioreactors and in vivo experiments with mouse models—towards our ultimate goal of a predictive model that can assist in the rational design of microbiota therapies.",Mathematical ecology models of host-microbiota interaction in auto microbiota transplants (auto-FMT),9854885,R01AI137269,"['Address', 'Allogenic', 'Anaerobic Bacteria', 'Antibiotic Prophylaxis', 'Antibiotic Therapy', 'Antibiotics', 'Autologous', 'Bioreactors', 'Blood Cells', 'Cancer Patient', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clostridium difficile', 'Communities', 'Complement', 'Complete Blood Count', 'Complication', 'Computational Technique', 'Data', 'Data Science', 'Data Set', 'Databases', 'Development', 'Dose', 'Ecology', 'Ecosystem', 'Engraftment', 'Enrollment', 'Experimental Models', 'Frequencies', 'Future', 'Genetic', 'Goals', 'Health', 'Hematopoietic', 'Hematopoietic Stem Cell Transplantation', 'Human', 'Immune system', 'In Vitro', 'Infection', 'Intervention', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Mathematics', 'Medical', 'Memorial Sloan-Kettering Cancer Center', 'Metadata', 'Modeling', 'Monitor', 'Mus', 'Neutropenia', 'Patients', 'Pharmaceutical Preparations', 'Play', 'Process', 'Randomized', 'Randomized Controlled Trials', 'Recovery', 'Research', 'Resources', 'Ribosomal RNA', 'Role', 'Sampling', 'Series', 'Shotgun Sequencing', 'Time', 'Transplantation', 'Validation', 'base', 'cancer therapy', 'clinical database', 'commensal bacteria', 'design', 'experimental study', 'fecal transplantation', 'gut microbiota', 'host microbiome', 'host microbiota', 'human microbiota', 'improved', 'improved outcome', 'in vivo', 'in vivo Model', 'indexing', 'large datasets', 'mathematical model', 'microbial', 'microbial community', 'microbiome', 'microbiota', 'microbiota transplantation', 'microorganism interaction', 'mortality', 'mouse model', 'next generation', 'patient population', 'patient subsets', 'predictive modeling', 'prevent', 'prophylactic', 'reconstitution', 'response', 'simulation', 'stem cells', 'success']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,R01,2020,804721,-0.005637672488334788
"Intestinal Homeostasis Induced by Commensals ABSTRACT While multidrug-resistance transporters including P-gp and MRP2 are generally studied for their role in exporting drugs and foreign compounds from the cell, our studies indicate that these efflux pumps expressed at the apical surface of intestinal epithelial cells provide a critical link in communication between sentinel functions of mucosal barriers and the immune system. Understanding how this P-gp/eCB anti-inflammatory arm is regulated will provide crucial insight into how dysfunction may promote intestinal inflammation and help identify potential new therapeutic targets. Because the resident microbiota is known to contribute to tolerance and homeostasis in the healthy intestine, the central hypothesis we aim to test is whether the normal microbiota actively drives the P-gp/eCB axis to prevent unnecessary inflammation. Our pilot studies indicate that the microbiota does influence P-gp expression and function, providing a unique foundation for further cause-effect studies. No data have previously demonstrated a link between the microbiota and eCBs or any other epithelial lipid signals, and may well provide great insight into a novel system. Bridging this gap could help explain how commensal bacteria can stabilize a state of tolerance and how genetic modification of specific pathway elements might predispose individuals to conditions of inflammatory bowel disease (IBD). To begin addressing these questions, in Aim 1 of this application will combine in vitro (including human colonoids) and in vivo murine model systems, as well as use healthy and UC patient stool, to more deeply understand the microbial consortia that collectively maximize P-gp expression and function. Aim 2 is designed to identify the microbial metabolites that drive activation of P-gp expression and eCB secretion to maintain an anti-inflammatory tone in the intestinal epithelium. Thus, transcriptomics and metabolite analyses will be performed to provide new information regarding microbial genes, gene clusters, and their metabolic products implicated in maintaining an anti- inflammatory tone in the intestinal epithelium through regulation of the P-gp/eCB axis. In Aim 3 we will employ novel computational methods will to uncover the inter-microbial network responses and the ecological structure of a stable community that is able to induce P-gp expression. Collectively, knowledge of the pathways that coordinate the maintenance of the P-gp/eCB axis will require a comprehensive understanding of distinct signals regulating intestinal homeostasis, how multiple signals are integrated in the complex intestinal environment, and pathways that modulate host-microbe interactions. Consequently, this proposal will directly advance novel biological principles with guidance of new therapeutic intervention strategies. Public Health Narrative Regulated recruitment and migration of acute inflammatory cells termed neutrophils (PMN) into the intestine and across the specialized epithelium that lines it is critical for host defense, yet dysregulation of this process is associated with disease. Because the resident microbiota is known to contribute to tolerance and homeostasis in the healthy intestine, the goal of this proposal will examine whether the normal microbiota actively drives the P-gp/eCB axis to maintain a homeostatic anti-inflammatory tone in the intestinal epithelium. Understanding this process could help explain how commensal bacteria can stabilize a state of tolerance and how genetic modification of specific pathway elements might predispose individuals to conditions of inflammatory bowel disease (IBD).",Intestinal Homeostasis Induced by Commensals,10029718,R01DK125407,"['ABCB1 gene', 'Acute', 'Address', 'Anti-Inflammatory Agents', 'Apical', 'Bile Acids', 'Biological', 'Biological Models', 'Cells', 'Colitis', 'Communication', 'Communities', 'Complex', 'Computing Methodologies', 'Cues', 'Data', 'Disease', 'Elements', 'Endocannabinoids', 'Environment', 'Epithelial', 'Epithelial Cells', 'Epithelium', 'Equilibrium', 'Ethanolamines', 'Eubacterium', 'Feces', 'Foundations', 'Functional disorder', 'Gene Cluster', 'Genes', 'Genetic', 'Genetic Transcription', 'Goals', 'Homeostasis', 'Host Defense', 'Human', 'Immune system', 'In Vitro', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Inflammatory Response', 'Injury', 'Intervention', 'Intestinal Mucosa', 'Intestines', 'Invaded', 'Knowledge', 'Lactobacillus', 'Link', 'Lipids', 'Machine Learning', 'Maintenance', 'Mediating', 'Metabolic', 'Modeling', 'Modification', 'Molecular', 'Mucous Membrane', 'Multi-Drug Resistance', 'Mus', 'Outcome', 'Output', 'P-Glycoprotein', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phenotype', 'Pilot Projects', 'Play', 'Process', 'Public Health', 'Pump', 'Regulation', 'Research', 'Resolution', 'Role', 'Sentinel', 'Severities', 'Signal Transduction', 'Structure', 'Submucosa', 'Surface', 'System', 'Testing', 'Time', 'Toxin', 'Work', 'Xenobiotic Metabolism', 'arm', 'bacterial community', 'base', 'commensal bacteria', 'design', 'efflux pump', 'first responder', 'healing', 'host colonization', 'host-microbe interactions', 'in vivo', 'inflammatory disease of the intestine', 'innate immune mechanisms', 'insight', 'intestinal epithelium', 'intestinal homeostasis', 'mathematical model', 'microbial', 'microbiome', 'microbiota', 'microorganism', 'migration', 'mouse model', 'neutrophil', 'new therapeutic target', 'normal microbiota', 'novel', 'novel therapeutic intervention', 'nuclear factor 1', 'prevent', 'recruit', 'response', 'theories', 'transcriptomics']",NIDDK,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2020,560048,-0.02982795441057232
"Developing a robust platform to identify unexpected and novel viruses in human brain tissue Project Summary / Abstract  Central nervous system infections with RNA viruses cause severe neurological deficits and death, and are extremely challenging to diagnose. This multiple PI proposal presents an innovative research program that will provide the foundation for a comprehensive viral diagnostics pipeline to rapidly diagnose RNA virus infections from surgical brain biopsies. The investigators will combine two cutting-edge techniques for viral detection, screening with double-stranded RNA immunohistochemistry (dsRNA IHC) to identify cases with a high likelihood of a viral etiology, and unbiased metagenomic next-generation sequencing (mNGS) for specific virus identification and genome analysis.  To develop the dsRNA IHC screening assay, six commercially available anti-dsRNA antibodies will be optimized in formalin-fixed, paraffin-embedded (FFPE) brain tissue and evaluated in a series of known infectious and non-infectious cases. The top candidates will be tested in a set of brain biopsies with inflammation of unclear etiology and compared to results of mNGS performed in parallel. The development of this assay will facilitate efficient screening of FFPE tissue samples from patients with neurological disease of unclear pathogenesis to identify those with a high likelihood of viral etiology. To improve mNGS methods for FFPE brain tissue, new laboratory techniques will be tested to increase the yield and quality of viral RNA extracted. In addition to virus identification, mNGS will be used to perform full-genome deep sequencing of viruses. These results can be applied to molecular epidemiology, disease surveillance, and understanding pathogen-host interactions. Overall, this project will enhance our understanding of RNA viruses that cause CNS infection, inform future studies of virus evolution and pathogenesis, and lay the foundation for the development of a comprehensive viral diagnostics pipeline that will substantially improve the care of patients with devastating CNS infections. This work aligns very well with the NINDS mission to seek and apply fundamental knowledge of the brain to reduce the burden of neurological disease.  This R21 Exploratory Neuroscience Research Grant proposal will support and expand the collaboration between the two PIs. The unique combination of expertise in neuropathology, infectious disease pathology, virology, clinical infectious disease, and metagenomic sequencing demonstrate the requisite skills and perspective needed to achieve the goals of this proposal. The stimulating environments of Brigham and Women’s Hospital, Harvard University, and Emory University are extremely well suited to the proposed research plan, which not only provide critical equipment and facilities infrastructure, but also mentorship, formal opportunities to present and discuss results, and eventual venues to implement this clinical testing prospectively. Project Narrative Viral infections of the central nervous system cause significant illness and death, and in many patients the specific virus causing infection is never found. This project will develop new tools to detect viruses in brain tissue, including previously unknown viruses, and sequence their genomes. The objectives of this project are to improve diagnostics for patients with devastating neurological illness, and to open new research directions in studying viruses that cause central nervous system infection.",Developing a robust platform to identify unexpected and novel viruses in human brain tissue,10105533,R21NS119660,"['Adenoviruses', 'Algorithms', 'Antibodies', 'Applications Grants', 'Arboviruses', 'Bacteria', 'Bioinformatics', 'Biological Assay', 'Biopsy', 'Brain', 'Case Study', 'Central Nervous System Infections', 'Central Nervous System Viral Diseases', 'Cerebrospinal Fluid', 'Cessation of life', 'Characteristics', 'Classification', 'Clinical', 'Collaborations', 'Communicable Diseases', 'Cytoplasmic Inclusion', 'DNA Viruses', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Surveillance', 'Double-Stranded RNA', 'Encephalitis', 'Enterovirus', 'Environment', 'Equipment', 'Etiology', 'Evolution', 'Exhibits', 'Formalin', 'Foundations', 'Freezing', 'Future', 'Genome', 'Genomics', 'Goals', 'Herpesviridae', 'Histologic', 'Histology', 'Hospitals', 'Human', 'Immunohistochemistry', 'Individual', 'Infection', 'Inflammation', 'Inflammatory', 'Infrastructure', 'Investigation', 'Knowledge', 'Laboratories', 'Liquid substance', 'Machine Learning', 'Mentorship', 'Metagenomics', 'Methods', 'Mission', 'Molecular Epidemiology', 'Molecular Target', 'National Institute of Neurological Disorders and Stroke', 'Neurologic', 'Neurologic Deficit', 'Neurologic Symptoms', 'Neurosciences Research', 'Nodule', 'Nuclear Inclusion', 'Nucleic Acids', 'Operative Surgical Procedures', 'Paraffin Embedding', 'Pathogenesis', 'Pathology', 'Patient Care', 'Patients', 'Pattern', 'Probability', 'Proteomics', 'RNA Virus Infections', 'RNA Viruses', 'Research', 'Research Personnel', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Series', 'Shotgun Sequencing', 'Specimen', 'Suggestion', 'Techniques', 'Testing', 'Tissue Embedding', 'Tissue Sample', 'Universities', 'Viral', 'Viral Genome', 'Virus', 'Virus Diseases', 'Woman', 'Work', 'assay development', 'base', 'brain tissue', 'cost', 'cross reactivity', 'deep sequencing', 'fungus', 'genome analysis', 'genome sequencing', 'human pathogen', 'improved', 'innovation', 'metagenomic sequencing', 'multimodality', 'nervous system disorder', 'neuropathology', 'next generation sequencing', 'novel virus', 'pathogen', 'programs', 'prospective test', 'rRNA Genes', 'rapid diagnosis', 'research clinical testing', 'screening', 'skills', 'tool', 'viral RNA', 'viral detection', 'viral genomics', 'virology', 'virus identification', 'whole genome']",NINDS,BRIGHAM AND WOMEN'S HOSPITAL,R21,2020,480375,-0.017143576022969665
"Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression Project Summary/Abstract This project will develop a new technological approach for the comprehensive analysis of adaptive immune responses, which holds the potential to catalyze new strategies to prevent and treat disease. Here we will apply immune profiling techniques recently invented by the PI to investigate the mechanisms of Epstein-Barr virus (EBV) adaptive immune control in clinical cohorts of infected patients. EBV is a highly prevalent pathogen infecting >90% of the world’s population. Primary EBV infection often causes infectious mononucleosis (IM) and long-term sequelae include numerous malignancies, lymphoproliferative disorders, and a strong association with multiple sclerosis. No EBV vaccine is approved to date, and the molecular mechanisms of immune protection from EBV-associated diseases are unclear. Unfortunately, prior technical barriers in high- throughput immune profiling methods have prevented a comprehensive understanding of adaptive immune protection against EBV diseases. A technological approach that identifies the critical features of EBV immune protection will advance new solutions for vaccine and therapeutic development. Therefore, we developed an experimental pipeline to enable rapid and cost-effective analysis of B- and T-cell responses to EBV that is scalable to dozens of human patients per experiment. We hypothesize that a comprehensive B- and T-cell analysis of carefully selected patient cohorts that either can or cannot suppress symptomatic infection will reveal function-based correlates of EBV control. To test this hypothesis, we will apply quantitative immune profiling technologies to analyze cryopreserved longitudinal samples from recently completed prospective clinical studies of IM. Patient samples in our cohort span pre- and post-infection through convalescence and encompass the full range of clinical IM severity scores (from 0, asymptomatic primary infection, to 6, essentially bedridden with IM). Immune profile data will be used to establish adaptive immune correlates of IM disease severity. In addition, we will analyze immune responses in apparently immunocompetent patients with chronic active EBV (CAEBV) disease, or patients who do not adequately suppress EBV infection, to gain insight regarding adaptive immune function and dysfunction in CAEBV. Finally, we will develop a new computational toolkit to rapidly identify immune correlates from high-throughput datasets. Successful completion of this project will constitute the first comprehensive functional B- and T-cell receptor analysis in a human clinical cohort. Our efforts will provide a repertoire-scale, mechanistic understanding of adaptive immunity to EBV and suggest new strategies for treatment and prevention of EBV-associated diseases. Our long-term goal is to develop human immune profiling techniques as a platform approach to accelerate the rational design of vaccines and therapeutics against pathogens of high public health importance, beginning with EBV. Project Narrative This project will apply new high-throughput immune profiling technologies to elucidate the features of effective Epstein-Barr virus (EBV) immune control. EBV causes a range of human diseases including infectious mononucleosis and several forms of cancer; however, limited EBV treatment options are available and no approved preventive EBV vaccines exist. Our long-term objective is to apply enhanced understanding of adaptive immunity to accelerate the rational development of new vaccines and therapeutics.",Comprehensive analysis of human adaptive immune receptors to elucidate correlates of Epstein-Barr virus disease suppression,9996359,DP5OD023118,"['Address', 'Antibodies', 'Antibody titer measurement', 'B cell repertoire', 'B-Lymphocytes', 'Burkitt Lymphoma', 'CD8-Positive T-Lymphocytes', 'Cells', 'Chronic', 'Clinical', 'Clinical Research', 'Convalescence', 'Cost Effectiveness Analysis', 'Cryopreservation', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'EBV-associated disease', 'Ensure', 'Epstein-Barr Virus Infections', 'Exhibits', 'Fatigue', 'Future', 'Goals', 'Herpesviridae', 'Herpesviridae Infections', 'Hodgkin Disease', 'Human', 'Human Herpesvirus 4', 'Immune', 'Immune System Diseases', 'Immune response', 'Immunocompetent', 'Immunologic Receptors', 'Incidence', 'Individual', 'Infection', 'Infectious Mononucleosis', 'Intervention', 'Knowledge', 'Lymphoproliferative Disorders', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Molecular', 'Multiple Sclerosis', 'Nasopharynx Carcinoma', 'Oncogenic', 'Patients', 'Phase', 'Population', 'Prevention', 'Preventive', 'Primary Infection', 'Public Health', 'Receptors, Antigen, B-Cell', 'Recovery', 'Research Personnel', 'Risk', 'Sampling', 'Serum', 'Severities', 'Severity of illness', 'Stomach Carcinoma', 'Symptoms', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Techniques', 'Technology', 'Teenagers', 'Testing', 'Therapeutic', 'Therapeutic antibodies', 'Vaccine Design', 'Vaccines', 'Viral', 'Virus Diseases', 'Work', 'adaptive immune response', 'adaptive immunity', 'base', 'career', 'cohort', 'cost', 'design', 'disorder control', 'experimental study', 'high dimensionality', 'human disease', 'immune function', 'insight', 'lead candidate', 'multiple sclerosis patient', 'neutralizing monoclonal antibodies', 'next generation', 'novel', 'novel therapeutics', 'novel vaccines', 'pathogen', 'pressure', 'prevent', 'prospective', 'response', 'therapeutic development', 'treatment strategy', 'vaccine development', 'vaccine trial', 'virology', 'young adult']",OD,UNIVERSITY OF KANSAS LAWRENCE,DP5,2020,363296,-0.008479642874375939
"Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens ﻿    DESCRIPTION (provided by applicant): Infections caused by antibiotic-resistant bacterial pathogens are exceedingly common in immunocompromised hosts. Patients undergoing allogeneic hematopoietic stem cell transplantation (allo-HSCT) are particularly susceptible to these infections and are the patient population our studies will focus upon. Our goal is to extend and further develop systems biology approaches that our group has pioneered to identify mechanisms by which the intestinal microbiota confers resistance to infection by Vancomycin-resistant Enterococcus (VRE), antibiotic-resistant Klebsiella pneumoniae (arKp) and Clostridium difficile (C. diff). Aim 1 of our project is to establish a clinical database from the hospital recrds of allo-HSCT patients during their initial hospitalization that will include all laboratory values,vital signs, pharmacy data, dietary data, symptoms and physical exam findings. Aim 2 will expand our fecal bank by collecting fecal samples from approximately 160 allo-HSCT patients per year and determining the presence/absence of VRE, arKp and C. diff by culture and PCR. We will use NGS of 16S rRNA genes to determine microbiota composition on each sample, will perform metagenomic and RNA sequencing to determine the bacterial transcriptome and perform metabolomic analyses on a selected subset of fecal samples. Aim 3 is to extend our mathematical modeling to identify specific members of the microbiota, metabolic pathways and metabolic products that correlate with resistance to VRE or arKp expansion in the GI tract or are associated with resistance to C. diff infection. The clinical database will be used to establish correlations between clinical treatments or events and changes in the intestinal microbiota or the expression of bacterial metabolic pathways. Ultimately, the computational platforms developed in aim 3 will identify bacterial species or consortia that are associated with resistance to infection and Aim 4 will test these associations in germ-free mouse models. We will culture bacterial species associated with resistance, colonize mice with these protective bacteria and test for resistance against VRE, arKp and C. diff. Samples obtained from these experimental studies will be subjected to metagenomic and metabolomic analyses to further refine, in an iterative fashion, computational models developed in aim 3. Our proposed studies will develop new and extend existing computational models to identify bacterial species and molecular mechanisms that confer resistance to antibiotic-resistant bacterial infections. PUBLIC HEALTH RELEVANCE: The normal bacteria inhabiting the human intestine provide a high level of resistance against antibiotic-resistant bacterial pathogens. We are investigating the intestinal flora of hospitalized patients and using mathematical modeling to identify bacterial species and their metabolic products that reduce the risk of infection by three prevalent antibiotic-resistant bacteria. These studies may lead to the development of new approaches to treat and prevent antibiotic-resistant infections.",Systems Biology of Microbiome-mediated Resilience to Antibiotic-resistant Pathogens,9922844,U01AI124275,"['16S ribosomal RNA sequencing', 'Allogenic', 'Antibiotic Resistance', 'Antibiotic Therapy', 'Antibiotics', 'Bacteria', 'Bacterial Antibiotic Resistance', 'Bacterial Infections', 'Categories', 'Clinical Treatment', 'Clostridium difficile', 'Collection', 'Computer Models', 'Data', 'Deposition', 'Development', 'Diagnostic radiologic examination', 'Diet', 'Dietary intake', 'Enrollment', 'Event', 'Feces', 'Gastrointestinal tract structure', 'Germ-Free', 'Gnotobiotic', 'Goals', 'Growth', 'Hematopoietic Stem Cell Transplantation', 'Hospitalization', 'Hospitals', 'Human', 'Immune', 'Immunocompromised Host', 'Infection', 'Integration Host Factors', 'Intestinal Content', 'Intestines', 'Laboratories', 'Lead', 'Machine Learning', 'Measurable', 'Mediating', 'Medical Records', 'Memorial Sloan-Kettering Cancer Center', 'Metabolic', 'Metabolic Pathway', 'Metagenomics', 'Modeling', 'Modification', 'Molecular', 'Mus', 'Patient risk', 'Patients', 'Pharmaceutical Preparations', 'Pharmacy facility', 'Population', 'Predisposition', 'Resistance', 'Resistance to infection', 'Risk', 'Sampling', 'Symptoms', 'Systems Biology', 'Testing', 'Toxic effect', 'Transplantation', 'Uncertainty', 'Vancomycin Resistance', 'Vancomycin resistant enterococcus', 'antibiotic resistant infections', 'antimicrobial drug', 'clinical database', 'clinically relevant', 'colonization resistance', 'commensal bacteria', 'commensal microbes', 'computational platform', 'computer studies', 'data warehouse', 'design', 'drug resistant pathogen', 'experimental study', 'falls', 'gut microbiota', 'host microbiome', 'immune activation', 'infection rate', 'infection risk', 'mathematical model', 'member', 'metabolome', 'metabolomics', 'metagenome', 'metagenomic sequencing', 'microbiome', 'microbiota', 'microorganism interaction', 'mouse model', 'network models', 'novel', 'novel strategies', 'parallel computer', 'pathogenic bacteria', 'patient population', 'prevent', 'public health relevance', 'rRNA Genes', 'resilience', 'resistance mechanism', 'resistant Klebsiella pneumoniae', 'text searching', 'transcriptome', 'transcriptome sequencing', 'transcriptomics']",NIAID,SLOAN-KETTERING INST CAN RESEARCH,U01,2020,1755886,-0.008919107381441535
"Gut Microbial and Metabolic Mediators of Rotavirus Vaccine Response Abstract Rotavirus (RV) infection causes life-threatening, dehydrating diarrhea and is the leading cause of diarrheal deaths among children <5 years old despite availability of a vaccine. Critically, the oral vaccine is less effective in middle- and low-income countries where disproportionately more deaths occur compared to high-income countries. Addressing this disparity in vaccine effectiveness is a major public health priority. Correlates of protection do not exist, and cellular responses against RV in humans remain incompletely understood. Mounting evidence supports a direct role for the gut microbiota in modulating humoral and cellular immune responses to oral vaccines, but little is known about their actual mechanism of action. In our pilot study, vaccine responders had a significantly greater abundance of Bifidobacterium longum and higher content of microbial genes associated with folate transformation in their gut compared to nonresponders. These data suggest that infants may depend on microbes such as B. longum to synthesize folate de novo as a mechanism for RV-specific immune cell expansion. We hypothesize that de novo folate synthesis by microbes such as B. longum facilitates RV-specific immune cell expansion, and that levels of folate modulate vaccine immunogenicity. We propose to study 330 infants from the US, Panama, and Peru where vaccine efficacy is known to be high, medium and low, respectively, by using both stored and prospectively collected longitudinal samples of blood and stool from infants 0 to 12 months of age. We have designed a novel RV “megapool” of immunogenic peptides to define cellular immune responses to RV vaccination in addition to assessing traditional serum RV-specific IgA and stool RV shedding after immunization (Aim 1). We will characterize gut microbial composition and function using metagenomic sequencing at multiple pre-vaccination time points in vaccine responders and nonresponders to determine if the abundance of B. longum and capacity to synthesize folate predict vaccine immunogenicity (Aim 2). We will analyze the metabolic byproducts to identify if folate or other metabolites enhance vaccine response (Aim 3). Our unique team of experts in vaccinology, immunology, microbiology, biochemistry, and bioinformatics will ensure successful integrative analysis and interpretation of these immunologic and multi-omics data. Completion of the study will provide a comprehensive characterization of microbial and metabolic biomarkers of RV vaccine responses, paving the way for targeted immune augmentation strategies. Project Narrative Rotavirus vaccine was developed to prevent severe diarrhea and is especially important in low- and middle- income countries where 85% of deaths due to rotaviruses occur; unfortunately, vaccine efficacy is lower in these countries compared to high-income countries. This proposal will study infants from three countries with different socioeconomic levels and use sophisticated methods to determine how specific bacteria in the infant gut (microbiome) may change the response to the vaccine. If successful, this study will have a significant impact on this important public health disparity as a targeted intervention to improve vaccine response, even if small, could save tens of thousands of lives every year.",Gut Microbial and Metabolic Mediators of Rotavirus Vaccine Response,9862588,R01HD100542,"['5 year old', 'Address', 'Age-Months', 'Antibodies', 'Antibody titer measurement', 'Bacteria', 'Bifidobacterium', 'Biochemistry', 'Bioinformatics', 'Biological Markers', 'Birth', 'Blood', 'Blood specimen', 'Cell Differentiation process', 'Cell Proliferation', 'Cells', 'Cessation of life', 'Child', 'Country', 'Data', 'Detection', 'Development', 'Diarrhea', 'Enrollment', 'Ensure', 'Feces', 'Folic Acid', 'Folic Acid Deficiency', 'Genes', 'Human', 'Immune', 'Immune Targeting', 'Immune response', 'Immunization', 'Immunoglobulin A', 'Immunologics', 'Immunology', 'Income', 'Infant', 'Infection', 'Intervention', 'Intestines', 'Life', 'Literature', 'Lymphocyte', 'Maintenance', 'Mediator of activation protein', 'Memory', 'Metabolic', 'Metabolic Biotransformation', 'Metabolism', 'Methods', 'Microbe', 'Microbiology', 'Multiomic Data', 'Nutrient', 'Oral', 'Panama', 'Pathway interactions', 'Peptides', 'Peru', 'Peruvian', 'Pilot Projects', 'Population', 'Preventive Intervention', 'Production', 'Public Health', 'Regulatory T-Lymphocyte', 'Role', 'Rotavirus', 'Rotavirus Infections', 'Rotavirus Vaccines', 'Sample Size', 'Sampling', 'Serum', 'Shotguns', 'Specificity', 'T cell response', 'T memory cell', 'T-Cell Proliferation', 'Therapeutic Intervention', 'Time', 'Vaccination', 'Vaccines', 'Vitamins', 'base', 'cell growth', 'design', 'folic acid supplementation', 'gut microbiome', 'gut microbiota', 'health disparity', 'immune function', 'immunogenic', 'immunogenicity', 'improved', 'infant gut microbiome', 'low and middle-income countries', 'low income country', 'metabolic profile', 'metabolomics', 'metagenomic sequencing', 'microbial', 'microbiome', 'novel', 'oral vaccine', 'prevent', 'prospective', 'public health priorities', 'response', 'sample collection', 'socioeconomics', 'statistical and machine learning', 'stool sample', 'vaccine effectiveness', 'vaccine efficacy', 'vaccine response', 'vaccinology']",NICHD,CHILDREN'S HOSPITAL OF LOS ANGELES,R01,2020,681780,-0.003729285088297387
"Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology PROJECT SUMMARY/ABSTRACT Candidate After the completion of my Ph.D. in Bioinformatics, I joined the Center for Statistical Genetics at the University of Michigan (U-M) as research fellow and acquired extensive training in analysis on high- dimensional biological data. I uncovered a strong interest in studying the genetics and genomics of psoriasis when working with Dr. James Elder at the U-M, and I developed a fascination in understanding the functional roles of long non-coding RNAs (lncRNAs) in cutaneous diseases. I joined the Department of Dermatology at the U-M as a faculty in summer of 2015, with secondary appointments in the Department of Computational Medicine & Bioinformatics and the Department of Biostatistics. In addition, I direct the new Center for Cutaneous Bioinformatics within the Department of Dermatology, and serve to supervise and implement an analysis pipeline for studies investigating the immunological mechanisms for different skin diseases. Career Development Plan I aim to become a future leader in combining in silico discovery and bench experiments to advance biomedical research in autoimmune skin disorders. My objective in seeking a Mentored Research Scientist Development Award is to acquire the additional knowledge, training, and experience necessary for me to become an independent scholar in developing novel systems biology approaches to decipher the pathology and mechanisms of cutaneous diseases. The five year training proposed will provide knowledge and experience in aspects that are critical to my success, and they are: i) To develop knowledge and experimental skills in cutaneous biology --- achieved by guidance from Dr. Elder (investigative dermatology), intense research meetings/conferences, and practical laboratory experience in cutaneous research; ii) To develop knowledge and skills to study immunological systems of autoimmune skin diseases --- achieved by supervision from Dr. Johann Gudjonsson (skin immunology), attending formal Immunology courses and seminars, and earning laboratory experience from immunology experiments; iii) To advance skills in developing statistical and computational approaches --- accomplished by mentoring from Dr. Goncalo Abecasis (computational biologist), research meetings, and conducting research projects requiring advanced skills and knowledge in quantitative science; iv) To cultivate my professional development through enhancing scientific connections, grantsmanship skills, and educator portfolio --- achieved by establishing connections with colleagues during meetings, visiting King’s College London as scholar, attending a grantsmanship workshop and bootcamp, and learning mentoring skills through teaching formal classes and mentoring research students. Through the intensive and comprehensive training, I will be well grounded in conducting basic science experiments and also be able to capitalize my advanced knowledge in quantitative science to model mechanisms in cutaneous diseases. Research Project The research project will use psoriasis as a disease model to study the roles of lncRNAs in complex cutaneous disorders. I will test the hypotheses that (i): some lncRNAs are key causal elements and potentiate pathogenic inflammatory reactions in psoriasis development and (ii) by combining in silico predictions and in vitro validations we are able to provide comprehensive characterization of skin-expressing lncRNAs in keratinocytes and lymphocytes to infer their pathological implications for psoriasis. This work will demonstrate how we can take advantages of the genomic data to develop an integrative biology framework to provide novel biological insights and understand pathological roles of lncRNAs. Significance Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture. It is estimated that over 4 million Americans and 100 million people worldwide suffer from this disease. While genetic association studies have revealed the disease loci are highly enriched in non-coding regions, it is very challenging to translate genetic signals to biologic effects. In fact, most of the causal genes have not yet been identified. Our preliminary results showed that lncRNA is a class of gene that has largely been understudied for their roles in psoriasis, and both genetic and transcriptomic data suggested they can play important functions in psoriasis pathogenesis. By combining in silico analysis and in vitro validation we can expand our knowledge of lncRNAs in skin biology, and generate important hypotheses for future experiments. The results of this project can also identify novel biomarkers, and ultimately assist in the therapeutic drug discovery. PROJECT NARRATIVE Psoriasis is a chronic immune-mediated skin disease with complex genetic architecture, and affects over 4 million Americans and 100 million people worldwide. Long non-coding RNAs (lncRNAs) is a class of gene that has largely been understudied, and recent studies suggested their potential roles in autoimmune diseases. This project aims to expand our knowledge of lncRNAs in skin biology, and advance identification of lncRNAs that play functional roles in psoriasis pathogenesis.",Integrative Biology Approach to Identify and Characterize Roles of lncRNAs Associated with Psoriasis Pathology,10003820,K01AR072129,"['Affect', 'American', 'Appointment', 'Autoimmune Diseases', 'Autoimmune Process', 'Basic Science', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Process', 'Biology', 'Biomedical Research', 'Biometry', 'Catalogs', 'Cellular Structures', 'Chromatin', 'Chronic', 'Complex', 'Coronary Arteriosclerosis', 'Cutaneous', 'Data', 'Dermatology', 'Development', 'Development Plans', 'Disease', 'Disease model', 'Disease susceptibility', 'Doctor of Philosophy', 'Economic Burden', 'Educational process of instructing', 'Educational workshop', 'Elderly', 'Elements', 'Enzyme-Linked Immunosorbent Assay', 'Enzymes', 'Epigenetic Process', 'Expression Profiling', 'Faculty', 'Flow Cytometry', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genetic study', 'Genomics', 'Genotype', 'Homing', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunologics', 'Immunology', 'In Vitro', 'Inflammatory', 'Inflammatory Response', 'Knowledge', 'Learning', 'London', 'Lymphocyte', 'Mediating', 'Medicine', 'Mentored Research Scientist Development Award', 'Mentors', 'Michigan', 'Modeling', 'Molecular', 'Pathogenesis', 'Pathogenicity', 'Pathologic', 'Pathology', 'Patients', 'Play', 'Process', 'Production', 'Proteins', 'Psoriasis', 'Public Health', 'Quality of life', 'Quantitative Reverse Transcriptase PCR', 'Reaction', 'Research', 'Research Project Grants', 'Role', 'Sampling', 'Science', 'Signal Transduction', 'Skin', 'Societies', 'Statistical Methods', 'Students', 'Supervision', 'Susceptibility Gene', 'Systems Biology', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Transcript', 'Translating', 'United States', 'Universities', 'Untranslated RNA', 'Validation', 'Visit', 'Work', 'analysis pipeline', 'base', 'candidate identification', 'career development', 'causal variant', 'cell type', 'cohort', 'college', 'comorbidity', 'cytokine', 'drug discovery', 'experience', 'experimental study', 'fascinate', 'genetic architecture', 'genetic association', 'genomic data', 'high dimensionality', 'human disease', 'human tissue', 'immunoregulation', 'in silico', 'insight', 'interest', 'keratinocyte', 'knock-down', 'laboratory experience', 'machine learning method', 'meetings', 'novel', 'novel marker', 'recruit', 'scaffold', 'skills', 'skin disorder', 'small hairpin RNA', 'statistical center', 'success', 'symposium', 'tool', 'transcriptome sequencing', 'transcriptomics']",NIAMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,K01,2020,98982,-0.01829290657322149

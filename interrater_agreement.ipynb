{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048c93e8-a35d-4ed7-80a6-f0f911f40453",
   "metadata": {},
   "source": [
    "# Interrater Reliability between two human raters and NLP algorithm.\n",
    "\n",
    "You'll have to change the code at places marked #TOCHANGE\n",
    "\n",
    "It feels like it makes most sense to compare each human rater pair-wise with the NLP algorithm. First of all, this seems to be the main question we are interested in: How does the NLP algorithm compare to humans? Secondly, if we calculated the overall agreement between the three raters (two human and one NLP), it could return an abnormally high/low value because the humans are agreeing/not agreeing with each other. I think the same is true for Fleiss' Kappa (Cohen's Kappa generalized to more than two raters).\n",
    "\n",
    "However, it may be useful to compare the humans pair-wise as well, just to provide some context for how the NLP algorithm is performing. (ie. If humans agree really well with each other, but not with NLP, something is strange.)\n",
    "\n",
    "Also, I think it makes sense to include a significance value for the following Null Hypothesis Significance Testing:\n",
    "    H_0: kappa = 0 (ratings are random)\n",
    "    H_A: kappa > 0 (ratings are better than random)\n",
    "    \n",
    "We can test this using a randomization test through the following algorithm: 1) Generate n fake datasets of random ratings of k categories between two raters. 2) For each fake dataset, calculate Cohen's kappa for a total of n values of Cohen's kappa. By definition, these kappas are generated under the null hypothesis. 3) Count the proportion of generated kappas that are greater than our observed kappas. By definition, this is the probability of obtaining the observed kappa, or more extreme, under the null hypothesis - also known as the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ffc8e870-673d-40f5-ba8a-1d82b17aff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "d601d39c-fc61-4d5e-984e-a21dd5b875f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"fake_data.csv\") #TOCHANGE needs path to real data csv\n",
    "\n",
    "#need to add \"abstract id\" to convert to tidy format\n",
    "id = pd.DataFrame({\"id\": range(len(ratings))})\n",
    "ratings_id = pd.concat([id, ratings], axis=1)\n",
    "\n",
    "#create three pairwise comparisons: rater_1 (Rater 1 and NLP), rater_2 (Rater 2 and NLP), human (Rater 1 and Rater 2)\n",
    "rater_1 = ratings_id.drop(columns = ['Rater 2'])\n",
    "rater_2 = ratings_id.drop(columns = ['Rater 1'])\n",
    "human = ratings_id.drop(columns = ['Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3f15782e-9485-42eb-8511-118df6500817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tidy(df):\n",
    "    df_str = df.applymap(str) #cohen's kappa needs ratings as strings\n",
    "    df_long = pd.melt(df_str, id_vars = \"id\") #creates tidy format\n",
    "    df_reorder = df_long[['variable', 'id', 'value']] #required order for cohens kappa\n",
    "    rating_list = [df_str.iloc[:, 1].values.tolist(), df_str.iloc[:, 2].values.tolist()] #needed for agreement\n",
    "    return(rating_list, df_reorder)\n",
    "\n",
    "def print_values(rater_list, rater_tidy, desc): #prints out Kappa and agreement\n",
    "    agree_obj = agreement.AnnotationTask(data = rater_tidy.values.tolist())\n",
    "    cm = ConfusionMatrix(rater_list[0], rater_list[1])\n",
    "    \n",
    "    print(desc + \":\")\n",
    "    print(\"Cohen's Kappa: \" + str(agree_obj.kappa()))\n",
    "    print(\"Agreement: \" + str(agree_obj.avg_Ao()))\n",
    "    \n",
    "    return(agree_obj.kappa())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a9edb87d-fbd6-4b2b-a59c-624ce8240760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP vs Rater 1:\n",
      "Cohen's Kappa: -0.01719104768083037\n",
      "Agreement: 0.0\n",
      "NLP vs Rater 2:\n",
      "Cohen's Kappa: -0.015873015873015876\n",
      "Agreement: 0.0\n",
      "Rater 1 vs Rater 2:\n",
      "Cohen's Kappa: -0.014230271668822774\n",
      "Agreement: 0.0\n"
     ]
    }
   ],
   "source": [
    "rater_1_list, rater_1_tidy = convert_to_tidy(rater_1)\n",
    "rater_2_list, rater_2_tidy = convert_to_tidy(rater_2)\n",
    "human_list, human_tidy = convert_to_tidy(human)\n",
    "    \n",
    "rater_1_kappa = print_values(rater_1_list, rater_1_tidy, \"NLP vs Rater 1\")\n",
    "rater_2_kappa = print_values(rater_2_list, rater_2_tidy, \"NLP vs Rater 2\")\n",
    "human_kappa = print_values(human_list, human_tidy, \"Rater 1 vs Rater 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4c457-8e10-4020-897c-44fd24adc835",
   "metadata": {},
   "source": [
    "Cohen's Kappa normal interpretation is as follows: <0.0 = Poor, 0.0-0.2 = Slight, 0.2-0.4 = Fair, 0.4-0.6 = Moderate, 0.6-0.8 = Substantial, 0.8-1.0 = Almost perfect.\n",
    "\n",
    "Agreement is just # correct/total. Cohen's Kappa should be almost identical to Agreement for large k, where k is number of categories raters had to pick from.\n",
    "\n",
    "For large k, I'm not sure Cohen's Kappa is a fair assessment, because it's much harder to pick the right one and Cohen's Kappa does not adequately correct for that. So if the normal interpretation is good for your NLP, go with that! Otherwise, you can qualify it, using the human:human pair for comparison - \"Cohen's Kappa and agreement for Rater 1:NLP pairs and Rater 2:NLP pairs were low; however, given the large number of categories raters had to choose from, this was expected, as raters had to choose the exact category out of __ options. We see these values are comparable to those from the Rater 1:Rater 2 pair, highlighting the difficulty of the task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c979ee3-128e-444c-8273-e626a1c0cc7d",
   "metadata": {},
   "source": [
    "To quantify if the pairs are doing better than expected from random chance, we can generate Kappas under the null.\n",
    "\n",
    "Below, I generate a large number (n_iterations) of \"rating datasets\" where each rater randomly choose from k categories (k_categories) with equal probability (our assumption under H_0) and calculate Cohen's Kappa for each of these.\n",
    "\n",
    "Then, I check what proportion of these null Cohen's Kappa are as extreme, or more extreme, than our observed Cohen's Kappa for each pair of raters. These have identical interpretation as p-values and can be used to argue the raters are doing better than random ratings. Unfortunately, this is not a super strong statement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "19d9a498-170e-4bfe-82aa-fdc29b4e8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "n_iterations = 100000\n",
    "k_categories = 59 #TOCHANGE How many categories did Raters have access to?\n",
    "\n",
    "def generate_ratings(n_ratings, k): \n",
    "    variable = [\"rater_1\"] * n_ratings + [\"rater_2\"] * n_ratings\n",
    "    rating_num = list(range(n_ratings)) * 2\n",
    "    value = np.random.randint(1, k, n_ratings * 2).tolist() #random ratings from k categories\n",
    "    \n",
    "    rating_num_str = list(map(str, rating_num)) #convert to string\n",
    "    value_str = list(map(str, value))\n",
    "    \n",
    "    ratings_list = [None] * n_ratings * 2\n",
    "    for i in range(n_ratings * 2):\n",
    "        ratings_list[i] = [variable[i], rating_num_str[i], value_str[i]]\n",
    "    \n",
    "    agree_obj = agreement.AnnotationTask(data = ratings_list)\n",
    "    return(agree_obj.kappa())\n",
    "\n",
    "kappa = [None] * n_iterations\n",
    "for i in range(n_iterations):\n",
    "    kappa[i] = generate_ratings(n_ratings = 60, k = k_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "715eebbe-1c91-4e8d-a330-43ae071d48d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81903\n",
      "0.74814\n",
      "0.67951\n"
     ]
    }
   ],
   "source": [
    "rater_1_pval = len([val for val in kappa if val >= rater_1_kappa])/n_iterations #proportion of values as extreme or more\n",
    "rater_2_pval = len([val for val in kappa if val >= rater_2_kappa])/n_iterations\n",
    "human_pval = len([val for val in kappa if val >= human_kappa])/n_iterations\n",
    "\n",
    "print(rater_1_pval)\n",
    "print(rater_2_pval)\n",
    "print(human_pval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
